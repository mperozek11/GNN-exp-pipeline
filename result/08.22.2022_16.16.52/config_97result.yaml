config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:21:05.632950'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_97fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_97fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_97fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_97fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_97fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_97fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_97fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_97fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_97fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_97fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.3448623776435853
  - 3.147399300336838
  - 3.031863397359848
  - 3.0465058743953706
  - 3.1410167276859284
  - 2.9454560160636905
  - 2.9416964650154114
  - 2.9300183773040773
  - 3.069920629262924
  - 3.2994359493255616
  - 3.011338767409325
  - 2.8929493069648746
  - 2.9339629828929903
  - 2.972534081339836
  - 2.896523025631905
  validation_losses:
  - 0.5545567274093628
  - 0.41220301389694214
  - 0.39593255519866943
  - 0.38706356287002563
  - 0.3984827697277069
  - 0.42534440755844116
  - 0.3956892490386963
  - 0.4171697795391083
  - 0.4503314197063446
  - 0.38050052523612976
  - 0.3832866847515106
  - 0.38135236501693726
  - 0.3828517198562622
  - 0.3925292491912842
  - 0.4003073275089264
loss_records_fold1:
  train_losses:
  - 2.8228883832693104
  - 2.785634931921959
  - 2.896688437461853
  - 2.8222251266241076
  - 2.871821162104607
  - 3.003108072280884
  - 2.841914087533951
  - 2.885547861456871
  - 2.869470375776291
  - 2.9161869883537292
  - 2.78671487569809
  - 2.8288749307394028
  - 2.7935246527194977
  - 2.7815222799777986
  - 2.8619120299816134
  - 2.8259909182786944
  - 2.7734111309051515
  - 2.8656424432992935
  - 2.865028360486031
  - 2.898632898926735
  validation_losses:
  - 0.3900658190250397
  - 0.427883505821228
  - 0.4042661190032959
  - 0.3955446481704712
  - 0.40846437215805054
  - 0.4133770167827606
  - 1.4983296394348145
  - 0.4454166889190674
  - 0.5384724736213684
  - 0.8145026564598083
  - 0.4646400213241577
  - 0.4447759687900543
  - 0.5156487822532654
  - 0.5986872911453247
  - 0.41769012808799744
  - 0.3883523643016815
  - 0.391613245010376
  - 0.38611671328544617
  - 0.39050954580307007
  - 0.3994010090827942
loss_records_fold2:
  train_losses:
  - 2.845414102077484
  - 2.805675482749939
  - 2.8249055445194244
  - 2.834238713979721
  - 2.7974175751209263
  - 2.8394356667995453
  - 2.84870285987854
  - 2.8238670229911804
  - 2.8161020040512086
  - 2.906615248322487
  - 2.802565622329712
  - 2.7974950551986697
  - 2.796928542852402
  - 2.8428484559059144
  - 2.8559119164943696
  - 2.816209661960602
  - 2.786434191465378
  - 2.7678474575281147
  - 2.808721843361855
  - 2.8506947576999666
  - 2.8178657770156863
  - 2.757144457101822
  - 2.7833727657794953
  - 2.8424999833106996
  - 2.7873845607042314
  - 2.834227016568184
  - 2.809124082326889
  - 2.8347941040992737
  - 2.816800364851952
  - 2.809401959180832
  - 2.75028977394104
  - 2.728998652100563
  - 2.7904334992170337
  - 2.7689392775297166
  - 2.7984763711690905
  - 2.8322995185852053
  - 2.7620046496391297
  - 2.794907695055008
  - 2.7839292645454408
  - 2.7394164472818376
  - 2.810354655981064
  - 2.7590825974941255
  - 2.7491363763809207
  - 2.7978492379188538
  - 2.7909223794937135
  - 2.7550922304391863
  - 2.796991464495659
  - 2.8323681116104127
  - 2.808637130260468
  - 2.7766438096761705
  - 2.7995179772377017
  - 2.76144460439682
  - 2.7525829493999483
  - 2.738933539390564
  - 2.7972254753112793
  - 2.802096861600876
  - 2.7276772499084476
  - 2.7387421131134033
  - 2.750067347288132
  - 2.724854272603989
  - 2.740046787261963
  - 2.795342427492142
  - 2.8082999289035797
  - 2.725962954759598
  - 2.753905227780342
  - 2.971312838792801
  - 2.8022401332855225
  - 2.724554827809334
  - 2.77150496840477
  - 2.788101252913475
  - 2.745558685064316
  - 2.7465631604194645
  - 2.809616446495056
  - 2.8291148334741596
  - 2.7700663208961487
  - 2.754737138748169
  - 2.767577347159386
  - 2.729282093048096
  - 2.75508785545826
  - 2.7806220650672913
  - 2.6677067250013353
  - 2.711431711912155
  - 2.7170433044433597
  - 2.7272814512252808
  - 2.7587281584739687
  - 2.7179733991622927
  - 2.7206598401069644
  - 2.835402575135231
  - 2.785195627808571
  - 2.7256628036499024
  - 2.7507930397987366
  - 2.782074972987175
  - 2.7041518032550815
  - 2.665503251552582
  - 2.700246512889862
  - 2.7183505594730377
  - 2.7149973779916765
  - 2.7096362054347995
  - 2.678032571077347
  - 2.6742542505264284
  validation_losses:
  - 0.3778427541255951
  - 0.38206157088279724
  - 0.3832454979419708
  - 0.38085871934890747
  - 0.383938729763031
  - 0.4749442934989929
  - 0.4343436658382416
  - 0.6914201378822327
  - 1.6085315942764282
  - 0.6243981719017029
  - 0.4175065755844116
  - 0.7595511078834534
  - 0.39559024572372437
  - 0.39031723141670227
  - 0.5618893504142761
  - 0.3781733810901642
  - 0.5570676326751709
  - 0.4357176423072815
  - 0.6833985447883606
  - 0.486201673746109
  - 0.38234421610832214
  - 0.4604247808456421
  - 0.6213893890380859
  - 0.6888650059700012
  - 0.6753324270248413
  - 0.9931605458259583
  - 0.8188310265541077
  - 0.392541766166687
  - 0.8098323941230774
  - 2.065077781677246
  - 0.5971729755401611
  - 0.802875816822052
  - 2.937645673751831
  - 0.38453754782676697
  - 0.38518449664115906
  - 0.40868231654167175
  - 1.385094404220581
  - 0.44019702076911926
  - 1.7035571336746216
  - 0.3857804238796234
  - 1.2737348079681396
  - 1.0108498334884644
  - 0.3913883566856384
  - 0.39561963081359863
  - 0.6826317310333252
  - 0.7412108778953552
  - 0.3893473446369171
  - 0.40154406428337097
  - 0.39383232593536377
  - 0.43069708347320557
  - 0.8640292882919312
  - 0.4143291115760803
  - 0.8805943131446838
  - 0.3832961916923523
  - 0.5318853855133057
  - 0.38899773359298706
  - 0.3919806480407715
  - 0.46513354778289795
  - 1.6001423597335815
  - 0.627799391746521
  - 0.4073570668697357
  - 0.3890506625175476
  - 0.3831130862236023
  - 0.9261082410812378
  - 6.211666107177734
  - 0.6323457956314087
  - 0.4107132852077484
  - 0.6587640047073364
  - 3.053966999053955
  - 1.0877774953842163
  - 0.6425959467887878
  - 1.313624382019043
  - 0.5080767273902893
  - 0.51076340675354
  - 0.48474958539009094
  - 0.40519973635673523
  - 0.5824471116065979
  - 0.5044306516647339
  - 0.5760024785995483
  - 0.4614425301551819
  - 0.5283785462379456
  - 0.4324193000793457
  - 0.4387822151184082
  - 0.45314469933509827
  - 0.5186434388160706
  - 0.5065552592277527
  - 0.5987228751182556
  - 0.5790429711341858
  - 0.5380355715751648
  - 0.7035369873046875
  - 0.47014397382736206
  - 0.6380707621574402
  - 0.5450475215911865
  - 0.7125349640846252
  - 0.5434468388557434
  - 0.41936028003692627
  - 0.7116840481758118
  - 0.48632603883743286
  - 0.41902315616607666
  - 0.6368857622146606
loss_records_fold3:
  train_losses:
  - 2.7245348632335666
  - 2.738713258504868
  - 2.7251665920019152
  - 2.696087494492531
  - 2.729297059774399
  - 2.7067303597927097
  - 2.7061505705118183
  - 2.701696586608887
  - 2.7038309633731843
  - 2.782660275697708
  - 2.7624174803495407
  - 2.7209425359964374
  - 2.6956337988376617
  - 2.713744354248047
  - 2.705213671922684
  - 2.6826097279787064
  - 2.6977495133876803
  - 2.6898493707180027
  - 2.6789736270904543
  - 2.6935288578271868
  - 2.6721930623054506
  - 2.718816035985947
  - 2.713961118459702
  - 2.7863667726516725
  - 2.7464224785566334
  - 2.7620334267616276
  - 2.6787542402744293
  - 2.6917398631572724
  - 2.741085320711136
  - 2.7369518816471103
  - 2.753670158982277
  - 2.7401187419891357
  - 2.7007531702518466
  - 2.678498470783234
  - 2.6689102411270142
  - 2.695384138822556
  - 2.6921961396932605
  - 2.6940334737300873
  - 2.6870775520801544
  - 2.716242802143097
  - 2.7319406777620316
  - 2.6420193642377856
  - 2.639476364850998
  - 2.613780388236046
  - 2.6953560352325443
  - 2.662390792369843
  - 2.6841179341077805
  - 2.68722805082798
  - 2.639694795012474
  - 2.6937406003475193
  - 2.6451710879802706
  - 2.6527218580245973
  - 2.6357114255428318
  - 2.6230939567089084
  - 2.65347812473774
  - 2.6108987092971803
  - 2.685789474844933
  - 2.619894564151764
  - 2.689641916751862
  - 2.7885202288627626
  - 2.640787133574486
  - 2.6028022676706315
  - 2.615379196405411
  - 2.665675988793373
  - 2.6535724610090257
  - 2.6173700094223022
  - 2.5972305089235306
  - 2.611460375785828
  - 2.636176985502243
  - 2.7060876905918123
  - 2.6059260070323944
  - 2.637007722258568
  - 2.611806681752205
  - 2.608734107017517
  - 2.6460408449172976
  - 2.6245780229568485
  - 2.657348626852036
  - 2.540899728238583
  - 2.613066700100899
  - 2.535213953256607
  - 2.5424387961626054
  - 2.579718726873398
  - 2.5592128545045854
  - 2.5412969172000888
  - 2.5292755872011186
  - 2.5934251934289936
  - 2.5621250808238987
  - 2.520106515288353
  - 2.5495814085006714
  - 2.637883886694908
  - 2.506362697482109
  - 2.488955944776535
  - 2.5560095191001895
  - 2.5535726666450502
  - 2.532051253318787
  - 2.535675570368767
  - 2.4623022913932804
  - 2.4742911517620088
  - 2.5224074661731724
  - 2.4685641825199127
  validation_losses:
  - 0.6685836315155029
  - 0.6796630024909973
  - 0.4865891933441162
  - 0.6140292286872864
  - 0.6067667007446289
  - 0.5351893901824951
  - 0.5290334820747375
  - 0.7667916417121887
  - 0.5903109908103943
  - 0.3757801353931427
  - 0.5068431496620178
  - 0.4231259226799011
  - 0.6625866889953613
  - 0.6475551724433899
  - 0.4206849932670593
  - 0.489677757024765
  - 0.5035495758056641
  - 0.43102023005485535
  - 0.48376739025115967
  - 0.545198917388916
  - 0.5419753193855286
  - 0.6281257271766663
  - 4.4451494216918945
  - 1.2663518190383911
  - 0.5131460428237915
  - 0.4772742986679077
  - 0.6400551199913025
  - 0.7396227121353149
  - 0.45598655939102173
  - 0.466855525970459
  - 0.47451621294021606
  - 0.6645159721374512
  - 0.4750332832336426
  - 0.6008156538009644
  - 1.4861202239990234
  - 0.5454480051994324
  - 0.6111463904380798
  - 0.6446527242660522
  - 0.5218979120254517
  - 0.41894298791885376
  - 0.5510185360908508
  - 0.7344884872436523
  - 0.5329985618591309
  - 0.5741812586784363
  - 0.5523877739906311
  - 0.6610017418861389
  - 0.46112313866615295
  - 1.2536273002624512
  - 0.6997561454772949
  - 1.1279785633087158
  - 4.124784469604492
  - 1.929426908493042
  - 0.8930845260620117
  - 0.8032209277153015
  - 0.8388732075691223
  - 2.290898323059082
  - 0.41605818271636963
  - 0.6924946308135986
  - 0.4269692897796631
  - 0.48085418343544006
  - 0.5074581503868103
  - 0.5301856398582458
  - 0.6183236837387085
  - 0.5585700273513794
  - 2.9948787689208984
  - 5.828744888305664
  - 1.3113903999328613
  - 2.162234306335449
  - 0.6224009394645691
  - 13.804686546325684
  - 18.80108642578125
  - 8.16658878326416
  - 3.4310951232910156
  - 23.591764450073242
  - 21.933767318725586
  - 16.106952667236328
  - 18.621376037597656
  - 46.664794921875
  - 15.875710487365723
  - 10.376008987426758
  - 3.7576401233673096
  - 5.229597091674805
  - 22.320405960083008
  - 28.348997116088867
  - 5.847473621368408
  - 8.849604606628418
  - 10.633707046508789
  - 148.79818725585938
  - 0.6318876147270203
  - 0.6975861191749573
  - 0.6036491990089417
  - 0.6798997521400452
  - 2.697317361831665
  - 1.8181979656219482
  - 1.2138231992721558
  - 10.430709838867188
  - 0.6451109647750854
  - 0.5786727666854858
  - 0.5471891164779663
  - 0.5833187103271484
loss_records_fold4:
  train_losses:
  - 2.6025211572647096
  - 2.7859760224819183
  - 2.8887255787849426
  - 2.803098812699318
  - 2.8273519396781923
  - 2.811996927857399
  - 2.7743862330913545
  - 2.798226916790009
  - 2.7436870694160462
  - 2.771934795379639
  - 2.7520127266645433
  validation_losses:
  - 1.094295859336853
  - 1.2500808238983154
  - 0.3793331980705261
  - 0.36858800053596497
  - 0.36883530020713806
  - 0.3737190067768097
  - 0.36879876255989075
  - 0.363554984331131
  - 0.3633595407009125
  - 0.36952927708625793
  - 0.3657964766025543
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8542024013722127, 0.8044596912521441, 0.8216123499142367,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.0, 0.2962962962962963, 0.23529411764705882, 0.023529411764705882]'
  mean_eval_accuracy: 0.8390591383588853
  mean_f1_accuracy: 0.11102396514161221
  total_train_time: '0:23:50.978010'
