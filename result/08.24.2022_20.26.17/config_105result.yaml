config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:51:06.968401'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_105fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_105fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_105fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_105fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_105fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_105fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_105fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_105fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_105fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_105fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 36.91518325805664
  - 17.50217462182045
  - 21.638991430401802
  - 15.440057080984117
  - 10.962619751691818
  - 14.270287251472475
  - 10.56018553376198
  - 12.233801886439323
  - 12.10531925559044
  - 10.412329381704332
  - 11.596385246515275
  - 6.996261632442475
  - 13.183886218070985
  - 25.684914708137512
  - 7.36100127696991
  - 5.5466363549232485
  - 7.709203836321831
  - 4.964090749621391
  - 7.337426513433456
  - 9.750656008720398
  - 5.71080938577652
  - 7.072441902756691
  - 7.4094848275184635
  - 5.132019031047822
  - 5.359720641374588
  - 5.324266868829728
  - 11.685672396421433
  - 8.80198231637478
  - 7.184885436296463
  - 6.792007380723954
  - 10.59589558839798
  - 6.690239298343659
  - 7.207146835327149
  - 4.973101043701172
  - 6.080519729852677
  - 5.68722911477089
  - 7.776519936323166
  - 5.668108302354813
  - 3.4145563155412675
  - 3.97910373210907
  - 4.40171312391758
  - 6.817756795883179
  - 4.787639638781548
  - 3.1230743408203128
  - 4.009064489603043
  - 5.2619549334049225
  - 4.288312840461731
  - 3.9807378828525546
  - 3.6156676799058918
  - 4.290111991763115
  - 4.346987971663475
  - 4.709518361091614
  - 4.490568536520004
  - 5.453293642401696
  - 4.761567237973213
  - 4.35691579580307
  - 3.334909522533417
  - 6.406453162431717
  - 3.8254336744546893
  - 4.413896769285202
  - 3.399300986528397
  - 3.002054876089096
  - 4.1150350928306585
  - 3.542819380760193
  - 3.309045815467835
  - 2.985849225521088
  - 3.1750751316547396
  - 3.6581674814224243
  - 3.372585356235504
  - 2.963630384206772
  - 3.181629478931427
  validation_losses:
  - 3.726152181625366
  - 4.99723482131958
  - 2.7575912475585938
  - 0.5309506058692932
  - 0.673866868019104
  - 0.6272379159927368
  - 0.6981860995292664
  - 0.5146026015281677
  - 1.0808335542678833
  - 0.5584285855293274
  - 0.8050441145896912
  - 0.45276162028312683
  - 0.9308946132659912
  - 0.5270681381225586
  - 0.4887516498565674
  - 0.5764256715774536
  - 0.6410237550735474
  - 0.3885602355003357
  - 0.4469667375087738
  - 0.4758451282978058
  - 0.39473870396614075
  - 0.40630578994750977
  - 0.43224969506263733
  - 0.39271050691604614
  - 0.39302605390548706
  - 0.44219499826431274
  - 0.4546358287334442
  - 0.44034823775291443
  - 0.5070536732673645
  - 1.0443575382232666
  - 0.5810937881469727
  - 0.3869378864765167
  - 0.4072757363319397
  - 0.5232570767402649
  - 0.40610626339912415
  - 0.43339258432388306
  - 0.39284372329711914
  - 0.3888278901576996
  - 0.3887035548686981
  - 0.4788844585418701
  - 0.4107290208339691
  - 0.4980411231517792
  - 0.38501980900764465
  - 0.3994443416595459
  - 0.37989434599876404
  - 0.4369296133518219
  - 0.6463773846626282
  - 0.3784744143486023
  - 1.286697506904602
  - 0.3915877044200897
  - 0.4352447986602783
  - 0.37937018275260925
  - 0.3759923279285431
  - 0.379328191280365
  - 0.42191407084465027
  - 0.37998032569885254
  - 0.4131825268268585
  - 0.4418102204799652
  - 0.45376163721084595
  - 0.4434850215911865
  - 0.38963714241981506
  - 0.38859209418296814
  - 0.390973836183548
  - 0.39109885692596436
  - 0.4304562211036682
  - 0.39603373408317566
  - 0.40008923411369324
  - 0.38629454374313354
  - 0.38960814476013184
  - 0.38662415742874146
  - 0.3850969076156616
loss_records_fold1:
  train_losses:
  - 3.3730577349662783
  - 2.938752841949463
  - 2.881247127056122
  - 3.0539134323596957
  - 3.0974474340677265
  - 3.1230377137660983
  - 3.12459774017334
  - 2.938748985528946
  - 3.1674593448638917
  - 3.0827274143695833
  - 4.218499916791916
  - 3.594596952199936
  - 3.1192097038030626
  - 3.2946707844734195
  - 3.05109156370163
  - 2.938891845941544
  - 3.0512639582157135
  - 2.9608436435461045
  - 3.0521227300167086
  - 3.9013089150190354
  - 3.702373594045639
  - 3.0931127905845646
  - 3.1999701857566833
  - 3.696264842152596
  - 3.037044769525528
  - 2.9919542193412783
  - 2.9718655407428742
  - 2.973141133785248
  - 2.9235416591167454
  - 2.9179255664348602
  - 2.922064393758774
  - 3.048781090974808
  - 2.8937578380107882
  - 2.9868802189826966
  - 3.2789197623729707
  - 2.894281986355782
  - 3.0223447293043137
  - 3.0152980476617817
  - 2.917584627866745
  - 3.0468268364667894
  - 3.185995274782181
  - 2.959386202692986
  - 2.9417750149965287
  - 2.874898660182953
  validation_losses:
  - 0.4003950357437134
  - 0.3981599807739258
  - 0.39604347944259644
  - 0.40567609667778015
  - 0.4014074504375458
  - 0.4227493703365326
  - 0.40684935450553894
  - 0.4093492329120636
  - 0.42428144812583923
  - 0.3951819837093353
  - 0.4355088174343109
  - 0.400638610124588
  - 0.41788598895072937
  - 0.4103941023349762
  - 0.4379720389842987
  - 0.4148920774459839
  - 0.3978980481624603
  - 0.4102439284324646
  - 0.4000774323940277
  - 0.41007131338119507
  - 0.43168875575065613
  - 0.4109516441822052
  - 0.43809184432029724
  - 0.4835599362850189
  - 0.39508071541786194
  - 0.39351585507392883
  - 0.4004952013492584
  - 0.4678974151611328
  - 0.39547839760780334
  - 0.40688449144363403
  - 0.39394041895866394
  - 0.3927710950374603
  - 0.4048959016799927
  - 0.39838066697120667
  - 0.3950144946575165
  - 0.41540178656578064
  - 0.39110666513442993
  - 0.452166348695755
  - 0.44910740852355957
  - 0.45839622616767883
  - 0.4497239887714386
  - 0.4067508280277252
  - 0.3968687653541565
  - 0.39330169558525085
loss_records_fold2:
  train_losses:
  - 2.895412462949753
  - 2.899055576324463
  - 3.028179475665093
  - 3.157111316919327
  - 2.875770384073258
  - 3.055108088254929
  - 2.878743582963944
  - 3.033329883217812
  - 3.316201534867287
  - 2.972186875343323
  - 2.9277075588703156
  - 3.011031997203827
  - 3.092071902751923
  - 2.937647408246994
  - 2.9340611696243286
  - 2.9892772257328035
  - 2.9950825899839404
  - 3.029473412036896
  - 2.898502764105797
  - 2.9449434340000153
  - 2.898209434747696
  - 3.0213096320629123
  - 2.8493241608142856
  - 2.9123275846242906
  - 2.9266664892435075
  - 2.9045415937900545
  - 2.9481268525123596
  - 2.908165916800499
  - 2.810222399234772
  - 2.8789338648319247
  - 2.9214845776557925
  - 3.1370875954627992
  - 3.290407040715218
  - 3.0375048935413362
  - 3.056851875782013
  - 2.9152812838554385
  - 2.9459744811058046
  - 2.8733646750450137
  - 2.875790509581566
  - 2.9192587703466417
  - 2.8900803923606873
  - 2.8625211209058765
  - 2.8233969390392306
  - 2.8311050087213516
  - 2.861318451166153
  - 2.96373205780983
  - 2.995540821552277
  - 2.9675516337156296
  - 2.821952791512013
  - 2.8785637438297274
  - 2.89063081741333
  - 2.8085521280765535
  - 2.915972477197647
  - 2.951324608922005
  - 2.9087492495775225
  - 2.85549173951149
  - 2.913769793510437
  - 2.8384698152542116
  - 2.9466873884201052
  - 2.9060539245605472
  - 3.048301023244858
  - 3.092882513999939
  - 3.2941144824028017
  - 3.343011236190796
  - 3.0614694565534593
  - 3.0588886737823486
  - 3.2005507528781894
  - 3.199975162744522
  - 3.2350458562374116
  - 3.0085224360227585
  - 2.9964337170124056
  - 2.944171816110611
  - 2.958496952056885
  - 3.0713970839977267
  - 3.099677672982216
  - 3.0264388382434846
  - 3.0258567273616794
  - 2.99154466688633
  - 2.94066518843174
  - 2.995934784412384
  - 2.9994645535945894
  - 2.9492578476667406
  - 3.122206598520279
  - 3.113838690519333
  - 2.9632895916700366
  - 2.951687151193619
  - 2.9642076909542086
  - 2.948177206516266
  - 3.0211092948913576
  - 2.9686250507831575
  - 2.95594020485878
  - 4.893650829792023
  - 4.03697536289692
  - 6.2165935218334205
  - 4.359829080104828
  - 3.0937101513147356
  - 2.9839734673500065
  - 3.049254083633423
  - 3.0126101613044742
  - 4.7729612469673155
  validation_losses:
  - 0.37270626425743103
  - 0.37230733036994934
  - 0.8075960278511047
  - 0.3751106560230255
  - 0.4256048798561096
  - 0.42346227169036865
  - 0.38184186816215515
  - 0.5645883679389954
  - 0.5199264287948608
  - 0.4409148395061493
  - 0.39784836769104004
  - 0.3752685487270355
  - 0.4318966567516327
  - 0.37510764598846436
  - 0.41194334626197815
  - 0.3774857521057129
  - 0.3841177523136139
  - 0.6633197069168091
  - 0.47056645154953003
  - 0.907529354095459
  - 1.5905815362930298
  - 1.127604603767395
  - 0.5708509683609009
  - 0.7932390570640564
  - 0.6733815670013428
  - 0.5933139324188232
  - 0.38548675179481506
  - 0.4334220588207245
  - 0.5246481895446777
  - 0.5905422568321228
  - 0.5962913036346436
  - 0.5698328018188477
  - 0.7661033272743225
  - 1.9371079206466675
  - 0.378447026014328
  - 0.38274481892585754
  - 0.37399768829345703
  - 0.3729723393917084
  - 0.4117365777492523
  - 0.3879627585411072
  - 0.3717914819717407
  - 0.4495774209499359
  - 0.3804815411567688
  - 3.8132245540618896
  - 0.8896450996398926
  - 0.39630237221717834
  - 0.37663283944129944
  - 0.38729462027549744
  - 0.36948323249816895
  - 0.3725242614746094
  - 0.3688334822654724
  - 0.37096017599105835
  - 0.36926838755607605
  - 0.38223713636398315
  - 0.3791353106498718
  - 0.4108358919620514
  - 0.38038092851638794
  - 0.4042976498603821
  - 0.38898858428001404
  - 0.3786245286464691
  - 0.43191835284233093
  - 2.851440191268921
  - 0.4716680943965912
  - 0.3909532129764557
  - 0.3891839385032654
  - 0.43140318989753723
  - 0.3860771656036377
  - 0.4458022713661194
  - 0.5469430685043335
  - 2.3594822883605957
  - 0.3828789293766022
  - 0.3839731812477112
  - 0.3915509581565857
  - 0.4052289128303528
  - 0.38636693358421326
  - 0.44632577896118164
  - 0.38437575101852417
  - 0.39481180906295776
  - 0.40496644377708435
  - 0.5790395140647888
  - 0.799144446849823
  - 0.4036600589752197
  - 0.4257897734642029
  - 0.6011274456977844
  - 0.4060107469558716
  - 0.3863302767276764
  - 0.38399672508239746
  - 0.3864569664001465
  - 0.4888530373573303
  - 0.3998614251613617
  - 0.6094794869422913
  - 1.1459791660308838
  - 0.39119836688041687
  - 4.745241641998291
  - 0.38288140296936035
  - 0.38397976756095886
  - 0.38688695430755615
  - 1657480320.0
  - 192931742875648.0
  - 10.533977508544922
loss_records_fold3:
  train_losses:
  - 5.978644794225693
  - 3.0377480149269105
  - 3.4250855803489686
  - 2.9602808684110644
  - 3.08262155354023
  - 3.297858285903931
  - 3.3869226902723315
  - 3.0784574270248415
  - 3.084768044948578
  - 3.2177821516990663
  - 3.083860829472542
  - 3.063284379243851
  - 3.0396378219127658
  - 2.9911199092864993
  - 2.9262195855379107
  - 3.0071958661079408
  - 2.954452925920487
  - 2.9772416412830354
  - 2.9824950993061066
  - 2.9455904960632324
  - 3.0504667401313785
  validation_losses:
  - 0.476060152053833
  - 0.44458428025245667
  - 0.3978522717952728
  - 0.3962078094482422
  - 0.3948211669921875
  - 0.40589800477027893
  - 0.42151522636413574
  - 0.4356727600097656
  - 0.4010300934314728
  - 0.39992889761924744
  - 0.4123830795288086
  - 0.4116809368133545
  - 0.39490845799446106
  - 0.4004218280315399
  - 0.4119415581226349
  - 0.4011053740978241
  - 0.39895951747894287
  - 0.3972960114479065
  - 0.4048417806625366
  - 0.3971914052963257
  - 0.3984525203704834
loss_records_fold4:
  train_losses:
  - 3.0820090860128406
  - 3.0358453750610352
  - 3.014775747060776
  - 3.06549391746521
  - 2.983451166749001
  - 2.9557010143995286
  - 2.9656350761651993
  - 2.9385254323482517
  - 3.04818714261055
  - 3.004238271713257
  - 3.1053636938333513
  validation_losses:
  - 0.4246571362018585
  - 0.4020287096500397
  - 0.4236748218536377
  - 0.4000934362411499
  - 0.3895505964756012
  - 0.3905515968799591
  - 0.39102184772491455
  - 0.3974546194076538
  - 0.38992419838905334
  - 0.39040613174438477
  - 0.3917364776134491
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 71 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 44 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.5077186963979416, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.11692307692307694, 0.0, 0.0]'
  mean_eval_accuracy: 0.7882878581575332
  mean_f1_accuracy: 0.023384615384615386
  total_train_time: '0:24:09.207835'
