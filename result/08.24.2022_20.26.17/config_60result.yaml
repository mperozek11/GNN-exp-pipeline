config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:47:43.260726'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_60fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_60fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_60fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_60fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_60fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_60fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_60fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_60fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_60fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_60fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 75.8312561005354
  - 40.252205953001976
  - 32.42908609509468
  - 31.235434387624267
  - 25.347994390130044
  - 27.780858099460602
  - 20.735293340682986
  - 11.579119983315469
  - 20.623218265175822
  - 17.47306225001812
  - 16.289416390657426
  - 26.915654629468918
  - 17.669327944517136
  - 15.379098823666574
  - 13.036243322491647
  - 13.282848769426346
  - 15.675322332978249
  - 8.569275042414665
  - 10.899815577268601
  - 11.666251742839814
  - 12.26942228972912
  - 12.472590929269792
  - 13.163862133026123
  - 9.755079799890519
  - 7.450321865081787
  - 8.427860869467258
  - 7.913348269462586
  - 8.749011746048927
  - 8.750036014616489
  - 7.716768860816956
  - 6.59941233098507
  - 6.424942129850388
  - 7.296255168318749
  - 7.190815144777298
  - 6.95089637041092
  - 6.532384246587753
  - 6.094880446791649
  - 6.635364371538163
  - 6.706010395288468
  - 6.5373072773218155
  - 6.367723312973976
  - 6.386305367946625
  - 6.235372257232666
  - 6.169302761554718
  - 6.522110962867737
  - 6.033766126632691
  - 6.408111134171486
  - 6.439115297794342
  - 6.49555613398552
  - 6.257210603356362
  - 6.325504708290101
  - 6.437751352787018
  - 6.397427710890771
  - 6.6791556715965275
  - 6.658702605962754
  - 6.8672199934721
  - 6.380930343270302
  - 6.714854496717454
  - 6.193900856375695
  - 6.301469433307648
  - 6.236490270495415
  - 6.404485756158829
  - 6.423905941843987
  - 6.185214895009995
  - 6.499621590971947
  - 6.510461640357971
  - 6.257050615549088
  - 6.480029404163361
  - 6.032274675369263
  - 6.308711916208267
  - 6.360330212116242
  - 6.449591279029846
  - 7.385963010787965
  - 10.605766767263413
  - 6.712623527646065
  - 6.688297772407532
  - 6.61097976565361
  - 6.413285738229752
  - 6.388799941539765
  - 6.2233657091856
  - 6.245785677433014
  - 6.463248065114022
  - 6.386129289865494
  - 6.342827641963959
  - 6.947026470303536
  - 6.223983442783356
  - 6.158864504098893
  - 6.294988045096398
  - 6.067241805791856
  - 6.244712096452713
  - 6.084640794992447
  - 6.591340351104737
  - 6.568365529179573
  - 6.732123690843583
  - 6.738554963469506
  - 7.205888509750366
  - 6.4078487932682044
  - 6.494535818696022
  - 6.2619084328413015
  - 6.3038233637809755
  validation_losses:
  - 0.7807118892669678
  - 0.6960464119911194
  - 0.9872703552246094
  - 0.42423731088638306
  - 0.6927796602249146
  - 0.8057793378829956
  - 0.4857853055000305
  - 0.43544232845306396
  - 0.6083670258522034
  - 0.4625251591205597
  - 0.5113434791564941
  - 0.48642486333847046
  - 0.5890485048294067
  - 0.5395548939704895
  - 0.6287767887115479
  - 0.46005746722221375
  - 0.5166119337081909
  - 0.41445672512054443
  - 0.45113441348075867
  - 0.42064106464385986
  - 0.40196216106414795
  - 0.4024835228919983
  - 0.47376421093940735
  - 0.46312686800956726
  - 0.4915607273578644
  - 0.4975357949733734
  - 0.474399596452713
  - 0.423505961894989
  - 0.5957935452461243
  - 0.41389337182044983
  - 0.39092570543289185
  - 0.42072734236717224
  - 0.39498597383499146
  - 0.39056459069252014
  - 0.4089585244655609
  - 0.4002479314804077
  - 0.39752063155174255
  - 0.479486346244812
  - 0.38670027256011963
  - 0.3851771354675293
  - 0.4038669168949127
  - 0.4010831117630005
  - 0.39898282289505005
  - 0.40067198872566223
  - 0.42848822474479675
  - 0.43598130345344543
  - 0.3956969380378723
  - 0.42945414781570435
  - 0.40183794498443604
  - 0.4066784679889679
  - 0.3936680257320404
  - 0.41637247800827026
  - 0.39802074432373047
  - 0.39252012968063354
  - 0.44877922534942627
  - 0.4432787299156189
  - 0.4252529442310333
  - 0.43112751841545105
  - 0.42295512557029724
  - 0.41365280747413635
  - 0.42535385489463806
  - 0.42849135398864746
  - 0.40215906500816345
  - 0.38815924525260925
  - 0.4633038640022278
  - 0.40297290682792664
  - 0.3965550363063812
  - 0.8315063118934631
  - 0.4194757640361786
  - 0.38568809628486633
  - 0.3935297429561615
  - 0.5046604871749878
  - 0.5941794514656067
  - 0.4446648955345154
  - 0.42112869024276733
  - 0.4108196496963501
  - 0.40933462977409363
  - 0.43648862838745117
  - 0.5069310665130615
  - 0.414946585893631
  - 0.4094148874282837
  - 0.39924097061157227
  - 0.42241746187210083
  - 0.3937966823577881
  - 0.4158707857131958
  - 0.4079146087169647
  - 0.3919069468975067
  - 0.39516934752464294
  - 0.3950461745262146
  - 0.38419944047927856
  - 0.4011767506599426
  - 21.146345138549805
  - 0.456559956073761
  - 0.4200196862220764
  - 0.4278651475906372
  - 7.779789447784424
  - 4552306.5
  - 2965033.25
  - 26157738.0
  - 0.4733901619911194
loss_records_fold1:
  train_losses:
  - 6.525309619307518
  - 6.4252360492944725
  - 6.320896020531655
  - 6.4976355850696565
  - 6.398458912968636
  - 6.391856595873833
  - 6.450435966253281
  - 6.348364320397377
  - 6.5592250555753715
  - 7.925869101285935
  - 7.128802940249443
  - 6.940911179780961
  - 6.511562776565552
  - 7.2030774593353275
  - 6.536780053377152
  - 6.46798484325409
  - 6.45070487856865
  - 6.210025146603584
  - 6.698123905062676
  - 6.567846518754959
  - 6.484494933485985
  - 6.19836669266224
  - 6.480040439963341
  - 6.367248889803887
  - 6.389790484309197
  - 6.518549129366875
  - 6.392613548040391
  - 6.5377483695745475
  - 6.481616598367691
  - 6.478858265280724
  - 6.4609551489353185
  - 6.3152703076601036
  - 6.434838938713074
  - 6.466698828339577
  - 6.4185878306627275
  - 6.201793920993805
  - 6.601355203986168
  - 6.492664664983749
  - 6.669427073001862
  - 6.472430014610291
  - 6.290549305081368
  - 6.263704788684845
  - 6.515432140231133
  - 6.238315054774285
  - 6.377033045887948
  - 6.919662356376648
  - 6.27288376390934
  - 6.381625831127167
  - 6.545626765489579
  - 6.439718475937844
  - 6.588420251011849
  - 6.347355911135674
  - 6.593937993049622
  - 6.338657030463219
  - 6.252720990777016
  - 6.306256866455079
  - 6.4320403218269355
  - 6.521145233511925
  - 6.6671236068010336
  - 6.5216544538736345
  - 6.451394250988961
  - 6.334342792630196
  - 6.438984978199006
  - 6.42940443456173
  - 6.30069899559021
  - 6.474339348077774
  - 6.352296397089958
  - 6.5782104492187505
  - 6.508931952714921
  - 6.6652149081230165
  - 6.341068899631501
  - 6.768876206874848
  - 6.403640878200531
  - 8.226968038082124
  - 6.979339960217477
  - 6.605122417211533
  - 6.219370758533478
  - 6.629574397206307
  - 6.371780517697335
  - 6.498568817973137
  - 6.584332305192948
  - 6.288916864991188
  - 6.156920915842057
  - 6.414048179984093
  - 6.689857387542725
  - 6.4932631850242615
  - 6.32013875246048
  - 6.406187400221825
  - 6.335319295525551
  - 6.387936821579934
  - 6.162198415398598
  - 6.475858509540558
  - 6.713504448533058
  - 6.541853475570679
  - 6.78435636907816
  - 6.42470504641533
  - 6.38446998000145
  - 6.520391067862511
  - 6.480875185132027
  - 6.2179196894168856
  validation_losses:
  - 0.43653541803359985
  - 0.41900619864463806
  - 0.4846016764640808
  - 0.407865047454834
  - 0.438663512468338
  - 0.4375775456428528
  - 0.4308236837387085
  - 0.5078549385070801
  - 5908619853824.0
  - 1.6355890035629272
  - 0.5582356452941895
  - 0.46064919233322144
  - 0.4241240918636322
  - 0.46836307644844055
  - 0.42665910720825195
  - 0.42991772294044495
  - 0.4237044155597687
  - 0.44200295209884644
  - 0.45214274525642395
  - 0.43664786219596863
  - 0.41444385051727295
  - 0.4089892506599426
  - 0.45382943749427795
  - 0.4657641053199768
  - 0.45379284024238586
  - 0.4338648021221161
  - 0.42033305764198303
  - 0.42875951528549194
  - 0.4206145405769348
  - 0.5751055479049683
  - 0.4124078154563904
  - 0.42841991782188416
  - 0.47095930576324463
  - 0.4174436926841736
  - 0.47838178277015686
  - 0.4791431725025177
  - 0.49921661615371704
  - 0.45480331778526306
  - 0.4263259768486023
  - 0.44274452328681946
  - 0.39907774329185486
  - 0.45169514417648315
  - 0.4640228748321533
  - 0.4738716185092926
  - 0.4955542981624603
  - 0.4361467659473419
  - 0.42576324939727783
  - 0.42982369661331177
  - 0.4229608178138733
  - 0.4407077729701996
  - 0.426435649394989
  - 0.4353795051574707
  - 0.4048299193382263
  - 0.4647372364997864
  - 0.4174112379550934
  - 0.4350369870662689
  - 0.44925016164779663
  - 0.42583805322647095
  - 0.45651209354400635
  - 0.4142645597457886
  - 0.43554556369781494
  - 0.4115711748600006
  - 0.45250648260116577
  - 0.43628379702568054
  - 0.4350188374519348
  - 0.41245847940444946
  - 0.4437590539455414
  - 0.4721480906009674
  - 0.46296659111976624
  - 0.4318860173225403
  - 0.40234631299972534
  - 0.4268942177295685
  - 90.78178405761719
  - 0.5971088409423828
  - 11892.1298828125
  - 204371488.0
  - 0.42797601222991943
  - 0.5139762759208679
  - 0.4153611660003662
  - 0.42197081446647644
  - 0.4339110553264618
  - 0.4318119287490845
  - 0.46387338638305664
  - 0.409678190946579
  - 0.43672481179237366
  - 0.4167841076850891
  - 0.4193233251571655
  - 0.42535892128944397
  - 0.4406590163707733
  - 0.42663145065307617
  - 0.42186665534973145
  - 0.44422245025634766
  - 0.43228858709335327
  - 0.42270416021347046
  - 0.4625127613544464
  - 0.4233127534389496
  - 0.4169933795928955
  - 0.41724053025245667
  - 0.5020735263824463
  - 0.42805737257003784
loss_records_fold2:
  train_losses:
  - 6.622416454553605
  - 6.372281321883202
  - 6.354097312688828
  - 6.446293041110039
  - 6.348491832613945
  - 6.648597180843353
  - 6.339430904388428
  - 6.352820709347725
  - 6.343257278203964
  - 6.379209759831429
  - 6.914979979395866
  - 6.294888606667519
  - 6.4326954096555715
  - 6.34046186208725
  - 6.734518274664879
  - 6.5599410668015485
  - 6.364716702699662
  - 6.584941273927689
  - 6.547150796651841
  - 10.398443132638931
  - 12.355676275491716
  - 7.427309739589692
  - 6.4913660258054735
  - 7.526632484793663
  - 7.148570740222931
  - 7.266239503026009
  - 6.56855637729168
  - 6.408161321282387
  - 6.505387973785401
  - 6.642235681414604
  - 6.375075003504754
  - 6.307433918118477
  - 6.764417034387589
  - 6.215204110741616
  - 6.384791594743729
  - 6.481265193223954
  - 6.442334246635437
  - 6.688481917977334
  - 6.702541202306747
  - 6.353168842196465
  - 6.4409538328647615
  - 6.5728497296571735
  - 6.745534291863442
  - 6.286648356914521
  - 6.593517959117889
  - 6.576223081350327
  - 6.575873678922654
  - 6.37472375035286
  - 6.431436240673065
  - 6.382023611664772
  - 6.424273037910462
  - 6.3851025104522705
  - 6.346433365345002
  - 6.172047439217568
  - 6.4640046745538715
  - 6.60487466454506
  - 6.639312717318536
  - 6.513888740539551
  - 6.269429704546929
  - 6.578639590740204
  - 6.259326365590096
  - 6.500810012221336
  - 6.423336523771287
  - 6.854738807678223
  - 6.405677312612534
  - 6.480272981524468
  - 6.583045160770417
  - 6.542721211910248
  - 6.3689812391996385
  - 6.423158106207848
  - 6.360132229328156
  - 6.52238684296608
  - 6.442117920517922
  - 6.3739060252904896
  - 6.509058877825737
  - 6.417100414633751
  - 6.488117569684983
  - 6.5271845877170565
  - 6.356503030657769
  - 6.910625740885735
  - 6.503987982869148
  - 6.521330131590367
  - 6.414409050345421
  - 6.4159813016653064
  - 6.507727652788162
  - 6.753859776258469
  - 6.307741534709931
  - 6.449354872107506
  - 6.3423194050788885
  - 6.453072467446328
  - 6.825555557012558
  - 6.941637858748436
  - 6.355893278121949
  - 6.3179029494524
  - 6.222727596759796
  - 6.520409539341927
  - 6.510758975148201
  - 6.758157050609589
  - 6.3925249695777895
  - 6.590499129891396
  validation_losses:
  - 311053344.0
  - 0.4001051187515259
  - 0.39829349517822266
  - 134623792.0
  - 0.5000813007354736
  - 0.4124220609664917
  - 0.4011671543121338
  - 0.4610315263271332
  - 0.41140544414520264
  - 0.5007185339927673
  - 0.39566853642463684
  - 0.42630141973495483
  - 0.4104119539260864
  - 0.3976970911026001
  - 263346384.0
  - 0.40426596999168396
  - 2362640384.0
  - 0.4199346601963043
  - 218630784.0
  - 0.43178966641426086
  - 0.41037988662719727
  - 0.39905643463134766
  - 0.45989593863487244
  - 0.3909282982349396
  - 0.4009620249271393
  - 0.43116921186447144
  - 0.3916165828704834
  - 0.4210403263568878
  - 0.3997650146484375
  - 0.43903976678848267
  - 0.4138187766075134
  - 0.426005095243454
  - 0.414897084236145
  - 0.40751272439956665
  - 0.4221210479736328
  - 0.4099053144454956
  - 0.4133565127849579
  - 0.4033485949039459
  - 0.40545088052749634
  - 0.43437230587005615
  - 0.39842677116394043
  - 2.276165008544922
  - 0.5071061849594116
  - 0.4116716980934143
  - 7.692568778991699
  - 0.39974677562713623
  - 0.46857255697250366
  - 0.3981553018093109
  - 0.40012064576148987
  - 0.41098201274871826
  - 0.6030803322792053
  - 1.762677788734436
  - 0.5455521941184998
  - 0.40526852011680603
  - 0.4005388915538788
  - 0.42381536960601807
  - 0.4170040190219879
  - 0.41781002283096313
  - 0.9542979001998901
  - 21.504878997802734
  - 0.415630042552948
  - 0.4662739634513855
  - 0.3958224654197693
  - 0.4057813584804535
  - 0.40014564990997314
  - 0.38861769437789917
  - 0.4045046865940094
  - 2.1751482486724854
  - 0.9802583456039429
  - 0.4158869981765747
  - 11.529794692993164
  - 0.4121672213077545
  - 0.3963005840778351
  - 0.43650907278060913
  - 0.3969830870628357
  - 0.39560064673423767
  - 0.41492003202438354
  - 0.4009090065956116
  - 0.405933678150177
  - 0.47727298736572266
  - 0.4108203649520874
  - 0.41664546728134155
  - 0.4164826571941376
  - 0.42221599817276
  - 0.5529935956001282
  - 0.4122290015220642
  - 0.4392707645893097
  - 0.43725040555000305
  - 0.39946678280830383
  - 0.4712908864021301
  - 0.4321344196796417
  - 0.41899633407592773
  - 0.39362531900405884
  - 0.41180741786956787
  - 0.41891998052597046
  - 0.4166181981563568
  - 0.47826656699180603
  - 0.42114171385765076
  - 0.40083786845207214
  - 0.4081166684627533
loss_records_fold3:
  train_losses:
  - 6.610780251026154
  - 6.504600182175636
  - 6.283235695958138
  - 6.452446699142456
  - 6.496236488223076
  - 6.658007708191872
  - 6.454392501711846
  - 6.510357287526131
  - 6.482642287015915
  - 6.665993890166283
  - 6.523889181017876
  - 6.5529760211706165
  - 6.5882951647043235
  - 6.473275998234749
  - 6.323292309045792
  - 6.520541799068451
  - 6.405323243141175
  - 6.690093955397606
  - 6.700063973665237
  - 6.307301834225655
  - 6.356239610910416
  - 6.479021349549294
  - 6.2912877112627035
  - 6.69410240650177
  - 6.358782160282136
  - 6.466286155581475
  - 6.396723210811615
  - 6.426411700248718
  - 6.485238026082516
  - 6.29806302189827
  - 6.637225082516671
  - 6.268622213602066
  - 6.36347149014473
  - 6.387895774841309
  - 6.38192772269249
  - 6.318213012814522
  - 6.367150005698204
  - 6.395803099870682
  - 6.420976179838181
  - 6.465858048200608
  - 6.325294890999794
  - 6.689540332555771
  - 6.524829187989235
  - 6.480264991521835
  - 6.292044572532177
  - 6.2400207936763765
  - 6.338640826940537
  - 6.201377525925636
  - 6.53581560254097
  - 6.735712721943855
  - 6.481212937831879
  - 6.53313094675541
  - 6.359698683023453
  - 6.3624042034149175
  - 6.592698922753335
  - 6.386235648393631
  - 6.178117883205414
  - 6.472881835699082
  - 6.548899099230766
  - 6.438033729791641
  - 6.956623500585557
  - 6.581971901655198
  - 6.498079681396485
  - 6.550215893983841
  - 6.356028071045876
  - 6.398925709724427
  - 6.2341469645500185
  - 6.616575926542282
  - 6.56613667011261
  - 6.367828768491745
  - 6.3061233282089235
  - 6.351393741369248
  - 6.5061181634664536
  - 6.560718058049679
  - 6.292987728118897
  - 6.347664493322373
  - 6.459642022848129
  - 6.4059271872043615
  - 6.360649833083153
  validation_losses:
  - 0.4162200689315796
  - 0.42963868379592896
  - 0.4114864468574524
  - 0.41508397459983826
  - 0.44395941495895386
  - 0.49020078778266907
  - 0.42133182287216187
  - 0.42079052329063416
  - 0.4544447064399719
  - 0.4077880382537842
  - 0.4180848300457001
  - 0.4073435366153717
  - 0.45689913630485535
  - 0.40665560960769653
  - 0.43227049708366394
  - 0.3979054391384125
  - 0.47379183769226074
  - 0.4092611074447632
  - 0.46586155891418457
  - 0.41446739435195923
  - 0.40627792477607727
  - 0.44413453340530396
  - 0.4187672436237335
  - 0.4104434549808502
  - 0.4092044532299042
  - 0.4392073154449463
  - 0.4519783854484558
  - 0.40288904309272766
  - 0.40265092253685
  - 0.40741804242134094
  - 0.43718257546424866
  - 0.42287611961364746
  - 0.41530585289001465
  - 0.4190148711204529
  - 0.4833173155784607
  - 0.4154011011123657
  - 0.4250621497631073
  - 0.49899357557296753
  - 0.42044544219970703
  - 0.4323814809322357
  - 0.42008987069129944
  - 0.5430128574371338
  - 0.41461801528930664
  - 0.4017067551612854
  - 0.4182303249835968
  - 0.42773565649986267
  - 0.49143415689468384
  - 0.4023285508155823
  - 0.45478302240371704
  - 0.45027169585227966
  - 0.42445263266563416
  - 0.4102938175201416
  - 0.42315638065338135
  - 0.40721839666366577
  - 0.4207630455493927
  - 0.4066051244735718
  - 0.4021483063697815
  - 0.4708428978919983
  - 0.4684644043445587
  - 0.39804738759994507
  - 0.4309140741825104
  - 0.43566763401031494
  - 0.414371132850647
  - 0.43714773654937744
  - 0.4088933765888214
  - 0.4356323778629303
  - 0.44097012281417847
  - 0.4248988628387451
  - 0.4241793751716614
  - 0.4220600128173828
  - 0.40428435802459717
  - 0.4723728597164154
  - 0.48784589767456055
  - 0.41685009002685547
  - 0.41813886165618896
  - 0.425509512424469
  - 0.43455731868743896
  - 0.4371053874492645
  - 0.434842973947525
loss_records_fold4:
  train_losses:
  - 6.392810016870499
  - 6.58735643029213
  - 6.411929431557656
  - 6.3482790946960455
  - 6.488766831159592
  - 6.983118760585786
  - 6.577212953567505
  - 6.886236140131951
  - 6.503268870711327
  - 6.559177997708321
  - 6.199440878629685
  - 6.390222501754761
  - 6.5316257089376455
  - 6.381073442101479
  - 6.396453332901001
  - 6.901200866699219
  - 6.529680302739144
  - 6.576695704460144
  - 6.503264099359512
  - 6.709396788477898
  - 6.483573761582375
  - 6.62962621152401
  validation_losses:
  - 0.4155898094177246
  - 0.4058395326137543
  - 0.40679091215133667
  - 0.4322315752506256
  - 0.3963701128959656
  - 0.4254762828350067
  - 0.41675233840942383
  - 0.41638004779815674
  - 0.42289936542510986
  - 0.40332916378974915
  - 0.42551758885383606
  - 0.4085538685321808
  - 0.4373270869255066
  - 0.41240328550338745
  - 0.40570738911628723
  - 0.4644864499568939
  - 0.43493616580963135
  - 0.432917982339859
  - 0.4304805397987366
  - 0.4078906774520874
  - 0.4109322428703308
  - 0.4041532576084137
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 79 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:42:38.913241'
