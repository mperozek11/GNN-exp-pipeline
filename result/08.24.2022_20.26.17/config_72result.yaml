config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:06:16.220277'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_72fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_72fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_72fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_72fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_72fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_72fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_72fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_72fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_72fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_72fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 40.76982807815075
  - 32.19908231496811
  - 27.146099388599396
  - 18.637794059515
  - 18.250450177490713
  - 17.279132995009423
  - 12.967003512382508
  - 10.458397728204728
  - 21.907351361215117
  - 18.46820552200079
  - 9.630886226892471
  - 11.103784805536272
  - 9.974510163068771
  - 7.590362468361855
  - 9.339926984906198
  - 8.937547132372856
  - 7.4809767544269565
  - 6.634297949075699
  - 6.425154691934586
  - 6.783465401828289
  - 7.361902254819871
  validation_losses:
  - 1.1151922941207886
  - 2.4296133518218994
  - 0.6581025123596191
  - 0.6398488283157349
  - 0.4624077081680298
  - 0.5305084586143494
  - 0.43726322054862976
  - 0.6023592948913574
  - 0.4437234401702881
  - 0.384006530046463
  - 0.400372177362442
  - 0.7110698223114014
  - 0.3837090730667114
  - 0.4044686257839203
  - 0.5450120568275452
  - 0.39414915442466736
  - 0.3963378667831421
  - 0.39043155312538147
  - 0.3966805636882782
  - 0.39191552996635437
  - 0.3911878168582916
loss_records_fold1:
  train_losses:
  - 6.319423574209214
  - 6.770276156067848
  - 6.846397086977959
  - 6.484173491597176
  - 6.141482448577881
  - 6.828998783230782
  - 6.847559225559235
  - 6.206294959783555
  - 6.670082616806031
  - 6.330259680747986
  - 6.146267369389534
  - 5.729793423414231
  - 6.168893957138062
  - 6.106699404120445
  - 6.205076256394387
  - 5.857530939579011
  - 5.9130327135324485
  - 5.98674401640892
  - 5.677374574542046
  - 5.910425889492036
  - 5.668149799108505
  - 5.793245181441307
  validation_losses:
  - 0.397163987159729
  - 0.6330320835113525
  - 0.40587684512138367
  - 0.400205135345459
  - 0.3986985683441162
  - 0.4810864329338074
  - 0.4948723316192627
  - 0.45643943548202515
  - 0.4087313115596771
  - 0.426857590675354
  - 0.42641910910606384
  - 0.44772428274154663
  - 0.4003675580024719
  - 0.3997248411178589
  - 0.3945847451686859
  - 0.4875057637691498
  - 0.4272713363170624
  - 0.41531985998153687
  - 0.39911308884620667
  - 0.39822709560394287
  - 0.40499258041381836
  - 0.4095616042613983
loss_records_fold2:
  train_losses:
  - 5.894471207261086
  - 6.04345018863678
  - 6.470872396230698
  - 6.869193431735039
  - 6.019431340694428
  - 5.8202890992164615
  - 5.773545476794244
  - 5.867688673734666
  - 5.818174031376839
  - 6.267582151293755
  - 5.953330072760583
  - 5.856196308135987
  - 5.897167554497719
  - 6.136854812502861
  - 6.284121191501618
  - 6.021590462327004
  - 5.9230807512998584
  - 6.02977834045887
  - 7.4616359084844595
  - 6.3748077929019935
  - 6.3259400755167015
  - 6.156848362088204
  - 6.328336304426194
  - 6.528191143274308
  - 6.228399232029915
  - 6.157065030932427
  - 6.019574305415154
  - 6.141664192080498
  - 6.265867120027543
  - 5.86289119720459
  - 5.9689515173435215
  - 7.158386611938477
  - 12.080220276117325
  - 8.405779087543488
  - 7.0346690863370895
  - 6.2816818952560425
  - 6.403131732344628
  - 6.03107457458973
  - 6.197978243231773
  - 6.191952309012414
  - 5.910351502895356
  - 5.873523107171059
  - 6.064339765906334
  - 6.001519733667374
  - 5.960248503088952
  - 6.176142975687981
  - 6.501358550786972
  - 6.083236540853978
  - 5.895651584863663
  - 5.925744053721428
  - 5.973396277427674
  - 6.141473066806793
  - 6.043762692809105
  - 5.988984414935112
  - 5.916457670927048
  - 5.930472207069397
  - 6.034872272610665
  - 6.145578846335411
  - 5.9169262737035755
  - 6.028356835246086
  - 5.957418993115425
  - 6.233861681818962
  - 5.88134150505066
  - 6.1846519172191625
  - 6.40777359455824
  - 5.841702914237977
  - 6.071615907549859
  - 5.9350974559783936
  - 5.874736621975899
  - 6.1654966324567795
  - 6.105927492678166
  - 6.085787189006806
  - 5.930120220780373
  - 5.869623699784279
  - 6.05624061524868
  - 5.8547987103462225
  - 5.963600727915765
  - 6.546250036358834
  - 6.088159453868866
  - 5.904686167836189
  - 5.849275013804436
  - 5.977347227931023
  - 5.938803744316101
  - 5.958151039481163
  - 5.888882207870484
  - 6.039685234427452
  - 5.949940061569214
  - 6.003848937153816
  - 5.936330664157868
  - 5.907199835777283
  - 5.8553928405046465
  - 6.019475305080414
  - 5.914584392309189
  - 6.049384608864784
  - 7.393186119198799
  - 6.111311835050583
  - 5.903695997595787
  - 5.972334730625153
  - 6.157454091310502
  - 5.907858037948609
  validation_losses:
  - 0.4016032814979553
  - 0.4182196259498596
  - 2.4477427005767822
  - 0.3902626633644104
  - 0.3774467706680298
  - 0.41882362961769104
  - 0.44617339968681335
  - 0.5802504420280457
  - 0.3988265097141266
  - 0.45830002427101135
  - 0.38645467162132263
  - 0.38158875703811646
  - 0.4393022656440735
  - 0.4641473591327667
  - 0.38531291484832764
  - 0.38193297386169434
  - 0.38405102491378784
  - 4.830397129058838
  - 0.4598303437232971
  - 0.388485312461853
  - 0.5033087730407715
  - 0.3842734396457672
  - 0.407471239566803
  - 0.38393545150756836
  - 0.44268694519996643
  - 0.4398272931575775
  - 0.4480361044406891
  - 50.33183288574219
  - 5.167679309844971
  - 2.723527431488037
  - 0.3845265209674835
  - 0.47086194157600403
  - 0.402849406003952
  - 0.39486992359161377
  - 0.3818874955177307
  - 0.38262733817100525
  - 0.40154147148132324
  - 0.38945820927619934
  - 0.4880106449127197
  - 0.3837558329105377
  - 0.4023495018482208
  - 0.4014749526977539
  - 0.4203418791294098
  - 0.4832668602466583
  - 0.38586100935935974
  - 0.40549391508102417
  - 0.4356134831905365
  - 0.3897440433502197
  - 0.3824114501476288
  - 0.41267818212509155
  - 0.4032926857471466
  - 0.3818187713623047
  - 1.0255862474441528
  - 0.3842218220233917
  - 0.3824305832386017
  - 0.4160718619823456
  - 0.41690734028816223
  - 0.4046260416507721
  - 0.42137768864631653
  - 0.3936762511730194
  - 0.3824349343776703
  - 0.38302555680274963
  - 0.3871191442012787
  - 0.4758897125720978
  - 0.38375145196914673
  - 0.3875507116317749
  - 0.3876970112323761
  - 0.38170477747917175
  - 0.5114118456840515
  - 0.3926681578159332
  - 0.38296452164649963
  - 0.39236992597579956
  - 0.38286057114601135
  - 0.394284725189209
  - 0.41048723459243774
  - 0.38857463002204895
  - 0.4183065593242645
  - 0.38310369849205017
  - 0.40267276763916016
  - 0.38833603262901306
  - 0.3883714973926544
  - 0.4233250021934509
  - 0.38268744945526123
  - 39.34115982055664
  - 0.3847474753856659
  - 0.392718642950058
  - 0.3824955224990845
  - 260.43304443359375
  - 3717278.5
  - 0.38515612483024597
  - 0.3842592239379883
  - 0.38452091813087463
  - 0.39179882407188416
  - 0.42535287141799927
  - 0.3870435357093811
  - 71610.0703125
  - 0.384791761636734
  - 0.4307725429534912
  - 0.3987240791320801
  - 0.3925633132457733
loss_records_fold3:
  train_losses:
  - 5.957205256819726
  - 6.098249673843384
  - 5.914621824026108
  - 5.879477864503861
  - 5.901664271950722
  - 5.865816000103951
  - 5.930098977684975
  - 5.9399366021156315
  - 5.941600994765759
  - 5.844824263453484
  - 5.912835657596588
  - 6.424556949734688
  - 6.1103809624910355
  - 6.138731694221497
  - 6.026321044564248
  - 5.899324664473534
  - 5.899439123272896
  - 5.9150633931159975
  - 5.974015787243843
  - 6.010498175024987
  - 5.982901406288147
  - 5.906003385782242
  - 6.025878819823266
  - 5.985023534297944
  - 6.2870696723461155
  - 5.928473564982415
  - 5.845978182554245
  - 6.007780626416206
  - 5.919018492102623
  - 6.037914663553238
  - 6.418817713856697
  - 5.971415582299233
  - 5.861249819397926
  - 5.892082476615906
  - 5.949685874581338
  - 5.915313896536827
  - 5.795924082398415
  - 5.890817034244538
  - 5.967550691962242
  - 6.364096286892892
  - 5.886154493689538
  - 5.897635737061501
  - 6.07876465022564
  - 5.819490051269532
  - 5.920331367850304
  - 5.821583265066147
  - 5.887549877166748
  - 5.981110191345215
  - 6.013216751813889
  - 5.828929075598717
  - 5.950490936636925
  - 5.890723159909249
  - 6.0263454794883735
  - 5.953198182582856
  - 5.877267816662789
  - 6.067746436595917
  - 5.834021152555943
  - 5.910803535580635
  - 5.943343567848206
  - 5.87547976076603
  - 6.027858686447144
  - 6.080137526988984
  - 5.997017905116081
  - 5.887936982512475
  - 5.909122753143311
  - 5.937698206305504
  - 5.909065473079682
  - 6.44531776458025
  - 5.915096178650856
  - 6.0303397834301
  - 5.935775354504585
  - 5.871147266030312
  - 5.864518085122109
  - 5.927679625153542
  - 5.9031989723443985
  - 5.900108453631401
  - 5.861022341251374
  - 6.029989336431027
  - 5.984886319935322
  - 5.905676256120206
  - 5.924563252925873
  - 5.8687128901481636
  - 6.037813726067544
  - 5.896940594911576
  - 5.841459906101227
  - 5.96458716839552
  - 5.783345776796342
  - 5.915126910805703
  - 5.904606312513351
  - 6.057693825662136
  - 6.001590368151665
  - 6.25520595908165
  - 6.140818393230439
  - 5.959770667552949
  - 5.8671199768781666
  - 5.90570776462555
  - 5.880439311265945
  - 6.405642485618592
  - 5.9622525662183765
  - 6.1951570332050325
  validation_losses:
  - 0.4752557575702667
  - 0.39885199069976807
  - 22238168.0
  - 990478848.0
  - 0.400471955537796
  - 0.407699853181839
  - 0.4051875174045563
  - 0.3989115059375763
  - 0.4034542143344879
  - 0.45220696926116943
  - 0.3990285396575928
  - 0.4417807459831238
  - 0.4043634235858917
  - 0.45945969223976135
  - 0.40220770239830017
  - 0.4037844240665436
  - 0.41415148973464966
  - 0.4021410346031189
  - 0.4215408265590668
  - 0.40219545364379883
  - 0.40035805106163025
  - 0.45139142870903015
  - 0.40490713715553284
  - 0.3991820514202118
  - 0.4470747709274292
  - 0.40987470746040344
  - 0.45137226581573486
  - 0.39952075481414795
  - 0.39829790592193604
  - 0.3991844356060028
  - 0.4145694375038147
  - 0.3975922763347626
  - 0.40036740899086
  - 0.3990958034992218
  - 0.42521023750305176
  - 0.4035032093524933
  - 0.41497641801834106
  - 0.3989529013633728
  - 0.42371073365211487
  - 0.40266960859298706
  - 0.39835256338119507
  - 0.4154165983200073
  - 0.4508335292339325
  - 0.39861223101615906
  - 0.3978556990623474
  - 0.3990040123462677
  - 0.4006477892398834
  - 0.4408486783504486
  - 0.4005051255226135
  - 0.4282976984977722
  - 0.3985229432582855
  - 0.4007492959499359
  - 0.42902663350105286
  - 0.40053990483283997
  - 0.3999439775943756
  - 0.3982327878475189
  - 0.433977872133255
  - 0.4107540249824524
  - 0.424062579870224
  - 0.39833858609199524
  - 0.39892250299453735
  - 0.47112441062927246
  - 0.40950629115104675
  - 0.3989197611808777
  - 0.40585216879844666
  - 0.3989461660385132
  - 0.39886710047721863
  - 0.41653749346733093
  - 0.41260242462158203
  - 0.39937105774879456
  - 0.39862966537475586
  - 0.4261671006679535
  - 0.39973005652427673
  - 0.4178963005542755
  - 0.44931140542030334
  - 0.40846315026283264
  - 0.3991847634315491
  - 0.47280454635620117
  - 0.4015367329120636
  - 0.3976200222969055
  - 0.4112527072429657
  - 0.39895567297935486
  - 0.4294458031654358
  - 0.4177437722682953
  - 0.4071369767189026
  - 0.4024937152862549
  - 0.44148552417755127
  - 0.401267409324646
  - 0.4133675694465637
  - 0.4042820632457733
  - 0.4047412574291229
  - 0.4482656419277191
  - 0.41218629479408264
  - 0.4085800349712372
  - 0.39930227398872375
  - 0.39836081862449646
  - 0.4531856179237366
  - 0.4007607102394104
  - 0.39805248379707336
  - 0.45243510603904724
loss_records_fold4:
  train_losses:
  - 6.000284552574158
  - 5.918211936950684
  - 5.951595821976662
  - 5.980226129293442
  - 5.959024298191071
  - 5.967413704097272
  - 5.940128095448017
  - 5.9415120631456375
  - 5.98689613044262
  - 6.148993045091629
  - 5.9752218157053
  validation_losses:
  - 0.4290403723716736
  - 0.4109620749950409
  - 0.3918541967868805
  - 0.39236176013946533
  - 0.40322181582450867
  - 0.407209575176239
  - 0.3969654142856598
  - 0.39667588472366333
  - 0.3913564383983612
  - 0.3951060473918915
  - 0.3930519223213196
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:26:26.582350'
