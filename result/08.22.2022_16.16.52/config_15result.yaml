config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.211269'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_15fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_15fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_15fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_15fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_15fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_15fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_15fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_15fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_15fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_15fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 23.332841205596925
  - 8.789819073677064
  - 4.380793428421021
  - 3.6477923631668094
  - 2.442686223983765
  - 2.2466273188591006
  - 1.7796595335006715
  - 1.5767446279525759
  - 1.7180860042572021
  - 1.082629778981209
  - 1.3058164536952974
  - 1.0765805006027223
  - 1.0543275237083436
  - 2.3277144193649293
  - 3.216225051879883
  - 1.075713437795639
  - 2.0456047475337984
  - 1.359305500984192
  - 1.3313783168792725
  - 1.094341379404068
  - 1.1155422389507295
  - 1.2081892490386963
  - 1.0042389988899232
  - 1.0950487554073334
  - 3.618406355381012
  - 2.4214987456798553
  - 7.873665428161622
  - 3.8867236733436585
  - 1.8513654351234436
  - 1.5748413681983948
  - 1.176036223769188
  - 1.7640626430511475
  - 2.512430185079575
  - 1.410887521505356
  - 1.2264686524868011
  - 1.2059651732444765
  - 2.18384028673172
  - 1.9057496786117554
  - 1.649242925643921
  - 10.759394758939743
  - 1.2358968138694764
  - 1.5424890637397768
  - 3.297388380765915
  - 9.29139120578766
  - 1.4713190376758576
  - 1.8104835510253907
  - 0.9096035540103913
  - 0.839709034562111
  - 0.9940347790718079
  - 1.0420558869838714
  - 1.1105376183986664
  - 1.1864284455776215
  - 0.8119066387414933
  - 0.8875260591506958
  - 1.0171500504016877
  - 0.8706649780273438
  - 0.8360880017280579
  - 0.8640552997589112
  - 1.0978338599205018
  - 0.9193507850170136
  - 1.0822065770626068
  - 0.9002506256103516
  - 1.1669361531734468
  - 0.9886952936649323
  - 0.9572972476482392
  - 0.9077546358108521
  - 0.9186915934085846
  - 0.8923827350139618
  - 1.3958105444908142
  - 0.8923390269279481
  - 0.9835695683956147
  - 1.3668328553438187
  - 1.2397136986255646
  - 1.560824078321457
  - 0.9642781376838685
  - 0.9249741971492768
  - 0.9084914267063141
  - 0.9504828572273255
  - 0.8880925416946411
  - 1.0272879242897035
  validation_losses:
  - 4.533519268035889
  - 2.459324836730957
  - 1.4925957918167114
  - 1.4538078308105469
  - 0.5694583654403687
  - 1.1002001762390137
  - 0.5176388621330261
  - 2.0095672607421875
  - 0.42831891775131226
  - 0.5263017416000366
  - 0.5961742401123047
  - 0.45525434613227844
  - 0.43379876017570496
  - 0.4978479743003845
  - 1.2571873664855957
  - 0.5154686570167542
  - 0.5201091766357422
  - 0.45075687766075134
  - 0.4079534411430359
  - 0.455905556678772
  - 0.41751348972320557
  - 0.3921014368534088
  - 0.4075433313846588
  - 0.565298318862915
  - 0.4366297125816345
  - 0.505247175693512
  - 0.44262072443962097
  - 0.5357367992401123
  - 0.9147329926490784
  - 0.869438886642456
  - 0.4861964285373688
  - 0.6527002453804016
  - 0.5200944542884827
  - 0.47691527009010315
  - 0.6008303165435791
  - 0.5034485459327698
  - 0.5013426542282104
  - 0.9127260446548462
  - 0.42716073989868164
  - 0.4866164028644562
  - 0.41719985008239746
  - 0.4565059244632721
  - 0.41108423471450806
  - 0.3875732719898224
  - 0.4411901831626892
  - 0.40735140442848206
  - 0.3971567153930664
  - 0.4088786244392395
  - 0.8021581172943115
  - 0.6467832326889038
  - 0.4366280138492584
  - 0.397763192653656
  - 0.4030979871749878
  - 0.42282360792160034
  - 0.39161744713783264
  - 0.39466238021850586
  - 0.3917505443096161
  - 0.40811946988105774
  - 0.3922743797302246
  - 0.40390607714653015
  - 0.40254488587379456
  - 0.4637450873851776
  - 0.47573745250701904
  - 0.5342245101928711
  - 0.4050638973712921
  - 0.41240179538726807
  - 0.47149890661239624
  - 0.47932755947113037
  - 0.4205330014228821
  - 0.4380919933319092
  - 0.4776158630847931
  - 0.8990805149078369
  - 0.6727744936943054
  - 0.7162927985191345
  - 0.42673176527023315
  - 0.4237956702709198
  - 0.42214542627334595
  - 0.40729188919067383
  - 0.413086473941803
  - 0.40540626645088196
loss_records_fold1:
  train_losses:
  - 0.8930945158004762
  - 0.9092696249485016
  - 0.8229698359966279
  - 0.7939855813980103
  - 0.8661730825901032
  - 0.8613863706588746
  - 0.8778153777122498
  - 2.078743410110474
  - 1.0429365873336793
  - 0.821859011054039
  - 1.0291586816310883
  - 0.982231217622757
  - 0.9280010998249054
  - 0.888890039920807
  - 0.9792019397020341
  - 0.8361766755580903
  - 0.8351809799671174
  validation_losses:
  - 0.4249257743358612
  - 0.43513816595077515
  - 0.429274320602417
  - 0.4024505317211151
  - 0.40474799275398254
  - 0.4272347092628479
  - 0.4013058543205261
  - 0.4149840772151947
  - 0.4156048893928528
  - 0.436219722032547
  - 0.4544789791107178
  - 0.4257148802280426
  - 0.4305035173892975
  - 0.4128420054912567
  - 0.40171200037002563
  - 0.40591961145401
  - 0.4114897549152374
loss_records_fold2:
  train_losses:
  - 0.8215453386306764
  - 0.8537705540657043
  - 0.8737975597381592
  - 0.8974934697151185
  - 0.8739623367786408
  - 0.8452692329883575
  - 0.8244236022233964
  - 0.8656755328178406
  - 0.928009957075119
  - 0.8391813576221466
  - 0.8222209334373475
  validation_losses:
  - 0.3860791027545929
  - 0.3945751190185547
  - 0.3860660195350647
  - 0.432861864566803
  - 0.37906527519226074
  - 0.3792409896850586
  - 0.38705021142959595
  - 0.3951645493507385
  - 0.396665096282959
  - 0.40620991587638855
  - 0.3874826729297638
loss_records_fold3:
  train_losses:
  - 0.880774086713791
  - 0.8305444121360779
  - 0.8402585685253143
  - 0.8096912503242493
  - 0.7943141341209412
  - 0.9173026502132416
  - 1.2346000850200654
  - 0.9843937426805497
  - 1.0530528366565706
  - 5.24073669910431
  - 0.9905773341655731
  - 1.2741325497627258
  - 1.3735710620880128
  - 1.1046138048171998
  - 0.8846261024475098
  - 0.904066425561905
  - 0.8219475507736207
  - 0.9427729815244675
  - 0.8571246087551118
  - 0.8173819929361343
  - 0.8184143722057343
  - 0.9308178484439851
  - 0.8655514717102051
  - 1.3474072813987732
  - 0.9703369200229646
  - 1.0716222107410431
  - 0.8256891548633576
  - 0.9155124008655549
  - 0.94887233376503
  - 0.9226265490055084
  - 0.9495699763298036
  - 0.8959123373031617
  - 0.8965083718299867
  - 1.0010862410068513
  - 0.8583542585372925
  validation_losses:
  - 0.407410204410553
  - 0.3953098654747009
  - 0.3957069516181946
  - 0.40032368898391724
  - 0.3992743194103241
  - 0.39202550053596497
  - 0.4299478232860565
  - 0.44910162687301636
  - 0.4553035795688629
  - 0.4310764968395233
  - 0.4636175036430359
  - 0.4038892686367035
  - 0.3909686207771301
  - 0.39159509539604187
  - 0.4408745765686035
  - 0.4017370641231537
  - 0.4117538630962372
  - 0.40828049182891846
  - 0.4421274960041046
  - 0.39886772632598877
  - 0.41357263922691345
  - 0.41496551036834717
  - 0.41721367835998535
  - 0.42736849188804626
  - 0.4103650152683258
  - 0.40528932213783264
  - 0.38541656732559204
  - 0.4088992774486542
  - 0.41912388801574707
  - 0.4133495092391968
  - 0.39749962091445923
  - 0.39311671257019043
  - 0.3949558436870575
  - 0.3934175968170166
  - 0.39109766483306885
loss_records_fold4:
  train_losses:
  - 0.8474445402622224
  - 0.9266925632953644
  - 0.9501361787319184
  - 0.8352888524532318
  - 0.8464992463588715
  - 0.8704506814479829
  - 0.8400134682655335
  - 0.8684416651725769
  - 0.8311159729957581
  - 1.4478638172149658
  - 0.8900230526924133
  - 0.9785536170005799
  - 0.8917578279972077
  - 0.8360876917839051
  - 0.801824027299881
  - 0.8075857967138291
  - 0.8607677340507508
  - 0.8345264732837677
  - 0.9805645406246186
  validation_losses:
  - 0.4518892467021942
  - 0.5045008659362793
  - 0.4609624147415161
  - 0.41634732484817505
  - 0.401065468788147
  - 0.43210548162460327
  - 0.4068042039871216
  - 0.40250062942504883
  - 0.4392656683921814
  - 0.4279431998729706
  - 0.42089855670928955
  - 0.3931616246700287
  - 0.46418389678001404
  - 0.42629873752593994
  - 0.4141797125339508
  - 0.41711556911468506
  - 0.4173234701156616
  - 0.3996885120868683
  - 0.4081432819366455
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 80 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 35 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
training_metrics:
  fold_eval_accs: '[0.8542024013722127, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.0
  total_train_time: '0:12:54.983295'
