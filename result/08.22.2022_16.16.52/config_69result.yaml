config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:48:44.799933'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_69fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_69fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_69fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_69fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_69fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_69fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_69fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_69fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_69fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_69fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.4136940360069277
  - 3.272150141000748
  - 3.1902091383934024
  - 3.3847656548023224
  - 3.2656421542167666
  - 3.0951479494571688
  - 3.2286891996860505
  - 3.052975976467133
  - 3.230708456039429
  - 3.13620622754097
  - 3.204409420490265
  - 3.1351920604705814
  - 3.2666969001293182
  - 3.0677560925483705
  - 3.1800593793392182
  - 3.6234653830528263
  - 3.0982254564762117
  - 3.085956346988678
  - 3.073397654294968
  - 3.018506383895874
  - 3.1602804958820343
  validation_losses:
  - 0.4579288959503174
  - 0.4178738594055176
  - 0.395768404006958
  - 0.42126014828681946
  - 0.4471825063228607
  - 0.4528074860572815
  - 0.4011256992816925
  - 0.3948196470737457
  - 0.39256465435028076
  - 0.44174572825431824
  - 0.39620134234428406
  - 0.4117295444011688
  - 0.38785457611083984
  - 0.3886188566684723
  - 0.4489678144454956
  - 0.3962819576263428
  - 0.4001382291316986
  - 0.3958091139793396
  - 0.39202719926834106
  - 0.39155319333076477
  - 0.39713871479034424
loss_records_fold1:
  train_losses:
  - 3.1567056834697724
  - 3.1343933761119844
  - 2.9947592586278917
  - 3.0292101144790653
  - 2.976297080516815
  - 3.048819947242737
  - 3.074376791715622
  - 2.9876944065093998
  - 3.027975690364838
  - 3.048830211162567
  - 3.017438066005707
  - 3.002117165923119
  - 3.0864836096763613
  - 3.046625643968582
  - 2.985366088151932
  - 2.971995782852173
  - 3.065648448467255
  - 2.989263039827347
  validation_losses:
  - 0.39984291791915894
  - 0.39801645278930664
  - 0.4016360342502594
  - 0.41051971912384033
  - 0.38356661796569824
  - 0.41920918226242065
  - 0.39686062932014465
  - 0.4074627459049225
  - 0.4073099195957184
  - 0.3984781801700592
  - 0.39299869537353516
  - 0.4152339696884155
  - 0.41350695490837097
  - 0.39531004428863525
  - 0.40098172426223755
  - 0.39183005690574646
  - 0.39331698417663574
  - 0.39946630597114563
loss_records_fold2:
  train_losses:
  - 3.0380671679973603
  - 2.986087936162949
  - 3.0342226266860965
  - 3.018507480621338
  - 2.9721018075942993
  - 2.979149702191353
  - 2.994400769472122
  - 3.0595363020896915
  - 2.984003460407257
  - 2.9894924461841583
  - 3.03243488073349
  - 2.985381633043289
  - 3.0127121567726136
  - 3.0360098242759705
  - 2.9872327685356144
  - 3.051074200868607
  - 3.0082559108734133
  - 3.027581769227982
  - 3.0634116113185885
  - 2.985054868459702
  - 3.0684054851531983
  - 2.993007236719132
  - 3.0071705758571627
  - 2.9534976065158847
  - 2.976790946722031
  - 2.9650990784168245
  - 2.9468713343143467
  - 3.116874808073044
  validation_losses:
  - 0.3787720501422882
  - 0.38048499822616577
  - 0.405412882566452
  - 0.40331652760505676
  - 0.39090532064437866
  - 0.7443873286247253
  - 0.39438945055007935
  - 0.38874879479408264
  - 0.40279626846313477
  - 0.39563801884651184
  - 0.39227715134620667
  - 0.392840176820755
  - 0.39623910188674927
  - 0.3862675428390503
  - 0.39912551641464233
  - 0.3841392397880554
  - 0.38538214564323425
  - 0.6228580474853516
  - 0.39415669441223145
  - 0.3956897258758545
  - 0.39172589778900146
  - 0.40788885951042175
  - 0.40466225147247314
  - 0.39015963673591614
  - 0.3914566934108734
  - 0.3880544602870941
  - 0.39590951800346375
  - 0.39345332980155945
loss_records_fold3:
  train_losses:
  - 3.0832144737243654
  - 3.04423702955246
  - 3.054631191492081
  - 3.085645073652268
  - 2.9482107698917392
  - 3.1190729796886445
  - 2.985977366566658
  - 2.9891768574714663
  - 3.023707389831543
  - 2.9557460069656374
  - 3.092079073190689
  - 3.083985537290573
  - 3.008991622924805
  - 3.046194016933441
  - 2.977338171005249
  - 3.0680195629596714
  - 2.972589129209519
  - 3.0064266324043274
  - 3.0165076553821564
  - 3.0871951937675477
  - 2.9932866752147675
  - 3.0899028062820437
  - 2.9709679037332535
  - 2.992215779423714
  - 3.0493791788816456
  - 2.9887286186218263
  - 2.9458495885133744
  - 3.0071958482265475
  - 2.982557398080826
  - 3.0327711373567583
  - 3.0187898874282837
  - 2.975787937641144
  - 2.9783268123865128
  - 3.0408849716186523
  - 3.046257841587067
  - 3.055872243642807
  - 2.9791152834892274
  - 2.9629464745521545
  - 2.986098635196686
  - 2.9407190144062043
  - 2.9310306787490847
  - 2.9371297746896747
  - 2.9824062615633014
  - 3.019366842508316
  - 3.044847989082337
  - 2.9197701811790466
  - 2.958069455623627
  - 3.005684340000153
  - 3.0124483704566956
  - 3.02004371881485
  - 3.045746174454689
  - 2.9726979196071626
  - 3.103501904010773
  - 2.969447457790375
  - 2.9849487841129303
  - 2.987348473072052
  - 3.0173638403415683
  - 2.9913639545440676
  - 3.00261310338974
  - 3.0219102919101717
  - 3.0094538509845736
  - 3.028340268135071
  - 2.9878282845020294
  - 2.989579433202744
  - 3.0808481335639955
  - 2.907760113477707
  - 3.022180712223053
  - 2.9757342398166657
  - 2.9894126594066623
  - 2.924148067831993
  - 3.0110288202762607
  - 2.9392735540866854
  - 2.9920215368270875
  - 2.9954269975423813
  - 3.017075496912003
  - 2.9306048989295963
  - 2.9471707284450535
  - 2.9590111613273624
  - 2.9425986230373384
  - 2.9422699868679048
  - 2.988016211986542
  - 3.0107337832450867
  - 2.9982815802097322
  - 2.957393503189087
  - 2.9267390072345734
  - 2.92988018989563
  - 2.9944832742214205
  - 3.04982505440712
  - 2.9188849329948425
  - 2.956269043684006
  - 2.9186059117317202
  - 3.0276550948619843
  - 3.022036361694336
  - 2.9243861913681033
  - 3.0712225914001468
  - 2.9743253350257874
  - 2.9364693760871887
  - 2.8656033724546432
  - 3.038800796866417
  - 2.9090019434690477
  validation_losses:
  - 0.4869304299354553
  - 0.5779857635498047
  - 1.5893677473068237
  - 0.7380419969558716
  - 0.9895943403244019
  - 0.3851511478424072
  - 0.4173209071159363
  - 0.5050865411758423
  - 1.4223278760910034
  - 0.3822319507598877
  - 0.37440499663352966
  - 0.43336841464042664
  - 0.3724609613418579
  - 0.6187446713447571
  - 0.5149685740470886
  - 0.465266615152359
  - 0.3854425251483917
  - 1.0615601539611816
  - 0.3958384692668915
  - 0.381188303232193
  - 0.38466936349868774
  - 0.4311576187610626
  - 0.43592268228530884
  - 1.7107198238372803
  - 0.5663743615150452
  - 0.6714661717414856
  - 1.2426068782806396
  - 0.6946858763694763
  - 0.8236221075057983
  - 1.3676679134368896
  - 0.7299240827560425
  - 1.1783936023712158
  - 0.3842669725418091
  - 0.40745866298675537
  - 0.38948121666908264
  - 0.3803350329399109
  - 0.3855770528316498
  - 0.37745362520217896
  - 0.3813687860965729
  - 0.7357804179191589
  - 0.40891730785369873
  - 0.3988039195537567
  - 0.705428421497345
  - 0.510194718837738
  - 1.8926640748977661
  - 0.3926585018634796
  - 1.383032202720642
  - 3.1913342475891113
  - 1.1466498374938965
  - 1.200110912322998
  - 1.1184509992599487
  - 1.3366583585739136
  - 0.39278334379196167
  - 0.4032403528690338
  - 0.4080403447151184
  - 0.4139818847179413
  - 0.3918941617012024
  - 0.433390349149704
  - 0.41269442439079285
  - 0.46292874217033386
  - 0.49043843150138855
  - 1.8583670854568481
  - 0.4002706706523895
  - 0.3994430899620056
  - 0.4402574300765991
  - 0.5863745808601379
  - 0.6139925718307495
  - 1.831312894821167
  - 0.5363218784332275
  - 4.366817474365234
  - 0.9137002825737
  - 0.37284159660339355
  - 0.4837813675403595
  - 0.5300973057746887
  - 0.5674470067024231
  - 1.1301212310791016
  - 0.5519042015075684
  - 1.2001583576202393
  - 1.941322922706604
  - 1.2757949829101562
  - 1.680166244506836
  - 0.9687082767486572
  - 0.40346458554267883
  - 0.7615463137626648
  - 0.7445182204246521
  - 1.5322026014328003
  - 0.38743025064468384
  - 0.6937400698661804
  - 2.161113739013672
  - 0.4494909644126892
  - 0.6447755694389343
  - 0.8622321486473083
  - 0.38038375973701477
  - 0.39375460147857666
  - 0.40276196599006653
  - 0.41954532265663147
  - 0.8165860176086426
  - 0.40758827328681946
  - 0.4275791347026825
  - 0.5532200336456299
loss_records_fold4:
  train_losses:
  - 2.899869328737259
  - 2.9581963598728183
  - 2.9381470799446108
  - 2.997734093666077
  - 3.005975991487503
  - 2.915454602241516
  - 2.9386123895645144
  - 2.901007959246636
  - 3.1062695473432544
  - 3.007248687744141
  - 3.0791148602962495
  - 3.05194531083107
  - 3.043665158748627
  - 2.9939413130283357
  - 3.0300733357667924
  - 2.997614258527756
  validation_losses:
  - 0.4416758716106415
  - 0.393080472946167
  - 0.39382708072662354
  - 0.3719361424446106
  - 0.4142480194568634
  - 0.3917483985424042
  - 0.7222983241081238
  - 0.39802682399749756
  - 0.38855117559432983
  - 0.40107008814811707
  - 0.3862863779067993
  - 0.3760371804237366
  - 0.3847058117389679
  - 0.394212931394577
  - 0.3810640871524811
  - 0.38002046942710876
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8456260720411664,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0816326530612245, 0.0]'
  mean_eval_accuracy: 0.8558693332861781
  mean_f1_accuracy: 0.0163265306122449
  total_train_time: '0:16:56.721145'
