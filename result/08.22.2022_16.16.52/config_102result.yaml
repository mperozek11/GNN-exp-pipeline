config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:32:09.906437'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_102fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_102fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_102fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_102fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_102fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_102fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_102fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_102fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_102fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_102fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.9667870521545412
  - 1.766022390127182
  - 1.6264582693576815
  - 1.5764279067516327
  - 1.6602270245552064
  - 1.6937365710735321
  - 1.6328877389431
  - 1.5256594985723497
  - 1.5429014027118684
  - 1.55718195438385
  - 1.5532424330711365
  - 1.5591054677963259
  - 1.5551046907901764
  - 1.6644015789031983
  - 1.6469173967838289
  - 1.5772608757019044
  - 1.5746982157230378
  validation_losses:
  - 0.6802114844322205
  - 0.39806658029556274
  - 0.40040507912635803
  - 0.39640286564826965
  - 0.40198665857315063
  - 0.42214128375053406
  - 0.40394580364227295
  - 0.39111196994781494
  - 0.4082338809967041
  - 0.4071241319179535
  - 0.4356842041015625
  - 0.42126643657684326
  - 0.39613696932792664
  - 0.3985407054424286
  - 0.3860854506492615
  - 0.3891436755657196
  - 0.3931363523006439
loss_records_fold1:
  train_losses:
  - 1.506587064266205
  - 1.5276343524456024
  - 1.595030689239502
  - 1.5311639845371248
  - 1.5548885941505433
  - 1.5607130885124207
  - 1.598389595746994
  - 1.562656891345978
  - 1.5528348922729494
  - 1.5794533610343935
  - 1.5501899600028992
  - 1.5720084667205811
  - 1.5876249849796296
  - 1.5571579754352571
  - 1.5248156547546388
  - 1.548782402276993
  - 1.6456569969654085
  - 1.7480702459812165
  - 1.6896414160728455
  validation_losses:
  - 0.394924134016037
  - 0.41439130902290344
  - 0.4046875834465027
  - 0.3877965211868286
  - 0.39377155900001526
  - 0.3876212239265442
  - 0.3956752419471741
  - 0.4078451693058014
  - 0.4161422550678253
  - 0.39833229780197144
  - 0.4192153811454773
  - 0.39299091696739197
  - 0.40772053599357605
  - 0.38922008872032166
  - 0.3980239927768707
  - 0.4006541967391968
  - 0.39499759674072266
  - 0.39734548330307007
  - 0.37799325585365295
loss_records_fold2:
  train_losses:
  - 1.5214708089828493
  - 1.7842769384384156
  - 1.577833652496338
  - 1.4953881412744523
  - 1.5111148715019227
  - 1.5281913578510284
  - 1.5157080292701721
  - 1.5626768887043
  - 1.565513449907303
  - 1.5120949149131775
  - 1.4933063566684723
  - 1.524838811159134
  - 1.5407989919185638
  - 1.5634551882743837
  - 1.5885053694248201
  - 1.5669033944606783
  - 1.5357941091060638
  - 1.5058871865272523
  - 1.5271447718143465
  - 1.5512986540794373
  - 1.4906581699848176
  - 1.5223113834857942
  - 1.5249605774879456
  - 1.5498972475528718
  - 1.5662913441658022
  - 1.5132615685462953
  - 1.570805823802948
  - 1.519588053226471
  - 1.613197386264801
  - 1.5228398740291595
  - 1.5057505995035172
  - 1.5427356183528902
  - 1.5122411847114563
  - 1.5032453536987305
  - 1.5747831404209138
  - 1.4984006464481354
  - 1.5176256716251375
  - 1.551534575223923
  - 1.4931709110736848
  - 1.59673370718956
  - 1.5164785087108612
  - 1.4810230255126955
  - 1.4929675042629242
  validation_losses:
  - 0.3957894444465637
  - 0.3883568346500397
  - 0.40487241744995117
  - 0.390453577041626
  - 0.3913081884384155
  - 0.39642396569252014
  - 0.40841197967529297
  - 0.39291149377822876
  - 0.4297524690628052
  - 0.39079010486602783
  - 0.4530490040779114
  - 0.4079659581184387
  - 0.6778976321220398
  - 0.7833915948867798
  - 0.45590630173683167
  - 0.3946014940738678
  - 0.4121755361557007
  - 0.39858511090278625
  - 0.42723792791366577
  - 0.3958788514137268
  - 0.39845025539398193
  - 0.4168216586112976
  - 0.39749711751937866
  - 0.4492466151714325
  - 0.43024662137031555
  - 0.40255478024482727
  - 0.38911643624305725
  - 0.46529534459114075
  - 0.5820327401161194
  - 0.518886923789978
  - 0.3847247064113617
  - 0.5161867737770081
  - 0.43334853649139404
  - 0.7308457493782043
  - 1.1774293184280396
  - 0.6176170706748962
  - 0.6842340230941772
  - 0.39212900400161743
  - 0.3898452818393707
  - 0.386269211769104
  - 0.3836364150047302
  - 0.388535737991333
  - 0.3945819139480591
loss_records_fold3:
  train_losses:
  - 1.5357418060302734
  - 1.5769585311412813
  - 1.5508132934570313
  - 1.5157057344913483
  - 1.4922665268182755
  - 1.5090255200862885
  - 1.5489977836608888
  - 1.5184645533561707
  - 1.462994033098221
  - 1.5695150315761568
  - 1.4984377324581146
  - 1.546678113937378
  - 1.5473579883575441
  - 1.6123745679855348
  - 1.527095901966095
  - 1.5172980397939684
  - 1.5533731579780579
  - 1.485265988111496
  - 1.488747411966324
  - 1.5054223835468292
  - 1.5370918929576876
  - 1.5026227831840515
  - 1.4792313396930696
  - 1.4953460097312927
  - 1.5141923367977144
  - 1.4852035343647003
  - 1.4645888328552248
  - 1.5255150258541108
  - 1.5050932407379152
  - 1.485762494802475
  - 1.5588211238384249
  - 1.4747317969799043
  - 1.468550604581833
  - 1.6962206304073335
  - 1.4989764869213105
  - 1.5722938895225527
  - 1.6370756566524507
  - 1.5089497923851014
  validation_losses:
  - 0.3898574113845825
  - 0.38366925716400146
  - 0.3936816155910492
  - 0.5206944942474365
  - 0.5754075646400452
  - 0.5503289699554443
  - 1.2057740688323975
  - 0.8767848014831543
  - 1.1298573017120361
  - 0.5284811854362488
  - 1.2088459730148315
  - 0.7563786506652832
  - 0.7321932315826416
  - 0.37862175703048706
  - 0.6244953870773315
  - 0.7042970657348633
  - 1.211645483970642
  - 0.48874807357788086
  - 1.2911850214004517
  - 1.2500481605529785
  - 0.5852541327476501
  - 0.8446924686431885
  - 1.405228614807129
  - 1.3891545534133911
  - 0.539519190788269
  - 0.7293025255203247
  - 1.5666509866714478
  - 0.39408448338508606
  - 0.3893599212169647
  - 0.4032914340496063
  - 0.43399661779403687
  - 0.5002146363258362
  - 0.39760270714759827
  - 0.38745182752609253
  - 0.3920147716999054
  - 0.3777208924293518
  - 0.38415998220443726
  - 0.37764284014701843
loss_records_fold4:
  train_losses:
  - 1.5128728389739992
  - 1.5352549254894257
  - 1.5533361792564393
  - 1.560469824075699
  - 1.528214341402054
  - 1.553687757253647
  - 1.462857261300087
  - 1.5326939225196838
  - 1.5369091928005219
  - 1.4868905246257782
  - 1.477714365720749
  - 1.545808917284012
  - 1.4996309816837312
  - 1.4615002632141114
  - 1.4879204511642456
  - 1.482257205247879
  - 1.5224170327186586
  - 1.4918227612972261
  - 1.4494760155677797
  - 1.5691547334194185
  - 1.5578509092330934
  - 1.4845555245876314
  - 1.5629566013813019
  - 1.5204778730869295
  - 1.4889843344688416
  - 1.5006815850734712
  - 1.4836582899093629
  - 1.5364682078361511
  - 1.4898551881313324
  - 1.4861229300498964
  - 1.4902285516262055
  - 1.5096516489982605
  - 1.4690482467412949
  - 1.5163408398628235
  - 1.5301513016223909
  - 1.5020670741796494
  - 1.4825980067253113
  - 1.482613730430603
  - 1.4691465914249422
  - 1.5662161469459535
  - 1.509743082523346
  - 1.5432582736015321
  - 1.4944729208946228
  - 1.493142718076706
  validation_losses:
  - 0.3814370632171631
  - 0.39348191022872925
  - 0.36868640780448914
  - 0.3810582458972931
  - 0.38434475660324097
  - 0.3708513379096985
  - 0.3708138167858124
  - 0.37290439009666443
  - 0.37506982684135437
  - 0.38031005859375
  - 0.39288952946662903
  - 0.4138289988040924
  - 0.381059467792511
  - 0.36342740058898926
  - 0.3700888156890869
  - 0.3937688171863556
  - 0.44874683022499084
  - 0.4137241840362549
  - 0.3888067305088043
  - 0.3766450583934784
  - 0.3964279592037201
  - 0.3874654471874237
  - 0.37583452463150024
  - 0.37780097126960754
  - 0.3928997218608856
  - 0.39289310574531555
  - 0.41726306080818176
  - 0.4306800961494446
  - 0.36922895908355713
  - 0.3706038296222687
  - 0.36637091636657715
  - 0.36814084649086
  - 0.4384096562862396
  - 0.48418372869491577
  - 0.4699224829673767
  - 0.4768829941749573
  - 0.5846431851387024
  - 0.7193685173988342
  - 0.3919535279273987
  - 0.3812309801578522
  - 0.3829893469810486
  - 0.36869895458221436
  - 0.37318840622901917
  - 0.37208986282348633
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 43 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 38 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 44 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:14:27.171838'
