config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 02:11:26.500442'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/108/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/108/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/108/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/108/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/108/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/108/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/108/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/108/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/108/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/108/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 7.930334836244583
  - 7.9605099856853485
  - 7.948843240737915
  - 8.127878338098526
  - 7.864186882972717
  - 7.828567624092102
  - 7.633463636040688
  - 8.029362916946411
  - 8.056479424238205
  - 8.318966060876846
  - 7.875387907028198
  - 7.454890713095665
  - 7.516125708818436
  - 7.824273526668549
  - 7.7325426042079926
  - 7.511824607849121
  - 7.723710596561432
  - 7.920879065990448
  - 7.839367717504501
  - 7.894234627485275
  - 7.441750645637512
  - 7.89954200387001
  - 7.776418179273605
  - 7.966931641101837
  - 7.654096007347107
  - 7.685575693845749
  - 7.566703945398331
  - 7.736249506473541
  - 7.556242525577545
  - 7.575206309556961
  - 7.694704353809357
  - 7.667172819375992
  - 7.425315260887146
  - 7.629093319177628
  - 7.927826792001724
  - 7.773508071899414
  - 8.414218753576279
  - 7.76155960559845
  - 7.700931966304779
  - 7.7369696497917175
  - 7.46748423576355
  - 8.180575400590897
  - 7.77295395731926
  - 7.706450313329697
  - 7.862338215112686
  - 7.491342931985855
  - 7.484187960624695
  - 7.430246323347092
  - 7.429012015461922
  - 7.5953081250190735
  - 7.488201320171356
  - 7.6009175181388855
  - 7.636426225304604
  - 7.436826318502426
  - 7.741640627384186
  - 7.3911485224962234
  - 7.5908403396606445
  - 7.7350189089775085
  - 7.501442342996597
  - 7.565447509288788
  - 7.438133358955383
  - 7.452283620834351
  - 7.322496652603149
  - 7.482120364904404
  - 7.371401429176331
  - 7.817310303449631
  - 7.435215055942535
  - 7.371948957443237
  - 7.908610224723816
  - 7.95462030172348
  - 7.3824261128902435
  - 7.518275678157806
  - 7.805076003074646
  - 7.447181314229965
  - 7.5251840353012085
  - 7.515220999717712
  - 7.446135997772217
  - 7.55592080950737
  - 7.7579681277275085
  - 7.575212270021439
  - 7.279804021120071
  - 7.505490034818649
  - 7.697247862815857
  - 7.073930710554123
  - 7.389038860797882
  - 7.485431641340256
  - 7.4385086596012115
  - 7.345273703336716
  - 7.406591206789017
  - 7.477465331554413
  - 7.538857340812683
  - 7.687579572200775
  - 7.554480075836182
  - 7.578088730573654
  - 7.385892361402512
  - 7.3602999448776245
  - 7.4240420162677765
  - 7.318893224000931
  - 7.392957419157028
  - 7.343643993139267
  validation_losses:
  - 0.3967292606830597
  - 0.3991661071777344
  - 0.41018179059028625
  - 0.4067046344280243
  - 0.40202513337135315
  - 0.41940563917160034
  - 0.39654433727264404
  - 0.40354859828948975
  - 0.40061813592910767
  - 0.39509230852127075
  - 0.3916279673576355
  - 0.40734416246414185
  - 0.42574289441108704
  - 0.43327775597572327
  - 0.4160764217376709
  - 0.4010830223560333
  - 0.4362785816192627
  - 0.4050798714160919
  - 0.4552835524082184
  - 0.48862266540527344
  - 0.4467315971851349
  - 0.5000731348991394
  - 0.4251151978969574
  - 0.3873099386692047
  - 0.3958885967731476
  - 0.3880040943622589
  - 0.40916913747787476
  - 0.44269758462905884
  - 0.43693986535072327
  - 0.40428096055984497
  - 0.5148942470550537
  - 0.4688027799129486
  - 0.48124760389328003
  - 0.45476067066192627
  - 0.5114843845367432
  - 0.4509139358997345
  - 0.40757328271865845
  - 0.4827553927898407
  - 0.40916162729263306
  - 0.4253910183906555
  - 0.4053833782672882
  - 0.4010556936264038
  - 0.4132824242115021
  - 0.3954044282436371
  - 0.40326276421546936
  - 0.3925124704837799
  - 0.39676520228385925
  - 0.3926754295825958
  - 0.48415860533714294
  - 0.4521293044090271
  - 0.4947200119495392
  - 0.5645027160644531
  - 0.4177859127521515
  - 0.4152320325374603
  - 0.4147002398967743
  - 0.397230327129364
  - 0.6098673939704895
  - 0.4536581039428711
  - 0.4303637742996216
  - 0.5614748001098633
  - 0.7647978663444519
  - 0.6982421278953552
  - 0.6182577610015869
  - 0.6474300026893616
  - 0.4582556188106537
  - 0.6003947854042053
  - 0.5646052956581116
  - 0.6264227628707886
  - 0.40442410111427307
  - 0.41491976380348206
  - 0.4466671645641327
  - 0.4668593406677246
  - 0.537826657295227
  - 0.39665487408638
  - 0.41222670674324036
  - 0.5736647844314575
  - 0.7652578949928284
  - 0.5620805025100708
  - 0.5349364280700684
  - 0.465048223733902
  - 0.5847744941711426
  - 0.43763819336891174
  - 0.5450929403305054
  - 0.7537778615951538
  - 0.4691828191280365
  - 0.7808536291122437
  - 0.6691139936447144
  - 0.8925023674964905
  - 0.5428369641304016
  - 0.4063000977039337
  - 0.7609903216362
  - 0.591447651386261
  - 0.5604446530342102
  - 0.5598506927490234
  - 0.4003254771232605
  - 0.5648467540740967
  - 0.39058277010917664
  - 0.6458429098129272
  - 0.6619929671287537
  - 0.6845613121986389
loss_records_fold2:
  train_losses:
  - 7.458873748779297
  - 7.6006284058094025
  - 7.653221249580383
  - 7.408353418111801
  - 7.477723062038422
  - 7.467222809791565
  - 7.460373401641846
  - 7.396892726421356
  - 7.521524682641029
  - 7.163302734494209
  - 7.31587427854538
  - 7.7015301287174225
  - 7.2561463713645935
  - 7.423564195632935
  - 7.404479414224625
  - 7.511474013328552
  - 7.148518651723862
  - 7.1978834718465805
  - 7.323584854602814
  - 7.258611008524895
  - 7.392709463834763
  - 7.241628468036652
  - 7.37647819519043
  - 7.130464687943459
  - 7.5589156448841095
  - 7.13706248998642
  - 7.349033236503601
  - 7.265833109617233
  - 7.358131945133209
  - 7.144454598426819
  - 7.099574908614159
  - 7.299554884433746
  - 7.525593340396881
  - 7.381895303726196
  - 7.445287346839905
  - 7.607098877429962
  - 7.484206706285477
  - 7.666226118803024
  - 7.365359246730804
  - 7.606492608785629
  - 7.278642117977142
  - 7.061118334531784
  - 7.213420569896698
  - 7.117725282907486
  - 7.407109439373016
  - 7.376522034406662
  - 7.490681320428848
  - 7.654214054346085
  - 7.271519169211388
  - 7.098430573940277
  - 7.105075836181641
  - 7.082326114177704
  - 7.236999541521072
  - 7.237896174192429
  - 7.216933995485306
  - 7.415442153811455
  - 7.552257001399994
  - 7.511179834604263
  - 7.171889245510101
  - 7.304943084716797
  - 7.540530353784561
  - 7.12833745777607
  - 7.11055400967598
  - 7.089397639036179
  - 7.036847472190857
  - 7.2174669206142426
  - 7.501597225666046
  - 7.150130987167358
  - 7.209155827760696
  - 7.262711137533188
  - 7.28691503405571
  - 7.285718649625778
  - 7.205433160066605
  - 7.049175351858139
  - 7.426191598176956
  - 7.2503268122673035
  - 7.396459460258484
  - 7.128440499305725
  - 7.216629177331924
  - 7.155046492815018
  - 7.227999985218048
  - 7.271818041801453
  - 7.193029373884201
  - 7.185728043317795
  - 7.284377813339233
  - 7.200061023235321
  - 7.417667418718338
  - 7.122304320335388
  - 7.269186913967133
  - 6.943255454301834
  - 7.251408636569977
  - 7.03741592168808
  - 7.077300995588303
  - 7.118558645248413
  - 7.079300582408905
  - 7.368831425905228
  - 7.153310269117355
  - 7.229166477918625
  - 7.282418310642242
  - 7.327897906303406
  validation_losses:
  - 1.2609750032424927
  - 0.5187627673149109
  - 0.4201643466949463
  - 0.4218888282775879
  - 0.5774434804916382
  - 0.5359494090080261
  - 0.5657185316085815
  - 0.40163475275039673
  - 0.43803635239601135
  - 0.6703980565071106
  - 0.9807246327400208
  - 0.4583412706851959
  - 0.3818061649799347
  - 0.6009919047355652
  - 0.5966495275497437
  - 0.40527451038360596
  - 0.48424261808395386
  - 0.5136589407920837
  - 0.49874231219291687
  - 0.792920708656311
  - 0.5229445099830627
  - 0.38456425070762634
  - 0.5928543210029602
  - 0.3976738452911377
  - 3.0234429836273193
  - 1.2340543270111084
  - 0.8926427364349365
  - 0.3902895152568817
  - 0.4461279511451721
  - 0.3997041881084442
  - 0.3924206495285034
  - 0.7934024333953857
  - 0.614296555519104
  - 0.46491751074790955
  - 0.3881596326828003
  - 0.3912252187728882
  - 0.40530115365982056
  - 0.7846640944480896
  - 0.8960592746734619
  - 0.48797839879989624
  - 0.48837903141975403
  - 0.37516579031944275
  - 0.7906816601753235
  - 1.5321964025497437
  - 0.39358389377593994
  - 0.38789278268814087
  - 0.43040138483047485
  - 0.3938446044921875
  - 0.3827807903289795
  - 0.46293795108795166
  - 0.5581537485122681
  - 0.6456968188285828
  - 0.6631144285202026
  - 0.5332641005516052
  - 0.41953516006469727
  - 0.4816620349884033
  - 0.4289036989212036
  - 1.164258599281311
  - 0.4408018887042999
  - 0.4327433407306671
  - 0.43633925914764404
  - 0.3937777876853943
  - 0.43063679337501526
  - 0.4673803448677063
  - 2.191448450088501
  - 0.8919393420219421
  - 2.2397360801696777
  - 0.9312379956245422
  - 0.40898045897483826
  - 0.5743191838264465
  - 0.6613772511482239
  - 0.40929245948791504
  - 0.5842738747596741
  - 0.6115860342979431
  - 0.6232132315635681
  - 0.6154276728630066
  - 0.8648051619529724
  - 0.8363509178161621
  - 0.5234666466712952
  - 0.5225242972373962
  - 0.5945337414741516
  - 0.4096369743347168
  - 0.4646157920360565
  - 0.4895873963832855
  - 0.562092661857605
  - 0.4995981454849243
  - 0.5217128992080688
  - 0.43227455019950867
  - 0.5238770246505737
  - 0.8065037131309509
  - 0.5553368926048279
  - 0.4298630654811859
  - 0.5656399726867676
  - 0.49049267172813416
  - 0.4570707380771637
  - 0.5392341017723083
  - 0.7089144587516785
  - 0.4919132888317108
  - 0.4909081757068634
  - 0.4967042803764343
loss_records_fold3:
  train_losses:
  - 7.4187351167202
  - 7.329615116119385
  - 7.346566379070282
  - 7.4808840453624725
  - 7.699689835309982
  - 7.352528095245361
  - 7.368505835533142
  - 7.384353041648865
  - 7.393298476934433
  - 7.523744493722916
  - 7.248303830623627
  - 7.0831649750471115
  - 7.140304148197174
  - 7.283422499895096
  - 7.292121887207031
  - 7.386404186487198
  - 7.306394577026367
  - 7.2941771149635315
  - 7.4127229154109955
  - 7.18959254026413
  - 7.511343836784363
  - 7.247377663850784
  - 7.153313785791397
  - 7.079969346523285
  - 7.1779389679431915
  - 7.31094029545784
  - 7.096746772527695
  - 7.070690616965294
  - 7.0980900675058365
  - 7.246001839637756
  - 7.395386755466461
  - 7.079730004072189
  - 7.249166667461395
  - 7.393088489770889
  - 7.437184005975723
  - 7.408917188644409
  - 7.079592287540436
  - 6.946274548768997
  - 7.638337075710297
  - 7.347280263900757
  - 7.164007365703583
  - 7.036108642816544
  - 7.179806083440781
  - 7.1857670545578
  - 7.1406210064888
  - 7.13757261633873
  - 7.192443326115608
  - 7.155318468809128
  - 7.318227916955948
  - 7.319657653570175
  - 7.256427228450775
  - 7.19958832859993
  - 7.1717990934848785
  - 7.2208695113658905
  - 7.319853603839874
  - 7.157877862453461
  - 7.236652314662933
  - 7.3088847398757935
  - 7.092600762844086
  - 7.26529061794281
  - 7.49589604139328
  - 7.282106548547745
  - 7.178302019834518
  - 7.058090955018997
  - 7.137299656867981
  - 7.083904653787613
  - 7.080401822924614
  - 7.2167737782001495
  - 7.288901478052139
  - 7.221001863479614
  - 7.20540526509285
  - 7.154387205839157
  - 7.144867241382599
  - 7.242578476667404
  - 7.169961661100388
  - 7.22330567240715
  - 7.147430896759033
  - 7.109801530838013
  - 7.275847554206848
  - 7.1667160987854
  - 7.170421123504639
  - 7.342905670404434
  - 7.254598885774612
  - 7.16167950630188
  - 7.079472541809082
  - 7.251836270093918
  - 7.351150810718536
  - 7.043350428342819
  - 7.143603473901749
  - 7.2858946621418
  - 7.513201326131821
  - 7.002220019698143
  - 7.347719132900238
  - 7.240750670433044
  - 7.179082304239273
  - 7.31071063876152
  - 7.036106705665588
  - 7.129752039909363
  - 7.2524369060993195
  - 6.9850956201553345
  validation_losses:
  - 0.6431193351745605
  - 1.08035409450531
  - 0.5265606045722961
  - 0.424936980009079
  - 0.687091588973999
  - 0.8958609700202942
  - 0.6905912160873413
  - 0.764071524143219
  - 0.47936171293258667
  - 0.561576783657074
  - 0.499053955078125
  - 0.48592516779899597
  - 0.43839165568351746
  - 0.48359328508377075
  - 0.39962005615234375
  - 0.5340321063995361
  - 0.6318636536598206
  - 0.5200769305229187
  - 0.49113327264785767
  - 0.6975483894348145
  - 0.634152889251709
  - 0.6915149092674255
  - 0.968481719493866
  - 1.2357412576675415
  - 1.015952467918396
  - 0.7750254273414612
  - 0.812249481678009
  - 0.7045181393623352
  - 0.8396008610725403
  - 0.4755646884441376
  - 0.6559423208236694
  - 0.63254314661026
  - 0.5643765330314636
  - 0.6648086309432983
  - 1.1650301218032837
  - 1.1013517379760742
  - 0.4412100315093994
  - 0.49670034646987915
  - 0.6698387861251831
  - 0.5383871793746948
  - 0.4627395570278168
  - 0.5411690473556519
  - 0.7188219428062439
  - 0.44210538268089294
  - 0.5875808596611023
  - 1.157877802848816
  - 0.5983565449714661
  - 0.4829252362251282
  - 1.58724045753479
  - 1.093795657157898
  - 0.6735976934432983
  - 0.4299345910549164
  - 0.5081743001937866
  - 0.5343769788742065
  - 0.5046131610870361
  - 0.5265917778015137
  - 0.5119491219520569
  - 0.46176859736442566
  - 0.3977830410003662
  - 0.6428695917129517
  - 0.8515042066574097
  - 0.454554945230484
  - 0.6319440603256226
  - 0.5679344534873962
  - 0.9277137517929077
  - 1.709515929222107
  - 0.6764379143714905
  - 0.6019466519355774
  - 1.1811048984527588
  - 1.4880467653274536
  - 1.4464436769485474
  - 0.9442530274391174
  - 0.7161254286766052
  - 0.6919902563095093
  - 1.731581687927246
  - 0.9607230424880981
  - 0.7181796431541443
  - 0.5566994547843933
  - 0.5587618350982666
  - 0.8471817970275879
  - 0.5028990507125854
  - 0.8993669748306274
  - 1.1095492839813232
  - 0.9809489846229553
  - 1.4404751062393188
  - 0.38039958477020264
  - 0.504322350025177
  - 0.6846357583999634
  - 0.58656907081604
  - 0.7099129557609558
  - 1.3550655841827393
  - 0.7843040823936462
  - 0.9638261795043945
  - 0.46518170833587646
  - 0.676838755607605
  - 0.4470579922199249
  - 0.5313661694526672
  - 0.5890377759933472
  - 17.93781852722168
  - 0.6697777509689331
loss_records_fold4:
  train_losses:
  - 7.299881130456924
  - 7.458671897649765
  - 7.182165771722794
  - 7.335124433040619
  - 7.063592851161957
  - 7.113367587327957
  - 6.988630294799805
  - 6.903482913970947
  - 7.033940225839615
  - 7.138965845108032
  - 7.02018341422081
  - 6.9530453979969025
  - 7.385551631450653
  - 7.355879783630371
  - 7.223066881299019
  - 7.275492697954178
  - 7.197582513093948
  - 7.0611156821250916
  - 7.218643039464951
  - 7.027413159608841
  - 7.251393437385559
  - 7.050738275051117
  - 7.06815180182457
  - 7.057115793228149
  - 7.1342843770980835
  - 7.007224231958389
  - 7.0392873883247375
  - 7.089549213647842
  - 7.497926354408264
  - 7.470295280218124
  - 7.34120112657547
  - 7.285192251205444
  - 7.249424308538437
  - 6.823196575045586
  - 7.079677373170853
  - 7.272160977125168
  - 7.163937568664551
  - 7.423082143068314
  - 7.408042252063751
  - 7.377702534198761
  - 7.185769021511078
  - 7.0754507929086685
  - 7.103671461343765
  - 6.846421658992767
  - 7.098141938447952
  - 7.157646298408508
  - 7.368041396141052
  - 7.628639578819275
  - 7.53750666975975
  - 7.573305398225784
  - 7.204530656337738
  - 6.999546155333519
  - 7.279469132423401
  - 7.330544054508209
  - 7.131057590246201
  - 7.042502731084824
  - 7.231270432472229
  - 7.290694236755371
  - 6.963831871747971
  - 7.028151869773865
  - 6.924668937921524
  - 6.966216564178467
  - 7.146863013505936
  - 7.549792170524597
  - 7.4100512862205505
  - 7.1116127371788025
  - 7.362346321344376
  - 7.319627404212952
  - 6.910301506519318
  - 6.762245565652847
  - 7.299310624599457
  - 6.996427774429321
  - 7.342507541179657
  - 7.566987574100494
  - 7.110128670930862
  - 7.321997106075287
  - 7.345648884773254
  - 7.30378058552742
  - 6.929269641637802
  - 7.138109445571899
  - 7.212516367435455
  - 7.105602830648422
  - 7.007752299308777
  - 6.910376757383347
  - 7.079284965991974
  - 7.186308801174164
  - 7.252454042434692
  - 7.210843473672867
  - 6.962577819824219
  - 7.633755415678024
  - 7.130548864603043
  - 7.180699378252029
  - 7.265719383955002
  - 7.277173846960068
  - 7.524969160556793
  - 7.321780115365982
  - 7.016503304243088
  - 7.101568222045898
  - 7.222640335559845
  - 7.267144978046417
  validation_losses:
  - 0.476742148399353
  - 0.5123401880264282
  - 0.4061903655529022
  - 0.358908474445343
  - 0.3468993902206421
  - 0.342305064201355
  - 0.32891222834587097
  - 0.45883357524871826
  - 0.5541294813156128
  - 0.4678634703159332
  - 0.5388604402542114
  - 0.44876614212989807
  - 0.37075933814048767
  - 0.4525658190250397
  - 0.3521551191806793
  - 0.456095427274704
  - 0.5186283588409424
  - 0.5045897960662842
  - 0.40256467461586
  - 0.4544270932674408
  - 0.4556315243244171
  - 0.4412614703178406
  - 0.6070305109024048
  - 0.3580790162086487
  - 0.3948204517364502
  - 0.4517606794834137
  - 0.42561259865760803
  - 0.4017694294452667
  - 0.5028796792030334
  - 0.4486108720302582
  - 0.4173122048377991
  - 0.4521104395389557
  - 0.37805795669555664
  - 0.4088456928730011
  - 0.3518635928630829
  - 0.4669441282749176
  - 0.5212360620498657
  - 0.4946975111961365
  - 0.37456846237182617
  - 0.428693950176239
  - 0.4284449517726898
  - 0.4671301543712616
  - 0.4034418761730194
  - 0.3876003623008728
  - 0.4135582745075226
  - 0.5016352534294128
  - 0.3719869554042816
  - 0.37507250905036926
  - 0.45838436484336853
  - 0.5985012650489807
  - 0.38602733612060547
  - 0.34904059767723083
  - 0.3639388382434845
  - 0.4655227065086365
  - 0.5650154948234558
  - 0.4755292534828186
  - 0.4769052267074585
  - 0.38710808753967285
  - 0.40920382738113403
  - 0.513295590877533
  - 0.42192593216896057
  - 0.5877392888069153
  - 0.8320175409317017
  - 0.3916032612323761
  - 0.4696497917175293
  - 0.5294631123542786
  - 0.48438218235969543
  - 0.46251991391181946
  - 0.462043434381485
  - 0.5139981508255005
  - 0.5322249531745911
  - 0.5865588784217834
  - 0.4821174442768097
  - 0.5332756042480469
  - 0.515509843826294
  - 0.48503223061561584
  - 0.43933796882629395
  - 0.5984688997268677
  - 0.4697206914424896
  - 0.4528041481971741
  - 0.4972667396068573
  - 0.5072373747825623
  - 0.47016918659210205
  - 0.48149365186691284
  - 0.5747552514076233
  - 0.4331812858581543
  - 0.3964308798313141
  - 0.4615538418292999
  - 0.6668692827224731
  - 0.5087989568710327
  - 0.39179086685180664
  - 0.43348556756973267
  - 0.4239324927330017
  - 0.397035151720047
  - 0.45204880833625793
  - 0.3833828270435333
  - 0.35351914167404175
  - 0.5969525575637817
  - 0.4448687434196472
  - 0.3962373435497284
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8216123499142367, 0.8353344768439108, 0.8319039451114922,
    0.8487972508591065]'
  fold_eval_f1: '[0.0, 0.17460317460317462, 0.23809523809523808, 0.26865671641791045,
    0.18518518518518517]'
  mean_eval_accuracy: 0.8390561911666754
  mean_f1_accuracy: 0.17330806286030165
  total_train_time: '0:06:19.957843'
