config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:16:37.982215'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_78fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_78fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_78fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_78fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_78fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_78fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_78fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_78fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_78fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_78fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 22.68399784564972
  - 8.369852149486542
  - 3.590798354148865
  - 4.206773740053177
  - 7.6200493514537815
  - 6.459848648309708
  - 7.901059621572495
  - 3.429715204238892
  - 4.627522206306458
  - 2.87557030916214
  - 2.752608096599579
  - 3.5543436825275423
  - 2.1284916877746585
  - 2.176032990217209
  - 2.9337547183036805
  - 5.794173109531403
  - 13.130165696144104
  - 5.52612339258194
  - 1.8444265693426134
  - 1.7279231131076813
  - 2.254593425989151
  - 1.9666276454925538
  - 7.532774341106415
  - 6.340351229906083
  - 2.5463165879249576
  - 1.717138433456421
  - 2.5008882462978366
  - 3.285383361577988
  - 5.120154273509979
  - 1.9821400463581087
  - 2.5210939407348634
  - 2.2142393589019775
  - 1.9503778219223022
  - 6.168861263990403
  - 2.889652818441391
  - 2.354070430994034
  - 1.9907424271106722
  - 1.5892352104187013
  - 2.311231917142868
  - 3.0327494144439697
  - 2.910252660512924
  - 1.9608504831790925
  - 1.7176039814949036
  - 3.8430645525455476
  - 1.8335918188095093
  - 2.2621196419000626
  - 2.1467454075813293
  - 3.572708261013031
  - 3.004823213815689
  - 2.3393361389636995
  - 1.9078559875488281
  - 3.7322541117668155
  - 2.1991816818714143
  - 1.8849673688411714
  - 1.8130644500255586
  - 1.8447514891624452
  - 3.2823031067848207
  - 3.4763566851615906
  - 2.0083341002464294
  - 1.687062841653824
  - 1.9663640499114992
  - 2.5254269659519197
  - 3.4374155402183533
  - 7.0358744859695435
  - 3.8231415033340457
  - 6.3247160673141485
  - 2.233873999118805
  - 2.0344906270504
  - 2.1114139914512635
  - 2.5493616223335267
  - 2.7901946365833283
  - 1.6817878603935243
  - 2.1593497455120088
  - 1.873973333835602
  - 2.729491037130356
  - 3.227908670902252
  - 2.495695000886917
  - 2.9317504525184632
  - 1.6742803156375885
  - 1.696127986907959
  - 1.8422915101051331
  - 1.6319477915763856
  - 2.0898564338684085
  - 2.2781643509864806
  - 1.961048495769501
  - 1.7224482715129854
  - 3.250039941072464
  - 2.0084428787231445
  - 1.6920013129711151
  - 1.8024402618408204
  - 1.621515339612961
  - 1.6610636353492738
  - 1.6610885858535767
  validation_losses:
  - 17.71648406982422
  - 0.9159888625144958
  - 0.5559835433959961
  - 0.5957133173942566
  - 0.5125364065170288
  - 1.115159273147583
  - 1.6440562009811401
  - 0.6928547024726868
  - 0.5108276605606079
  - 0.5595678091049194
  - 0.4031796157360077
  - 0.5528356432914734
  - 0.48329082131385803
  - 0.4320102035999298
  - 0.42841649055480957
  - 0.5680282115936279
  - 0.586677610874176
  - 0.39875105023384094
  - 0.3869834244251251
  - 0.44174692034721375
  - 0.41366279125213623
  - 0.5612878203392029
  - 0.4436549246311188
  - 0.4663446247577667
  - 0.4066600501537323
  - 0.40310218930244446
  - 0.4020565450191498
  - 0.42772459983825684
  - 0.3959393799304962
  - 0.39313408732414246
  - 0.41695263981819153
  - 0.6038953065872192
  - 0.45001551508903503
  - 0.4082285165786743
  - 0.4539061188697815
  - 0.4012795686721802
  - 0.39058226346969604
  - 0.3906155228614807
  - 0.40951353311538696
  - 0.4020671546459198
  - 0.5653956532478333
  - 0.4156402051448822
  - 0.3999844789505005
  - 0.39720237255096436
  - 0.4169588088989258
  - 0.42026177048683167
  - 0.4045581817626953
  - 0.45713281631469727
  - 0.4189143180847168
  - 0.41999998688697815
  - 0.3985637426376343
  - 0.4233839213848114
  - 0.3945906162261963
  - 0.42453667521476746
  - 0.38972538709640503
  - 0.39013421535491943
  - 0.4277772009372711
  - 0.4087624251842499
  - 0.4136478006839752
  - 0.42145848274230957
  - 0.42421483993530273
  - 0.4024912118911743
  - 0.476010799407959
  - 0.6784620881080627
  - 2.6991639137268066
  - 0.5806671380996704
  - 0.4158090651035309
  - 0.41614440083503723
  - 0.4468904137611389
  - 0.44699427485466003
  - 0.40732541680336
  - 0.45195072889328003
  - 0.41050803661346436
  - 0.4102934002876282
  - 0.3891923129558563
  - 0.5020802617073059
  - 0.400781512260437
  - 0.4342704117298126
  - 0.4373430609703064
  - 0.40590277314186096
  - 0.4112612307071686
  - 0.404445081949234
  - 0.40459808707237244
  - 0.42635634541511536
  - 0.4231914281845093
  - 0.4029901325702667
  - 0.44834092259407043
  - 0.396126389503479
  - 0.40312108397483826
  - 0.4093870520591736
  - 0.4118202328681946
  - 0.4102274477481842
  - 0.40971478819847107
loss_records_fold1:
  train_losses:
  - 1.6208764016628265
  - 1.5890233635902407
  - 1.7084859490394593
  - 1.8079227030277254
  - 1.730524617433548
  - 1.7767255663871766
  - 1.863067764043808
  - 1.778452378511429
  - 1.7051219165325167
  - 1.5953112453222276
  - 1.6810582995414736
  - 1.7234058558940888
  - 1.6929103314876557
  - 1.6733073592185974
  - 2.0120207488536836
  - 1.862706255912781
  - 1.6590417087078095
  - 1.6916456699371338
  - 1.7051310241222382
  - 1.6707872033119202
  - 1.6277136147022249
  - 2.8378467917442323
  - 1.8425529420375826
  - 1.6057840913534165
  - 1.595665866136551
  - 1.6222332179546357
  - 1.695887577533722
  - 1.6128636419773104
  - 1.637128609418869
  - 1.6310288190841675
  - 1.6884923517704011
  - 1.6286069989204408
  - 1.7761195361614228
  - 1.890241825580597
  - 1.6575558364391327
  - 1.6077634572982789
  - 1.729149341583252
  - 1.8755539953708649
  - 1.695007395744324
  - 1.7122194051742554
  - 1.6308726310729982
  - 1.62748681306839
  - 1.9151109993457796
  - 1.621029579639435
  - 1.5698131501674653
  - 1.5610001266002655
  - 1.7136218786239625
  - 1.6374308228492738
  - 1.6799676418304443
  - 1.873842465877533
  - 1.624015098810196
  - 1.7195661902427675
  - 1.6568813145160677
  - 1.6235040426254272
  - 1.6704661846160889
  - 1.6954473316669465
  - 1.6018059492111207
  - 1.623708724975586
  - 2.037306010723114
  - 1.6549165308475495
  - 1.6255154371261598
  - 2.78331578373909
  - 1.75809063911438
  - 1.9201413810253145
  - 2.205925899744034
  - 1.6550583302974702
  - 1.6490901589393616
  - 1.6128860950469972
  - 1.7807723104953768
  - 1.6855075478553774
  - 1.6731868445873261
  - 2.409086912870407
  - 1.910748052597046
  - 1.6136424541473389
  - 1.6503611087799073
  - 1.7006254315376284
  - 1.66614807844162
  - 1.6901071310043336
  - 1.6242754340171814
  - 1.5823257923126222
  validation_losses:
  - 0.40725404024124146
  - 0.42943042516708374
  - 0.47036120295524597
  - 0.4768378436565399
  - 0.4087461829185486
  - 0.41739338636398315
  - 0.4242033362388611
  - 0.4364960491657257
  - 0.44239622354507446
  - 0.42867526412010193
  - 0.4514849781990051
  - 0.43666452169418335
  - 0.41913294792175293
  - 0.4508725106716156
  - 0.41493669152259827
  - 0.4698781669139862
  - 0.4423016905784607
  - 0.41844862699508667
  - 0.40665480494499207
  - 0.4190402925014496
  - 0.4233039319515228
  - 0.4206787049770355
  - 0.4105282425880432
  - 0.42683979868888855
  - 0.4167538285255432
  - 0.4401620030403137
  - 0.44953787326812744
  - 0.41371414065361023
  - 0.4227190911769867
  - 0.4155491888523102
  - 0.4321673512458801
  - 0.4160323739051819
  - 0.4195009469985962
  - 0.4301162362098694
  - 0.4148392677307129
  - 0.44729721546173096
  - 0.4148982763290405
  - 0.4673751890659332
  - 0.42479458451271057
  - 0.42111828923225403
  - 0.41382598876953125
  - 0.4177820384502411
  - 0.4213201403617859
  - 0.4403301477432251
  - 0.4120492935180664
  - 0.399995893239975
  - 0.4239194691181183
  - 0.436910480260849
  - 0.4332624673843384
  - 0.43558529019355774
  - 0.4060368537902832
  - 0.42425817251205444
  - 0.40886878967285156
  - 0.4248825013637543
  - 0.40779170393943787
  - 0.40925997495651245
  - 0.41854357719421387
  - 0.41237038373947144
  - 0.40936943888664246
  - 0.4196412265300751
  - 0.42734771966934204
  - 0.4373123049736023
  - 0.4433217942714691
  - 0.42566409707069397
  - 0.4086601734161377
  - 0.4293355345726013
  - 0.41301196813583374
  - 0.40133175253868103
  - 0.43709617853164673
  - 0.47195565700531006
  - 0.4378432035446167
  - 0.402617871761322
  - 0.40657198429107666
  - 0.4375899136066437
  - 0.43523159623146057
  - 0.44480156898498535
  - 0.44696056842803955
  - 0.42139872908592224
  - 0.42303723096847534
  - 0.413616418838501
loss_records_fold2:
  train_losses:
  - 1.6272051453590395
  - 1.612350443005562
  - 1.6946200728416443
  - 1.6136089980602266
  - 1.6700447499752045
  - 1.6354994654655457
  - 1.641606867313385
  - 1.6210365295410156
  - 1.639649510383606
  - 1.6306619673967362
  - 1.7023052960634233
  - 1.6948171377182009
  - 1.6265088081359864
  - 1.662325030565262
  - 1.6631566107273104
  - 1.5840424001216888
  validation_losses:
  - 0.3932940363883972
  - 0.4184497594833374
  - 0.40825480222702026
  - 0.42364779114723206
  - 0.388823002576828
  - 0.41916149854660034
  - 0.40307626128196716
  - 0.3972420394420624
  - 0.4026574194431305
  - 0.41367730498313904
  - 0.4095589816570282
  - 0.4111325740814209
  - 0.40894073247909546
  - 0.4151987135410309
  - 0.40213605761528015
  - 0.4010409414768219
loss_records_fold3:
  train_losses:
  - 1.6045325219631197
  - 1.5891366183757782
  - 1.6171435058116914
  - 1.7252843141555787
  - 1.7466633260250093
  - 1.6552518606185913
  - 1.589051252603531
  - 1.6335930824279785
  - 1.5977643013000489
  - 1.619070076942444
  - 1.5339506819844246
  - 1.6128553807735444
  - 1.6006106674671174
  - 1.6351502299308778
  - 1.6021602571010591
  - 1.7443644821643831
  - 1.6875325441360474
  - 1.661586481332779
  - 1.5652018010616304
  - 1.6002770304679872
  - 1.5877544999122621
  - 1.6612855553627015
  - 1.605148822069168
  - 1.6084218502044678
  - 1.6854923069477081
  - 1.7174858868122103
  - 1.6289749145507812
  - 1.7308514714241028
  - 1.6527752399444582
  - 1.6153750956058504
  validation_losses:
  - 0.41959142684936523
  - 0.4097892642021179
  - 0.4112224280834198
  - 0.4184657633304596
  - 0.478824257850647
  - 0.42514124512672424
  - 0.4468207359313965
  - 0.40744826197624207
  - 0.43918702006340027
  - 0.415282279253006
  - 0.4359567165374756
  - 0.4115059971809387
  - 0.4304472804069519
  - 0.4255372881889343
  - 0.41798195242881775
  - 0.4310440421104431
  - 0.40821966528892517
  - 0.44325417280197144
  - 0.4041634500026703
  - 0.4066131114959717
  - 0.44021859765052795
  - 0.40132245421409607
  - 0.40406471490859985
  - 0.4825323522090912
  - 0.41767704486846924
  - 0.4253593683242798
  - 0.4205392301082611
  - 0.4132506251335144
  - 0.4026493430137634
  - 0.4008764624595642
loss_records_fold4:
  train_losses:
  - 1.6573867857456208
  - 1.6775592029094697
  - 1.656339168548584
  - 1.637676012516022
  - 1.654571771621704
  - 1.5881112694740296
  - 1.5966826558113099
  - 1.72433660030365
  - 1.6177575945854188
  - 1.6596454083919525
  - 1.6557156801223756
  - 1.6543084383010864
  - 1.59356746673584
  - 1.6770153284072877
  - 1.6570453166961672
  - 1.6256124556064606
  - 1.6395750224590302
  - 1.661655080318451
  - 1.6735799729824068
  - 1.620052629709244
  - 1.6728507280349731
  - 1.6541645288467408
  - 1.6282653987407685
  - 1.601187127828598
  - 1.5992364823818208
  - 1.614378345012665
  - 1.6018518090248108
  - 1.692229187488556
  - 1.7306514203548433
  - 1.6022915065288545
  - 1.6563020169734957
  - 1.667508727312088
  - 1.657902717590332
  - 1.619855907559395
  - 1.6375460624694824
  - 1.6437557220458985
  - 1.6953036963939667
  - 1.713726055622101
  - 1.6045181274414064
  - 1.6161510169506075
  - 1.6325209796428681
  - 1.5896064221858979
  validation_losses:
  - 0.42990583181381226
  - 0.4006543457508087
  - 0.40755975246429443
  - 0.4113273620605469
  - 0.4143730700016022
  - 0.4156130254268646
  - 0.4004383385181427
  - 0.4108811914920807
  - 0.4029790163040161
  - 0.40018394589424133
  - 0.44585710763931274
  - 0.4243789613246918
  - 0.42530572414398193
  - 0.4049120545387268
  - 0.41452422738075256
  - 0.42975181341171265
  - 0.4156077802181244
  - 0.4092291593551636
  - 0.42144814133644104
  - 0.4073379635810852
  - 0.45716843008995056
  - 0.4381249248981476
  - 0.3988763093948364
  - 0.4110858738422394
  - 0.4356246292591095
  - 0.41516271233558655
  - 0.42117631435394287
  - 0.41222184896469116
  - 0.40000930428504944
  - 0.40520530939102173
  - 0.43415921926498413
  - 0.4059770107269287
  - 0.4081261456012726
  - 0.39673206210136414
  - 0.40386763215065
  - 0.4223962128162384
  - 0.4113163948059082
  - 0.41821151971817017
  - 0.41260260343551636
  - 0.40781137347221375
  - 0.4093210995197296
  - 0.39962249994277954
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 93 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 80 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 42 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:22:25.416599'
