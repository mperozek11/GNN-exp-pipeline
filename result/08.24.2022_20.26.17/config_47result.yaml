config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:29:12.179825'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_47fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_47fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_47fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_47fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_47fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_47fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_47fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_47fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_47fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_47fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 25.764861965179445
  - 11.329058694839478
  - 12.671141695976258
  - 4.559340262413025
  - 4.233045703172684
  - 3.903334188461304
  - 4.427927601337433
  - 2.2081942439079287
  - 2.621655997633934
  - 2.368920087814331
  - 2.2728128731250763
  - 1.1934324383735657
  - 1.105357438325882
  - 1.0353722393512725
  - 1.0659770846366883
  - 1.1751088440418245
  - 1.5684140264987947
  - 1.486205792427063
  - 1.221706509590149
  - 1.0732230424880982
  - 1.001691323518753
  - 0.9195941448211671
  - 1.7446796298027039
  - 7.27031518816948
  - 3.5149549305438996
  - 1.041037231683731
  - 1.1716568887233734
  - 1.1429853916168213
  - 1.0985406696796418
  - 0.9246620535850525
  - 0.9187388122081757
  - 2.8625647306442263
  - 1.6546322464942933
  - 1.1309353291988373
  - 1.0435027837753297
  - 2.026872754096985
  - 2.901754301786423
  - 4.088832461833954
  - 1.5617368400096894
  - 1.0445124447345735
  - 1.1680092155933381
  - 1.0694065392017365
  - 2.256455409526825
  - 1.2014933109283448
  - 1.110386598110199
  - 1.062187021970749
  - 1.1493842780590058
  - 1.0940137267112733
  - 1.1616266071796417
  - 0.9666026830673218
  - 1.1424646317958833
  - 0.852771556377411
  - 0.8972154796123505
  - 0.8808622419834138
  - 1.0149837374687196
  - 0.9838074505329133
  - 0.8036003977060319
  - 0.8384192258119584
  - 0.9613764464855195
  - 0.9272605061531067
  - 0.8352900862693787
  - 0.9881276667118073
  - 2.724921178817749
  - 2.8098535597324372
  - 2.7403096675872805
  - 1.0386548042297363
  - 1.0317244946956634
  - 0.8927308917045593
  - 2.27380411028862
  - 0.9130451261997223
  - 0.9663063764572144
  - 0.8522501230239868
  - 2.429143351316452
  - 0.88195458650589
  - 0.8694921374320984
  - 0.9258767187595368
  validation_losses:
  - 4.649999618530273
  - 2.785762310028076
  - 3.3753151893615723
  - 0.6757009625434875
  - 2.474968194961548
  - 1.8736474514007568
  - 1.1471004486083984
  - 0.938612699508667
  - 0.8607459664344788
  - 1.1525213718414307
  - 0.9176679849624634
  - 0.7785784006118774
  - 0.4402006268501282
  - 0.4288817346096039
  - 0.4922955632209778
  - 0.5231894850730896
  - 0.6397786736488342
  - 0.6382781267166138
  - 0.4970310628414154
  - 0.43518954515457153
  - 0.5348640084266663
  - 0.4814613461494446
  - 0.4111277163028717
  - 0.5315000414848328
  - 0.5346765518188477
  - 0.5669429898262024
  - 0.4392985999584198
  - 0.4534595012664795
  - 0.46314767003059387
  - 0.4756222665309906
  - 0.39876800775527954
  - 0.7721863389015198
  - 0.4525786340236664
  - 0.41610482335090637
  - 0.4730266034603119
  - 0.39584216475486755
  - 0.40724870562553406
  - 0.47408196330070496
  - 0.4348856508731842
  - 0.4342992901802063
  - 0.41546663641929626
  - 0.4265047311782837
  - 0.5919063687324524
  - 0.459524005651474
  - 0.4193321466445923
  - 0.419556200504303
  - 0.48280778527259827
  - 0.4250093698501587
  - 0.4330557882785797
  - 0.39601123332977295
  - 0.3969661295413971
  - 0.41850754618644714
  - 0.4550742506980896
  - 0.43377184867858887
  - 0.4443216025829315
  - 0.39572519063949585
  - 0.4081713557243347
  - 0.3864781856536865
  - 0.429837703704834
  - 0.4139135479927063
  - 0.4122920632362366
  - 0.3963228464126587
  - 0.38445955514907837
  - 0.43418556451797485
  - 0.40738651156425476
  - 0.41957440972328186
  - 0.39438626170158386
  - 0.4653213620185852
  - 0.40118545293807983
  - 0.4115792214870453
  - 0.41561079025268555
  - 0.40274253487586975
  - 0.38949495553970337
  - 0.39564985036849976
  - 0.3836844861507416
  - 0.39120060205459595
loss_records_fold1:
  train_losses:
  - 0.9137374103069306
  - 1.0104044675827026
  - 0.8916798770427704
  - 0.937542086839676
  - 2.091790997982025
  - 0.8462582767009735
  - 0.8682992815971375
  - 0.8826389193534852
  - 0.9193232625722886
  - 0.8890851259231568
  - 0.9452530741691589
  - 0.8432550549507142
  - 0.8794766008853913
  - 0.8429904460906983
  - 0.9126865923404694
  - 1.0109643280506135
  - 0.9103908002376557
  - 0.93551065325737
  - 1.02449831366539
  - 0.8769814729690553
  - 0.9602904975414277
  - 0.8889025092124939
  - 0.9013664186000825
  - 2.5633328735828402
  - 5.367278754711151
  - 1.5660209298133851
  - 1.19162015914917
  - 1.3796867609024048
  - 1.0919799625873565
  - 0.9819846570491791
  - 0.9338381946086884
  - 1.3471193313598633
  - 0.8976819813251495
  - 0.899183076620102
  - 0.87393778860569
  - 5.168420881032944
  - 2.2121341943740847
  - 9.58117317557335
  - 1.911385613679886
  - 1.8569531440734863
  - 1.1893170297145843
  - 0.9912900090217591
  - 0.9639438331127167
  - 1.0177542090415954
  - 0.9519802927970886
  - 0.8592485070228577
  - 0.959904357790947
  - 12.719546288251877
  - 0.934356951713562
  - 0.9492958456277848
  - 0.9663098216056825
  - 2.9758508980274203
  - 3.2463383913040165
  - 0.9626213371753694
  - 3.896830028295517
  - 1.0050971686840058
  - 0.8365203440189362
  - 0.8218334019184113
  - 1.0340069472789766
  validation_losses:
  - 0.43723800778388977
  - 0.45575469732284546
  - 0.450804203748703
  - 0.4340951442718506
  - 0.4171690046787262
  - 0.3981987237930298
  - 0.40853169560432434
  - 0.47883033752441406
  - 0.42613574862480164
  - 0.4188705384731293
  - 0.4607253074645996
  - 0.4037524163722992
  - 0.4240973889827728
  - 0.4251715838909149
  - 0.43711966276168823
  - 0.43226343393325806
  - 0.4502279758453369
  - 0.5283759236335754
  - 0.4162243902683258
  - 0.4503253996372223
  - 0.445122092962265
  - 0.4345860183238983
  - 0.43806779384613037
  - 0.7352556586265564
  - 0.602715790271759
  - 0.4494648873806
  - 0.44343292713165283
  - 0.47606155276298523
  - 0.45179638266563416
  - 0.46895891427993774
  - 0.43095675110816956
  - 0.5168552994728088
  - 0.4704202711582184
  - 0.43100830912590027
  - 0.4110417068004608
  - 0.4011611044406891
  - 1.7458717823028564
  - 0.42414504289627075
  - 1.1498979330062866
  - 0.5891863107681274
  - 0.5529566407203674
  - 0.42275479435920715
  - 0.43229445815086365
  - 0.4512263238430023
  - 0.41810595989227295
  - 0.41706156730651855
  - 0.4054073989391327
  - 0.4073464572429657
  - 0.5784533619880676
  - 0.4049096703529358
  - 0.39523443579673767
  - 0.4595875144004822
  - 0.5660248398780823
  - 0.4235060513019562
  - 0.41289761662483215
  - 0.3999210298061371
  - 0.4082809090614319
  - 0.40788838267326355
  - 0.4072708189487457
loss_records_fold2:
  train_losses:
  - 0.9919273734092713
  - 1.9898958206176758
  - 1.1104037582874298
  - 0.89064878821373
  - 0.8466555893421174
  - 1.907792788743973
  - 1.244221591949463
  - 1.8117590963840486
  - 0.9821034193038941
  - 1.546588009595871
  - 5.578260695934296
  - 2.035561847686768
  - 1.000791782140732
  - 1.0877409040927888
  - 1.0940023422241212
  - 0.8270715713500977
  - 0.9227782845497132
  - 1.7469070672988893
  - 1.230726033449173
  - 1.2109216034412384
  - 3.058760726451874
  - 2.399587094783783
  - 1.0368451595306396
  - 0.9543969690799714
  - 1.222090405225754
  - 0.9599789559841156
  - 0.9032552152872086
  - 0.8639234781265259
  - 5.782884132862091
  - 0.9985925972461701
  - 1.3635599792003632
  - 0.8728542745113373
  - 0.906153953075409
  - 0.9654741942882539
  - 0.8524560153484345
  - 0.8648216843605042
  - 0.9241127133369447
  - 0.8743923246860504
  - 1.075106465816498
  - 0.8594752490520478
  - 0.8573700368404389
  - 0.8856799721717835
  - 1.5800593376159668
  - 0.9237105667591096
  - 0.8685427069664002
  - 0.8915178775787354
  - 0.827262344956398
  - 0.9469227075576783
  - 0.8796949803829194
  - 1.079280024766922
  validation_losses:
  - 0.3804415166378021
  - 0.41238996386528015
  - 0.3844054639339447
  - 0.38463619351387024
  - 0.38065797090530396
  - 0.37695789337158203
  - 0.4118727147579193
  - 0.39164087176322937
  - 0.4195094406604767
  - 0.4233909845352173
  - 0.39281752705574036
  - 0.4645930826663971
  - 0.4710839092731476
  - 0.3961290121078491
  - 0.4019707441329956
  - 0.40527552366256714
  - 0.4034152925014496
  - 0.5242944955825806
  - 0.4269554316997528
  - 0.4397052228450775
  - 0.8781068325042725
  - 0.48886606097221375
  - 0.4337945282459259
  - 0.4350268244743347
  - 0.4055926501750946
  - 0.45338761806488037
  - 0.39894604682922363
  - 0.3960128128528595
  - 0.41052401065826416
  - 0.4477235674858093
  - 0.3936834931373596
  - 0.4137178957462311
  - 0.40768349170684814
  - 0.4000764489173889
  - 0.46718519926071167
  - 0.5065253376960754
  - 0.3782939314842224
  - 0.39995670318603516
  - 0.4008552134037018
  - 0.38274434208869934
  - 0.38214877247810364
  - 0.39436179399490356
  - 0.39898407459259033
  - 0.4475831091403961
  - 0.408090740442276
  - 0.40605032444000244
  - 0.4128163754940033
  - 0.4123750925064087
  - 0.4104107618331909
  - 0.4180024266242981
loss_records_fold3:
  train_losses:
  - 1.9727859258651734
  - 1.0508832871913911
  - 0.9643462598323822
  - 0.966252875328064
  - 0.8949548780918122
  - 0.868031108379364
  - 0.9342694163322449
  - 0.8890424013137818
  - 1.0868324756622314
  - 0.9784479737281799
  - 0.8850050628185273
  - 0.938918298482895
  - 0.8535744726657868
  - 0.8304373681545258
  - 0.8828310728073121
  - 0.8947871267795563
  - 0.8642790794372559
  validation_losses:
  - 0.4493710398674011
  - 0.5234172940254211
  - 0.4121762812137604
  - 0.41638925671577454
  - 0.4080584645271301
  - 0.43344148993492126
  - 0.41204050183296204
  - 0.42171144485473633
  - 0.4220150411128998
  - 0.42591002583503723
  - 0.43657568097114563
  - 0.4055076539516449
  - 0.4040217697620392
  - 0.4071347713470459
  - 0.40097182989120483
  - 0.40596693754196167
  - 0.41293561458587646
loss_records_fold4:
  train_losses:
  - 0.8786112427711488
  - 0.8904357075691224
  - 0.848703145980835
  - 0.8353241622447968
  - 0.8167458802461625
  - 0.8323451578617096
  - 0.8605938255786896
  - 0.838144713640213
  - 0.8423148095607758
  - 0.8727058410644531
  - 0.8696138620376588
  - 0.945922350883484
  - 0.9042245745658875
  - 0.8674582183361054
  - 0.8747858166694642
  - 0.9101539552211761
  - 2.7062912523746494
  - 0.8642600119113922
  - 1.7136089891195299
  - 0.9044443488121033
  - 0.8802693963050843
  - 0.8251728296279908
  - 1.1229715168476104
  - 0.8895888566970825
  - 0.8265829384326935
  - 0.8007285594940186
  - 4.185497516393662
  - 1.0732271730899812
  - 0.9227249026298523
  - 0.8493958413600922
  - 0.7987435758113861
  - 0.8612986087799073
  - 0.8266591846942902
  - 0.8866850018501282
  - 0.8818049967288971
  - 0.888026374578476
  - 0.918443387746811
  - 0.8248175024986267
  - 0.855813729763031
  - 0.8444406390190125
  - 0.9604205250740052
  - 0.8626360923051835
  - 0.8689651012420655
  - 0.8476509988307953
  - 0.8689871013164521
  - 0.8249760925769807
  - 0.8803355515003205
  - 0.8328868329524994
  - 0.8219029039144516
  - 0.8466277897357941
  - 0.8536655306816101
  - 0.8650731384754181
  - 0.8507445156574249
  - 0.8488787651062012
  - 0.8333924531936646
  - 0.9443441331386566
  - 0.8209530413150787
  - 0.8345603108406068
  - 0.8198934197425842
  - 0.8914634585380554
  - 0.8328053534030915
  - 0.8117181599140167
  - 2.1694522202014923
  - 0.8711827874183655
  - 0.9159377336502076
  - 1.3056690812110903
  - 0.8505556225776673
  - 0.8735704123973846
  - 0.8125843524932862
  - 0.8245607614517212
  - 0.8265790700912476
  - 0.9967392563819886
  - 1.0546302855014802
  - 0.8943471312522888
  - 1.0353229880332948
  - 0.9725937187671662
  - 0.8347681611776352
  - 0.9151323854923249
  - 0.940552181005478
  - 0.8288457036018372
  - 0.862848961353302
  - 0.933039551973343
  - 0.8385379195213318
  - 0.8634480476379395
  - 0.8428953468799592
  - 0.8406813025474549
  validation_losses:
  - 0.4213722050189972
  - 0.40651270747184753
  - 0.40614616870880127
  - 0.413478285074234
  - 0.4118763506412506
  - 0.40880340337753296
  - 0.4029200077056885
  - 0.41859883069992065
  - 0.40049970149993896
  - 0.4112527668476105
  - 0.4011431932449341
  - 0.40366843342781067
  - 0.4141232371330261
  - 0.4209161698818207
  - 0.41770297288894653
  - 0.40901505947113037
  - 0.43395206332206726
  - 7.255579948425293
  - 0.4054276943206787
  - 0.4136360287666321
  - 0.40731561183929443
  - 0.43577370047569275
  - 0.4314287304878235
  - 0.4112212359905243
  - 0.42580050230026245
  - 0.4135163128376007
  - 0.4273841679096222
  - 0.40942689776420593
  - 0.4265103340148926
  - 0.4415699541568756
  - 0.4246293604373932
  - 0.406645804643631
  - 0.4095142185688019
  - 0.4422744810581207
  - 0.4544339179992676
  - 0.4109265208244324
  - 0.44284456968307495
  - 0.4428409934043884
  - 0.41634461283683777
  - 0.4263169467449188
  - 0.43414098024368286
  - 0.4484034776687622
  - 0.43898940086364746
  - 0.40554606914520264
  - 0.4115626811981201
  - 0.43900564312934875
  - 0.4985247850418091
  - 0.4202745258808136
  - 0.4191209375858307
  - 0.4367152452468872
  - 0.40853044390678406
  - 0.446463406085968
  - 0.42869827151298523
  - 0.42677778005599976
  - 0.4140583574771881
  - 0.4332278072834015
  - 0.42851346731185913
  - 0.4380606412887573
  - 0.407356858253479
  - 0.41290009021759033
  - 0.3942587077617645
  - 0.41176092624664307
  - 0.43456608057022095
  - 0.4188291132450104
  - 0.40998101234436035
  - 0.431404709815979
  - 0.4696446657180786
  - 0.4185926914215088
  - 0.4229607582092285
  - 0.4182533323764801
  - 0.40749049186706543
  - 0.4084433913230896
  - 0.42494210600852966
  - 0.42262977361679077
  - 0.4207152724266052
  - 0.420922189950943
  - 0.42278850078582764
  - 0.46285346150398254
  - 0.42587313055992126
  - 0.44255250692367554
  - 0.41770610213279724
  - 0.4193204939365387
  - 0.40608376264572144
  - 0.4121948480606079
  - 0.40942415595054626
  - 0.40810030698776245
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 76 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 59 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 50 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 86 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8576329331046312, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:24:36.339833'
