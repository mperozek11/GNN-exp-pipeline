config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:37:11.194783'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_94fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_94fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_94fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_94fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_94fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_94fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_94fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_94fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_94fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_94fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 24.017698872089387
  - 9.624967133998872
  - 3.6820543229579927
  - 4.703262293338776
  - 7.169790494441987
  - 5.205134850740433
  - 4.804220896959305
  - 3.646979770064354
  - 3.613509637117386
  - 2.407827311754227
  - 3.521715742349625
  - 4.160920643806458
  - 3.155060261487961
  - 2.2461958050727846
  - 2.809345293045044
  - 3.933357781171799
  - 11.538725584745407
  - 4.500964176654816
  - 2.0607706516981126
  - 1.8031075596809387
  - 2.386794477701187
  - 1.9356290400028229
  - 6.586905139684678
  - 4.952275049686432
  - 3.3068790793418885
  - 1.8740889728069305
  - 3.6460147678852084
  - 2.5352467834949497
  - 3.2082119047641755
  - 1.7724087595939637
  - 2.078599786758423
  - 1.9067444026470186
  - 1.7470236897468567
  - 4.042843598127365
  - 3.325104439258576
  - 2.3735568821430206
  - 2.1448415875434876
  - 1.6135086655616762
  - 2.428552132844925
  - 3.128360313177109
  - 3.6531824409961704
  - 2.4150364577770236
  - 1.8005719602108003
  - 3.525600337982178
  - 3.988090378046036
  - 1.7871695071458817
  - 2.092180663347244
  validation_losses:
  - 19.288179397583008
  - 0.8264222145080566
  - 0.6379481554031372
  - 0.8924773335456848
  - 0.7042908072471619
  - 0.7000814080238342
  - 0.9346024990081787
  - 0.5844523906707764
  - 0.4748513400554657
  - 0.7854684591293335
  - 0.6796467900276184
  - 0.45064184069633484
  - 0.6064172983169556
  - 0.4083101749420166
  - 0.4447227716445923
  - 0.8073012232780457
  - 0.47504138946533203
  - 0.4297475516796112
  - 0.38298192620277405
  - 0.4627029299736023
  - 0.40963390469551086
  - 0.4225257337093353
  - 0.48059993982315063
  - 0.4397375285625458
  - 0.4515424370765686
  - 0.40935301780700684
  - 0.3854805529117584
  - 0.4383023679256439
  - 0.406717985868454
  - 0.3889133036136627
  - 0.4444708824157715
  - 0.39798593521118164
  - 0.42081576585769653
  - 0.42525434494018555
  - 0.4076286554336548
  - 0.3867631256580353
  - 0.38754504919052124
  - 0.39801692962646484
  - 0.3932959735393524
  - 0.39832839369773865
  - 0.4564322233200073
  - 0.42009952664375305
  - 0.4021878242492676
  - 0.40622183680534363
  - 0.4156193733215332
  - 0.4223968982696533
  - 0.42092615365982056
loss_records_fold1:
  train_losses:
  - 1.691650664806366
  - 1.6797672808170319
  - 2.943022620677948
  - 2.448513126373291
  - 1.7029486238956453
  - 2.1582545757293703
  - 3.67805507183075
  - 1.7092030227184296
  - 1.9915637388825418
  - 1.7415396571159363
  - 2.0765487492084502
  - 1.8582728147506715
  - 1.9011479318141937
  - 1.7386729717254639
  - 1.6751745581626893
  - 2.5662081450223924
  - 2.75602240562439
  - 1.9149393260478975
  - 1.7585126638412476
  validation_losses:
  - 0.41940951347351074
  - 0.4046512544155121
  - 0.4082796275615692
  - 0.5664461255073547
  - 0.40560728311538696
  - 0.46064648032188416
  - 0.4181007742881775
  - 0.4548583924770355
  - 0.5340962409973145
  - 0.43485745787620544
  - 0.42811641097068787
  - 0.5213125944137573
  - 0.6354262232780457
  - 0.4334007501602173
  - 0.4322618246078491
  - 0.4096417725086212
  - 0.4138297140598297
  - 0.4174836277961731
  - 0.4215388596057892
loss_records_fold2:
  train_losses:
  - 2.4322811365127563
  - 2.574189913272858
  - 3.055702358484268
  - 4.254177176952362
  - 2.253108763694763
  - 1.6975261569023132
  - 3.6153146207332614
  - 1.7659567594528198
  - 1.805422616004944
  - 1.668173211812973
  - 1.7138530313968658
  - 1.7542978763580324
  - 1.717850548028946
  - 2.0123975753784182
  - 1.7248760640621186
  - 1.7559809982776642
  - 1.6932460606098176
  - 1.7629808306694033
  - 2.1063590168952944
  - 1.636210060119629
  - 1.5963066756725313
  - 1.7467342168092728
  - 1.7908613443374635
  - 1.6372808158397676
  - 1.6508609324693682
  - 1.7220737636089325
  - 2.6031777918338777
  - 2.2810627281665803
  - 1.7894454121589662
  - 1.6679916918277742
  - 1.647621387243271
  - 1.6139371871948243
  - 1.6916742384433747
  - 2.0305841445922854
  - 1.6709452629089356
  - 1.7046828329563142
  - 1.696484535932541
  - 2.3984428465366365
  - 1.6405527710914614
  - 1.6198362708091736
  - 2.5712160170078278
  - 1.9647773861885072
  - 1.9093373775482179
  - 1.7669541001319886
  - 1.6478363156318665
  - 1.6771665275096894
  - 1.6600091457366943
  - 1.5833924561738968
  - 2.7436284840106966
  validation_losses:
  - 0.3905840814113617
  - 0.4199228882789612
  - 0.39164793491363525
  - 0.39510971307754517
  - 0.39735540747642517
  - 0.3812658488750458
  - 0.39528754353523254
  - 0.4158656597137451
  - 0.38090720772743225
  - 0.3942875862121582
  - 0.4040759205818176
  - 0.39567020535469055
  - 0.40969452261924744
  - 0.41304969787597656
  - 0.9587807655334473
  - 0.39816004037857056
  - 0.5557787418365479
  - 0.3951340317726135
  - 0.4069026708602905
  - 0.3899807631969452
  - 0.3974335193634033
  - 0.39789676666259766
  - 0.3911212384700775
  - 0.42047202587127686
  - 0.4077088534832001
  - 0.4024994671344757
  - 4.871468544006348
  - 0.4050666391849518
  - 0.42566898465156555
  - 0.4054522216320038
  - 0.393765926361084
  - 0.39724019169807434
  - 0.3889196515083313
  - 0.4068363904953003
  - 0.40574267506599426
  - 0.4066125452518463
  - 0.44076424837112427
  - 0.3952105641365051
  - 0.38149577379226685
  - 0.4099322259426117
  - 0.4147174656391144
  - 0.41370463371276855
  - 0.4321766197681427
  - 0.4032497704029083
  - 0.4027739465236664
  - 0.39550477266311646
  - 0.39807820320129395
  - 0.40486887097358704
  - 0.3962786793708801
loss_records_fold3:
  train_losses:
  - 1.6301224052906038
  - 1.6753141164779664
  - 1.5895219266414644
  - 1.6682838916778566
  - 1.5626023590564728
  - 1.8786431074142458
  - 1.6454110622406006
  - 1.6405798733234407
  - 1.651401698589325
  - 1.6263541996479036
  - 1.6868107855319978
  - 1.5635560154914856
  - 2.4594926297664643
  - 1.5683816879987718
  - 1.6035699069499971
  - 1.9155589759349825
  - 1.651293969154358
  - 1.594169706106186
  - 1.672573733329773
  - 1.9409214437007904
  - 1.731255891919136
  - 1.8622089982032777
  - 2.136887210607529
  - 1.9989929884672166
  - 2.263559067249298
  - 1.9577396273612977
  - 1.6965297222137452
  - 1.7114043831825256
  - 1.640701311826706
  - 2.4541449666023256
  - 1.7247297942638398
  - 1.6912003934383393
  - 1.6502945840358736
  - 1.676063084602356
  - 1.6497269690036775
  - 1.5833414673805237
  - 1.6409731030464174
  - 1.7246434271335602
  - 1.6843273878097536
  - 1.6405900120735168
  - 1.7182936251163483
  - 1.60157987177372
  - 1.732532948255539
  - 1.691474610567093
  - 1.6391643464565278
  - 1.5740971803665162
  - 1.6018096506595612
  - 1.625755023956299
  - 1.6793544530868532
  - 1.61240314245224
  - 1.6153987228870392
  - 1.631050479412079
  validation_losses:
  - 0.4065706729888916
  - 0.411182701587677
  - 0.45329439640045166
  - 0.4201156497001648
  - 0.4215250611305237
  - 0.41669943928718567
  - 0.4109605550765991
  - 0.41070541739463806
  - 0.40402254462242126
  - 0.4316573441028595
  - 0.41106775403022766
  - 0.45860445499420166
  - 0.4190922677516937
  - 0.4088299572467804
  - 0.41147589683532715
  - 0.42435163259506226
  - 0.42464855313301086
  - 0.4220515191555023
  - 0.40870481729507446
  - 0.4321846067905426
  - 0.4569254219532013
  - 0.4440421462059021
  - 0.42117106914520264
  - 0.4488694965839386
  - 0.40242496132850647
  - 0.4765763282775879
  - 0.4004920721054077
  - 0.4141737222671509
  - 0.40660250186920166
  - 0.3945518434047699
  - 0.47423994541168213
  - 0.40612098574638367
  - 0.4356952905654907
  - 0.41797125339508057
  - 0.40876781940460205
  - 0.4087977707386017
  - 0.4612690806388855
  - 0.42747631669044495
  - 0.4254422187805176
  - 0.4196203649044037
  - 0.4103713035583496
  - 0.44699129462242126
  - 0.4087102711200714
  - 0.41561517119407654
  - 0.40347158908843994
  - 0.4182884395122528
  - 0.4044857621192932
  - 0.4074515700340271
  - 0.41338199377059937
  - 0.40125197172164917
  - 0.4111037850379944
  - 0.412108838558197
loss_records_fold4:
  train_losses:
  - 1.633042734861374
  - 1.656159943342209
  - 1.7319901645183564
  - 1.678090262413025
  - 1.6341809034347534
  - 1.6477306604385378
  - 1.6191432595252992
  - 1.5594521254301072
  - 1.6332078754901886
  - 1.6273562073707581
  - 1.5966624438762667
  - 1.5968941032886506
  - 1.5900315999984742
  - 1.7229097068309784
  - 1.6925478756427765
  - 1.688841211795807
  - 1.6516255497932435
  - 1.5873037904500962
  - 1.648366630077362
  - 1.6638057291507722
  - 1.6269727110862733
  - 1.6481433153152467
  - 1.647030383348465
  - 1.7543667256832123
  - 1.6616037786006927
  - 1.6004173278808596
  - 1.5929696440696717
  - 1.6297974646091462
  - 1.6966727435588838
  - 1.6092282593250276
  - 1.6213909447193147
  - 1.5987720906734468
  validation_losses:
  - 0.3963056802749634
  - 0.39149847626686096
  - 0.4200471341609955
  - 0.4000104069709778
  - 0.39768165349960327
  - 0.3992959260940552
  - 0.39348965883255005
  - 0.4240562319755554
  - 0.4112899601459503
  - 0.3924941420555115
  - 0.3991726040840149
  - 0.42247655987739563
  - 0.40893638134002686
  - 0.40004000067710876
  - 0.4114258289337158
  - 0.4300052523612976
  - 0.4157869517803192
  - 0.39445510506629944
  - 0.39901894330978394
  - 0.421989768743515
  - 0.40248748660087585
  - 0.4034474194049835
  - 0.42175301909446716
  - 0.4033423960208893
  - 0.3959371745586395
  - 0.41199982166290283
  - 0.41464656591415405
  - 0.41834449768066406
  - 0.4047355055809021
  - 0.4065450131893158
  - 0.4002581834793091
  - 0.3978149890899658
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 47 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 49 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 52 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 32 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:17:13.999562'
