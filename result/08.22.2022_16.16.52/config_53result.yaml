config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:18:08.832417'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_53fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_53fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_53fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_53fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_53fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_53fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_53fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_53fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_53fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_53fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.5189335227012637
  - 3.3167510211467746
  - 3.2330006599426273
  - 3.2066375434398653
  - 3.142806112766266
  - 3.136839210987091
  - 3.1428338229656223
  - 3.1000104516744615
  - 3.02235706448555
  - 3.0217861890792848
  - 3.077249950170517
  - 3.076255458593369
  - 3.1110423862934113
  - 3.013742357492447
  - 3.0083840966224673
  - 3.047890967130661
  - 3.1892546057701114
  - 3.2361756652593616
  - 3.069323033094406
  - 3.027239179611206
  - 3.0316783398389817
  - 3.0996059596538545
  - 2.99759556055069
  - 3.03206724524498
  - 3.0624381780624392
  - 2.9332919955253605
  validation_losses:
  - 0.4415014684200287
  - 0.4246051609516144
  - 0.42770567536354065
  - 0.3987763524055481
  - 0.40567144751548767
  - 0.3912717401981354
  - 0.3980773091316223
  - 0.39816033840179443
  - 0.43291157484054565
  - 0.39259201288223267
  - 0.4437006711959839
  - 0.3955127000808716
  - 0.3938806653022766
  - 0.44094911217689514
  - 0.3846961557865143
  - 0.44351112842559814
  - 0.39721694588661194
  - 0.39265406131744385
  - 0.395979642868042
  - 0.42812687158584595
  - 0.415793776512146
  - 0.3939030170440674
  - 0.40000617504119873
  - 0.3975220024585724
  - 0.38747647404670715
  - 0.39492520689964294
loss_records_fold1:
  train_losses:
  - 3.1254771888256077
  - 3.11106173992157
  - 3.0016882836818697
  - 2.955732822418213
  - 3.1486586153507234
  - 3.129673734307289
  - 3.077323079109192
  - 3.0374624073505405
  - 3.0093251496553424
  - 3.0342272162437443
  - 2.9528841912746433
  - 3.021286773681641
  - 3.0149986863136293
  - 2.943254679441452
  - 2.9836756825447086
  - 2.994257128238678
  - 3.065529239177704
  - 3.0159375727176667
  - 3.0617319107055665
  - 2.96728630065918
  - 2.9821917831897737
  - 2.9481959700584413
  - 3.0143094182014467
  - 3.09873380959034
  - 3.0541168212890626
  - 3.0061221778392793
  - 3.002895271778107
  - 3.020236736536026
  - 2.9768953144550325
  - 2.970867627859116
  - 2.9916285008192065
  - 3.0266551434993745
  - 3.015132135152817
  - 2.98171558380127
  - 2.980707108974457
  - 2.9809753179550174
  - 2.9935615181922914
  - 2.9824999928474427
  - 3.0298725903034214
  - 2.933029913902283
  - 2.9123371779918674
  validation_losses:
  - 0.4043686091899872
  - 0.3926849663257599
  - 0.4291231632232666
  - 0.40141400694847107
  - 0.5948864221572876
  - 0.40316042304039
  - 0.40062713623046875
  - 0.3943846821784973
  - 0.4042966663837433
  - 0.4005053639411926
  - 0.41154640913009644
  - 0.3906756341457367
  - 0.4275321364402771
  - 0.4153038263320923
  - 0.3935738801956177
  - 0.4043203890323639
  - 0.4171907901763916
  - 0.3888307213783264
  - 0.39591073989868164
  - 0.48663100600242615
  - 0.3904688060283661
  - 0.40885162353515625
  - 0.4108917713165283
  - 0.40686529874801636
  - 0.3873830735683441
  - 0.4069001078605652
  - 0.40576332807540894
  - 0.3966463804244995
  - 0.40095409750938416
  - 0.39180052280426025
  - 0.40208345651626587
  - 0.4030335545539856
  - 0.3968215584754944
  - 0.39884066581726074
  - 0.41376763582229614
  - 0.3973405063152313
  - 0.40096554160118103
  - 0.40689533948898315
  - 0.4097347557544708
  - 0.41733768582344055
  - 0.3914934992790222
loss_records_fold2:
  train_losses:
  - 3.0805393338203433
  - 2.969643169641495
  - 3.0361263096332554
  - 2.9991332799196244
  - 2.9165099978446962
  - 2.948173463344574
  - 2.9583653271198274
  - 3.0184438228607178
  - 2.9176220238208774
  - 2.9681755363941194
  - 2.934928470849991
  - 2.991230481863022
  - 2.9740506768226624
  - 2.9144864201545717
  - 2.9643727362155916
  - 2.9463529109954836
  - 2.892807787656784
  - 2.962485945224762
  - 2.987537479400635
  - 3.000970995426178
  - 2.9203230649232865
  - 2.9311447888612747
  - 2.987424659729004
  - 2.9672053098678592
  - 2.955857288837433
  - 2.9444385290145876
  - 2.9150642275810243
  - 2.9999843001365663
  - 2.957523500919342
  - 2.929539102315903
  - 2.9747364401817324
  - 2.9046176433563233
  - 2.9913287281990053
  - 2.8775844991207125
  - 2.8927723288536074
  - 2.9305226087570193
  - 2.9506024658679966
  - 2.984497994184494
  - 2.9329611361026764
  - 2.8600538969039917
  - 2.9178655683994297
  - 3.0065735161304477
  - 2.9633762478828434
  - 2.8851345002651216
  - 3.0024820208549503
  - 2.996248531341553
  - 2.982733064889908
  - 2.923927143216133
  - 2.9162102878093723
  - 2.881045582890511
  - 2.8978226423263553
  - 2.880499202013016
  - 2.99985907971859
  - 2.9171521723270417
  - 2.9279392898082737
  - 2.842584526538849
  - 2.9305300533771517
  - 2.929266381263733
  - 2.9484835028648377
  - 2.891284316778183
  - 2.8750180780887606
  - 2.8898089647293093
  - 2.9127274781465533
  - 2.8932507544755937
  - 2.835633555054665
  - 2.94382501244545
  - 2.7847650527954104
  - 2.8547642886638642
  - 2.938408762216568
  - 2.923098081350327
  - 2.9075176626443864
  - 2.951953905820847
  - 2.93987666964531
  - 2.9567801088094714
  - 2.901450121402741
  - 2.907504948973656
  - 2.877767461538315
  - 2.8669822782278063
  - 2.9393878102302553
  - 2.868451550602913
  - 2.886222490668297
  - 2.8027117967605593
  - 2.891212606430054
  - 2.79765414595604
  - 2.7963216185569766
  - 2.849101030826569
  - 2.8750267386436463
  - 2.822761404514313
  - 2.835016041994095
  - 2.9375480204820636
  - 2.796577084064484
  - 2.821181383728981
  - 2.848299312591553
  - 2.8047653406858446
  - 2.876665645837784
  - 2.9293846398591996
  - 2.941606777906418
  - 2.8734083056449893
  - 2.8881641387939454
  - 2.886821922659874
  validation_losses:
  - 0.3831753134727478
  - 0.3829811215400696
  - 0.39221036434173584
  - 0.3856506645679474
  - 0.40754958987236023
  - 0.3951515555381775
  - 0.40365323424339294
  - 0.38351261615753174
  - 0.3800002336502075
  - 0.39062994718551636
  - 0.39300182461738586
  - 0.5020803213119507
  - 0.5330645442008972
  - 0.7974721789360046
  - 1.0429731607437134
  - 0.5437765717506409
  - 0.42279523611068726
  - 0.44478869438171387
  - 0.3942112326622009
  - 0.6606989502906799
  - 0.5225418210029602
  - 1.2629241943359375
  - 0.3795950412750244
  - 0.6250569820404053
  - 0.4605292081832886
  - 0.5401424169540405
  - 0.640032172203064
  - 0.5574474334716797
  - 0.38535913825035095
  - 0.37989485263824463
  - 0.40924403071403503
  - 0.4923168122768402
  - 0.39867889881134033
  - 0.3926060199737549
  - 0.5284208059310913
  - 0.7130316495895386
  - 0.38746756315231323
  - 0.5089273452758789
  - 0.4036486744880676
  - 1.0558830499649048
  - 2.2344517707824707
  - 0.7241913676261902
  - 0.4350396692752838
  - 0.6889364123344421
  - 0.41783979535102844
  - 1.053312063217163
  - 1.9804562330245972
  - 0.6588359475135803
  - 0.46420347690582275
  - 0.9304500222206116
  - 1.1229497194290161
  - 0.9913287162780762
  - 0.4273972511291504
  - 0.5163766741752625
  - 0.8830772042274475
  - 0.38429954648017883
  - 0.6204281449317932
  - 1.590660810470581
  - 1.1962101459503174
  - 0.3818780481815338
  - 0.390573114156723
  - 0.5832867622375488
  - 0.6549673080444336
  - 0.9104248285293579
  - 0.47220367193222046
  - 0.5978056788444519
  - 0.6325028538703918
  - 0.6722774505615234
  - 0.397368848323822
  - 0.383845716714859
  - 0.3964172899723053
  - 0.405828058719635
  - 0.4000619351863861
  - 0.45189860463142395
  - 0.3897581100463867
  - 0.5002773404121399
  - 0.5283142924308777
  - 0.48527032136917114
  - 0.546407163143158
  - 0.5557319521903992
  - 0.624371349811554
  - 0.5397538542747498
  - 0.3936624526977539
  - 0.6639463305473328
  - 0.9250285625457764
  - 0.561138927936554
  - 0.8165728449821472
  - 1.2248456478118896
  - 0.8622711300849915
  - 0.6976830959320068
  - 1.779414415359497
  - 0.7121890187263489
  - 0.6590733528137207
  - 1.0055344104766846
  - 0.4496438503265381
  - 0.5663696527481079
  - 0.5392360687255859
  - 0.5378422141075134
  - 0.7856433987617493
  - 0.8164419531822205
loss_records_fold3:
  train_losses:
  - 2.839517050981522
  - 2.815930813550949
  - 2.81411312520504
  - 2.8582239508628846
  - 2.949837499856949
  - 2.9021851420402527
  - 2.8938288390636444
  - 3.161025494337082
  - 2.933059883117676
  - 2.9432379961013795
  - 2.9171977043151855
  - 2.8920461744070054
  - 2.9374662935733795
  - 2.9265846550464634
  - 2.9738802254199985
  - 2.9215303003788
  - 2.9423752784729005
  - 2.968410408496857
  - 2.9525255978107454
  - 2.9301025927066804
  - 2.907378423213959
  - 2.887918689846993
  - 2.859903982281685
  - 2.9105679988861084
  - 2.8646580040454865
  - 2.904341691732407
  - 2.9065268725156788
  - 3.003742915391922
  - 2.8508105874061584
  - 2.8799167662858967
  - 2.8682521998882295
  - 2.9215003848075867
  - 2.801564007997513
  - 2.8630006790161135
  - 2.8921806514263153
  - 2.918348115682602
  - 2.93681021630764
  - 2.861725664138794
  - 2.944023343920708
  - 2.8610094070434573
  - 2.8224407702684404
  - 2.8407510340213777
  - 2.911344885826111
  - 2.8523594021797183
  - 2.8804001808166504
  - 2.8271288812160495
  - 2.8442654818296433
  - 2.8677217066287994
  - 2.813087260723114
  - 2.844503700733185
  - 2.881486648321152
  - 2.87508197426796
  - 2.8215881764888766
  validation_losses:
  - 1.0551923513412476
  - 0.7876262068748474
  - 10.748828887939453
  - 0.6007422804832458
  - 0.46995308995246887
  - 0.4662003517150879
  - 1.0849741697311401
  - 0.49151110649108887
  - 0.5226060748100281
  - 0.5242141485214233
  - 0.5411529541015625
  - 0.5496622323989868
  - 0.547633171081543
  - 3.253140687942505
  - 0.8080997467041016
  - 0.596012532711029
  - 0.6290035843849182
  - 0.742451548576355
  - 0.6789706945419312
  - 0.742878258228302
  - 0.7692723870277405
  - 0.6689189672470093
  - 0.9212194681167603
  - 0.8152925372123718
  - 2.056849718093872
  - 4.0601582527160645
  - 1.4900803565979004
  - 0.38705939054489136
  - 0.937705934047699
  - 1.3218064308166504
  - 0.7253758907318115
  - 0.7964251041412354
  - 1.7313547134399414
  - 1.4764920473098755
  - 0.7925766110420227
  - 0.502974271774292
  - 0.7735046148300171
  - 0.5432003140449524
  - 0.5325170755386353
  - 0.6283815503120422
  - 0.5446458458900452
  - 0.6713651418685913
  - 0.677331805229187
  - 0.8062902688980103
  - 0.6650286912918091
  - 0.6150835156440735
  - 0.9893560409545898
  - 0.7887799739837646
  - 0.7619426846504211
  - 0.6626409292221069
  - 0.6293662190437317
  - 0.6115422248840332
  - 0.5690152049064636
loss_records_fold4:
  train_losses:
  - 2.9072589844465258
  - 2.845324152708054
  - 2.8295871108770374
  - 2.829201063513756
  - 2.9864817708730698
  - 2.996457573771477
  - 2.8920267283916474
  - 2.84247339963913
  - 2.9042142599821092
  - 2.787164679169655
  - 2.8320729911327365
  - 2.8440992534160614
  - 2.821309131383896
  - 2.765256625413895
  - 2.7649084329605103
  - 2.8791221499443056
  - 2.8617424428462983
  - 2.885082319378853
  - 2.922471335530281
  - 2.8974167555570602
  - 2.893633979558945
  - 2.839386770129204
  - 2.9203526914119724
  - 2.856203901767731
  - 2.838267314434052
  - 2.8097896158695224
  - 2.8616168797016144
  - 2.8306983768939973
  - 2.8684042155742646
  - 2.7953648656606678
  - 2.7916126608848573
  - 2.829976144433022
  - 2.838264226913452
  - 2.878769552707672
  - 2.8791218400001526
  - 2.849302536249161
  - 2.845875543355942
  - 2.8273036539554597
  - 2.813171237707138
  - 2.840939521789551
  - 2.8028491675853733
  - 2.86846986413002
  - 2.789278444647789
  - 2.7655187606811524
  - 2.8021796703338624
  - 2.796520107984543
  - 2.8805472254753113
  - 2.935004633665085
  - 2.8123928159475327
  - 2.8521228283643723
  - 2.80070937871933
  - 2.8257750898599627
  - 2.660154038667679
  - 2.8621143102645874
  - 2.9057285249233247
  - 2.6865596652030947
  - 2.842153710126877
  - 2.8017292559146885
  - 2.701529583334923
  - 2.8871657431125644
  - 2.8213954448699954
  - 2.9698722660541534
  - 2.8440436989068987
  - 2.828641599416733
  - 2.762384158372879
  - 2.77243909239769
  - 2.7919418990612033
  - 2.7322710156440735
  - 2.782434916496277
  - 2.868181437253952
  - 2.7754421114921572
  - 2.7954630225896837
  - 2.765572762489319
  - 2.737311300635338
  - 2.7973772972822193
  - 2.8424212217330935
  - 2.818745270371437
  - 2.8313129365444185
  - 2.7629938244819643
  - 2.837354558706284
  - 2.804336810112
  - 2.867422187328339
  - 2.88615782558918
  - 2.9255299866199493
  - 2.8825694561004642
  - 2.8616502344608308
  - 2.8200047314167023
  - 2.9079879343509676
  - 2.8376884281635286
  - 2.861276239156723
  - 2.823294794559479
  - 2.808979645371437
  - 2.8079836040735247
  - 2.807038736343384
  - 2.8533249676227572
  - 2.748255056142807
  - 2.8682335913181305
  - 2.843576729297638
  - 2.7680279135704042
  - 2.8040496557950974
  validation_losses:
  - 0.4787004590034485
  - 0.4948789179325104
  - 0.4781273901462555
  - 0.5847913026809692
  - 0.4438505470752716
  - 0.45402541756629944
  - 0.5963727235794067
  - 0.477254182100296
  - 0.5500893592834473
  - 0.6843763589859009
  - 0.44943147897720337
  - 0.5654658079147339
  - 0.5325446724891663
  - 0.5744248628616333
  - 0.5141053795814514
  - 0.7459256649017334
  - 2.022905111312866
  - 0.570597767829895
  - 0.874099612236023
  - 0.657931387424469
  - 0.5300476551055908
  - 0.613264799118042
  - 0.7182746529579163
  - 0.6488859057426453
  - 0.5637763142585754
  - 0.5322679877281189
  - 0.6257023811340332
  - 0.6011903285980225
  - 0.5135381817817688
  - 0.5895117521286011
  - 0.5200119614601135
  - 0.5885897874832153
  - 0.6930219531059265
  - 0.7300114631652832
  - 0.6193504929542542
  - 0.5617154836654663
  - 0.6209080219268799
  - 0.6859117150306702
  - 0.5043439865112305
  - 0.5579727292060852
  - 0.7753477692604065
  - 0.6030840277671814
  - 0.6896726489067078
  - 0.45739829540252686
  - 0.46880999207496643
  - 0.5875966548919678
  - 0.4976707696914673
  - 0.7009680271148682
  - 0.6339772343635559
  - 0.6282827854156494
  - 0.5539360642433167
  - 0.521755039691925
  - 0.7126085758209229
  - 0.5878944396972656
  - 0.5197913646697998
  - 0.7257860898971558
  - 0.7562365531921387
  - 0.5197116136550903
  - 0.6125772595405579
  - 0.5296869874000549
  - 0.48073530197143555
  - 0.4232020080089569
  - 0.4857326149940491
  - 0.6875096559524536
  - 0.4345925450325012
  - 0.518337070941925
  - 0.7074853181838989
  - 0.6420845985412598
  - 0.5264714360237122
  - 0.5894011855125427
  - 0.46605250239372253
  - 0.49951624870300293
  - 0.5378544926643372
  - 0.5868114233016968
  - 0.5211524963378906
  - 0.47525036334991455
  - 0.5138459801673889
  - 0.5369997620582581
  - 0.6108387112617493
  - 0.49128958582878113
  - 0.5983731746673584
  - 0.5725244283676147
  - 0.49919646978378296
  - 0.3858723044395447
  - 0.35898059606552124
  - 0.440273642539978
  - 0.3947497010231018
  - 0.38857385516166687
  - 0.3760676085948944
  - 0.39550766348838806
  - 0.5358222126960754
  - 0.4525168538093567
  - 0.40304887294769287
  - 0.37119632959365845
  - 0.360167533159256
  - 0.41152289509773254
  - 0.5175817012786865
  - 0.47407054901123047
  - 0.5016432404518127
  - 0.48498037457466125
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 41 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 53 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8061749571183533, 0.8096054888507719,
    0.8333333333333334]'
  fold_eval_f1: '[0.0, 0.023255813953488372, 0.34682080924855485, 0.12598425196850394,
    0.12612612612612614]'
  mean_eval_accuracy: 0.8325328759291024
  mean_f1_accuracy: 0.12443740025933465
  total_train_time: '0:30:27.260119'
