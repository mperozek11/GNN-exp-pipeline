config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:21:36.387980'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_125fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_125fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_125fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_125fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_125fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_125fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_125fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_125fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_125fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_125fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 52.27574157714844
  - 31.46597580313683
  - 16.598092472553255
  - 12.438060170412065
  - 21.769618046283725
  - 20.048756617307664
  - 15.53639048933983
  - 12.000182148814202
  - 23.590032309293747
  - 19.78646969795227
  - 8.055258148908615
  - 6.458512288331986
  - 16.17641189992428
  - 23.664728021621706
  - 18.697135311365127
  - 14.728055101633073
  - 9.949717289209367
  - 4.81881799697876
  - 7.811726343631745
  - 13.711799281835557
  - 7.996192061901093
  - 10.175657790899278
  - 8.090332287549973
  - 5.071332237124444
  - 6.91193682551384
  - 8.376239487528801
  - 11.205808645486833
  - 10.674971133470535
  - 10.988476687669754
  - 5.192167353630066
  - 10.745583683252335
  - 5.770526903867722
  - 6.399742564558983
  - 6.695138931274414
  - 6.199362868070603
  - 9.270909363031388
  - 6.701271280646324
  - 8.145681977272034
  - 3.98844268321991
  - 6.366959339380265
  - 4.659026423096657
  - 8.044669231772422
  - 7.1558510065078735
  - 4.7637364864349365
  - 11.543292647600175
  - 9.666597312688829
  - 7.200427722930908
  - 4.94394913315773
  - 5.060325485467911
  - 4.991256076097489
  - 7.01341382265091
  - 8.371492683887482
  - 4.295027315616608
  - 7.669269573688507
  - 5.299465447664261
  - 4.006364393234253
  - 3.487773221731186
  - 5.245863234996796
  - 5.966081476211548
  - 4.09866658449173
  - 3.8770302295684815
  - 3.1582063376903537
  - 3.7695117712020876
  - 4.207579523324966
  - 3.237725901603699
  - 3.471226078271866
  - 3.624860402941704
  - 3.6452557265758516
  - 3.9955880641937256
  - 3.243314266204834
  - 4.261075025796891
  - 3.9487193644046785
  - 3.4951176702976228
  - 3.1599164783954623
  - 3.0263398587703705
  - 3.3205955177545547
  - 3.2995307028293612
  - 3.260286816954613
  - 3.2731376826763157
  - 4.077886199951172
  - 3.462015995383263
  - 3.941780722141266
  - 3.4696564376354218
  - 3.2267015039920808
  - 3.266970890760422
  - 3.547299283742905
  - 3.1840882122516634
  validation_losses:
  - 11.14480209350586
  - 1.9082316160202026
  - 1.0190651416778564
  - 0.7126007676124573
  - 0.6680059432983398
  - 2.3818562030792236
  - 0.7436965703964233
  - 0.8048218488693237
  - 0.8090658783912659
  - 0.6471028923988342
  - 0.5710455179214478
  - 0.6786876320838928
  - 0.7166330814361572
  - 0.9994351267814636
  - 0.9021268486976624
  - 0.5336348414421082
  - 0.44823575019836426
  - 0.4668302834033966
  - 0.4440953731536865
  - 0.45654717087745667
  - 0.5096288323402405
  - 0.43920060992240906
  - 0.44734394550323486
  - 0.42023172974586487
  - 0.40209078788757324
  - 0.4881000518798828
  - 0.39987632632255554
  - 0.4707752764225006
  - 0.3979187309741974
  - 0.4598919153213501
  - 0.3989463150501251
  - 0.4000527262687683
  - 0.43680527806282043
  - 0.4468699097633362
  - 0.428269624710083
  - 0.3953739404678345
  - 0.404496431350708
  - 0.39287295937538147
  - 0.3890204429626465
  - 0.4644874930381775
  - 0.49709978699684143
  - 0.6094139218330383
  - 0.4807336926460266
  - 0.40446391701698303
  - 0.7269781231880188
  - 0.4253345727920532
  - 0.3900577425956726
  - 0.39527660608291626
  - 0.38606512546539307
  - 0.4113336503505707
  - 0.4038902223110199
  - 0.3949560821056366
  - 0.40338853001594543
  - 0.7982190251350403
  - 0.4624476730823517
  - 0.4599912166595459
  - 0.4041951894760132
  - 0.40100187063217163
  - 0.460464209318161
  - 0.4153411090373993
  - 0.4219134449958801
  - 0.3954710364341736
  - 0.5682533383369446
  - 0.4154963493347168
  - 0.393703818321228
  - 0.3829209506511688
  - 0.4105220437049866
  - 0.40005943179130554
  - 0.3870090842247009
  - 0.4010132849216461
  - 0.4010887145996094
  - 0.3811016380786896
  - 0.41059163212776184
  - 0.38144218921661377
  - 0.40003132820129395
  - 0.41704991459846497
  - 0.416507363319397
  - 0.47546786069869995
  - 0.3940134048461914
  - 0.40691322088241577
  - 0.5988112092018127
  - 0.4264344274997711
  - 0.416103720664978
  - 0.41328492760658264
  - 0.3848513960838318
  - 0.3861124813556671
  - 0.3958849310874939
loss_records_fold1:
  train_losses:
  - 3.2402986347675324
  - 3.265703779459
  - 3.468689388036728
  - 3.8588556468486788
  - 4.295016321539879
  - 3.3607256382703783
  - 3.4179770708084107
  - 3.4814692318439486
  - 3.0164914190769196
  - 3.427861112356186
  - 3.283937394618988
  - 3.28012415766716
  - 3.0563393712043765
  - 3.1790990829467773
  - 3.2271128684282306
  - 3.399691012501717
  - 3.2398096829652787
  - 3.0851192712783817
  - 3.3931384682655334
  - 3.2173813432455063
  - 3.1401410043239597
  - 3.0280297636985782
  - 3.128905287384987
  - 3.0936571776866915
  - 3.1336802661418917
  - 3.326030144095421
  - 3.071415859460831
  - 3.200354439020157
  - 3.1237789154052735
  - 3.044579777121544
  - 3.0956497818231585
  - 3.0130488842725756
  - 3.056123355031014
  - 3.1382146894931795
  - 3.117425000667572
  - 3.1027482539415363
  - 3.1553178250789644
  - 3.255462044477463
  - 3.161671316623688
  - 2.983434367179871
  - 3.1469645857810975
  - 3.0051882803440098
  - 2.982495748996735
  - 2.9268705725669864
  - 3.0026791930198673
  - 2.976003995537758
  - 3.0649783611297607
  - 3.1131989866495133
  - 3.1037816345691684
  - 3.564118891954422
  - 3.274901679158211
  - 4.506627994775772
  - 3.415483379364014
  - 3.9079082906246185
  - 3.430306720733643
  - 3.500950288772583
  - 3.0538000106811527
  - 3.110313630104065
  - 3.269454258680344
  - 2.988006961345673
  - 3.3131099492311478
  - 5.163337200880051
  - 5.380381536483765
  - 3.5254165768623356
  - 3.432353687286377
  - 3.439216506481171
  - 3.1990544289350513
  - 3.4401048183441163
  - 3.3492499530315403
  - 3.3171079754829407
  - 4.926968163251877
  - 8.780270612239837
  - 3.627650934457779
  - 4.597811609506607
  - 3.7066197723150256
  - 3.1375567615032196
  - 3.4724117070436478
  - 3.6320324659347536
  - 3.3760602653026583
  - 3.3806420981884004
  - 3.183020091056824
  - 3.3455363869667054
  - 3.3338499009609226
  - 3.338963055610657
  - 3.408961695432663
  - 3.2306940793991092
  - 3.239763081073761
  - 3.414145529270172
  - 3.131772974133492
  - 3.4290800571441653
  - 3.283530554175377
  - 3.3411021411418917
  - 3.30896053314209
  - 3.237413889169693
  - 3.598791551589966
  - 3.224512314796448
  - 3.1733348309993747
  - 3.243215024471283
  - 3.184777331352234
  - 3.9228323042392734
  validation_losses:
  - 0.42190515995025635
  - 0.3956051766872406
  - 0.552567720413208
  - 0.40487855672836304
  - 0.42192569375038147
  - 0.43172886967658997
  - 0.4184741973876953
  - 0.39645707607269287
  - 0.40614232420921326
  - 0.43011730909347534
  - 0.4186776280403137
  - 0.4089919626712799
  - 0.41050589084625244
  - 0.40702396631240845
  - 0.4690387547016144
  - 0.44355034828186035
  - 0.3976402282714844
  - 0.4319433271884918
  - 0.42863988876342773
  - 0.42467761039733887
  - 0.39745011925697327
  - 0.41652631759643555
  - 0.3871511220932007
  - 0.43003493547439575
  - 0.5599390864372253
  - 0.44720160961151123
  - 0.44691532850265503
  - 0.4979265034198761
  - 0.4114949107170105
  - 1.0466994047164917
  - 0.4131788909435272
  - 0.43469223380088806
  - 0.42756786942481995
  - 0.45726513862609863
  - 0.44693490862846375
  - 0.5258736610412598
  - 0.3948926627635956
  - 0.661557674407959
  - 0.41214504837989807
  - 0.41449931263923645
  - 0.4870133399963379
  - 0.40578189492225647
  - 0.4009677469730377
  - 0.4125714898109436
  - 0.4206268787384033
  - 0.3955414891242981
  - 0.424909383058548
  - 0.5095300674438477
  - 0.4169761538505554
  - 26.437480926513672
  - 0.4208083748817444
  - 0.822840690612793
  - 0.4006079435348511
  - 0.4549039304256439
  - 0.4359174966812134
  - 0.407941073179245
  - 0.39981603622436523
  - 0.4702169895172119
  - 0.4504327178001404
  - 0.4972246289253235
  - 0.4630531072616577
  - 0.4995056986808777
  - 0.488003671169281
  - 0.4167099893093109
  - 0.45434415340423584
  - 0.4155273735523224
  - 0.416422039270401
  - 0.43360039591789246
  - 0.4147104322910309
  - 0.8071774840354919
  - 7.217540740966797
  - 0.5185693502426147
  - 0.43657705187797546
  - 0.4183824956417084
  - 0.40572723746299744
  - 0.4093833565711975
  - 0.44447001814842224
  - 0.4224582612514496
  - 0.42037636041641235
  - 0.42072224617004395
  - 0.42356809973716736
  - 0.4661407470703125
  - 0.40927281975746155
  - 0.4186171293258667
  - 0.46775299310684204
  - 0.4124049246311188
  - 0.4194701313972473
  - 0.42165279388427734
  - 0.41225147247314453
  - 0.4357668459415436
  - 0.44043999910354614
  - 0.41739609837532043
  - 0.41319507360458374
  - 0.42070671916007996
  - 0.44318240880966187
  - 0.44501569867134094
  - 0.43083813786506653
  - 0.4165278673171997
  - 0.42059388756752014
  - 0.4064708948135376
loss_records_fold2:
  train_losses:
  - 3.303358513116837
  - 3.443909162282944
  - 3.3937118738889698
  - 3.230533814430237
  - 3.32301199734211
  - 3.289242881536484
  - 3.1893864631652833
  - 3.3632477551698687
  - 3.6460883915424347
  - 3.3943326711654667
  - 3.383224141597748
  - 3.481236308813095
  - 3.114720153808594
  - 3.2032482832670213
  - 3.1231264114379886
  - 3.145249974727631
  - 3.316751050949097
  - 3.2856216669082645
  - 3.2588406205177307
  - 3.194917470216751
  - 3.3020336866378788
  - 3.4357640504837037
  - 3.406466728448868
  - 3.3105510234832765
  - 3.3561597883701326
  - 3.208463373780251
  - 3.2144518673419955
  - 3.1870435506105426
  validation_losses:
  - 0.5107899904251099
  - 0.3975105583667755
  - 0.4636880159378052
  - 0.4147980809211731
  - 0.41776177287101746
  - 0.4188316762447357
  - 0.4018422067165375
  - 0.4041958451271057
  - 0.41148611903190613
  - 0.40805310010910034
  - 0.44972336292266846
  - 0.500312328338623
  - 0.39789536595344543
  - 0.403561532497406
  - 0.39813488721847534
  - 0.40983134508132935
  - 0.39736682176589966
  - 0.4192798137664795
  - 0.4043941795825958
  - 0.39965367317199707
  - 0.3917592167854309
  - 0.4405372142791748
  - 0.3956834375858307
  - 0.40386900305747986
  - 0.40316855907440186
  - 0.40282386541366577
  - 0.39170411229133606
  - 0.40147700905799866
loss_records_fold3:
  train_losses:
  - 3.274639278650284
  - 3.282636114954949
  - 3.2004218041896824
  - 3.188564270734787
  - 3.1633101493120197
  - 3.3300941407680513
  - 3.21867133975029
  - 3.1434385001659395
  - 3.2507690608501436
  - 3.229485619068146
  - 3.2581174492836
  - 3.221801716089249
  - 3.246602523326874
  - 3.315212017297745
  - 3.192897927761078
  - 3.254868358373642
  - 3.244629287719727
  - 3.134809392690659
  - 3.205309116840363
  - 3.380525088310242
  - 3.338624227046967
  - 3.1535985589027407
  - 3.3893109619617463
  - 3.21511447429657
  - 3.30137734413147
  - 3.141064497828484
  - 3.1998186945915226
  - 3.1633144050836566
  - 3.1790319085121155
  - 3.1763767242431644
  validation_losses:
  - 0.42525050044059753
  - 0.438774436712265
  - 0.4080520272254944
  - 0.4125484228134155
  - 0.44245636463165283
  - 0.4112641215324402
  - 0.421399861574173
  - 0.4385131597518921
  - 0.4238758087158203
  - 0.40351542830467224
  - 0.4151054918766022
  - 0.4141998589038849
  - 0.4361512064933777
  - 0.4152602255344391
  - 0.40406009554862976
  - 0.44828224182128906
  - 0.4176449477672577
  - 0.4191564917564392
  - 0.4105488955974579
  - 0.47584274411201477
  - 0.45505645871162415
  - 0.4228504002094269
  - 0.41288691759109497
  - 0.42576828598976135
  - 0.4191132187843323
  - 0.4193648099899292
  - 0.41827619075775146
  - 0.4140903651714325
  - 0.418121874332428
  - 0.4112122654914856
loss_records_fold4:
  train_losses:
  - 3.2312442123889924
  - 3.2114596486091616
  - 3.2792813360691073
  - 3.1967278957366947
  - 3.3306798458099367
  - 3.4127292275428776
  - 3.2511382400989532
  - 3.3418165147304535
  - 3.3369737327098847
  - 3.2060698807239536
  - 3.3000427126884464
  - 3.2478366553783418
  - 3.1291716158390046
  - 3.3887065649032593
  - 3.3405449092388153
  - 3.092886906862259
  - 3.2262283205986027
  - 3.2294621527194978
  - 3.1899700939655307
  - 3.268042850494385
  - 3.1601400434970857
  - 3.215130198001862
  - 3.2241619110107425
  - 3.201422333717346
  - 3.181577274203301
  - 3.046911054849625
  - 3.194558966159821
  - 3.225779575109482
  - 3.262528604269028
  - 3.2186550915241243
  - 3.2411747694015505
  - 3.3500032365322117
  - 3.1955421030521394
  - 3.375998938083649
  - 3.26865268945694
  - 3.1538632810115814
  - 3.172766771912575
  - 3.2241310060024264
  - 3.1827814877033234
  - 3.1193460643291475
  - 3.1723777890205387
  - 3.2227280080318454
  - 3.1524109661579134
  - 3.300470694899559
  - 3.142950123548508
  - 3.2075026094913484
  - 3.2139258682727814
  - 7.944959688186646
  - 7.00252098441124
  - 3.678025105595589
  - 3.5191885948181154
  - 3.153423309326172
  - 3.1678384423255923
  - 3.2287135481834413
  - 3.24025958776474
  - 3.309175354242325
  - 3.3217377781867983
  - 3.2473992288112643
  - 3.281584489345551
  - 3.3094145715236665
  - 3.24660764336586
  - 3.182157075405121
  - 3.3203317672014236
  - 3.2032523036003115
  - 3.10024608373642
  - 3.146791270375252
  - 3.276609480381012
  - 3.168949091434479
  - 3.3135155737400055
  - 3.246635764837265
  - 3.081888681650162
  - 3.1512795329093937
  - 3.134179896116257
  - 3.2009355366230015
  - 3.133936643600464
  - 3.2350080966949464
  - 3.1578867197036744
  - 3.1376613259315493
  - 3.2602058887481693
  - 3.209599229693413
  - 3.2196805477142334
  - 3.3017973959445954
  - 3.2636626183986666
  - 3.2947985470294956
  - 3.2009880244731903
  - 3.295580744743347
  - 3.1549770295619965
  - 3.216753149032593
  - 3.1631627023220066
  - 3.4190433382987977
  - 3.1257283806800844
  - 3.2173222303390503
  - 3.3374327570199966
  - 3.1014506399631503
  - 3.277516329288483
  - 3.201476711034775
  - 3.3574861228466037
  - 3.2726547479629517
  - 3.309424036741257
  - 3.2462030917406084
  validation_losses:
  - 0.4196608066558838
  - 0.42349839210510254
  - 0.40109771490097046
  - 0.40425053238868713
  - 0.465047687292099
  - 0.4014272391796112
  - 0.4096165597438812
  - 0.4039638936519623
  - 0.41526657342910767
  - 0.4032971262931824
  - 0.4015575349330902
  - 0.4040676951408386
  - 0.42747974395751953
  - 0.4058137536048889
  - 0.3915056586265564
  - 0.40482857823371887
  - 0.4309898614883423
  - 0.41838955879211426
  - 0.41558220982551575
  - 0.4261976480484009
  - 0.4140390157699585
  - 0.40027737617492676
  - 0.4004715085029602
  - 0.41572362184524536
  - 0.3944005072116852
  - 0.3906514644622803
  - 0.4669879972934723
  - 0.41345280408859253
  - 0.4261605441570282
  - 0.4004516899585724
  - 0.39684441685676575
  - 0.41070830821990967
  - 0.42624005675315857
  - 0.425712525844574
  - 0.4208965003490448
  - 0.4009956419467926
  - 1.0227912664413452
  - 3629.579833984375
  - 1048666.875
  - 0.40972623229026794
  - 0.4182104468345642
  - 0.4126089811325073
  - 0.39962083101272583
  - 0.4061073660850525
  - 0.4237484335899353
  - 0.3982582092285156
  - 0.41983744502067566
  - 251.59735107421875
  - 1755.7689208984375
  - 9.834900856018066
  - 10.727792739868164
  - 9.842119216918945
  - 74.45574951171875
  - 36.051509857177734
  - 5.934294700622559
  - 73.5977783203125
  - 80.54554748535156
  - 54.8456916809082
  - 74.14308166503906
  - 413.3794860839844
  - 17.241708755493164
  - 7.464162826538086
  - 1646.8685302734375
  - 41.63240051269531
  - 5.94993257522583
  - 33.634979248046875
  - 92.56938171386719
  - 1085.8966064453125
  - 333.53265380859375
  - 409.5871887207031
  - 757.1428833007812
  - 29.89613914489746
  - 57.682064056396484
  - 959.5
  - 63.38157272338867
  - 538.2294921875
  - 168.58084106445312
  - 390417.9375
  - 68022.7109375
  - 17330.982421875
  - 1415978.75
  - 1158449.625
  - 582167.0625
  - 1202568.875
  - 326967.75
  - 1168453.125
  - 16401978.0
  - 1432350.75
  - 3633226.5
  - 57.75978469848633
  - 708.77734375
  - 206981.40625
  - 48787320.0
  - 27395492.0
  - 3464398.5
  - 1801826.5
  - 28742298.0
  - 816002.5
  - 1277163.625
  - 113193160.0
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 87 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:20:22.855034'
