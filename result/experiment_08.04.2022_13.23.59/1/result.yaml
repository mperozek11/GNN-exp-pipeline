config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 13:35:12.197822'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/1/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/1/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/1/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/1/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/1/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/1/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/1/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/1/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/1/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/1/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 31.045243471860886
  - 30.067060500383377
  - 30.03745001554489
  - 30.460020154714584
  - 31.26043175160885
  - 30.678686812520027
  - 31.139704674482346
  - 30.284605145454407
  - 30.321559235453606
  - 30.0321224629879
  - 30.150853723287582
  - 30.262567162513733
  - 31.17782251536846
  - 30.796562895178795
  - 30.675176680088043
  - 30.060418531298637
  - 30.688519224524498
  - 30.947092443704605
  - 30.326330840587616
  - 30.210817605257034
  - 29.97966131567955
  - 30.286986231803894
  - 30.418812066316605
  - 30.073755770921707
  - 30.5601494461298
  - 30.29188632965088
  - 30.383312091231346
  - 30.206093326210976
  - 30.289083525538445
  - 30.30213874578476
  - 30.02258710563183
  - 29.921470627188683
  - 30.629043832421303
  - 30.21915164589882
  - 30.79816699028015
  - 30.494830459356308
  - 30.581197142601013
  - 30.113236650824547
  - 29.625002786517143
  - 30.47796307504177
  - 30.007428601384163
  - 29.889105334877968
  - 30.778450548648834
  - 29.87232233583927
  - 29.830803588032722
  - 30.133202970027924
  - 30.244253128767014
  - 30.45376254618168
  - 29.95089226961136
  - 31.059305369853973
  - 29.797059804201126
  - 29.93103778362274
  - 30.587566763162613
  - 30.720734909176826
  - 29.935543477535248
  - 30.385650977492332
  - 30.361264884471893
  - 29.843256920576096
  - 30.65519092977047
  - 29.927457734942436
  - 30.336541429162025
  - 31.845125894993544
  - 30.19296458363533
  - 30.407273679971695
  - 29.995363757014275
  - 30.52266936004162
  - 30.28918021917343
  - 29.774928137660027
  - 30.261187702417374
  - 29.93298852443695
  - 30.264494001865387
  - 30.296908676624298
  - 29.94145604968071
  - 30.076828822493553
  - 30.935436889529228
  - 30.725385516881943
  - 30.22271879017353
  - 30.45934408903122
  - 30.789189845323563
  - 32.84276306629181
  - 30.586821854114532
  - 30.333272472023964
  - 30.427711471915245
  - 30.76070348918438
  - 30.41730970144272
  - 30.30770169198513
  - 30.092242151498795
  - 30.643711484968662
  - 30.40082496404648
  - 30.86567921936512
  - 30.581002309918404
  - 30.401571035385132
  - 30.24519395828247
  - 30.15359678864479
  - 30.6450926810503
  - 30.282891400158405
  - 30.970557987689972
  - 30.324071422219276
  - 30.17490543425083
  - 30.364912509918213
  validation_losses:
  - 0.4080667197704315
  - 0.4085575044155121
  - 0.4178359806537628
  - 0.41090837121009827
  - 0.4094305634498596
  - 0.4241950213909149
  - 0.40907567739486694
  - 0.4075110852718353
  - 19.99200439453125
  - 64.63575744628906
  - 12.126463890075684
  - 12982529.0
  - 4966744.5
  - 314.1073913574219
  - 208690.640625
  - 8978594816.0
  - 14000644423680.0
  - 29129263022080.0
  - 18188427329536.0
  - 24831141609472.0
  - 308.7275085449219
  - 194.7142333984375
  - 240072.40625
  - 42559524864.0
  - 31542.23828125
  - 3162498048.0
  - 16907118837760.0
  - 94344140816384.0
  - 19348.1171875
  - 12742377.0
  - 504700862464.0
  - 22137968525312.0
  - 23714938748928.0
  - 15929504169984.0
  - 39097934544896.0
  - 31760482041856.0
  - 114.68788146972656
  - 288361536.0
  - 359839694848.0
  - 12146427559936.0
  - 13222222495744.0
  - 89671400947712.0
  - 11395709009920.0
  - 13204587544576.0
  - 114.66841888427734
  - 399132160.0
  - 12015587295232.0
  - 23911093764096.0
  - 82082873212928.0
  - 64465957552128.0
  - 3309.418701171875
  - 705170240.0
  - 34.609580993652344
  - 14927285714944.0
  - 124693839347712.0
  - 20759435018240.0
  - 20291503783936.0
  - 23745053851648.0
  - 17341737861120.0
  - 53700856905728.0
  - 40954702594048.0
  - 22206197268480.0
  - 8703628017664.0
  - 39080414937088.0
  - 119549122838528.0
  - 48148416299008.0
  - 11704956092416.0
  - 5249879244800.0
  - 27517551378432.0
  - 0.42356348037719727
  - 0.4195869565010071
  - 0.4114352762699127
  - 0.4095643162727356
  - 0.43195414543151855
  - 1980539745599488.0
  - 8513871258058752.0
  - 1.095383627333632e+16
  - 0.429034560918808
  - 0.4114020764827728
  - 155789541376.0
  - 88367423488.0
  - 332202934272.0
  - 441178587136.0
  - 497337597952.0
  - 471157342208.0
  - 480933543936.0
  - 462916059136.0
  - 435496189952.0
  - 422789021696.0
  - 414931124224.0
  - 439937335296.0
  - 369180442624.0
  - 443157217280.0
  - 366233583616.0
  - 226272460800.0
  - 216860508160.0
  - 309598257152.0
  - 318743248896.0
  - 262871564288.0
  - 269470924800.0
loss_records_fold2:
  train_losses:
  - 30.61958396434784
  - 30.765106976032257
  - 30.17020134627819
  - 29.988411471247673
  - 31.008059814572334
  - 30.36934420466423
  - 30.64677993953228
  - 30.595987424254417
  - 30.349344208836555
  - 30.266019076108932
  - 30.40764382481575
  - 30.274731636047363
  - 30.25620099902153
  - 30.900126323103905
  - 30.684450298547745
  - 31.79505844414234
  - 30.388574585318565
  - 30.293140336871147
  - 30.04589806497097
  - 30.357345819473267
  - 30.389332503080368
  - 30.58654949069023
  - 30.404852613806725
  - 30.6022911965847
  - 30.819238759577274
  - 30.898318141698837
  - 30.316187158226967
  - 31.1513312458992
  - 30.68079686164856
  - 30.074963226914406
  - 31.18939968198538
  - 30.58457611501217
  - 30.496309772133827
  - 30.363998219370842
  - 30.714663356542587
  - 30.554344475269318
  - 31.172484531998634
  - 30.43826073408127
  - 30.267215982079506
  - 30.060624442994595
  - 31.370070844888687
  - 30.721248999238014
  - 30.132101237773895
  - 30.23039910197258
  - 30.229120582342148
  - 30.326837196946144
  - 30.20949311554432
  - 30.35543328523636
  - 30.51052263379097
  - 30.49321459233761
  - 30.96391797065735
  - 30.137834295630455
  - 30.097789123654366
  - 30.691244825720787
  - 30.25939190387726
  - 30.570985361933708
  - 30.61164389550686
  - 30.717892050743103
  - 30.51270353794098
  - 30.490360110998154
  - 30.44014471024275
  - 30.32331056892872
  - 30.48301175236702
  - 30.42142552137375
  - 30.314219057559967
  - 30.465364649891853
  - 30.618485316634178
  - 30.472101762890816
  - 30.43330818414688
  - 30.98214688897133
  - 30.567770764231682
  - 30.63547645509243
  - 30.057860985398293
  - 30.250663578510284
  - 30.16704922914505
  - 30.45643338561058
  - 30.363379880785942
  - 30.925136506557465
  - 30.216736882925034
  - 30.65854026377201
  - 30.261575132608414
  - 30.71253775060177
  - 30.09824848175049
  - 30.761805437505245
  - 30.820337310433388
  - 30.222208634018898
  - 30.642586439847946
  - 30.35436700284481
  - 30.608381658792496
  - 31.480786934494972
  - 30.22990293800831
  - 30.038238883018494
  - 30.816734492778778
  - 30.463136434555054
  - 30.457763671875
  - 30.295913338661194
  - 30.817995600402355
  - 31.265177831053734
  - 30.6307542771101
  - 31.867872446775436
  validation_losses:
  - 318498144256.0
  - 204566708224.0
  - 285332635648.0
  - 338045468672.0
  - 313120194560.0
  - 267825725440.0
  - 243556253696.0
  - 294257819648.0
  - 214460055552.0
  - 245473148928.0
  - 203291295744.0
  - 206634631168.0
  - 243692716032.0
  - 268452003840.0
  - 243660177408.0
  - 257314242560.0
  - 231579320320.0
  - 348476145664.0
  - 310416506880.0
  - 247987929088.0
  - 324022272000.0
  - 214607167488.0
  - 268912623616.0
  - 278065184768.0
  - 338043404288.0
  - 218865156096.0
  - 264563818496.0
  - 262793773056.0
  - 320704577536.0
  - 220083240960.0
  - 257091649536.0
  - 297169551360.0
  - 270281326592.0
  - 331967430656.0
  - 214393716736.0
  - 202074554368.0
  - 235278303232.0
  - 232830369792.0
  - 283466858496.0
  - 220535013376.0
  - 224180600832.0
  - 308741701632.0
  - 307011190784.0
  - 227626172416.0
  - 271424290816.0
  - 275319947264.0
  - 231444283392.0
  - 297467838464.0
  - 274615861248.0
  - 307517521920.0
  - 326836518912.0
  - 250492731392.0
  - 339565641728.0
  - 231673380864.0
  - 292608802816.0
  - 221290872832.0
  - 216629706752.0
  - 244301627392.0
  - 242860834816.0
  - 198887063552.0
  - 229093539840.0
  - 251470495744.0
  - 235716214784.0
  - 242512674816.0
  - 215557423104.0
  - 294746685440.0
  - 232313044992.0
  - 222978031616.0
  - 251879030784.0
  - 263877836800.0
  - 256462405632.0
  - 246038200320.0
  - 266590748672.0
  - 318299176960.0
  - 219512012800.0
  - 206795489280.0
  - 304713269248.0
  - 290314780672.0
  - 204546588672.0
  - 344646909952.0
  - 302958149632.0
  - 251677360128.0
  - 240483663872.0
  - 256255754240.0
  - 325296357376.0
  - 270698725376.0
  - 289839742976.0
  - 316109225984.0
  - 357783044096.0
  - 240890806272.0
  - 233825828864.0
  - 286710431744.0
  - 222997184512.0
  - 275999784960.0
  - 326229098496.0
  - 304657661952.0
  - 206867152896.0
  - 225669758976.0
  - 291178676224.0
  - 330267262976.0
loss_records_fold3:
  train_losses:
  - 30.42209529876709
  - 30.42162176966667
  - 30.400822937488556
  - 30.35819073021412
  - 30.445589989423752
  - 30.243365705013275
  - 30.170401230454445
  - 30.80536939203739
  - 30.088056579232216
  - 30.593063950538635
  - 30.420745372772217
  - 30.804411344230175
  - 30.2216687053442
  - 30.083818197250366
  - 30.509883522987366
  - 30.356543824076653
  - 30.608833491802216
  - 31.19234761595726
  - 30.176726311445236
  - 30.602149173617363
  - 30.26726657152176
  - 31.096297219395638
  - 30.921686440706253
  - 31.637643486261368
  - 30.420194000005722
  - 30.901696503162384
  - 30.43012124300003
  - 30.882076777517796
  - 30.356045439839363
  - 30.575963392853737
  - 30.209092132747173
  - 30.277097046375275
  - 30.614146500825882
  - 30.517522230744362
  - 30.934513747692108
  - 30.584180116653442
  - 30.741365149617195
  - 30.27719959616661
  - 30.049606278538704
  - 30.36407081782818
  - 30.324418231844902
  - 30.07776816189289
  - 30.609796807169914
  - 30.790092155337334
  - 30.252620458602905
  - 30.00372639298439
  - 30.295731961727142
  - 30.694766990840435
  - 30.352774932980537
  - 30.757615596055984
  - 30.73040121793747
  - 31.25414216518402
  - 30.139705553650856
  - 30.507110029459
  - 30.366012185811996
  - 30.367377623915672
  - 30.51057606935501
  - 30.474033907055855
  - 30.12951284646988
  - 30.82684935629368
  - 30.99164789915085
  - 30.319822654128075
  - 30.807666543871164
  - 30.58169586956501
  - 30.28508225083351
  - 30.41472600400448
  - 30.352078840136528
  - 30.94669832289219
  - 30.49147918820381
  - 30.909821078181267
  - 30.40267165005207
  - 30.28653174638748
  - 30.666365444660187
  - 30.805192962288857
  - 30.016361236572266
  - 30.814551159739494
  - 30.520093485713005
  - 30.91350170969963
  - 30.446436390280724
  - 30.158110812306404
  - 30.631884932518005
  - 30.54461333155632
  - 30.799385719001293
  - 31.7609231621027
  - 30.585039541125298
  - 30.051369085907936
  - 31.72787408530712
  - 30.39126768708229
  - 30.32114413380623
  - 30.232497930526733
  - 30.485965192317963
  - 30.342119432985783
  - 30.300556376576424
  - 30.396556794643402
  - 30.588746905326843
  - 30.54308757930994
  - 30.291642397642136
  - 30.408122777938843
  - 30.50417198240757
  - 30.742071196436882
  validation_losses:
  - 206093467648.0
  - 242397741056.0
  - 244540506112.0
  - 278036086784.0
  - 288652886016.0
  - 224195772416.0
  - 263219281920.0
  - 230486573056.0
  - 266231857152.0
  - 360168685568.0
  - 198687457280.0
  - 317894000640.0
  - 283803713536.0
  - 310662594560.0
  - 229362712576.0
  - 295552909312.0
  - 237347618816.0
  - 343835672576.0
  - 353186480128.0
  - 304110665728.0
  - 235310907392.0
  - 226552414208.0
  - 298310598656.0
  - 239974105088.0
  - 278672900096.0
  - 237947895808.0
  - 227139796992.0
  - 218817495040.0
  - 300619759616.0
  - 257272479744.0
  - 293060149248.0
  - 234747396096.0
  - 281613533184.0
  - 214919004160.0
  - 285094641664.0
  - 240855990272.0
  - 224162938880.0
  - 279471423488.0
  - 227060154368.0
  - 287335809024.0
  - 330039787520.0
  - 311048601600.0
  - 287005179904.0
  - 257855717376.0
  - 293760139264.0
  - 238425767936.0
  - 261528207360.0
  - 292764614656.0
  - 268294225920.0
  - 244421853184.0
  - 238260584448.0
  - 286136434688.0
  - 270725136384.0
  - 288616873984.0
  - 302538293248.0
  - 276705312768.0
  - 309583544320.0
  - 271036137472.0
  - 257843085312.0
  - 261221892096.0
  - 261995167744.0
  - 256423444480.0
  - 245491826688.0
  - 272857972736.0
  - 316439592960.0
  - 275552436224.0
  - 251272200192.0
  - 329688252416.0
  - 231920664576.0
  - 249360138240.0
  - 257786511360.0
  - 240542957568.0
  - 283702263808.0
  - 346553221120.0
  - 267757551616.0
  - 223441846272.0
  - 254238228480.0
  - 284274753536.0
  - 210097045504.0
  - 285346136064.0
  - 265731162112.0
  - 202203824128.0
  - 278516924416.0
  - 263381286912.0
  - 275630391296.0
  - 258069676032.0
  - 224471498752.0
  - 334661058560.0
  - 237656899584.0
  - 298439049216.0
  - 290243543040.0
  - 236287262720.0
  - 321356464128.0
  - 235081400320.0
  - 248523849728.0
  - 263849590784.0
  - 225994571776.0
  - 222093246464.0
  - 265731424256.0
  - 212080590848.0
loss_records_fold4:
  train_losses:
  - 31.441632717847824
  - 30.66960372030735
  - 30.219307154417038
  - 30.077305659651756
  - 30.12487307190895
  - 31.311285853385925
  - 30.242403209209442
  - 30.432994291186333
  - 30.92078658938408
  - 31.740788742899895
  - 30.221373543143272
  - 30.137444868683815
  - 30.403850600123405
  - 30.585881784558296
  - 30.86132822930813
  - 30.973904997110367
  - 30.181677922606468
  - 30.68760271370411
  - 30.305397137999535
  - 29.972743406891823
  - 30.507502794265747
  - 30.40080739557743
  - 30.22902798652649
  - 30.132564172148705
  - 30.339406050741673
  - 30.165482744574547
  - 30.063186302781105
  - 31.74712073802948
  - 30.37363025546074
  - 30.610910952091217
  - 30.89689989387989
  - 30.243198558688164
  - 30.76967816054821
  - 30.884503826498985
  - 30.967338502407074
  - 30.776299372315407
  - 30.733687296509743
  - 30.183394744992256
  - 30.747972577810287
  - 30.261295214295387
  - 30.403259083628654
  - 31.151699781417847
  - 30.98536765575409
  - 30.434908628463745
  - 30.36409718543291
  - 31.14488285779953
  - 30.468478932976723
  - 30.17427785694599
  - 30.17734345793724
  - 30.624754667282104
  - 30.991086050868034
  - 30.17735680937767
  - 30.389161109924316
  - 30.97719419002533
  - 30.779036715626717
  - 30.19611147046089
  - 30.145722463726997
  - 30.413506031036377
  - 30.424881160259247
  - 30.363510817289352
  - 30.574816018342972
  - 30.76897308975458
  - 30.29546308517456
  - 31.051714941859245
  - 30.759892255067825
  - 30.446551367640495
  - 30.425946667790413
  - 30.382228046655655
  - 30.92107191681862
  - 30.336846068501472
  - 30.611753225326538
  - 30.598420798778534
  - 30.573056936264038
  - 30.29776580631733
  - 29.906480833888054
  - 30.699710816144943
  - 30.03328140079975
  - 31.8592139929533
  - 30.49610786139965
  - 30.10784325003624
  - 30.682892724871635
  - 30.31522062420845
  - 29.970435723662376
  - 30.509095400571823
  - 30.394866555929184
  - 30.132261142134666
  - 30.347264856100082
  - 30.573755711317062
  - 30.25101873278618
  - 30.023782707750797
  - 30.093213990330696
  - 30.492430478334427
  - 30.245499536395073
  - 30.40788520872593
  - 30.69029489159584
  - 31.39378459006548
  - 30.131807044148445
  - 30.524017214775085
  - 30.41979668289423
  - 31.104273922741413
  validation_losses:
  - 258346647552.0
  - 207648063488.0
  - 311562141696.0
  - 238982348800.0
  - 320130744320.0
  - 319253839872.0
  - 316214509568.0
  - 280505614336.0
  - 237715308544.0
  - 217273827328.0
  - 355844653056.0
  - 246273327104.0
  - 233046638592.0
  - 291338092544.0
  - 289575141376.0
  - 322217410560.0
  - 307680182272.0
  - 233338929152.0
  - 276021379072.0
  - 280444665856.0
  - 258478620672.0
  - 321068302336.0
  - 284933586944.0
  - 270041235456.0
  - 349873602560.0
  - 219563343872.0
  - 227223715840.0
  - 299529306112.0
  - 332808781824.0
  - 308358610944.0
  - 390593609728.0
  - 342500081664.0
  - 328283422720.0
  - 192760856576.0
  - 308373389312.0
  - 239464218624.0
  - 252004024320.0
  - 267794120704.0
  - 217210060800.0
  - 296010448896.0
  - 279086759936.0
  - 323953491968.0
  - 283046739968.0
  - 277409890304.0
  - 392099233792.0
  - 275883556864.0
  - 212176502784.0
  - 289621966848.0
  - 225864826880.0
  - 299248648192.0
  - 244762198016.0
  - 250540965888.0
  - 250557317120.0
  - 285105160192.0
  - 229639536640.0
  - 273818583040.0
  - 302287454208.0
  - 225757675520.0
  - 227641655296.0
  - 267951865856.0
  - 270693171200.0
  - 237409796096.0
  - 262776946688.0
  - 310104489984.0
  - 221774856192.0
  - 326182404096.0
  - 268264980480.0
  - 335813410816.0
  - 362010279936.0
  - 291290185728.0
  - 249304416256.0
  - 276225097728.0
  - 294668959744.0
  - 308921106432.0
  - 224892616704.0
  - 308970749952.0
  - 293022892032.0
  - 213475459072.0
  - 218905280512.0
  - 236233703424.0
  - 259289284608.0
  - 204921405440.0
  - 278593994752.0
  - 287822020608.0
  - 291198140416.0
  - 263598030848.0
  - 308333281280.0
  - 307430883328.0
  - 262753665024.0
  - 255366119424.0
  - 236630245376.0
  - 334666072064.0
  - 314308067328.0
  - 275813924864.0
  - 289612627968.0
  - 287808618496.0
  - 264813772800.0
  - 311458627584.0
  - 295317929984.0
  - 208615686144.0
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 87 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:19:51.096229'
