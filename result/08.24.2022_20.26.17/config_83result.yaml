config:
  aggregation: mean
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:23:11.378906'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_83fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_83fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_83fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_83fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_83fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_83fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_83fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_83fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_83fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_83fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 0.9286261141300202
  - 0.8465534865856171
  - 0.7901746034622192
  - 0.750272822380066
  - 0.782249015569687
  - 0.7736378729343415
  - 0.7816153347492218
  - 0.7370565474033356
  - 0.787971431016922
  - 0.7659903764724731
  - 0.8287581205368042
  validation_losses:
  - 0.4966716170310974
  - 0.4213182032108307
  - 0.41574954986572266
  - 0.3945434093475342
  - 0.40072545409202576
  - 0.3879638612270355
  - 0.39009952545166016
  - 0.3980084955692291
  - 0.3935626447200775
  - 0.39796388149261475
  - 0.3985650837421417
loss_records_fold1:
  train_losses:
  - 0.7629759609699249
  - 0.7706351161003113
  - 0.7917023658752442
  - 0.7846609234809876
  - 0.7790258407592774
  - 0.7746962428092957
  - 0.739220803976059
  - 0.771082878112793
  - 0.851721853017807
  - 0.7687410473823548
  - 0.7829789936542512
  - 0.7695546090602875
  - 0.753643274307251
  - 0.7568462789058685
  validation_losses:
  - 0.39568832516670227
  - 0.3966607451438904
  - 0.3934529423713684
  - 0.39112648367881775
  - 0.3906592130661011
  - 0.39390894770622253
  - 0.3875260651111603
  - 0.4004395008087158
  - 0.3970367908477783
  - 0.3941989243030548
  - 0.3971785306930542
  - 0.3934588134288788
  - 0.3932249844074249
  - 0.3854652941226959
loss_records_fold2:
  train_losses:
  - 0.7676157593727112
  - 0.7448558986186982
  - 0.7848792433738709
  - 0.7488613903522492
  - 0.7825464069843293
  - 0.7871084094047547
  - 0.7564790010452271
  - 0.7553456664085388
  - 0.7592695891857147
  - 0.7495358645915986
  - 0.7883911073207855
  - 0.776108068227768
  - 0.7774992823600769
  - 0.81746945977211
  - 0.7729062676429749
  - 0.7641228079795838
  - 0.7746771216392517
  validation_losses:
  - 0.38939839601516724
  - 0.3865927755832672
  - 0.40619295835494995
  - 0.3861059546470642
  - 0.3904569149017334
  - 0.39713025093078613
  - 0.41234636306762695
  - 0.40211784839630127
  - 0.39457574486732483
  - 0.39423269033432007
  - 0.447498619556427
  - 0.3924562931060791
  - 0.3863547444343567
  - 0.39490512013435364
  - 0.3938041925430298
  - 0.385648250579834
  - 0.38749435544013977
loss_records_fold3:
  train_losses:
  - 0.7815928936004639
  - 0.7381753683090211
  - 0.7878794074058533
  - 0.7807541847229005
  - 0.7692917048931123
  - 0.7532783448696136
  - 0.772314190864563
  - 0.7576895117759705
  - 0.737069058418274
  - 0.7640900194644928
  - 0.789586079120636
  - 0.9136120140552522
  - 0.7641249716281892
  - 0.7702228426933289
  - 0.7689106941223145
  - 0.758021205663681
  - 0.7613640904426575
  - 0.7540781855583192
  - 0.7644673585891724
  - 0.7220703601837158
  - 0.7475167155265808
  - 0.7491196215152741
  - 0.7403173506259919
  - 0.7579669833183289
  - 0.7659594297409058
  - 0.8397731125354767
  - 0.7524553716182709
  - 0.804105705022812
  - 0.7907556474208832
  - 0.7891819894313813
  - 0.8040622651576996
  validation_losses:
  - 0.3699696362018585
  - 0.36612966656684875
  - 0.3915044069290161
  - 0.3723301887512207
  - 0.3679477870464325
  - 0.37428900599479675
  - 0.4069649577140808
  - 0.3850878179073334
  - 0.3748975992202759
  - 0.3708062171936035
  - 0.38151246309280396
  - 0.3858508765697479
  - 0.3688705563545227
  - 0.36644452810287476
  - 0.37561818957328796
  - 0.38228464126586914
  - 0.3936536908149719
  - 0.3814573884010315
  - 0.37433314323425293
  - 0.3996868431568146
  - 0.3742314279079437
  - 0.37635403871536255
  - 0.37659671902656555
  - 0.39722537994384766
  - 0.4231436252593994
  - 0.3888470530509949
  - 0.3735860586166382
  - 0.3756054937839508
  - 0.37294432520866394
  - 0.37742310762405396
  - 0.3772132396697998
loss_records_fold4:
  train_losses:
  - 0.7414028108119965
  - 0.748548698425293
  - 0.7813876926898957
  - 0.73139006793499
  - 0.7424481093883515
  - 0.7486518979072572
  - 0.743660169839859
  - 0.8168972671031952
  - 0.7783113062381745
  - 0.8025451362133027
  - 0.7383726358413697
  - 0.7533327639102936
  - 0.7582159698009492
  - 0.7663175523281098
  - 0.738462370634079
  - 0.7406516253948212
  - 0.8442531883716584
  - 0.7619774401187898
  validation_losses:
  - 0.3780127167701721
  - 0.3851149380207062
  - 0.38139650225639343
  - 0.38176077604293823
  - 0.3801196813583374
  - 0.3844860792160034
  - 0.38296252489089966
  - 0.40819424390792847
  - 0.38863247632980347
  - 0.3937567174434662
  - 0.3827621042728424
  - 0.3977562487125397
  - 0.38620269298553467
  - 0.3917141556739807
  - 0.3825092911720276
  - 0.38794824481010437
  - 0.3808015286922455
  - 0.38400986790657043
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 31 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:07:37.535217'
