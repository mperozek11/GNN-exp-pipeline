config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:59:17.793649'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_120fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_120fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_120fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_120fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_120fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_120fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_120fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_120fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_120fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_120fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 77.356514108181
  - 33.86286160051823
  - 27.116427795588972
  - 45.677034206688404
  - 33.25133247822523
  - 16.164740464091302
  - 26.222665044665337
  - 15.552429980039598
  - 16.84407379627228
  - 35.867692860960965
  - 12.918701058626176
  - 27.956393456459047
  - 26.027227926254273
  - 15.273542934656144
  - 10.660783204436303
  - 7.260088089108468
  - 11.773788100481035
  - 10.103290900588036
  - 8.183466747403145
  - 8.89329852461815
  - 8.850175699591636
  - 7.460383373498917
  - 8.769085314869882
  - 8.447107350826263
  - 7.699698123335839
  - 9.106567452847958
  - 7.432115453481675
  - 6.898567815124989
  - 8.430467101931573
  - 6.843774375319481
  - 6.32168941795826
  - 6.914782562851906
  - 6.0674056887626655
  - 6.899881887435914
  - 6.231788614392281
  - 6.393176734447479
  - 6.190467536449432
  - 6.130765265226365
  - 5.878839814662934
  - 6.5871841117739685
  - 6.31926461160183
  - 8.956627556681633
  - 7.194048902392388
  - 6.410228672623635
  - 6.148178601264954
  - 6.187196385860443
  - 5.867919450998307
  - 5.986715975403786
  - 6.246074402332306
  - 6.099521818757058
  - 6.163400274515152
  - 6.3453358024358755
  - 6.033182317018509
  - 6.157476294040681
  - 5.894328382611275
  - 5.926356700062752
  - 5.924177977442742
  - 5.9449806988239295
  - 6.165183410048485
  - 9.6612130433321
  - 12.412313333153726
  - 8.5558747112751
  - 7.21863698065281
  - 6.063681426644326
  - 5.922558996081353
  - 6.069497814774514
  - 5.867814111709595
  - 6.096328365802766
  - 6.0598218023777015
  - 5.800648352503777
  - 5.839067721366883
  - 5.954061874747277
  - 5.926992323994637
  - 5.994103237986565
  - 6.163961163163186
  - 5.9087400257587435
  - 5.947485142946244
  - 6.333705996721983
  - 6.277596607804298
  - 6.033010686933995
  - 6.432450535893441
  - 6.319066989421845
  - 6.526147338747979
  - 6.010593774914742
  - 6.092959439754487
  - 5.923130889236927
  - 5.8335739687085155
  - 5.91010933816433
  - 5.904453369975091
  - 6.120129427313805
  - 6.055693531036377
  - 6.13754056096077
  - 5.9085768371820455
  - 6.036281713843346
  - 6.26280754506588
  - 5.970271131396294
  - 6.013683104515076
  - 5.920447584986687
  - 5.871667116880417
  - 6.170153823494911
  validation_losses:
  - 1.5106303691864014
  - 0.488583505153656
  - 1.0449130535125732
  - 1.7103908061981201
  - 0.6394606232643127
  - 0.7840785384178162
  - 0.7664301991462708
  - 0.412896990776062
  - 0.3904148042201996
  - 0.41167548298835754
  - 4.3332414627075195
  - 0.4352411925792694
  - 0.40990757942199707
  - 0.38219738006591797
  - 0.41682711243629456
  - 0.3894893229007721
  - 0.41408494114875793
  - 0.3876343369483948
  - 0.4673610031604767
  - 0.387684166431427
  - 0.39365023374557495
  - 1.3455708026885986
  - 0.7586772441864014
  - 1.3951690196990967
  - 1.179169774055481
  - 0.4169589877128601
  - 0.49344757199287415
  - 0.4839498698711395
  - 0.4141550660133362
  - 0.38205453753471375
  - 0.3847644329071045
  - 0.40438833832740784
  - 0.38913217186927795
  - 0.3952258825302124
  - 0.3907245099544525
  - 0.389187216758728
  - 0.4025215804576874
  - 0.392007052898407
  - 0.457736611366272
  - 0.4024330973625183
  - 0.38586145639419556
  - 0.40963971614837646
  - 0.42374375462532043
  - 0.42056143283843994
  - 0.4287322759628296
  - 0.38846465945243835
  - 0.38871827721595764
  - 0.387745201587677
  - 0.44085684418678284
  - 0.3908989429473877
  - 0.902588427066803
  - 0.39320462942123413
  - 0.3943333029747009
  - 0.4035361111164093
  - 0.416134238243103
  - 0.4037797749042511
  - 0.4855642020702362
  - 0.39264020323753357
  - 0.4515586793422699
  - 23.245624542236328
  - 25.996835708618164
  - 0.9769697785377502
  - 829138.0
  - 1169738368.0
  - 50200976.0
  - 6583.51171875
  - 943020352.0
  - 940594.1875
  - 26512.126953125
  - 2096959872.0
  - 21774312.0
  - 639370.75
  - 84.90828704833984
  - 1989.401611328125
  - 3.4678030014038086
  - 13.314149856567383
  - 0.3925720751285553
  - 1.6885117292404175
  - 0.3912929594516754
  - 2.1724159717559814
  - 2.7471368312835693
  - 1.2841084003448486
  - 3.1044201850891113
  - 1.677172303199768
  - 0.8588162064552307
  - 0.4506027400493622
  - 0.5001609921455383
  - 52.79588317871094
  - 103329.7890625
  - 15089238.0
  - 1230440.75
  - 50.7601203918457
  - 0.39071688055992126
  - 0.8728647232055664
  - 1101.458740234375
  - 70.7852783203125
  - 1.1974406242370605
  - 103074.9765625
  - 6.750725746154785
  - 0.4684906005859375
loss_records_fold1:
  train_losses:
  - 5.932463279366494
  - 5.805376625061036
  - 5.806956425309181
  - 5.909186786413193
  - 6.093854096531868
  - 6.052537456154823
  - 6.016885960102082
  - 5.843795064091683
  - 5.990426927804947
  - 5.890741357207299
  - 5.992223709821701
  - 6.109890511631966
  - 5.896438938379288
  - 5.826113662123681
  - 5.8327917695045475
  - 5.815030026435853
  - 5.91449491083622
  - 5.955339902639389
  - 5.960648751258851
  - 5.801993951201439
  - 5.866347831487656
  - 5.963465207815171
  - 5.769599455595017
  - 5.812495049834252
  - 5.835609450936317
  - 5.8707161247730255
  - 5.969772690534592
  - 6.22632651925087
  - 5.977183896303178
  - 5.923196104168892
  - 5.942840477824212
  - 5.883008587360383
  - 5.967341586947441
  - 5.950527553260327
  - 5.949856466054917
  - 6.059522214531899
  - 6.018342977762223
  - 5.790369153022766
  - 6.084137213230133
  - 5.876073560118676
  - 5.915349569916725
  - 5.859306478500367
  - 5.870000663399697
  - 5.943602132797242
  - 6.097232803702354
  - 5.930739834904671
  - 5.860056394338608
  - 5.950720739364624
  - 5.869414684176445
  - 5.9547751396894455
  - 5.85700301527977
  - 6.119258379936219
  - 5.896013224124909
  - 6.1594949007034305
  - 5.833336466550827
  - 5.8971674859523775
  - 5.852481538057328
  - 5.817295667529106
  - 5.888095805048943
  - 6.270744368433952
  - 5.81458420753479
  - 5.935050052404404
  - 6.0231484353542335
  - 5.809008154273034
  - 5.895118382573128
  - 5.98114197254181
  - 6.002807575464249
  - 5.8568265914917
  - 6.024153977632523
  validation_losses:
  - 9.008028030395508
  - 0.42183172702789307
  - 0.4199175536632538
  - 0.44445887207984924
  - 0.4754314124584198
  - 0.771202802658081
  - 0.40827080607414246
  - 0.4051804840564728
  - 12448.6015625
  - 0.4105941355228424
  - 3308.15966796875
  - 38502.54296875
  - 5824.9091796875
  - 79686.53125
  - 85048.9609375
  - 19343.9140625
  - 30188.71484375
  - 9845.484375
  - 27067.240234375
  - 52383.85546875
  - 0.4086264669895172
  - 0.4188646674156189
  - 9055.01953125
  - 520052.5
  - 35960.78125
  - 0.4107038378715515
  - 88454.359375
  - 246.830810546875
  - 793.4096069335938
  - 0.3992326557636261
  - 0.4101963937282562
  - 0.4144022762775421
  - 73796080.0
  - 0.4077269434928894
  - 0.43126195669174194
  - 0.40396857261657715
  - 1396275.625
  - 15.067265510559082
  - 263895.9375
  - 2000720.625
  - 0.40109169483184814
  - 0.40521854162216187
  - 10286520.0
  - 186770.828125
  - 0.42635002732276917
  - 249077552.0
  - 71436206080.0
  - 0.40415215492248535
  - 0.42601609230041504
  - 0.45712825655937195
  - 0.40953564643859863
  - 0.40707141160964966
  - 0.44980502128601074
  - 0.4148557484149933
  - 0.41985630989074707
  - 0.4105483293533325
  - 0.4244527816772461
  - 0.40594926476478577
  - 0.43692028522491455
  - 0.4072127640247345
  - 0.40687859058380127
  - 0.40772655606269836
  - 0.422328382730484
  - 0.4173608720302582
  - 0.41017258167266846
  - 0.41531023383140564
  - 0.4229094684123993
  - 0.4198472797870636
  - 0.40673935413360596
loss_records_fold2:
  train_losses:
  - 5.992453321814537
  - 5.897317039966584
  - 6.155587702989578
  - 5.980522161722184
  - 5.9272064745426185
  - 6.110904595255852
  - 5.997264152765275
  - 6.085917130112648
  - 6.195375104248524
  - 5.883355140686035
  - 6.7700088948011405
  - 5.925695937871933
  - 6.2323385149240496
  - 6.307124242186546
  - 13.058207680284978
  - 6.877161806821824
  - 6.309603223204613
  - 6.78325617313385
  - 6.009420728683472
  - 6.388490375876427
  - 6.139673838019371
  - 6.261226451396943
  - 6.021722266077996
  - 6.090096580982209
  - 5.904994794726372
  validation_losses:
  - 0.41423749923706055
  - 0.504116952419281
  - 0.3944472372531891
  - 0.39191704988479614
  - 0.39481478929519653
  - 0.4218064546585083
  - 0.4049730896949768
  - 0.3854134678840637
  - 0.3819214701652527
  - 0.38505831360816956
  - 688287.75
  - 0.432310551404953
  - 3611.955078125
  - 1766805.0
  - 0.41269651055336
  - 0.3866410255432129
  - 0.38404980301856995
  - 0.3831373155117035
  - 0.41828247904777527
  - 0.41701075434684753
  - 0.41775643825531006
  - 0.38905030488967896
  - 0.38133472204208374
  - 0.38711950182914734
  - 0.38157182931900024
loss_records_fold3:
  train_losses:
  - 5.967477655410767
  - 6.248633036017418
  - 5.913922545313835
  - 5.890965136885644
  - 5.861041483283043
  - 6.083254408836365
  - 6.1638804405927665
  - 5.906678766012192
  - 6.124778243899346
  - 5.9896681874990465
  - 5.9530822187662125
  - 5.914614665508271
  - 5.867582905292512
  - 5.984276902675629
  - 6.005717028677464
  - 5.933769553899765
  - 5.868552930653095
  - 6.07104866206646
  - 5.948792347311974
  - 5.874341279268265
  validation_losses:
  - 0.5660536885261536
  - 0.4402543306350708
  - 0.3954033851623535
  - 0.3997563123703003
  - 0.3996940851211548
  - 0.42192065715789795
  - 0.3990583121776581
  - 0.39440545439720154
  - 0.41654127836227417
  - 0.3989163637161255
  - 0.44205838441848755
  - 0.4229293167591095
  - 0.39932072162628174
  - 0.47795769572257996
  - 0.4038088023662567
  - 0.4024212658405304
  - 0.3942689001560211
  - 0.3991042375564575
  - 0.40085500478744507
  - 0.3970312476158142
loss_records_fold4:
  train_losses:
  - 6.000152227282524
  - 6.026971212029458
  - 5.984274962544442
  - 5.876339185237885
  - 6.076431918144227
  - 6.2052963286638265
  - 5.913276010751725
  - 6.273843979835511
  - 6.066702264547349
  - 6.101682505011559
  - 5.934570226073266
  validation_losses:
  - 0.410757839679718
  - 0.41176077723503113
  - 0.3909982144832611
  - 0.3928453326225281
  - 0.3971386253833771
  - 0.3993183672428131
  - 0.3918154835700989
  - 0.3906918168067932
  - 0.39163681864738464
  - 0.3915193974971771
  - 0.39192020893096924
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 69 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:23:56.032902'
