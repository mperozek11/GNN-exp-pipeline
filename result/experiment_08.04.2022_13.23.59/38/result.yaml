config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 17:19:50.129259'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/38/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/38/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/38/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/38/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/38/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/38/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/38/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/38/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/38/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/38/fold_4_state_dict.pt
loss_records_fold3:
  train_losses:
  - 3.799774616956711
  - 3.693087786436081
  - 3.8370548486709595
  - 3.8056880831718445
  - 3.7657440900802612
  - 3.6737826764583588
  - 3.7155798375606537
  - 3.5988776087760925
  - 3.7979326248168945
  - 3.6315779387950897
  - 3.6176799535751343
  - 3.629839152097702
  - 3.731331318616867
  - 3.6662282943725586
  - 3.6847062408924103
  - 3.5540711730718613
  - 3.6076657474040985
  - 3.5822056233882904
  - 3.435820147395134
  - 3.6115473806858063
  - 3.7781564593315125
  - 3.708084285259247
  - 3.9763497412204742
  - 3.864077538251877
  - 3.8012100160121918
  - 4.047017395496368
  - 4.023709237575531
  - 3.849658191204071
  - 3.7531803846359253
  - 4.045488268136978
  - 3.7833269834518433
  - 3.7477826476097107
  - 3.944320023059845
  - 3.7857572436332703
  - 3.8771129846572876
  - 4.001412749290466
  - 3.7528333365917206
  - 3.653436928987503
  - 3.9726232290267944
  - 3.901271939277649
  - 3.6015922129154205
  - 3.678835391998291
  - 4.122847467660904
  - 3.9954912066459656
  - 3.696999043226242
  - 3.70963978767395
  - 3.700773745775223
  - 3.6859516501426697
  - 3.531317412853241
  - 3.744666874408722
  - 3.7294155657291412
  - 3.5859539210796356
  - 3.7311439216136932
  - 3.9038213193416595
  - 3.5285427421331406
  - 3.71559277176857
  - 3.716937392950058
  - 3.5306637436151505
  - 3.5946914851665497
  - 3.6592269241809845
  - 3.6244454085826874
  - 3.549331411719322
  - 3.827455163002014
  - 3.529044434428215
  - 3.670716255903244
  - 3.717733472585678
  - 3.8336152732372284
  - 4.170569062232971
  - 3.7511687874794006
  - 3.659648358821869
  - 3.598117381334305
  - 3.6683687567710876
  - 3.6727233827114105
  - 3.659274399280548
  - 3.6147714853286743
  - 3.7015821635723114
  - 3.7221635580062866
  - 3.519999712705612
  - 3.5839242041110992
  - 3.6647379398345947
  - 3.717688202857971
  - 3.854682832956314
  - 3.673364073038101
  - 3.5557635724544525
  - 3.623531013727188
  - 3.8685085773468018
  - 3.732287347316742
  - 3.6397987008094788
  - 3.7508952915668488
  - 3.8641506135463715
  - 3.693090721964836
  - 3.852428525686264
  - 3.7275323271751404
  - 3.5541708022356033
  - 3.7412911355495453
  - 3.5735702514648438
  - 3.63351047039032
  - 3.6895760893821716
  - 3.9202146232128143
  - 3.6880257427692413
  validation_losses:
  - 0.6073395609855652
  - 0.6932112574577332
  - 0.5120828747749329
  - 0.5254247188568115
  - 0.4276479482650757
  - 0.4325002431869507
  - 0.6545156836509705
  - 0.5597658753395081
  - 0.6949540972709656
  - 0.5885279178619385
  - 0.388137549161911
  - 0.4534611701965332
  - 0.5433530807495117
  - 0.5126187801361084
  - 0.48857933282852173
  - 0.4745978116989136
  - 0.4375292658805847
  - 0.4403320848941803
  - 0.4711967706680298
  - 0.43237027525901794
  - 0.49871912598609924
  - 0.7192758321762085
  - 0.44755661487579346
  - 0.3810645341873169
  - 0.39428454637527466
  - 0.386203408241272
  - 1.1208418607711792
  - 1.24762761592865
  - 0.9967029690742493
  - 0.5891512632369995
  - 0.579334020614624
  - 0.6394827365875244
  - 0.43217119574546814
  - 0.4349408745765686
  - 0.40395569801330566
  - 0.3788989186286926
  - 0.39685383439064026
  - 0.594504714012146
  - 0.5242356061935425
  - 0.5644453763961792
  - 0.7603396773338318
  - 0.5482156872749329
  - 0.5454061031341553
  - 0.6764068007469177
  - 0.6284257173538208
  - 0.802527904510498
  - 0.6473662853240967
  - 0.6643157601356506
  - 0.5996209979057312
  - 0.7922701835632324
  - 0.951134443283081
  - 0.37537091970443726
  - 0.5385064482688904
  - 0.7702757120132446
  - 0.38903793692588806
  - 0.6208596229553223
  - 0.4378010928630829
  - 0.6149151921272278
  - 0.7083280682563782
  - 0.5240940451622009
  - 0.7032332420349121
  - 0.654956579208374
  - 0.5257779955863953
  - 0.5815309882164001
  - 0.4384786784648895
  - 0.3909264802932739
  - 0.39551660418510437
  - 0.47452354431152344
  - 0.8680054545402527
  - 0.632652223110199
  - 0.5157418251037598
  - 0.6320816278457642
  - 0.5820565223693848
  - 0.5456746816635132
  - 0.6956456303596497
  - 0.5687786936759949
  - 0.38251397013664246
  - 0.3806167542934418
  - 0.4285384714603424
  - 0.635887622833252
  - 0.6386977434158325
  - 0.6686592102050781
  - 1.0420105457305908
  - 0.5063649415969849
  - 0.5068848133087158
  - 0.5641525983810425
  - 0.4196595549583435
  - 0.4347095787525177
  - 0.41447001695632935
  - 0.4324668049812317
  - 0.5373379588127136
  - 0.830970823764801
  - 0.5072729587554932
  - 0.5436450839042664
  - 0.5540076494216919
  - 0.5447179675102234
  - 0.5086385607719421
  - 0.46686556935310364
  - 0.48589345812797546
  - 0.6903586983680725
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 48 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 85 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.855917667238422, 0.8027444253859348,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.0, 0.04545454545454545, 0.27672955974842767, 0.023529411764705882]'
  mean_eval_accuracy: 0.8459202018237224
  mean_f1_accuracy: 0.06914270339353581
  total_train_time: '0:01:32.253650'
