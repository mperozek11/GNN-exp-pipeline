config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:38:44.028710'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_54fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_54fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_54fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_54fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_54fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_54fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_54fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_54fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_54fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_54fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.9980609834194185
  - 1.609584003686905
  - 1.727301001548767
  - 1.6592695772647859
  - 1.6786035358905793
  - 1.6100933492183687
  - 1.6577173233032227
  - 1.6088570773601534
  - 1.5443639039993287
  - 1.6089736104011536
  - 1.6057777404785156
  - 1.580200868844986
  - 1.521328365802765
  - 1.5689012229442598
  - 1.5109849840402605
  - 1.531985342502594
  - 1.5840879142284394
  - 1.5643713235855103
  - 1.5505082786083222
  - 1.5439549267292023
  - 1.6063278436660768
  - 1.5482129573822023
  - 1.5456502795219422
  - 1.6635484099388123
  - 1.5717894196510316
  - 1.5515722632408142
  - 1.6053021550178528
  - 1.5949385881423952
  - 1.5670125603675844
  - 1.551099467277527
  - 1.5207667410373689
  - 1.5849314451217653
  validation_losses:
  - 0.42234042286872864
  - 0.4121119976043701
  - 0.4586910009384155
  - 0.45830073952674866
  - 0.4178050756454468
  - 0.41044336557388306
  - 0.4077959954738617
  - 0.39686110615730286
  - 0.3987868130207062
  - 0.47416481375694275
  - 0.4215167164802551
  - 0.38518837094306946
  - 0.3935409188270569
  - 0.38933777809143066
  - 0.3848426938056946
  - 0.43563419580459595
  - 0.4160259962081909
  - 0.4259357452392578
  - 0.3871329426765442
  - 0.4123343527317047
  - 0.3947490453720093
  - 0.3863162100315094
  - 0.39685288071632385
  - 0.4011719822883606
  - 0.38187795877456665
  - 0.4604123830795288
  - 0.3971579074859619
  - 0.39232519268989563
  - 0.3861846327781677
  - 0.39601707458496094
  - 0.38857489824295044
  - 0.39401131868362427
loss_records_fold1:
  train_losses:
  - 1.5750604152679444
  - 1.54630446434021
  - 1.5074229001998902
  - 1.5547455310821534
  - 1.5147404074668884
  - 1.5139392495155335
  - 1.5492600560188294
  - 1.549819850921631
  - 1.5032054245471955
  - 1.5444971680641175
  - 1.61745445728302
  - 1.558309328556061
  - 1.6401759505271913
  validation_losses:
  - 0.3908051550388336
  - 0.3881857991218567
  - 0.41281622648239136
  - 0.4315076172351837
  - 0.39890748262405396
  - 0.395710825920105
  - 0.4244837760925293
  - 0.4047709107398987
  - 0.4023338258266449
  - 0.4012676775455475
  - 0.39477109909057617
  - 0.3926110565662384
  - 0.39425212144851685
loss_records_fold2:
  train_losses:
  - 1.5399701654911042
  - 1.619553303718567
  - 1.5140513420104982
  - 1.53328897356987
  - 1.5200171947479248
  - 1.5421386301517488
  - 1.5126011669635773
  - 1.5695122718811036
  - 1.542360132932663
  - 1.51496022939682
  - 1.5746703684329988
  - 1.5437848925590516
  - 1.5469714224338533
  - 1.5048541843891146
  - 1.5054659664630892
  - 1.573542183637619
  - 1.5710552990436555
  - 1.5515921711921692
  - 1.550967347621918
  - 1.5320618450641632
  - 1.524919790029526
  - 1.5210688829421999
  - 1.561873334646225
  - 1.5064934849739076
  validation_losses:
  - 0.3918110430240631
  - 0.3857722580432892
  - 0.38993576169013977
  - 0.3967767655849457
  - 0.4689680337905884
  - 0.3955135643482208
  - 0.3902661204338074
  - 0.3963046967983246
  - 0.38881322741508484
  - 0.397763729095459
  - 0.42664825916290283
  - 0.3885057270526886
  - 0.38732269406318665
  - 0.39189979434013367
  - 0.40318286418914795
  - 0.38880637288093567
  - 0.3921888470649719
  - 0.4226912260055542
  - 0.3992500603199005
  - 0.3912777006626129
  - 0.38875532150268555
  - 0.3967611491680145
  - 0.39072078466415405
  - 0.39049968123435974
loss_records_fold3:
  train_losses:
  - 1.5717443585395814
  - 1.53257172703743
  - 1.5790948510169984
  - 1.5546639800071718
  - 1.581249797344208
  - 1.5129879593849183
  - 1.5997814893722535
  - 1.5509450495243073
  - 1.5670127034187318
  - 1.5362349331378937
  - 1.540395772457123
  - 1.5652000129222872
  - 1.6773537814617159
  - 1.5421646416187287
  - 1.5258519411087037
  - 1.510639986395836
  - 1.5154781579971315
  - 1.4852690607309342
  - 1.505013930797577
  - 1.4907522141933442
  - 1.520965188741684
  - 1.559351772069931
  - 1.546429741382599
  - 1.449931186437607
  - 1.506166297197342
  - 1.498021012544632
  - 1.5330220699310304
  - 1.5049440264701843
  - 1.5531510651111604
  - 1.5132320284843446
  - 1.5013704657554627
  - 1.5446223974227906
  - 1.486798107624054
  - 1.5170654177665712
  - 1.4808758914470674
  - 1.469449019432068
  - 1.4799621164798737
  - 1.515869450569153
  - 1.509866636991501
  - 1.5438291311264039
  - 1.4989105939865113
  - 1.4642898678779603
  - 1.5292913317680359
  - 1.481377822160721
  - 1.4719771683216096
  - 1.5162971138954164
  - 1.4945518612861635
  - 1.4716881930828096
  - 1.5428997874259949
  - 1.5597786486148835
  - 1.5217442274093629
  - 1.4844378232955933
  - 1.5071609675884248
  - 1.5098999559879305
  - 1.4575129628181458
  - 1.4847638189792634
  - 1.47992821931839
  - 1.5521116495132448
  - 1.4647485315799713
  - 1.495348185300827
  - 1.4237056821584702
  - 1.5145317733287813
  - 1.488498604297638
  - 1.4713901579380035
  - 1.5096209943294525
  - 1.4493556886911394
  - 1.4651106595993042
  - 1.4941805601119995
  - 1.5509802579879761
  - 1.4983261644840242
  - 1.5369969606399536
  - 1.4899344682693483
  - 1.5633352041244508
  - 1.5313806712627411
  - 1.510694533586502
  - 1.504281395673752
  - 1.510626196861267
  - 1.500396263599396
  - 1.4951906621456148
  - 1.4989563882350923
  - 1.4976730823516846
  - 1.5389690041542055
  - 1.4732171952724458
  - 1.5282762765884401
  - 1.4878707885742188
  - 1.4597035348415375
  - 1.5332334756851198
  - 1.4855152785778047
  - 1.5518335521221163
  - 1.5282724380493165
  - 1.5087362229824066
  - 1.7011461377143862
  - 1.5366641342639924
  - 1.5585431218147279
  - 1.477237331867218
  - 1.5136134147644045
  - 1.4541228771209718
  - 1.493293857574463
  - 1.5083508670330048
  - 1.4957430958747864
  validation_losses:
  - 0.4534246325492859
  - 2.629910469055176
  - 0.38071438670158386
  - 0.3813275694847107
  - 0.3849725127220154
  - 0.5118218064308167
  - 0.6646351218223572
  - 0.3896266222000122
  - 1.9872002601623535
  - 2.080940008163452
  - 0.39144980907440186
  - 0.3965945839881897
  - 0.38915494084358215
  - 0.37328773736953735
  - 0.3776128590106964
  - 0.3959612548351288
  - 0.41405895352363586
  - 0.4078570604324341
  - 0.674157440662384
  - 0.4336296617984772
  - 0.439649373292923
  - 0.4789775311946869
  - 0.4366287291049957
  - 0.3853655159473419
  - 0.4161001145839691
  - 0.41017112135887146
  - 0.4692884385585785
  - 0.43847140669822693
  - 0.5072537660598755
  - 0.8786417245864868
  - 0.3806930184364319
  - 0.38484200835227966
  - 0.4646746814250946
  - 0.4011155068874359
  - 0.39556360244750977
  - 0.5065039992332458
  - 0.5282501578330994
  - 0.837300181388855
  - 1.5091984272003174
  - 0.3802260756492615
  - 0.40393614768981934
  - 0.4261171817779541
  - 0.5193753838539124
  - 0.5046157836914062
  - 1.2741293907165527
  - 1.155042052268982
  - 0.40085819363594055
  - 0.3815639913082123
  - 0.4005184769630432
  - 0.42811334133148193
  - 0.46588921546936035
  - 0.4155310094356537
  - 0.4280472695827484
  - 0.49890556931495667
  - 0.48630473017692566
  - 0.41015106439590454
  - 0.5328153967857361
  - 0.4522075057029724
  - 0.4488197863101959
  - 0.6932842135429382
  - 0.47427552938461304
  - 0.6629596948623657
  - 0.5524201989173889
  - 0.36685410141944885
  - 0.47542813420295715
  - 0.5190367102622986
  - 0.4428170919418335
  - 0.8064566850662231
  - 0.43075281381607056
  - 0.40620720386505127
  - 0.5135967135429382
  - 0.44188952445983887
  - 0.5558328628540039
  - 0.4589759111404419
  - 0.4091965854167938
  - 0.3805730640888214
  - 0.40420061349868774
  - 0.3918200433254242
  - 0.4256707727909088
  - 0.420908123254776
  - 0.43816277384757996
  - 0.4868614375591278
  - 0.5638847351074219
  - 0.5413839221000671
  - 0.48303845524787903
  - 0.5284260511398315
  - 0.5232750177383423
  - 0.5237971544265747
  - 0.4562585949897766
  - 0.49580082297325134
  - 31.609630584716797
  - 0.399103045463562
  - 0.38377824425697327
  - 0.39153140783309937
  - 0.494915246963501
  - 0.40361034870147705
  - 0.4547969400882721
  - 0.4492526352405548
  - 0.5724067687988281
  - 0.6109004616737366
loss_records_fold4:
  train_losses:
  - 1.4676454961299896
  - 1.4945212841033937
  - 1.5051763236522675
  - 1.4584859490394593
  - 1.5157703816890717
  - 1.4766026616096497
  - 1.4654409021139145
  - 1.4827901721000671
  - 1.4842701017856599
  - 1.4983456015586853
  - 1.4988348543643952
  - 1.4633848488330843
  - 1.491413974761963
  - 1.4479142427444458
  - 1.4656273543834688
  - 1.4967044949531556
  - 1.514262992143631
  - 1.4156185030937196
  - 1.5066501557826997
  - 1.4563597768545151
  - 1.471321988105774
  - 1.4523729145526887
  - 1.484878659248352
  - 1.4661912441253664
  - 1.4908007919788362
  - 1.53450163602829
  - 1.5077923834323883
  - 1.5007339894771576
  - 1.556686517596245
  - 1.5311095595359803
  - 1.4691418647766115
  - 1.4902617931365967
  - 1.4973095953464508
  - 1.4992189288139344
  - 1.5317264199256897
  - 1.4931845366954803
  - 1.4940360128879548
  - 1.4458167105913162
  - 1.478180241584778
  - 1.4635234475135803
  - 1.520733916759491
  - 1.4578327417373658
  - 1.4542988240718842
  - 1.5306956708431245
  - 1.5060399591922762
  - 1.4894279658794405
  - 1.519322919845581
  - 1.462409257888794
  - 1.5068909287452699
  - 1.5049823045730593
  - 1.4707021474838258
  - 1.420431447029114
  - 1.4557878255844117
  - 1.4872944712638856
  - 1.480650418996811
  - 1.556000852584839
  - 1.539847368001938
  - 1.4977740168571474
  - 1.481523334980011
  - 1.517044997215271
  - 1.4997243821620942
  - 1.4382606089115144
  - 1.4727192461490632
  - 1.434411844611168
  - 1.4778824627399445
  - 1.4333352148532867
  - 1.465744322538376
  - 1.494974207878113
  - 1.4999760627746583
  - 1.5125545084476473
  - 1.487162208557129
  - 1.494069355726242
  - 1.4983569741249085
  - 1.5724893510341644
  - 1.4795146644115449
  - 1.4799173116683961
  - 1.4788986802101136
  - 1.4485520839691164
  - 1.4699933588504792
  - 1.4975464642047882
  - 1.4167869061231615
  - 1.4160990178585053
  - 1.439163613319397
  - 1.4636792600154878
  - 1.499024546146393
  - 1.4902119517326355
  - 1.5238610923290254
  - 1.4800274491310121
  - 1.4979671180248262
  - 1.4497449398040771
  - 1.4537998378276826
  - 1.437740170955658
  - 1.468277955055237
  - 1.4339653134346009
  - 1.4604499995708466
  - 1.4628916442394257
  - 1.4926845073699953
  - 1.4838909327983858
  - 1.4823957920074464
  - 1.615972876548767
  validation_losses:
  - 0.42098599672317505
  - 0.4423006474971771
  - 0.43977123498916626
  - 0.38185200095176697
  - 0.41415631771087646
  - 0.4228915274143219
  - 0.37143924832344055
  - 0.4944162368774414
  - 0.3813526928424835
  - 0.5022382140159607
  - 0.35892048478126526
  - 0.4064599871635437
  - 0.43672654032707214
  - 0.4034322202205658
  - 0.37549468874931335
  - 0.4579831659793854
  - 0.5220390558242798
  - 0.4240393042564392
  - 0.4301154315471649
  - 0.4315027892589569
  - 0.4004124104976654
  - 0.4596833884716034
  - 0.4115603268146515
  - 0.4002860188484192
  - 0.36458390951156616
  - 0.4866589903831482
  - 0.45067286491394043
  - 0.3809298276901245
  - 0.38057589530944824
  - 0.3774878680706024
  - 0.3692161440849304
  - 0.3890579640865326
  - 0.39475080370903015
  - 0.3882550597190857
  - 0.36905890703201294
  - 0.4076899588108063
  - 0.4229240119457245
  - 0.3937457203865051
  - 0.4679906964302063
  - 0.5125268697738647
  - 0.4095745086669922
  - 0.40546220541000366
  - 0.4411906898021698
  - 0.4039686322212219
  - 0.38673120737075806
  - 0.38383904099464417
  - 0.42146265506744385
  - 0.3788202106952667
  - 0.3599027097225189
  - 0.5073975324630737
  - 0.424483984708786
  - 0.4734973907470703
  - 0.45515352487564087
  - 0.39213159680366516
  - 0.4049340486526489
  - 0.3891698122024536
  - 0.3743521571159363
  - 0.39048200845718384
  - 0.3563249409198761
  - 0.4174107313156128
  - 0.38715437054634094
  - 0.376634806394577
  - 0.3847891390323639
  - 0.41607609391212463
  - 0.40475842356681824
  - 0.39934447407722473
  - 0.470137357711792
  - 0.42720991373062134
  - 0.45410189032554626
  - 0.432882159948349
  - 0.4716797471046448
  - 0.3686985671520233
  - 0.37794098258018494
  - 0.37514257431030273
  - 0.3786733150482178
  - 0.3980043828487396
  - 0.35525208711624146
  - 0.40793323516845703
  - 0.46324509382247925
  - 0.43751394748687744
  - 0.3827040195465088
  - 0.42589014768600464
  - 0.5396479964256287
  - 0.41744279861450195
  - 0.48500093817710876
  - 0.40263158082962036
  - 0.3567502200603485
  - 0.40105053782463074
  - 0.49698400497436523
  - 0.36909326910972595
  - 0.3592371642589569
  - 1.3704943656921387
  - 0.38434529304504395
  - 0.7525728344917297
  - 0.42182621359825134
  - 0.3868655562400818
  - 0.3833984434604645
  - 0.6455258727073669
  - 0.446805864572525
  - 0.402750700712204
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 32 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8181818181818182,
    0.8470790378006873]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.17187500000000003, 0.1359223300970874]'
  mean_eval_accuracy: 0.8479749842325216
  mean_f1_accuracy: 0.06155946601941749
  total_train_time: '0:23:50.260423'
