config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 03:33:27.471822'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/120/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/120/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/120/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/120/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/120/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/120/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/120/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/120/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/120/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/120/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 423.3567384928465
  - 207.2697260081768
  - 167.77915999293327
  - 157.6542684622109
  - 115.27166309580207
  - 123.14665187895298
  - 102.11414012312889
  - 62.08503641188145
  - 127.18142785131931
  - 86.37021803855896
  - 150.92839799821377
  - 133.66265061497688
  - 115.53793428838253
  - 71.21121993660927
  - 58.50449879467487
  - 77.94511905312538
  - 71.13647915422916
  - 46.644956439733505
  - 51.41704837977886
  - 57.40137493610382
  - 45.66537743806839
  - 66.31752893328667
  - 65.09435373544693
  - 52.43488536775112
  - 38.20651015639305
  - 41.58868020772934
  - 36.5965561568737
  - 39.347615376114845
  - 42.86172675341368
  - 39.44654972106218
  - 35.13191409409046
  - 32.64538870751858
  - 37.981486424803734
  - 36.42668356001377
  - 35.10207839310169
  - 35.40202623605728
  - 32.0459568798542
  - 35.01292157173157
  - 42.296641901135445
  - 38.73683451116085
  - 34.07055492699146
  - 34.17786091566086
  - 31.90371947735548
  - 33.587961718440056
  - 34.8487913608551
  - 32.14828944206238
  - 32.383200868964195
  - 33.160782873630524
  - 32.854045525193214
  - 31.90616585314274
  - 31.83715268969536
  - 32.44383339583874
  - 31.996780216693878
  - 31.95935209095478
  - 33.20428095757961
  - 32.715306997299194
  - 33.388037860393524
  - 33.4751093685627
  - 31.377898082137108
  - 31.916942983865738
  - 31.507277488708496
  - 32.37226186692715
  - 33.72906690835953
  - 31.535267531871796
  - 32.87983027100563
  - 32.41083736717701
  - 36.73003479838371
  - 38.751806393265724
  - 32.98418661952019
  - 31.65188355743885
  - 31.60926128923893
  - 32.327307134866714
  - 34.145953848958015
  - 32.5833694934845
  - 32.73994863033295
  - 33.32213522493839
  - 33.658349737524986
  - 34.723046869039536
  - 84.74056379497051
  - 34.040335580706596
  - 36.46676653623581
  - 32.99992421269417
  - 32.37553606927395
  - 32.74824310839176
  - 42.35646326839924
  - 49.1194409430027
  - 50.74501685798168
  - 34.887521490454674
  - 32.15797856450081
  - 33.548962235450745
  - 31.54460321366787
  - 33.49007837474346
  - 33.54071828722954
  - 34.01609665155411
  - 33.852625995874405
  - 34.35074147582054
  - 33.30438697338104
  - 32.279258489608765
  - 31.444398254156113
  - 31.888015687465668
  validation_losses:
  - 3.279139995574951
  - 2.4797582626342773
  - 0.90061354637146
  - 0.5932995080947876
  - 0.6149526834487915
  - 0.7570238709449768
  - 0.576370894908905
  - 0.7882905006408691
  - 0.6426858901977539
  - 0.5146276950836182
  - 0.4678385555744171
  - 0.7655559182167053
  - 0.40355154871940613
  - 0.4863470792770386
  - 0.5313006043434143
  - 0.4407515227794647
  - 0.42980819940567017
  - 0.4032253623008728
  - 0.4796581268310547
  - 0.40409499406814575
  - 0.5380340218544006
  - 0.41857627034187317
  - 0.4157254695892334
  - 0.45530009269714355
  - 0.45590656995773315
  - 0.4188385009765625
  - 0.3855432868003845
  - 0.44208642840385437
  - 0.5258030295372009
  - 0.44205355644226074
  - 0.41030099987983704
  - 0.42383062839508057
  - 0.4180774688720703
  - 0.4811638593673706
  - 0.4182739853858948
  - 0.4141274392604828
  - 0.400508850812912
  - 0.4298734664916992
  - 1.3325790166854858
  - 0.39538392424583435
  - 0.4163326025009155
  - 0.4066089689731598
  - 0.5249629616737366
  - 0.3936620056629181
  - 0.43167561292648315
  - 0.7019018530845642
  - 0.4009634554386139
  - 0.45431411266326904
  - 0.3951258659362793
  - 0.4140792191028595
  - 0.3992033302783966
  - 0.44871217012405396
  - 0.4064427614212036
  - 0.40020060539245605
  - 0.44901880621910095
  - 0.5146281123161316
  - 0.413936972618103
  - 0.42665600776672363
  - 0.41932806372642517
  - 0.42313432693481445
  - 0.43414106965065
  - 0.4699987769126892
  - 0.395149827003479
  - 0.3914353549480438
  - 0.42886969447135925
  - 0.4863068759441376
  - 6.547836780548096
  - 0.5101557970046997
  - 0.40970107913017273
  - 0.4220215678215027
  - 0.39605259895324707
  - 0.46863242983818054
  - 0.41796937584877014
  - 0.4423038363456726
  - 0.40551769733428955
  - 11.8634614944458
  - 0.40244388580322266
  - 0.41290581226348877
  - 0.40927278995513916
  - 0.42716947197914124
  - 0.4102456271648407
  - 0.4015410542488098
  - 0.42615124583244324
  - 0.41420242190361023
  - 0.7887579798698425
  - 0.6090824604034424
  - 1.7718416452407837
  - 2692183.5
  - 0.4738951325416565
  - 0.3968145251274109
  - 0.4233817756175995
  - 0.4745168387889862
  - 0.45479142665863037
  - 0.4183844029903412
  - 0.4215780198574066
  - 0.47605428099632263
  - 0.4216639995574951
  - 0.450110524892807
  - 0.40290072560310364
  - 0.4543699026107788
loss_records_fold3:
  train_losses:
  - 30.98369300365448
  - 32.70412749052048
  - 32.828289180994034
  - 33.95013888925314
  - 36.360132694244385
  - 32.61514665186405
  - 31.743688866496086
  - 32.28773294389248
  - 44.67840948700905
  - 47.45665590465069
  - 39.77274824678898
  - 34.23727747797966
  - 33.28249731659889
  - 31.233346313238144
  - 35.766714081168175
  - 33.98750960826874
  - 31.414302304387093
  - 32.15866121649742
  - 34.547374844551086
  - 32.963947743177414
  - 33.72758515179157
  - 31.625179797410965
  - 32.15470305085182
  - 32.631529837846756
  - 32.79238659143448
  - 31.63412217795849
  - 36.880790784955025
  - 32.85572004318237
  - 31.575290322303772
  - 32.397153824567795
  - 31.887728184461594
  - 31.943082869052887
  - 33.41216999292374
  - 31.90307129919529
  - 31.583072811365128
  - 32.37963588535786
  - 32.583286464214325
  - 32.837145641446114
  - 33.67427872121334
  - 32.12427657842636
  - 33.628171384334564
  - 32.89951804280281
  - 33.19073602557182
  - 31.979679882526398
  - 32.098604902625084
  - 32.3866401463747
  - 31.550186306238174
  - 31.96276979148388
  - 31.35049419105053
  - 32.794885724782944
  - 32.30273100733757
  - 33.5560837239027
  - 32.35604125261307
  - 32.18180891871452
  - 33.63871058821678
  - 31.25263673067093
  - 32.861148327589035
  - 33.551397919654846
  - 35.900366470217705
  - 32.44595609605312
  - 32.182133078575134
  - 31.686954081058502
  - 32.60420882701874
  - 31.248246014118195
  - 31.964821949601173
  - 32.37362012267113
  - 31.980078920722008
  - 32.23710857331753
  - 32.34570150077343
  - 31.65665239095688
  - 31.698735237121582
  - 32.77566160261631
  - 31.894975036382675
  - 32.5097918510437
  - 31.091532960534096
  - 32.06842851638794
  - 31.933200001716614
  - 32.46984422206879
  - 33.27257043123245
  - 32.79392600804567
  - 31.720089122653008
  - 32.94831094145775
  - 31.46004731953144
  - 31.653666019439697
  - 32.5590363740921
  - 31.931934103369713
  - 31.679849967360497
  - 31.72062674164772
  - 31.109961569309235
  - 31.894095361232758
  - 31.598198741674423
  - 31.44424331188202
  - 31.842458590865135
  - 32.603616908192635
  - 31.037753522396088
  - 31.805697351694107
  - 31.30058379471302
  - 31.517556726932526
  - 33.46842607855797
  - 32.66366058588028
  validation_losses:
  - 0.46118271350860596
  - 0.4444539248943329
  - 0.48843637108802795
  - 0.48036789894104004
  - 0.40902063250541687
  - 0.4534722864627838
  - 0.4194750487804413
  - 0.45850667357444763
  - 53540.48828125
  - 178.46360778808594
  - 0.4259338676929474
  - 0.4686133861541748
  - 0.4170282781124115
  - 0.44213125109672546
  - 0.6293012499809265
  - 28036.55859375
  - 0.41109246015548706
  - 0.45567116141319275
  - 0.4966987669467926
  - 0.45835477113723755
  - 0.41426312923431396
  - 0.4158189594745636
  - 0.49490734934806824
  - 0.5126902461051941
  - 0.41997772455215454
  - 0.40692442655563354
  - 0.6836600303649902
  - 0.42562198638916016
  - 0.40385082364082336
  - 0.4117983877658844
  - 0.40885981917381287
  - 0.45329156517982483
  - 0.4413420855998993
  - 0.42618119716644287
  - 0.44250521063804626
  - 0.39568251371383667
  - 0.500627338886261
  - 43535.3359375
  - 24196243456.0
  - 0.4637702703475952
  - 0.4244016706943512
  - 0.5377002954483032
  - 0.4226878583431244
  - 0.4107334315776825
  - 0.4331204295158386
  - 0.4212118983268738
  - 0.4080308973789215
  - 0.41067931056022644
  - 0.4078114330768585
  - 0.44238653779029846
  - 0.5081124305725098
  - 0.42642056941986084
  - 0.4543674886226654
  - 2599217302339584.0
  - 5.419176360117535e+17
  - 1381766135808.0
  - 0.4194331467151642
  - 0.4037230610847473
  - 0.4194323420524597
  - 0.43554767966270447
  - 0.44672125577926636
  - 0.4096255600452423
  - 0.4200078248977661
  - 0.4098411798477173
  - 0.5325129628181458
  - 0.4242057204246521
  - 0.4310525953769684
  - 0.40963467955589294
  - 0.44657257199287415
  - 0.4207259714603424
  - 0.4189269542694092
  - 0.4612407386302948
  - 0.4685683846473694
  - 0.4153156578540802
  - 0.40932825207710266
  - 0.41276249289512634
  - 0.41406095027923584
  - 0.44892609119415283
  - 0.49250492453575134
  - 0.4085577130317688
  - 0.42172524333000183
  - 0.4191279113292694
  - 0.4077894985675812
  - 0.4187924861907959
  - 0.41646286845207214
  - 0.4336239993572235
  - 0.5224071145057678
  - 0.4507850408554077
  - 0.4252432584762573
  - 0.43662068247795105
  - 0.46306923031806946
  - 0.5260851383209229
  - 0.4225043058395386
  - 0.4317803382873535
  - 0.4146910011768341
  - 0.4040641188621521
  - 0.43599286675453186
  - 0.41367143392562866
  - 0.4270514249801636
  - 0.4120006263256073
loss_records_fold4:
  train_losses:
  - 33.741264030337334
  - 32.079043962061405
  - 34.31696669757366
  - 32.39418935775757
  - 32.51837669312954
  - 32.19598951935768
  - 32.22100828588009
  - 33.34717255830765
  - 32.1414822191
  - 32.91604959964752
  - 33.532806634902954
  - 32.193575605750084
  - 33.126740247011185
  - 32.94719980657101
  - 34.15514661371708
  - 32.79744404554367
  - 33.002998888492584
  - 32.62613005191088
  - 31.26640409231186
  - 31.80062560737133
  - 31.009987726807594
  - 32.04560101032257
  - 31.747375890612602
  - 31.41757471859455
  - 32.31799137592316
  - 33.78941126167774
  - 33.77629915624857
  - 33.155029624700546
  - 32.219473496079445
  - 32.07138569653034
  - 33.78667601943016
  - 32.559760734438896
  - 32.969898104667664
  - 31.85863834619522
  - 33.06291651725769
  - 31.8789441883564
  - 32.30166728794575
  - 32.53810399770737
  - 30.87393769621849
  - 32.80937443673611
  - 32.664839550852776
  - 32.62450397014618
  - 32.92680607736111
  - 32.09695006906986
  - 31.77984807640314
  - 32.02394342422485
  - 32.11199817061424
  - 32.43508620560169
  - 31.603308722376823
  - 32.75913526117802
  - 31.66888077557087
  - 32.47565054893494
  - 31.117150634527206
  - 32.69236758351326
  - 33.38337804377079
  - 31.582972928881645
  - 32.071000799536705
  - 33.90210674703121
  - 31.566108122467995
  - 32.04880651831627
  - 35.34547716379166
  - 35.10676318407059
  - 31.090994998812675
  - 32.26045148074627
  - 31.209644839167595
  - 32.47810687124729
  - 32.45975540578365
  - 32.141373723745346
  - 32.4641997218132
  - 31.66626825928688
  - 32.331775441765785
  - 31.800072848796844
  - 31.59724712371826
  - 32.11858095228672
  - 31.241215586662292
  - 33.17299868166447
  - 31.804542273283005
  - 32.473337933421135
  - 32.23569469153881
  - 31.978320360183716
  - 31.517229571938515
  - 31.223656475543976
  - 32.345860958099365
  - 30.96402034163475
  - 31.333115339279175
  - 33.22626832127571
  - 31.817960768938065
  - 33.2569088190794
  - 33.38136497139931
  - 31.83715310692787
  - 32.27182138711214
  - 32.80822575092316
  - 32.99457934498787
  - 31.85363630950451
  - 32.25026758015156
  - 32.182648688554764
  - 31.592811822891235
  - 32.22031080722809
  - 31.55058892071247
  - 32.87897361814976
  validation_losses:
  - 0.4630536437034607
  - 0.42212727665901184
  - 0.4397629499435425
  - 0.40593627095222473
  - 0.44017747044563293
  - 0.4058102071285248
  - 0.3981870710849762
  - 0.40205100178718567
  - 0.4286845624446869
  - 0.39834606647491455
  - 0.4140439033508301
  - 0.407382994890213
  - 0.4594863951206207
  - 0.42059090733528137
  - 0.42333105206489563
  - 0.4051011800765991
  - 0.45167970657348633
  - 0.40794193744659424
  - 0.42315009236335754
  - 0.40598028898239136
  - 0.448827862739563
  - 0.41237038373947144
  - 0.41251930594444275
  - 0.4151637554168701
  - 0.41195401549339294
  - 0.4809234142303467
  - 0.44035959243774414
  - 0.5245028734207153
  - 0.41376248002052307
  - 0.4185938537120819
  - 0.40276601910591125
  - 0.41134947538375854
  - 0.43796178698539734
  - 0.3981461822986603
  - 0.40963760018348694
  - 0.3982454240322113
  - 0.44586580991744995
  - 0.42879825830459595
  - 0.40948033332824707
  - 0.41145655512809753
  - 0.43359389901161194
  - 0.4130445718765259
  - 0.4152175188064575
  - 0.4070550501346588
  - 0.3992007076740265
  - 0.41569650173187256
  - 0.4035804271697998
  - 0.4315473437309265
  - 0.4137728214263916
  - 0.40689054131507874
  - 0.3975731134414673
  - 0.4054931104183197
  - 0.44953882694244385
  - 0.44177067279815674
  - 0.40061289072036743
  - 0.4030671715736389
  - 0.40067699551582336
  - 0.42736995220184326
  - 0.40943700075149536
  - 0.4110196530818939
  - 0.4908044636249542
  - 0.4383528530597687
  - 0.4152393341064453
  - 0.4209693372249603
  - 0.41950350999832153
  - 0.41339850425720215
  - 0.449258029460907
  - 0.40070420503616333
  - 0.42686933279037476
  - 0.40413743257522583
  - 0.43560218811035156
  - 0.40204790234565735
  - 0.4046386778354645
  - 0.4093746542930603
  - 0.3987294137477875
  - 0.44621509313583374
  - 0.4717485010623932
  - 0.42012539505958557
  - 0.40483522415161133
  - 0.40054258704185486
  - 0.40335559844970703
  - 0.4331551790237427
  - 0.4082747995853424
  - 0.42891648411750793
  - 0.43889427185058594
  - 0.401917964220047
  - 0.4383821189403534
  - 0.5116161704063416
  - 0.40624019503593445
  - 0.40055206418037415
  - 0.44802433252334595
  - 0.4568477272987366
  - 0.40113434195518494
  - 0.42310720682144165
  - 0.4196949005126953
  - 0.4210037887096405
  - 60096033849344.0
  - 0.41008511185646057
  - 0.41609588265419006
  - 0.4183429479598999
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:15:47.379657'
