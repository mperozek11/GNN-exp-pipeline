config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:32:36.412296'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_18fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_18fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_18fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_18fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_18fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_18fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_18fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_18fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_18fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_18fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.7689757823944092
  - 1.5218738973140717
  - 1.5099609434604646
  - 1.4952501714229585
  - 1.49607253074646
  - 1.4655520528554917
  - 1.4541592538356782
  - 1.533159500360489
  - 1.4776311695575715
  - 1.4560085117816925
  - 1.435550892353058
  - 1.4438992559909822
  - 1.4686219334602357
  - 1.4405008792877199
  - 1.4807976901531221
  - 1.4617955207824709
  - 1.4531247019767761
  - 1.4300608217716217
  - 1.4212479054927827
  - 1.447188037633896
  - 1.4593335151672364
  - 1.4584698021411897
  - 1.460231900215149
  - 1.4770664930343629
  - 1.4823084473609924
  - 1.4666839063167574
  - 1.4793327033519745
  - 1.4846707105636598
  - 1.4484800517559053
  - 1.425045520067215
  - 1.3949628472328186
  - 1.4432879090309143
  - 1.4348401188850404
  - 1.4238630801439287
  - 1.5000543475151062
  - 1.434209218621254
  - 1.4610729277133943
  - 1.453369301557541
  - 1.4144413292407991
  - 1.4387577861547471
  - 1.4104301035404205
  - 1.4243839502334597
  - 1.4069008231163025
  - 1.4217896223068238
  - 1.4196293532848359
  - 1.3911620557308197
  - 1.4049423933029175
  - 1.3931952118873596
  - 1.422857302427292
  - 1.4048927396535875
  - 1.4230098307132721
  - 1.4042636275291445
  - 1.3741488963365556
  - 1.4050591349601746
  - 1.4515012443065645
  - 1.4103597819805147
  - 1.4240115702152254
  - 1.3861419826745989
  - 1.3968270242214205
  - 1.3687874943017961
  - 1.4811645805835725
  - 1.4073456227779388
  - 1.3887184500694276
  - 1.4067247688770295
  - 1.4057066082954408
  - 1.419974571466446
  - 1.4052308708429337
  - 1.4252680897712708
  - 1.459182369709015
  - 1.4246295213699343
  - 1.4251387119293213
  - 1.4588559329509736
  - 1.4259556353092195
  - 1.4497612595558167
  - 1.4105211675167084
  - 1.4120866060256958
  - 1.3764447063207628
  - 1.431150734424591
  - 1.3954706311225893
  - 1.3970767259597778
  - 1.3834975600242616
  - 1.3764550268650055
  - 1.441729748249054
  - 1.4125084578990936
  - 1.3779680788517
  - 1.4385133266448975
  - 1.4193214178085327
  - 1.4121979594230654
  - 1.3921322584152223
  - 1.4105697333812715
  - 1.3724853992462158
  - 1.4222388386726381
  - 1.3855458974838257
  - 1.3923553168773652
  - 1.3813263833522798
  - 1.3917085826396942
  - 1.368288791179657
  - 1.3589951246976852
  - 1.3800861656665804
  - 1.4180846929550173
  validation_losses:
  - 0.42326441407203674
  - 0.3943191170692444
  - 0.3997417986392975
  - 0.472534716129303
  - 0.40265947580337524
  - 0.3834683299064636
  - 0.5007256269454956
  - 0.4551747739315033
  - 0.4002293646335602
  - 0.4355386793613434
  - 0.39862632751464844
  - 0.3930914103984833
  - 0.40130072832107544
  - 0.37597811222076416
  - 0.5462861061096191
  - 0.38952022790908813
  - 0.3913387060165405
  - 0.3789859414100647
  - 0.37894773483276367
  - 0.3810487687587738
  - 0.3921560049057007
  - 0.38637909293174744
  - 0.43381866812705994
  - 0.4112781584262848
  - 0.38678041100502014
  - 0.3972129225730896
  - 0.4307643175125122
  - 0.517058253288269
  - 0.9290266036987305
  - 0.4295070767402649
  - 0.3821282684803009
  - 0.4983745813369751
  - 0.4773904085159302
  - 2.255681276321411
  - 0.602723240852356
  - 0.37685441970825195
  - 0.4718935787677765
  - 0.7946003675460815
  - 0.40096399188041687
  - 0.48553237318992615
  - 0.6311571598052979
  - 0.3801926374435425
  - 0.38152623176574707
  - 0.7430030703544617
  - 0.4570741355419159
  - 0.4472324550151825
  - 0.5633694529533386
  - 0.7226580381393433
  - 0.4601719081401825
  - 0.37765100598335266
  - 0.3862271010875702
  - 0.6381925344467163
  - 0.43240880966186523
  - 0.37792545557022095
  - 0.4585818350315094
  - 0.4380960464477539
  - 0.3869214355945587
  - 0.74897700548172
  - 0.4409564137458801
  - 0.6595789194107056
  - 0.42319777607917786
  - 0.37948137521743774
  - 0.3750654458999634
  - 0.3962002098560333
  - 0.42570725083351135
  - 0.4198567569255829
  - 0.40327295660972595
  - 0.40699297189712524
  - 0.714156985282898
  - 0.921432375907898
  - 0.5107075572013855
  - 0.4462836682796478
  - 0.5157871246337891
  - 0.41532081365585327
  - 0.41355493664741516
  - 0.397649347782135
  - 0.40777096152305603
  - 0.3905075788497925
  - 0.43797722458839417
  - 0.49283623695373535
  - 0.4151792824268341
  - 0.5001905560493469
  - 0.4169701635837555
  - 0.3910670876502991
  - 0.40882647037506104
  - 0.4309976100921631
  - 0.4996734857559204
  - 0.4539196789264679
  - 0.45058467984199524
  - 0.4615266025066376
  - 0.48556947708129883
  - 0.39625898003578186
  - 0.5100383758544922
  - 0.5553922057151794
  - 0.4837469160556793
  - 0.5708523988723755
  - 0.49071598052978516
  - 0.44737666845321655
  - 0.5830501317977905
  - 0.46189993619918823
loss_records_fold1:
  train_losses:
  - 1.3864533483982087
  - 1.395754235982895
  - 1.4389543890953065
  - 1.407955914735794
  - 1.4078431189060212
  - 1.4122529029846191
  - 1.4218397319316864
  - 1.4035631239414217
  - 1.4001429200172426
  - 1.4219413936138154
  - 1.4222673654556275
  - 1.4559985071420671
  validation_losses:
  - 0.4757903218269348
  - 0.44589149951934814
  - 0.5803791284561157
  - 0.4427865147590637
  - 0.5108487606048584
  - 0.5530017614364624
  - 0.4884924292564392
  - 0.47812455892562866
  - 0.39743247628211975
  - 0.39979633688926697
  - 0.40479955077171326
  - 0.4033222496509552
loss_records_fold2:
  train_losses:
  - 1.378665578365326
  - 1.3745469689369203
  - 1.3932018637657166
  - 1.365611657500267
  - 1.4127403736114503
  - 1.4147606730461122
  - 1.4018455982208253
  - 1.3865640103816987
  - 1.3647950053215028
  - 1.4369752317667008
  - 1.3881569266319276
  - 1.3879686176776886
  - 1.4180880010128023
  - 1.3833453416824342
  - 1.3580971360206604
  - 1.402460938692093
  - 1.384009838104248
  - 1.354848110675812
  - 1.350013142824173
  - 1.3242421537637712
  - 1.3974898040294648
  - 1.3782851815223696
  - 1.3575690925121309
  - 1.3779295086860657
  - 1.3583396077156067
  - 1.3655645549297333
  - 1.3532692879438402
  - 1.3641598224639893
  - 1.3525337040424348
  - 1.3800821602344513
  - 1.382938152551651
  - 1.3910853683948519
  - 1.3649492919445039
  - 1.3413460433483124
  - 1.3805155873298647
  - 1.430410474538803
  - 1.3604795396327973
  - 1.3813623011112215
  - 1.3347650051116944
  - 1.3395063281059265
  - 1.4352752566337585
  - 1.3777190357446671
  - 1.3754096865653993
  - 1.3225017458200456
  - 1.3392333924770357
  - 1.3088533341884614
  - 1.3307933986186982
  - 1.3374118387699128
  - 1.4066569149494172
  - 1.3526783883571625
  - 1.3559152305126192
  - 1.3526051163673403
  - 1.399860954284668
  - 1.3614847749471666
  - 1.3615580320358278
  - 1.366999214887619
  - 1.372698360681534
  - 1.4016825914382935
  - 1.3843845248222353
  - 1.3460346519947053
  - 1.3818757236003876
  - 1.3363739550113678
  - 1.3270111382007599
  - 1.3548650264739992
  - 1.3518340647220612
  - 1.3064012825489044
  - 1.3818337976932527
  - 1.337824559211731
  - 1.3296271085739138
  - 1.3828374564647676
  - 1.3682089924812317
  - 1.3878907591104508
  - 1.3580240070819856
  - 1.322376748919487
  - 1.3111790299415589
  - 1.34661662876606
  - 1.3553520500659944
  - 1.3680938959121705
  - 1.361778771877289
  - 1.306329542398453
  - 1.3291979730129242
  - 1.355680513381958
  - 1.3333511650562286
  - 1.3078412622213365
  - 1.3293536543846132
  - 1.3231331944465639
  - 1.3580561757087708
  - 1.3045287370681764
  - 1.392073029279709
  - 1.321215268969536
  - 1.3392023980617525
  - 1.3882540941238404
  - 1.3827885568141938
  - 1.3455490052700043
  - 1.335218119621277
  - 1.3736340999603271
  - 1.3126665264368058
  - 1.3373955607414247
  - 1.2848514914512634
  - 1.3405912756919862
  validation_losses:
  - 0.4010162651538849
  - 0.49909746646881104
  - 0.44842079281806946
  - 0.5097185373306274
  - 0.4910452365875244
  - 0.6207271218299866
  - 0.5025349855422974
  - 0.5346072316169739
  - 0.4926285743713379
  - 0.4014792740345001
  - 0.3796936571598053
  - 0.42993536591529846
  - 0.6528794765472412
  - 0.5344623923301697
  - 0.4832416772842407
  - 0.5515996217727661
  - 0.5218477845191956
  - 0.49842554330825806
  - 0.4854702055454254
  - 0.67445969581604
  - 0.8594298362731934
  - 0.6720525622367859
  - 0.5479063391685486
  - 0.5058835744857788
  - 0.5660399794578552
  - 0.7157982587814331
  - 0.6424574851989746
  - 0.6107835173606873
  - 0.5873384475708008
  - 0.5908279418945312
  - 0.5405137538909912
  - 0.927647590637207
  - 0.45536866784095764
  - 0.5360451936721802
  - 0.529585599899292
  - 0.5302393436431885
  - 0.515223503112793
  - 0.5520035028457642
  - 0.4971857964992523
  - 0.6205348968505859
  - 0.47451621294021606
  - 0.5718997120857239
  - 0.5480552315711975
  - 0.5561099648475647
  - 0.7582873106002808
  - 0.8519822955131531
  - 1.1036580801010132
  - 1.0845807790756226
  - 0.7530904412269592
  - 0.5872549414634705
  - 0.6281799077987671
  - 0.6540805101394653
  - 0.5473541617393494
  - 0.5393896698951721
  - 0.5720958709716797
  - 0.6978322267532349
  - 0.7332985997200012
  - 0.4899060130119324
  - 0.4816599488258362
  - 0.5165197253227234
  - 0.44565314054489136
  - 0.45372888445854187
  - 0.473104864358902
  - 0.5046529173851013
  - 0.49505743384361267
  - 0.562504231929779
  - 0.6128877997398376
  - 0.5416399836540222
  - 0.5715831518173218
  - 0.6576138138771057
  - 0.5275521278381348
  - 0.4397583603858948
  - 0.5539988279342651
  - 0.6432524919509888
  - 0.8112798929214478
  - 0.731317400932312
  - 0.6025054454803467
  - 0.5223733186721802
  - 0.4819585382938385
  - 0.5001440048217773
  - 0.5443050265312195
  - 0.5469120740890503
  - 0.5030589699745178
  - 0.6184197068214417
  - 0.5045352578163147
  - 0.5480504631996155
  - 0.5669698119163513
  - 0.5526768565177917
  - 0.6009297370910645
  - 0.5683057904243469
  - 0.5590463280677795
  - 0.6782062649726868
  - 0.47467607259750366
  - 0.3934902548789978
  - 0.5246268510818481
  - 0.5870137214660645
  - 0.5968813300132751
  - 0.5831915736198425
  - 0.64078289270401
  - 0.5372124314308167
loss_records_fold3:
  train_losses:
  - 1.3989120543003084
  - 1.4128886491060257
  - 1.3722755670547486
  - 1.3773729443550111
  - 1.3801625549793244
  - 1.4285952985286714
  - 1.3992314517498017
  - 1.380167615413666
  - 1.3257403373718262
  - 1.3196941852569581
  - 1.357031899690628
  - 1.3317175686359406
  - 1.3320332288742067
  - 1.340356168150902
  - 1.3041865676641464
  - 1.3286178886890412
  - 1.3240009605884553
  - 1.3426122784614565
  - 1.319180750846863
  - 1.3562840819358826
  - 1.3458344340324402
  - 1.3457999765872957
  - 1.2975026220083237
  - 1.3799975097179413
  - 1.3228904366493226
  - 1.283660137653351
  - 1.3192006081342698
  - 1.3116791993379593
  - 1.3439567208290102
  - 1.3536349534988403
  - 1.3539237618446351
  - 1.3395554006099701
  - 1.3405417919158937
  - 1.3301167964935303
  - 1.3033751130104065
  - 1.3097731947898865
  - 1.340777796506882
  - 1.3440355271100999
  - 1.2902272731065751
  - 1.4567394882440567
  validation_losses:
  - 0.5283669829368591
  - 0.3568632900714874
  - 0.46467310190200806
  - 0.4451439082622528
  - 0.3762357234954834
  - 0.5210796594619751
  - 0.5546345114707947
  - 0.5846107602119446
  - 0.5844472050666809
  - 0.6648517847061157
  - 0.7267134785652161
  - 0.6374123096466064
  - 0.7881433367729187
  - 0.7239372730255127
  - 0.6554873585700989
  - 0.6348212957382202
  - 0.5206035375595093
  - 0.562850296497345
  - 0.61232990026474
  - 0.5197946429252625
  - 0.4662467837333679
  - 0.5004498958587646
  - 0.4729529619216919
  - 0.5665028691291809
  - 0.6475206017494202
  - 0.6256929039955139
  - 0.5392565131187439
  - 0.5287664532661438
  - 0.5400855541229248
  - 0.5538439154624939
  - 0.5388371348381042
  - 0.5612082481384277
  - 0.6617422103881836
  - 0.6802038550376892
  - 0.6441593766212463
  - 0.6525444388389587
  - 0.6332228183746338
  - 0.6092318892478943
  - 0.5616772174835205
  - 0.4917462468147278
loss_records_fold4:
  train_losses:
  - 1.3556750655174257
  - 1.3546035289764404
  - 1.3660682141780853
  - 1.31565437912941
  - 1.3030395060777664
  - 1.3341311722993852
  - 1.36348415017128
  - 1.366783905029297
  - 1.3581735074520112
  - 1.3407966077327729
  - 1.31999471783638
  - 1.3557423233985901
  - 1.318388617038727
  - 1.316867619752884
  - 1.3736928641796113
  - 1.3232294142246248
  - 1.3511469006538392
  - 1.3137395203113558
  - 1.3298131376504898
  - 1.3444727271795274
  - 1.3614272594451906
  - 1.3404600262641908
  - 1.3470125079154969
  - 1.3596774518489838
  - 1.3195452690124512
  - 1.338817149400711
  - 1.3041308581829072
  - 1.3198161602020264
  - 1.3206863164901734
  - 1.3486837029457093
  - 1.2654652625322342
  - 1.3095095396041871
  - 1.3587648153305054
  - 1.3344696938991547
  - 1.3305725514888764
  - 1.3366109907627106
  - 1.271848237514496
  - 1.2975236237049104
  - 1.3165779352188112
  - 1.264637640118599
  - 1.2594699621200562
  - 1.2587051749229432
  - 1.3090355575084687
  - 1.366441935300827
  - 1.367363613843918
  - 1.3700982928276062
  - 1.3400535583496094
  - 1.3049457728862763
  - 1.3313683390617372
  - 1.366826105117798
  - 1.4014297395944597
  - 1.349070066213608
  - 1.3161889255046846
  - 1.3071452856063843
  - 1.360218095779419
  - 1.2953582584857941
  - 1.3021947830915452
  - 1.3381320893764497
  - 1.3091921776533129
  - 1.307207638025284
  - 1.2603004217147828
  - 1.3017854005098344
  - 1.3062829911708833
  - 1.3543422788381578
  - 1.2734914481639863
  - 1.3032206118106844
  - 1.277925705909729
  - 1.2593235790729524
  - 1.3123492658138276
  - 1.3098771035671235
  - 1.2763773649930954
  - 1.2813860327005386
  - 1.271937531232834
  - 1.3153246074914933
  - 1.2932882905006409
  - 1.2825970113277436
  - 1.2799192130565644
  - 1.288498216867447
  - 1.3345215320587158
  - 1.2929821491241456
  - 1.3657318532466889
  - 1.3163162142038347
  - 1.373512101173401
  - 1.2978079676628114
  - 1.2908919274806978
  - 1.2514605700969696
  - 1.2982427775859833
  - 1.2589751303195955
  - 1.2678444266319275
  - 1.2869414180517198
  - 1.3514559328556062
  - 1.3943089663982393
  - 1.2850411951541902
  - 1.266543832421303
  - 1.3040375947952272
  - 1.2597125411033632
  - 1.3208377599716188
  - 1.3210847675800323
  - 1.4099578380584719
  - 1.2705452978610994
  validation_losses:
  - 0.6121896505355835
  - 0.767365038394928
  - 0.6988871693611145
  - 0.8321006894111633
  - 0.5645390748977661
  - 0.7267241477966309
  - 0.6196945905685425
  - 0.6425334215164185
  - 0.4261300563812256
  - 0.6668936610221863
  - 0.643477737903595
  - 0.49865058064460754
  - 0.545417308807373
  - 0.7138558626174927
  - 0.5583418607711792
  - 0.6033883094787598
  - 0.572881281375885
  - 0.652713418006897
  - 0.6502177715301514
  - 0.41162770986557007
  - 0.5325467586517334
  - 0.5831549763679504
  - 0.6328787207603455
  - 0.5574774742126465
  - 0.3675404489040375
  - 0.4374345541000366
  - 0.5771254301071167
  - 0.48950275778770447
  - 0.5272515416145325
  - 0.5111966133117676
  - 0.4461202323436737
  - 0.6343297958374023
  - 0.6444316506385803
  - 0.623611330986023
  - 0.568945586681366
  - 0.6799428462982178
  - 0.4283141493797302
  - 0.3926302492618561
  - 0.5789835453033447
  - 0.606566309928894
  - 0.4989367127418518
  - 0.9084262847900391
  - 0.5124258995056152
  - 0.5532713532447815
  - 0.46381139755249023
  - 0.3795146644115448
  - 0.3642752170562744
  - 0.40443307161331177
  - 0.3508361279964447
  - 0.41842591762542725
  - 0.4208166301250458
  - 0.4911646842956543
  - 0.456112802028656
  - 0.4094235599040985
  - 0.42648637294769287
  - 0.41441410779953003
  - 0.39010533690452576
  - 0.46145525574684143
  - 0.5253525376319885
  - 0.5847436189651489
  - 0.5401265621185303
  - 0.5734073519706726
  - 0.4075765907764435
  - 0.5092009902000427
  - 0.553044319152832
  - 0.6973397731781006
  - 0.5198639035224915
  - 0.508722186088562
  - 0.6489676237106323
  - 0.6214547157287598
  - 0.5279513597488403
  - 0.5162651538848877
  - 0.42796581983566284
  - 0.6139289736747742
  - 0.7409002780914307
  - 0.7873543500900269
  - 0.6156615614891052
  - 0.6334365010261536
  - 0.7045420408248901
  - 0.8191667199134827
  - 0.49045833945274353
  - 0.38709065318107605
  - 0.5287167429924011
  - 0.5566930770874023
  - 0.5864826440811157
  - 0.5343194007873535
  - 0.5561482906341553
  - 0.6439652442932129
  - 0.5582515001296997
  - 0.5141515135765076
  - 0.6266307830810547
  - 0.5602691769599915
  - 0.6340805888175964
  - 0.619331955909729
  - 0.7299467921257019
  - 0.6467146277427673
  - 0.7735903859138489
  - 0.6710991859436035
  - 0.6289408802986145
  - 0.4890063405036926
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 40 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8404802744425386, 0.8353344768439108, 0.8078902229845626, 0.8147512864493996,
    0.8419243986254296]'
  fold_eval_f1: '[0.17699115044247787, 0.1272727272727273, 0.25333333333333335, 0.19402985074626863,
    0.1320754716981132]'
  mean_eval_accuracy: 0.8280761318691681
  mean_f1_accuracy: 0.17674050669858407
  total_train_time: '0:29:15.167306'
