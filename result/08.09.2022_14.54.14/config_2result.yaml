config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cpu
  dropout: 0.0
  epochs: 30
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.05
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 5
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-09 14:54:34.027764'
fold_0_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_2fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_2fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_2fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_2fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_2fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_2fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_2fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_2fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_2fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_2fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 7.202898558974266
  - 6.345912089943886
  - 5.986040198802948
  - 5.953713884949685
  - 6.006337279081345
  - 5.985220861434937
  - 5.89977149963379
  - 5.883752182126045
  - 6.018899896740914
  - 6.094227784872055
  - 6.2141318410635
  - 5.999194371700288
  - 6.0280383914709095
  - 5.986295205354691
  - 6.129400503635407
  - 6.121760421991349
  - 6.0182393282651905
  - 6.101403848826886
  - 6.033683872222901
  - 5.983573025465012
  - 6.329475066065789
  - 6.150121027231217
  - 6.1282683491706855
  - 6.119407087564468
  - 6.036125949025155
  - 6.125760100781918
  - 5.990899255871773
  - 6.046102085709572
  - 6.180466568470002
  - 6.046274253726006
  validation_losses:
  - 0.412067174911499
  - 0.4111068546772003
  - 0.3915742337703705
  - 0.47858089208602905
  - 0.4273304343223572
  - 0.41384652256965637
  - 0.4180023670196533
  - 0.44406646490097046
  - 0.40678560733795166
  - 13.675333023071289
  - 222873472.0
  - 6300277669888.0
  - 0.41012054681777954
  - 0.45994535088539124
  - 55214329856.0
  - 0.4096289873123169
  - 59552866304.0
  - 0.41014859080314636
  - 0.4151933789253235
  - 0.4093325436115265
  - 7855221760.0
  - 78890483712.0
  - 0.4224664270877838
  - 22025302016.0
  - 0.42398521304130554
  - 0.40934091806411743
  - 64356933632.0
  - 73944956928.0
  - 0.4099402129650116
  - 0.4156738817691803
loss_records_fold1:
  train_losses:
  - 6.116224107146263
  - 6.0360551327466965
  - 6.153767216205598
  - 6.092967864871025
  - 6.1602831274271015
  - 6.054720693826676
  - 5.997762581706048
  - 6.090531811118126
  - 5.9966741681098945
  - 6.140476858615876
  - 6.074250862002373
  - 6.347599709033966
  - 6.218090656399728
  - 6.208447483181954
  - 6.154837727546692
  - 5.991156509518624
  - 6.130048462748528
  - 6.062712225317956
  - 6.030130475759506
  - 6.082250672578812
  - 6.141946336627007
  - 6.021684309840203
  - 6.121720588207245
  - 6.034591430425644
  - 6.057795268297196
  - 6.108784127235413
  - 6.0392560243606574
  - 6.061080032587052
  - 6.107131007313729
  - 6.28808236271143
  validation_losses:
  - 39255953408.0
  - 30919499776.0
  - 0.41095170378685
  - 0.40925049781799316
  - 0.42820751667022705
  - 0.4177907109260559
  - 0.4318165183067322
  - 16913072128.0
  - 0.4346221387386322
  - 0.41166675090789795
  - 0.4096459448337555
  - 0.4684339761734009
  - 0.420674204826355
  - 0.4256047308444977
  - 22316294144.0
  - 0.4247280955314636
  - 14747417600.0
  - 13977111552.0
  - 36160598016.0
  - 0.4135217070579529
  - 45038968832.0
  - 0.4093776345252991
  - 0.42094293236732483
  - 0.4124264717102051
  - 42406600704.0
  - 0.44676369428634644
  - 12835098624.0
  - 45157748736.0
  - 35601907712.0
  - 0.42023319005966187
loss_records_fold2:
  train_losses:
  - 6.048551344871521
  - 6.108730632066727
  - 6.0363559365272526
  - 6.099931645393372
  - 6.084562200307847
  - 6.023620319366455
  - 6.092714327573777
  - 6.090179356932641
  - 6.048598571121693
  - 6.116163912415505
  - 6.12133279144764
  - 5.996169939637184
  - 6.087568950653076
  - 6.073135638237
  - 6.0448736429214485
  - 6.071678501367569
  - 6.015423399209976
  validation_losses:
  - 5744792064.0
  - 1930273280.0
  - 23837046784.0
  - 0.4375069737434387
  - 16069495808.0
  - 25346748416.0
  - 0.4209921658039093
  - 0.410835862159729
  - 0.4063962399959564
  - 0.4071262776851654
  - 14188195840.0
  - 0.40961578488349915
  - 28204404736.0
  - 0.41508230566978455
  - 0.40814217925071716
  - 0.40631812810897827
  - 0.40621134638786316
loss_records_fold3:
  train_losses:
  - 6.08039235174656
  - 6.069827169179916
  - 6.0876218557357795
  - 6.2735324770212175
  - 5.996421679854393
  - 6.110879489779473
  - 6.133189614117146
  - 6.214313504099846
  - 6.011812147498131
  - 6.107362936437131
  - 6.058553621172905
  - 6.2155744403600695
  - 6.056382870674134
  - 6.037760326266289
  - 6.154967291653157
  - 6.101090696454048
  - 6.059386464953423
  - 6.151475512981415
  - 6.185407133400441
  - 6.1128918707370765
  - 6.139977675676346
  - 6.0059156864881516
  - 6.126370918750763
  - 5.996379885077477
  - 6.3040345251560215
  - 6.057633554935456
  - 6.180859264731407
  - 6.066552102565765
  - 6.079261150956154
  - 6.192530477046967
  validation_losses:
  - 0.4062553346157074
  - 0.4102230966091156
  - 0.4238934814929962
  - 0.41964513063430786
  - 0.41028717160224915
  - 32255823872.0
  - 0.4099884033203125
  - 14804786176.0
  - 0.4354768991470337
  - 0.40779101848602295
  - 29428146176.0
  - 54298767360.0
  - 0.4087381958961487
  - 27242088448.0
  - 26172018688.0
  - 0.40760815143585205
  - 0.40972021222114563
  - 0.46452978253364563
  - 0.40946730971336365
  - 46025183232.0
  - 0.4177648723125458
  - 0.40617597103118896
  - 28974178304.0
  - 0.4144773483276367
  - 36021821440.0
  - 50604512.0
  - 0.40626260638237
  - 49518968832.0
  - 0.4072597622871399
  - 0.40752798318862915
loss_records_fold4:
  train_losses:
  - 6.092201679944992
  - 6.06649212539196
  - 6.135797780752182
  - 6.012210065126419
  - 12.397755211591722
  - 7.492982283234596
  - 6.396853181719781
  - 6.3264617979526525
  - 6.207634150981903
  - 6.157950910925866
  - 6.080203370749951
  validation_losses:
  - 51479990272.0
  - 4388408832.0
  - 9512507392.0
  - 0.4204093813896179
  - 0.6009085178375244
  - 0.4111115336418152
  - 0.415400892496109
  - 0.42162811756134033
  - 0.4052838087081909
  - 0.40481463074684143
  - 0.4181421399116516
training fold messages:
  fold 0 training message: completed 30 epochs without stopping early
  fold 1 training message: completed 30 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 3 training message: completed 30 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:04:17.096043'
