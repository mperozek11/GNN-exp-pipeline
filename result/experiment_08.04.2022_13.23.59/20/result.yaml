config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 15:42:09.956461'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/20/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/20/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/20/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/20/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/20/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/20/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/20/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/20/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/20/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/20/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 160.79031562805176
  - 27.159010350704193
  - 22.91545832157135
  - 18.651585042476654
  - 12.371636599302292
  - 12.727850884199142
  - 11.501794219017029
  - 12.070218712091446
  - 14.359246760606766
  - 14.389939993619919
  - 15.548778861761093
  - 11.830395013093948
  - 9.349236696958542
  - 17.40858155488968
  - 18.393478602170944
  - 15.465189635753632
  - 13.03246483206749
  - 9.124341815710068
  - 8.375601202249527
  - 10.950122594833374
  - 8.98663741350174
  - 9.17835333943367
  - 10.19106775522232
  - 8.323936223983765
  - 15.522600561380386
  - 14.117340981960297
  - 14.637897282838821
  - 16.249912708997726
  - 11.166697442531586
  - 9.004801526665688
  - 9.300632745027542
  - 10.458446323871613
  - 9.123422354459763
  - 9.714147865772247
  - 12.946618556976318
  - 10.594198793172836
  - 12.981692999601364
  - 15.510392934083939
  - 15.144219994544983
  - 12.012582182884216
  - 11.776345402002335
  - 11.419927179813385
  - 11.982540845870972
  - 27.793232798576355
  - 12.421190708875656
  - 10.616476848721504
  - 9.572297930717468
  - 10.777723856270313
  - 9.016072124242783
  - 10.43248775601387
  - 11.325494259595871
  - 10.51887309551239
  - 9.014931917190552
  - 10.563646763563156
  - 10.288590878248215
  - 10.13872915506363
  - 13.390076845884323
  - 11.827552765607834
  - 8.046728104352951
  - 9.550837755203247
  - 12.524828791618347
  - 9.872868239879608
  - 8.031510323286057
  - 14.357753545045853
  - 9.72590658068657
  - 12.91509622335434
  - 9.110588610172272
  - 8.065348744392395
  - 10.396189749240875
  - 8.19398808479309
  - 8.65004625916481
  - 10.02655166387558
  - 9.881428450345993
  - 18.447407722473145
  - 8.725229680538177
  - 7.645242869853973
  - 7.420431271195412
  - 7.428952902555466
  - 7.423938274383545
  - 8.035298466682434
  - 7.411116570234299
  - 7.280317917466164
  - 8.241310745477676
  - 7.74117511510849
  - 7.480256617069244
  - 8.388748109340668
  - 8.941744059324265
  - 8.616145730018616
  - 8.802520543336868
  - 9.904353976249695
  - 7.95546481013298
  - 8.321050226688385
  - 7.47799625992775
  - 8.232189267873764
  - 7.929140985012054
  - 7.987779587507248
  - 7.611967533826828
  - 7.246212661266327
  - 7.594663619995117
  - 8.306560188531876
  validation_losses:
  - 1.1805156469345093
  - 1.30173921585083
  - 1.1689342260360718
  - 2.1118505001068115
  - 2.0598742961883545
  - 0.688098132610321
  - 0.4204282760620117
  - 0.6306804418563843
  - 0.4955090582370758
  - 0.5285693407058716
  - 1.603650689125061
  - 0.4030292332172394
  - 0.39217299222946167
  - 0.9556441307067871
  - 0.9497216939926147
  - 0.6057162284851074
  - 0.539890706539154
  - 0.39332151412963867
  - 0.38451918959617615
  - 0.4797181189060211
  - 0.556232213973999
  - 0.44472169876098633
  - 0.4490719735622406
  - 0.6013311743736267
  - 0.42300114035606384
  - 0.999426543712616
  - 0.41084402799606323
  - 0.5179600119590759
  - 0.6796149015426636
  - 0.4543750286102295
  - 0.39313817024230957
  - 0.4115867018699646
  - 0.4532386064529419
  - 0.634375810623169
  - 0.5401093363761902
  - 0.4960755407810211
  - 0.42433929443359375
  - 0.4254515469074249
  - 0.377156138420105
  - 0.8019979000091553
  - 0.5131440162658691
  - 0.3985270857810974
  - 0.39107921719551086
  - 0.4414174556732178
  - 0.5084351897239685
  - 0.4320543706417084
  - 0.40566834807395935
  - 0.39097917079925537
  - 0.5336865782737732
  - 0.45685887336730957
  - 0.37866032123565674
  - 0.3843687176704407
  - 0.4231437146663666
  - 0.47576507925987244
  - 0.6551313996315002
  - 0.7925254106521606
  - 0.38013947010040283
  - 0.44604170322418213
  - 0.46053674817085266
  - 0.530232310295105
  - 0.40806737542152405
  - 0.3832630515098572
  - 0.379456102848053
  - 0.46724459528923035
  - 0.3754352629184723
  - 0.40021175146102905
  - 0.3890231251716614
  - 0.39125490188598633
  - 0.5911301970481873
  - 0.39138564467430115
  - 0.39595863223075867
  - 0.40901318192481995
  - 0.4480484127998352
  - 0.4844056963920593
  - 0.3839249014854431
  - 0.3930513262748718
  - 0.4239172339439392
  - 0.38260018825531006
  - 0.46520987153053284
  - 0.3781239986419678
  - 0.39307069778442383
  - 0.389888197183609
  - 0.41792112588882446
  - 0.39116793870925903
  - 0.3930468261241913
  - 0.43436527252197266
  - 0.42955002188682556
  - 0.47828352451324463
  - 0.5866112112998962
  - 0.42293256521224976
  - 0.3829253017902374
  - 0.39104345440864563
  - 0.4059250056743622
  - 0.4223569631576538
  - 0.4878549575805664
  - 0.40081673860549927
  - 0.3970295190811157
  - 0.43377286195755005
  - 0.45167994499206543
  - 0.3911139667034149
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 67 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 35 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 9 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 33 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:03:33.434150'
