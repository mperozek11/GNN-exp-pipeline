config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.197696'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_8fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_8fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_8fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_8fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_8fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_8fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_8fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_8fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_8fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_8fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 58.652089740335946
  - 15.358638260513544
  - 20.04103983640671
  - 20.931312689185145
  - 13.209911608695984
  - 12.886724770069122
  - 16.17171213030815
  - 13.915543657541276
  - 12.663506975769998
  - 10.088219551742078
  - 12.640973588824274
  - 10.305827493965626
  - 7.546993872523308
  - 6.932622450590134
  - 7.608639946579934
  - 7.130162557959557
  - 7.046837395429612
  - 9.206107969582082
  - 11.424616661667825
  - 8.687863159179688
  - 7.383932477235795
  - 7.09918042421341
  - 7.14960649907589
  - 7.076480269432068
  - 6.161418366432191
  - 6.464660407602787
  - 6.080790513753891
  - 6.000645169615746
  - 6.157594446837902
  - 6.67533732354641
  - 5.722898906469346
  validation_losses:
  - 2.681030511856079
  - 1.1648175716400146
  - 0.43678808212280273
  - 0.43568870425224304
  - 1.7093727588653564
  - 0.3858760595321655
  - 0.44444727897644043
  - 0.47553977370262146
  - 0.48482173681259155
  - 0.7149704694747925
  - 0.3916364014148712
  - 0.4714980125427246
  - 0.39868634939193726
  - 0.4104239344596863
  - 0.3899000883102417
  - 0.3939419388771057
  - 0.4753427505493164
  - 0.3861626088619232
  - 0.3858852982521057
  - 0.3876456320285797
  - 0.38823580741882324
  - 0.43188565969467163
  - 0.519081711769104
  - 0.43352535367012024
  - 1.7868292331695557
  - 0.39410024881362915
  - 0.38022685050964355
  - 0.38467633724212646
  - 0.385585218667984
  - 0.38630157709121704
  - 0.3817613422870636
loss_records_fold1:
  train_losses:
  - 5.8521209299564365
  - 5.887477001547814
  - 6.18076693713665
  - 6.55258164703846
  - 6.002062502503396
  - 5.912982612848282
  - 6.024505931138993
  - 5.88846534639597
  - 6.690279799699784
  - 6.216249850392342
  - 6.135637839138508
  - 6.399243195354939
  - 5.717270156741143
  - 6.003290215134621
  - 5.70699904859066
  - 6.095216301083565
  - 5.759649083018303
  - 5.633436587452889
  - 5.800594261288643
  - 6.044373080134392
  - 5.815324871242047
  - 5.68869570195675
  - 5.860159115493298
  - 5.888446253538132
  - 5.670573434233666
  - 5.628826859593392
  - 5.64634450674057
  - 6.091863539814949
  - 5.9569973886013035
  - 5.7800417155027395
  - 5.766694134473801
  - 5.608625367283821
  - 5.644807615876198
  - 5.85428800880909
  - 5.723323921859265
  - 5.559458082914353
  - 5.984616631269455
  - 5.7463059425354
  - 5.5490958333015445
  - 5.767532441020013
  - 5.633487361669541
  - 5.638098418712616
  - 5.875111263990402
  - 5.790522342920304
  - 5.6813664704561235
  - 5.642487025260926
  - 5.733528211712837
  - 5.803393211960793
  - 5.637508696317673
  - 5.685429120063782
  - 5.948903098702431
  - 5.610372173786164
  - 5.9489845693111425
  - 5.846472063660622
  - 5.8430860400199895
  - 5.705082020163537
  - 6.297494284808636
  - 5.803898262977601
  - 5.6715938538312916
  - 5.638539639115334
  - 5.762712088227272
  - 5.690268365293742
  - 5.727859297394753
  - 5.727909138798714
  - 5.700007882714272
  - 5.857077746093274
  - 5.635239559412003
  - 5.660602569580078
  - 6.081284445524216
  - 5.900028482079506
  - 5.792772710323334
  - 5.580656531453133
  - 5.717001715302468
  - 5.681342226266861
  - 5.75816787481308
  - 5.776645368337632
  - 5.681340304017067
  - 5.914094415307045
  - 5.831232658028603
  - 5.704263755679131
  - 5.653465712070465
  - 5.513245016336441
  - 5.717381602525712
  - 5.562603494524956
  - 5.734090517461301
  - 5.772858881950379
  - 5.666303709149361
  - 5.5847276747226715
  - 5.95584262907505
  - 5.8358051717281345
  - 6.228600025177002
  - 5.997533169388771
  - 5.842465472221375
  - 5.965346169471741
  - 5.757410085201264
  - 5.879663789272309
  - 6.095787730813027
  - 5.858380943536758
  - 6.2960125565528875
  - 5.809119373559952
  validation_losses:
  - 0.40514427423477173
  - 0.39618808031082153
  - 0.4032942056655884
  - 0.8681179881095886
  - 0.4048828184604645
  - 0.44254499673843384
  - 0.7275238633155823
  - 0.5130508542060852
  - 0.43141844868659973
  - 0.40068843960762024
  - 0.4536176919937134
  - 0.4032432436943054
  - 0.40009012818336487
  - 0.3984226882457733
  - 0.4678278863430023
  - 0.456153005361557
  - 0.5374853610992432
  - 0.4995155930519104
  - 0.5073961615562439
  - 0.5888001322746277
  - 0.42308309674263
  - 0.3968658745288849
  - 0.4667159616947174
  - 0.3991660475730896
  - 0.39255255460739136
  - 0.39764273166656494
  - 0.5125628709793091
  - 0.39755138754844666
  - 0.39219099283218384
  - 0.39742571115493774
  - 0.3937233090400696
  - 0.39385178685188293
  - 0.46268391609191895
  - 0.39432403445243835
  - 0.395650714635849
  - 0.3964473009109497
  - 0.39392799139022827
  - 0.393276572227478
  - 0.5755890011787415
  - 0.5257195830345154
  - 0.5378467440605164
  - 0.5989047288894653
  - 0.5382599234580994
  - 0.617217481136322
  - 0.39592453837394714
  - 0.5454268455505371
  - 0.45507949590682983
  - 0.4042864739894867
  - 0.5956429243087769
  - 0.6464033722877502
  - 0.45052310824394226
  - 0.41585418581962585
  - 0.44255170226097107
  - 0.4293449819087982
  - 0.4485764801502228
  - 0.48714861273765564
  - 0.4377230107784271
  - 0.40885818004608154
  - 0.39046546816825867
  - 0.4033854007720947
  - 0.3998466730117798
  - 0.40988689661026
  - 0.4102272391319275
  - 0.3941381573677063
  - 0.49410828948020935
  - 0.4220767319202423
  - 0.410017192363739
  - 0.4589032530784607
  - 0.45976853370666504
  - 0.43410590291023254
  - 0.41373172402381897
  - 0.49714967608451843
  - 0.47435277700424194
  - 0.46960967779159546
  - 0.4291405975818634
  - 0.42130419611930847
  - 0.39201268553733826
  - 0.5942947864532471
  - 0.4758221209049225
  - 0.4262182116508484
  - 0.5271772146224976
  - 0.42368561029434204
  - 0.3924514651298523
  - 0.39875420928001404
  - 0.4003223180770874
  - 0.5244132280349731
  - 0.3965080976486206
  - 0.5820520520210266
  - 0.4725415110588074
  - 0.7265449166297913
  - 0.4339735209941864
  - 0.4144548177719116
  - 0.41045624017715454
  - 0.40229251980781555
  - 0.4036548435688019
  - 0.5110088586807251
  - 0.40942347049713135
  - 0.5077698826789856
  - 0.445252001285553
  - 0.4141598343849182
loss_records_fold2:
  train_losses:
  - 5.965181130170823
  - 5.955544123053551
  - 5.98604072034359
  - 5.9681316912174225
  - 6.148439413309098
  - 5.952959948778153
  - 6.108569023013115
  - 6.251997238397599
  - 5.895045837759972
  - 6.021864369511604
  - 5.865443587303162
  - 6.039595618844032
  - 6.041249924898148
  - 6.9526940256357195
  - 8.912329724431038
  - 6.677724218368531
  - 7.246232786774636
  - 6.266093754768372
  - 6.067071515321732
  - 6.104821580648423
  - 7.336889871954918
  - 5.95509437918663
  - 5.890725830197335
  - 5.841272306442261
  - 6.067727077007294
  - 6.138096809387207
  - 5.948752009868622
  - 6.071480685472489
  - 5.9809955805540085
  - 6.436561730504036
  - 6.1487084358930595
  - 6.422459328174591
  - 6.274232226610184
  - 6.324063909053803
  - 5.953072202205658
  - 5.97203953564167
  - 6.252133384346962
  - 6.178341507911682
  - 6.06018680036068
  - 6.314875292778016
  - 6.019443100690842
  - 6.046978649497032
  - 6.057610669732094
  - 5.927499502897263
  - 5.9986114501953125
  - 5.993038442730904
  - 5.953881821036339
  - 6.071168968081475
  - 5.918666133284569
  - 6.060156229138375
  - 6.361763317883015
  - 6.138750311732292
  - 6.1149526894092565
  - 5.967861512303353
  - 5.906607526540757
  - 5.924644881486893
  - 6.084432455897332
  - 6.023968026041985
  - 6.01651945412159
  - 5.974495476484299
  - 6.0144136369228365
  - 5.996310377120972
  - 5.99081635773182
  - 6.422823041677475
  - 5.903216674923897
  - 6.100793820619583
  - 6.0993280738592155
  - 5.956984969973565
  - 5.993290084600449
  - 6.188261926174164
  - 6.003172069787979
  - 6.404774165153504
  - 5.9716752707958225
  - 6.01118121445179
  - 5.843646895885468
  - 5.927789115905762
  - 5.962891483306885
  - 5.9954491525888445
  validation_losses:
  - 0.3810350000858307
  - 0.4047243893146515
  - 0.39007318019866943
  - 0.38546279072761536
  - 0.390625
  - 0.3926815390586853
  - 0.38115194439888
  - 0.3900110423564911
  - 0.38289937376976013
  - 0.4478800296783447
  - 0.3978934586048126
  - 0.3864569664001465
  - 0.41933441162109375
  - 0.815563976764679
  - 0.3897724449634552
  - 0.4007780849933624
  - 0.43538886308670044
  - 0.3884902894496918
  - 0.3952949345111847
  - 5.229890823364258
  - 0.38162121176719666
  - 0.4026243984699249
  - 0.38769882917404175
  - 0.38880231976509094
  - 0.3984634578227997
  - 0.38159480690956116
  - 0.3810606002807617
  - 0.3944917321205139
  - 2608.705810546875
  - 0.3795805275440216
  - 281.5846862792969
  - 0.42137226462364197
  - 0.5713534951210022
  - 20.2125244140625
  - 1.2575139999389648
  - 1.4451344013214111
  - 0.3894711136817932
  - 0.4106559157371521
  - 0.4835011959075928
  - 0.4205072224140167
  - 0.3864233195781708
  - 0.6183422803878784
  - 0.3869117200374603
  - 0.38709574937820435
  - 0.38504326343536377
  - 17.835899353027344
  - 0.7213769555091858
  - 0.3837517201900482
  - 0.40304702520370483
  - 0.39764946699142456
  - 0.41978541016578674
  - 0.43216392397880554
  - 0.38382869958877563
  - 0.3898996412754059
  - 0.3884974718093872
  - 0.38669002056121826
  - 0.3840121328830719
  - 0.39648061990737915
  - 0.4024673104286194
  - 0.38736116886138916
  - 0.39517098665237427
  - 0.4010418951511383
  - 0.4111272096633911
  - 0.38120773434638977
  - 0.38218340277671814
  - 0.3843832015991211
  - 0.4209153354167938
  - 0.3959265351295471
  - 0.41234803199768066
  - 0.3854011297225952
  - 0.3868784010410309
  - 0.3987319767475128
  - 0.38784363865852356
  - 0.3945944309234619
  - 0.392219215631485
  - 0.3923722505569458
  - 0.39202961325645447
  - 0.391215980052948
loss_records_fold3:
  train_losses:
  - 5.929480183124543
  - 5.902822670340538
  - 5.81013123691082
  - 6.18895879983902
  - 6.120162573456764
  - 6.117207562923432
  - 5.852480414509774
  - 6.017701488733292
  - 5.818728435039521
  - 5.862556649744511
  - 5.835016682744026
  - 5.95371290743351
  - 5.818469819426537
  - 5.9553492218256
  - 6.037653091549874
  validation_losses:
  - 0.4403998553752899
  - 0.3982528746128082
  - 0.4190870225429535
  - 0.46989157795906067
  - 0.47403836250305176
  - 0.3985172212123871
  - 0.4339940547943115
  - 0.39818915724754333
  - 0.42164888978004456
  - 0.41315460205078125
  - 0.4042890965938568
  - 0.3975799083709717
  - 0.4011242389678955
  - 0.40555325150489807
  - 0.3980138599872589
loss_records_fold4:
  train_losses:
  - 5.9686541423201565
  - 6.030495837330818
  - 5.858902037143707
  - 6.0395442098379135
  - 5.931530871987343
  - 5.943117311596871
  - 6.035103997588158
  - 5.88802684545517
  - 6.120008930563927
  - 5.879951247572899
  - 6.1520030289888386
  - 5.84517257809639
  - 6.088102012872696
  - 6.203321531414986
  - 5.819117298722268
  - 5.969694641232491
  - 5.918029889464378
  - 6.202433681488038
  - 5.998043286800385
  - 5.8856306165456775
  - 5.840684053301811
  - 5.911756806075573
  - 6.269506880640984
  - 6.258275899291039
  - 5.910862338542938
  - 6.088789720833302
  - 6.134591099619866
  - 6.041716611385346
  - 5.8006591439247135
  validation_losses:
  - 0.4014990031719208
  - 0.40060850977897644
  - 0.39664342999458313
  - 0.42809733748435974
  - 0.39065760374069214
  - 0.39690476655960083
  - 0.3971487283706665
  - 0.3895680904388428
  - 0.40090495347976685
  - 0.41145405173301697
  - 0.3963989317417145
  - 0.3899463415145874
  - 0.40521693229675293
  - 0.44737595319747925
  - 0.39747002720832825
  - 0.39573225378990173
  - 0.41170036792755127
  - 0.3912355899810791
  - 0.3912450969219208
  - 0.3984304666519165
  - 0.3909628391265869
  - 0.3910816013813019
  - 0.4551694095134735
  - 0.3896036148071289
  - 0.39868873357772827
  - 0.3920382857322693
  - 0.3888295590877533
  - 0.39309242367744446
  - 0.4009239971637726
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 31 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 78 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8542024013722127, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.0
  total_train_time: '0:24:45.583704'
