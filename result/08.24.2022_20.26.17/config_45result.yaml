config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:27:12.572124'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_45fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_45fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_45fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_45fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_45fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_45fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_45fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_45fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_45fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_45fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 60.682984590530396
  - 25.848875296115878
  - 22.337331247329715
  - 15.486630254983902
  - 13.999836695194245
  - 11.546610647439957
  - 9.857511335611344
  - 8.038811659812927
  - 6.404008311033249
  - 4.56679311990738
  - 5.799152684211731
  - 5.86931843161583
  - 11.315463900566101
  - 9.6346115231514
  - 5.558929219841957
  - 9.031554532051087
  - 13.0888145506382
  - 8.749647793173791
  - 10.128039795160294
  - 7.315743225812913
  - 6.349925184249878
  - 9.968616616725923
  - 6.778549015522003
  - 6.780797004699707
  - 11.932525509595871
  - 9.407540401816368
  - 7.107835334539414
  - 10.491937011480331
  - 8.240013873577118
  - 5.905527192354203
  - 8.67271602153778
  - 5.564325198531151
  - 5.996127027273179
  - 6.60260626077652
  - 8.488070660829544
  - 6.39148622751236
  - 6.0161751210689545
  - 5.428846901655198
  - 4.92164341211319
  - 4.865401089191437
  - 6.002761137485504
  - 6.699169343709946
  - 4.5570117950439455
  - 3.82646464407444
  - 3.526583969593048
  - 6.799003720283508
  - 8.931700956821443
  - 8.20960208773613
  - 5.376195049285889
  - 6.081119161844254
  - 4.952920711040497
  - 4.87648857831955
  - 3.6685288548469543
  - 4.1669622480869295
  - 4.200216475129127
  - 5.288550269603729
  - 5.119877433776856
  - 3.779508197307587
  - 4.8471633225679405
  - 3.1963948607444763
  - 4.400916492938996
  - 4.165933066606522
  - 4.311593466997147
  - 5.10001277923584
  - 7.835198643803597
  - 6.24828161597252
  - 3.7895546793937687
  - 3.5836611926555637
  - 3.7161468625068665
  - 3.4011179864406587
  - 4.1756055712699895
  - 8.275308269262315
  - 4.32289822101593
  - 4.747184675931931
  - 3.335867860913277
  - 4.134533372521401
  - 4.143683698773384
  - 3.45013345181942
  - 3.8149612843990326
  - 3.352925100922585
  - 3.614631310105324
  - 3.324586266279221
  - 3.271445643901825
  - 3.508851182460785
  - 3.5641129076480866
  - 3.418577152490616
  - 3.4404239654541016
  - 3.2815639615058902
  - 3.845284324884415
  - 3.4566233068704606
  - 3.7005671918392182
  - 3.72020777463913
  - 4.167223441600799
  - 3.4177626192569734
  - 3.424262070655823
  - 3.6815957814455036
  - 3.9891637921333314
  - 3.540934228897095
  - 3.3110905528068546
  - 3.303728532791138
  validation_losses:
  - 2.830965042114258
  - 6.32990837097168
  - 2.481685161590576
  - 0.8284267783164978
  - 0.6064867973327637
  - 0.5899458527565002
  - 0.736129105091095
  - 0.6126396059989929
  - 0.5786895155906677
  - 0.4696716070175171
  - 0.4608759582042694
  - 0.4545508027076721
  - 1.2263199090957642
  - 0.428494930267334
  - 1.4800010919570923
  - 0.5791603922843933
  - 0.5036011934280396
  - 0.681047260761261
  - 0.6348705291748047
  - 0.5615921020507812
  - 0.715571939945221
  - 0.3999045491218567
  - 0.4245395064353943
  - 0.4743679165840149
  - 0.8248798847198486
  - 0.5190467238426208
  - 0.5074706673622131
  - 0.5506201982498169
  - 0.5295096039772034
  - 0.498786598443985
  - 0.5956363677978516
  - 0.4901156723499298
  - 0.4204537868499756
  - 0.8261460661888123
  - 0.5903236865997314
  - 0.8702482581138611
  - 0.4518108069896698
  - 0.45103663206100464
  - 0.6428439617156982
  - 0.8773229122161865
  - 0.5137761831283569
  - 0.4838520884513855
  - 0.42373141646385193
  - 0.42931902408599854
  - 0.39094170928001404
  - 0.6344826817512512
  - 0.4813404381275177
  - 0.4090617299079895
  - 0.4724351167678833
  - 0.5610206127166748
  - 0.46960580348968506
  - 0.4117034077644348
  - 0.5084805488586426
  - 0.44966620206832886
  - 0.4749325215816498
  - 0.4288657307624817
  - 0.46067357063293457
  - 0.4383591115474701
  - 0.4661833345890045
  - 0.4060145616531372
  - 0.49285775423049927
  - 0.5415632724761963
  - 0.4002392590045929
  - 0.4311385452747345
  - 0.48353543877601624
  - 0.39642438292503357
  - 0.45901724696159363
  - 0.39512506127357483
  - 0.4284663498401642
  - 0.41797927021980286
  - 0.43281757831573486
  - 1.1104427576065063
  - 1.0802605152130127
  - 0.6522173285484314
  - 0.4863469898700714
  - 0.657337486743927
  - 0.4359641969203949
  - 0.5824116468429565
  - 0.5755115151405334
  - 0.39789921045303345
  - 0.42988747358322144
  - 0.3944516181945801
  - 0.5456902384757996
  - 0.40341371297836304
  - 0.534400463104248
  - 0.6780393123626709
  - 0.613131582736969
  - 0.4326356053352356
  - 0.4452734589576721
  - 0.5034410357475281
  - 0.6687783002853394
  - 0.49945399165153503
  - 0.4139692783355713
  - 0.4803217649459839
  - 0.4441588521003723
  - 0.5290737152099609
  - 0.4392257034778595
  - 0.42182114720344543
  - 0.4441205561161041
  - 0.4090682864189148
loss_records_fold1:
  train_losses:
  - 3.231391727924347
  - 3.316509866714478
  - 3.246115815639496
  - 3.2758916437625887
  - 3.3570312142372134
  - 3.256087452173233
  - 3.4557012736797335
  - 3.2060281097888947
  - 3.7916043221950533
  - 3.3765645146369936
  - 3.454499399662018
  - 3.1447960376739506
  - 3.2003141701221467
  - 3.528422617912293
  - 3.2428281605243683
  - 3.0968194365501405
  - 3.0972918033599854
  - 3.0847871720790865
  - 3.1436288177967073
  - 3.034542030096054
  - 3.061067223548889
  - 3.387357982993126
  - 3.058054003119469
  - 3.199787431955338
  - 3.1262861251831056
  - 3.1599528163671495
  - 3.1503040909767153
  validation_losses:
  - 0.4063246250152588
  - 0.4421997666358948
  - 0.41630759835243225
  - 0.42183491587638855
  - 0.42750051617622375
  - 0.43609681725502014
  - 0.41845422983169556
  - 0.44201651215553284
  - 0.5076475143432617
  - 0.42254576086997986
  - 0.5021488070487976
  - 0.40264275670051575
  - 0.45787230134010315
  - 0.5164426565170288
  - 0.47093671560287476
  - 0.40047338604927063
  - 0.409135103225708
  - 0.3954523801803589
  - 0.390638530254364
  - 0.4357357919216156
  - 0.46899545192718506
  - 0.4323413372039795
  - 0.4279240071773529
  - 0.43291768431663513
  - 0.442482590675354
  - 0.4277624487876892
  - 0.43006381392478943
loss_records_fold2:
  train_losses:
  - 3.1419159173965454
  - 3.314650869369507
  - 3.3300551712512974
  - 3.148224538564682
  - 3.1671704202890396
  - 3.2551521539688113
  - 3.0491508841514587
  - 3.1581937044858934
  - 3.3418316483497623
  - 3.275875499844551
  - 3.076881903409958
  - 3.1821614027023317
  - 3.1680261492729187
  - 3.247263026237488
  - 3.233898985385895
  - 3.1755866706371307
  - 3.615282762050629
  - 3.2550059437751773
  - 3.637560433149338
  - 3.3061267226934437
  - 4.677654445171356
  - 3.6646939277648927
  - 3.8849364519119263
  - 3.3869525462388994
  - 3.320464599132538
  - 3.335279303789139
  - 3.3872202873229984
  - 3.1830127596855164
  - 3.4337955176830293
  - 3.270352676510811
  - 3.511892056465149
  - 3.3016261905431747
  - 3.1023206949234012
  - 3.1016329646110536
  - 3.1826609849929812
  - 3.4366059184074405
  - 3.2816433489322665
  - 3.197786033153534
  - 3.14724093079567
  - 3.231008040904999
  - 3.344867950677872
  - 3.122943896055222
  - 3.064823079109192
  - 3.098524603247643
  - 3.1487260639667514
  - 3.1160184085369114
  - 3.3254850029945375
  - 3.0955789715051654
  - 3.0901869118213656
  - 3.166916698217392
  - 3.0732168793678287
  - 3.146020153164864
  - 3.1140995025634766
  - 3.0708271235227587
  - 3.1062957465648653
  - 3.0670665919780733
  - 3.129820364713669
  - 3.2005339443683627
  - 3.279005724191666
  - 3.136729872226715
  - 3.054190266132355
  - 3.169850927591324
  - 3.059462821483612
  - 3.212425023317337
  - 3.426503810286522
  - 3.1258573770523075
  - 3.1551594734191895
  - 3.233256715536118
  - 3.419137191772461
  - 3.380082428455353
  - 3.153761395812035
  - 3.156822830438614
  - 3.0642882883548737
  - 3.112438291311264
  - 3.2079470068216325
  - 3.3094821333885194
  - 3.211921858787537
  - 3.120545756816864
  - 3.117377978563309
  validation_losses:
  - 0.3898743689060211
  - 0.40209898352622986
  - 0.42561790347099304
  - 0.4352279007434845
  - 0.40754440426826477
  - 0.39394283294677734
  - 0.39780429005622864
  - 0.459590345621109
  - 0.43367403745651245
  - 0.45444414019584656
  - 0.3817915916442871
  - 0.37485599517822266
  - 0.41637206077575684
  - 0.7865729331970215
  - 0.4107474982738495
  - 0.4452924132347107
  - 0.3848629593849182
  - 0.3893396258354187
  - 0.4876767694950104
  - 1.5677589178085327
  - 0.5404911041259766
  - 0.4847506880760193
  - 0.4002225399017334
  - 0.770547091960907
  - 2.003667116165161
  - 2.971808433532715
  - 1.808313250541687
  - 0.919992983341217
  - 1.2062909603118896
  - 1.0474275350570679
  - 1.3220382928848267
  - 1.1324440240859985
  - 1.9864975214004517
  - 2.2958827018737793
  - 0.7032612562179565
  - 0.5885342359542847
  - 0.6993513107299805
  - 0.4725918769836426
  - 0.6628422737121582
  - 0.4405500888824463
  - 0.4288575053215027
  - 0.38426417112350464
  - 0.39976826310157776
  - 0.4214658737182617
  - 0.4033283293247223
  - 0.6581445932388306
  - 0.42304524779319763
  - 0.4042412042617798
  - 0.6752662658691406
  - 0.40198686718940735
  - 0.41044604778289795
  - 0.44718503952026367
  - 0.3949670195579529
  - 0.53631991147995
  - 0.4459810256958008
  - 0.3927308917045593
  - 0.5491495132446289
  - 0.7596407532691956
  - 0.6161893010139465
  - 0.39798498153686523
  - 0.3973034918308258
  - 0.3827870488166809
  - 0.41101017594337463
  - 0.42355260252952576
  - 0.41935837268829346
  - 0.44280651211738586
  - 0.42047667503356934
  - 0.4096919596195221
  - 0.40829703211784363
  - 0.43327826261520386
  - 0.43477219343185425
  - 0.4042050242424011
  - 0.550606906414032
  - 0.454807311296463
  - 0.4445056617259979
  - 0.434418648481369
  - 0.4178532361984253
  - 0.4071764349937439
  - 0.4042837917804718
loss_records_fold3:
  train_losses:
  - 3.2330232799053196
  - 3.124621495604515
  - 3.1826732605695724
  - 3.170589652657509
  - 3.005520388484001
  - 3.1814220905303956
  - 3.090411847829819
  - 3.163665854930878
  - 3.076063668727875
  - 3.05418119430542
  - 3.105717957019806
  - 3.079488044977188
  - 3.284906989336014
  - 3.1834329545497897
  - 3.091411375999451
  - 3.048187786340714
  - 3.102796417474747
  - 3.163418710231781
  - 3.119945311546326
  - 3.1425446510314945
  - 3.0812317728996277
  - 3.635003608465195
  - 3.367545819282532
  - 3.2642963528633118
  - 3.178741067647934
  - 3.0209173381328585
  - 3.2025428950786594
  - 3.1427017927169802
  - 3.190339481830597
  - 3.143674075603485
  - 3.2009090662002566
  - 3.229648619890213
  - 3.2292314171791077
  - 3.1878644049167635
  - 3.289025875926018
  - 3.260610908269882
  - 3.2406455993652346
  - 3.148493611812592
  - 3.18915091753006
  - 3.068917191028595
  - 3.1440383672714236
  - 3.1409694015979768
  - 3.1564277529716493
  - 3.0594911724328995
  - 3.1846934854984283
  - 3.2381940960884097
  - 3.4007258057594303
  - 3.1846180319786073
  - 3.2801434040069584
  - 3.2457524955272676
  validation_losses:
  - 0.412679523229599
  - 0.3907497525215149
  - 2.2299323081970215
  - 20.70924186706543
  - 0.4043256640434265
  - 0.3957449495792389
  - 0.44905126094818115
  - 0.43569186329841614
  - 0.4053989052772522
  - 0.41617316007614136
  - 0.4702095091342926
  - 0.4644298553466797
  - 0.5321093201637268
  - 416.260498046875
  - 1.7039419412612915
  - 0.393857479095459
  - 0.4108196198940277
  - 0.39545124769210815
  - 46.00027084350586
  - 2221.828857421875
  - 224.51487731933594
  - 3.283804416656494
  - 5.679441452026367
  - 0.43693819642066956
  - 0.7573422193527222
  - 0.5255398750305176
  - 0.4392581880092621
  - 0.4113888442516327
  - 0.42315858602523804
  - 0.42903974652290344
  - 0.48158520460128784
  - 0.41644108295440674
  - 0.48450785875320435
  - 0.4233958125114441
  - 0.42224663496017456
  - 0.416226863861084
  - 0.4324135482311249
  - 0.4514102041721344
  - 0.42065903544425964
  - 0.41542524099349976
  - 0.41594842076301575
  - 0.40375903248786926
  - 0.42735058069229126
  - 0.4565601944923401
  - 0.4252985119819641
  - 0.4266338646411896
  - 0.42028263211250305
  - 0.41818735003471375
  - 0.4111495316028595
  - 0.4145135283470154
loss_records_fold4:
  train_losses:
  - 3.2070573151111605
  - 3.1684105455875398
  - 3.127139091491699
  - 3.227203214168549
  - 3.1400425374507908
  - 3.2034835875034333
  - 3.089981520175934
  - 3.1762847363948823
  - 3.168398576974869
  - 3.1889619886875153
  - 3.1428497016429904
  - 3.2854474961757663
  - 3.1230384349823
  - 3.2267530381679537
  - 3.1534193605184555
  - 3.084905481338501
  - 3.0430390536785126
  - 3.1472586750984193
  - 3.285002970695496
  - 3.1521988928318025
  - 3.194206428527832
  - 3.124174076318741
  - 3.1135911107063294
  - 3.318339961767197
  - 3.138832485675812
  - 3.187145060300827
  - 3.1919465541839602
  - 3.169791239500046
  - 3.149749231338501
  - 3.165611332654953
  - 3.339702391624451
  - 3.102956533432007
  - 3.229051995277405
  - 3.2675929188728334
  - 3.2980065047740936
  - 3.2550329983234407
  - 3.224791127443314
  - 3.2156537353992465
  - 3.1778963416814805
  - 3.312366908788681
  - 3.4741116166114807
  - 3.2485384702682496
  - 3.1643442630767824
  - 3.1823527216911316
  - 3.2501661717891697
  - 3.384552532434464
  validation_losses:
  - 0.3922233581542969
  - 0.3955831825733185
  - 0.38354572653770447
  - 0.405533105134964
  - 0.41635066270828247
  - 0.40624019503593445
  - 0.40695488452911377
  - 0.4119791090488434
  - 0.4075024127960205
  - 0.4278341233730316
  - 0.44314926862716675
  - 0.43594345450401306
  - 0.4203792214393616
  - 0.4207563102245331
  - 0.3988878130912781
  - 0.41315022110939026
  - 0.40952256321907043
  - 0.39841166138648987
  - 0.4155484139919281
  - 0.41135966777801514
  - 0.39496710896492004
  - 0.4145292341709137
  - 0.3938334286212921
  - 0.43683987855911255
  - 0.4097702205181122
  - 0.40411049127578735
  - 0.3998430371284485
  - 0.41185706853866577
  - 0.40902507305145264
  - 0.4585787355899811
  - 0.4003582298755646
  - 0.4070611298084259
  - 0.4124983847141266
  - 0.4064924418926239
  - 0.4408138394355774
  - 0.4423028528690338
  - 0.40612590312957764
  - 0.41257160902023315
  - 0.3985759913921356
  - 0.41972917318344116
  - 0.4148022532463074
  - 0.4163833558559418
  - 0.4097663462162018
  - 0.40993478894233704
  - 0.40517374873161316
  - 0.4054848849773407
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 79 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 50 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 46 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:29:09.350972'
