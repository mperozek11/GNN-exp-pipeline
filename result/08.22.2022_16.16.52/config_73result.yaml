config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:51:43.832004'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_73fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_73fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_73fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_73fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_73fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_73fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_73fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_73fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_73fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_73fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 43.69409999251366
  - 6.547598657011986
  - 6.115916085243225
  - 14.490133351087572
  - 13.189425867795945
  - 9.478124165534973
  - 7.323275190591812
  - 5.075375863909722
  - 5.854632538557053
  - 12.610344135761261
  - 11.512662401795389
  - 7.528133919835091
  - 5.258354157209396
  - 7.1693761408329015
  - 5.3248521566391
  - 6.085666066408158
  - 5.102868059277535
  - 3.8474430054426194
  - 3.020663842558861
  - 3.346048554778099
  - 4.569529089331627
  - 3.545125561952591
  - 4.264137071371079
  - 3.305683144927025
  - 4.896923720836639
  - 4.127563217282296
  - 4.707237738370895
  - 4.109600132703782
  - 4.964111000299454
  - 2.9443596422672274
  - 3.561374640464783
  - 3.7237897455692295
  - 4.11945258975029
  - 3.8937910676002505
  - 3.700779616832733
  - 3.7004308611154557
  - 6.13144525885582
  - 3.317000198364258
  - 3.179390522837639
  - 3.7650598883628845
  - 3.3304470121860508
  - 2.935598683357239
  - 2.8817879259586334
  - 3.5991392165422442
  - 2.9608089059591296
  - 3.3964717119932177
  - 3.007059296965599
  - 3.961975932121277
  - 3.0648830175399784
  - 3.261018490791321
  - 3.486318224668503
  - 3.6268472522497177
  - 3.1177760750055317
  - 3.6208168745040896
  - 3.3851187109947207
  - 3.1751478701829914
  - 3.294956731796265
  - 4.492400881648064
  - 3.5484911650419235
  - 3.250752329826355
  - 3.148970276117325
  - 3.010123330354691
  - 3.611449781060219
  - 3.541750425100327
  - 3.507366895675659
  - 3.031211414933205
  - 3.0396344840526583
  - 3.1625210642814636
  - 2.9983172178268434
  - 3.0061539292335513
  - 3.034312230348587
  - 2.969769424200058
  - 2.8958170950412754
  - 2.939060550928116
  - 3.162386187911034
  - 2.9745826691389086
  - 2.939076042175293
  - 2.908019307255745
  - 2.956459465622902
  - 3.2756501972675327
  - 2.934063044190407
  - 2.8618098318576815
  - 2.9115228176116945
  - 2.932578384876251
  - 2.9175606906414036
  - 3.0120385110378267
  - 2.9541515052318577
  - 3.0074984937906266
  - 2.969266325235367
  - 2.886329081654549
  - 2.939518338441849
  - 3.1147941887378696
  - 2.89567523598671
  - 2.8889535427093507
  - 2.826711505651474
  - 2.897652390599251
  validation_losses:
  - 2.1726999282836914
  - 0.8314986228942871
  - 1.7231396436691284
  - 0.5933179259300232
  - 0.6897528767585754
  - 0.7183140516281128
  - 0.6401742696762085
  - 0.490131676197052
  - 0.44301313161849976
  - 0.4083288311958313
  - 0.5524315237998962
  - 0.406913161277771
  - 0.4085771143436432
  - 0.41838595271110535
  - 0.584252655506134
  - 0.4125339984893799
  - 0.40194588899612427
  - 0.40206679701805115
  - 0.4215582013130188
  - 0.4227437674999237
  - 0.39893069863319397
  - 0.4371541142463684
  - 0.518425464630127
  - 0.3864137828350067
  - 0.39250391721725464
  - 0.38555824756622314
  - 0.3912927210330963
  - 0.4097698926925659
  - 0.3851644694805145
  - 0.38588058948516846
  - 0.41453874111175537
  - 0.38891786336898804
  - 0.38706281781196594
  - 0.3850211799144745
  - 0.38695260882377625
  - 0.38319459557533264
  - 0.42256981134414673
  - 0.391727477312088
  - 0.3996744155883789
  - 0.44383302330970764
  - 0.3844034671783447
  - 0.38415631651878357
  - 0.3853180706501007
  - 0.38962048292160034
  - 0.44973883032798767
  - 0.3875041604042053
  - 0.382469117641449
  - 0.3765074610710144
  - 0.39446163177490234
  - 0.39729997515678406
  - 0.3919282555580139
  - 0.438005268573761
  - 2.8521742820739746
  - 0.4394208490848541
  - 0.42144575715065
  - 0.47311827540397644
  - 0.38613682985305786
  - 0.46665632724761963
  - 0.40389978885650635
  - 0.41452738642692566
  - 0.38279274106025696
  - 0.3783663213253021
  - 0.4183620512485504
  - 0.40874016284942627
  - 0.3932644724845886
  - 0.39293116331100464
  - 0.4447895884513855
  - 0.39051681756973267
  - 0.3896214962005615
  - 0.40202969312667847
  - 0.40685147047042847
  - 0.4046993851661682
  - 0.4177699089050293
  - 0.4104234576225281
  - 0.3902231454849243
  - 0.39477670192718506
  - 0.3961525559425354
  - 0.37974363565444946
  - 0.39455655217170715
  - 0.39666852355003357
  - 0.38253262639045715
  - 0.38131362199783325
  - 0.37755465507507324
  - 0.37793654203414917
  - 0.4323970079421997
  - 0.37913617491722107
  - 0.4214008152484894
  - 0.3794528543949127
  - 0.3836371600627899
  - 0.39832669496536255
  - 0.3798324763774872
  - 0.38048702478408813
  - 0.3843575417995453
  - 0.3753421902656555
  - 0.3779866099357605
  - 0.38555312156677246
loss_records_fold1:
  train_losses:
  - 2.7916730076074603
  - 2.817822200059891
  - 2.8116145551204683
  - 2.851818832755089
  - 2.884411409497261
  - 2.834281760454178
  - 2.755304065346718
  - 2.9511994272470474
  - 2.791361612081528
  - 2.906977951526642
  - 2.9420649766922
  - 2.89168858230114
  - 2.8222990423440937
  - 2.91947158575058
  - 2.981333661079407
  - 2.8702874898910524
  - 3.6780657827854157
  - 4.050484895706177
  - 3.1910386592149735
  - 2.9439755916595463
  - 2.953109320998192
  - 2.9665984839200976
  - 2.898937457799912
  - 2.7911458611488342
  - 2.814195948839188
  - 2.900046467781067
  - 3.013047516345978
  - 2.9103630632162094
  - 2.8077613651752475
  validation_losses:
  - 0.4657033383846283
  - 0.39476320147514343
  - 0.3944110572338104
  - 0.4147980809211731
  - 0.3940686285495758
  - 0.4023314416408539
  - 0.3980023264884949
  - 0.404487282037735
  - 0.4688586890697479
  - 0.4236970543861389
  - 0.3940252959728241
  - 0.39958426356315613
  - 0.45595604181289673
  - 0.41391655802726746
  - 0.4072190225124359
  - 0.44980618357658386
  - 0.6483867168426514
  - 0.4012839198112488
  - 0.4227551519870758
  - 0.6483216881752014
  - 0.6596388816833496
  - 0.4148794710636139
  - 0.5316595435142517
  - 0.40522921085357666
  - 0.4034090042114258
  - 0.4009040296077728
  - 0.40024352073669434
  - 0.4000939726829529
  - 0.40001043677330017
loss_records_fold2:
  train_losses:
  - 2.8809017449617387
  - 2.865007716417313
  - 2.884716731309891
  - 2.8912674456834795
  - 2.855837154388428
  - 2.9002292245626453
  - 2.940314441919327
  - 2.98410436809063
  - 3.0428427040576937
  - 3.001163637638092
  - 2.9298204839229585
  - 2.9398436307907105
  - 3.0420274436473846
  - 3.0361530721187595
  - 2.9531904041767123
  - 2.9977039963006975
  - 3.1494782090187075
  - 3.1645910382270817
  - 3.463807925581932
  - 3.129929593205452
  - 2.953720995783806
  - 3.0035533994436268
  - 2.970058906078339
  validation_losses:
  - 0.418417364358902
  - 0.44828376173973083
  - 1.3488481044769287
  - 0.41285791993141174
  - 0.3786112368106842
  - 0.409633994102478
  - 0.39281925559043884
  - 0.5352364182472229
  - 224.7221221923828
  - 0.7499767541885376
  - 0.3872066140174866
  - 0.4235612154006958
  - 0.3985607326030731
  - 0.3830294907093048
  - 0.38586294651031494
  - 0.3808375597000122
  - 439.02386474609375
  - 0.4047282636165619
  - 0.4104340970516205
  - 0.3771602213382721
  - 0.3836260735988617
  - 0.3833409547805786
  - 0.3698740005493164
loss_records_fold3:
  train_losses:
  - 2.9520089358091357
  - 2.9635548114776613
  - 2.9148730039596558
  - 3.2048737049102787
  - 3.016918247938156
  - 3.1854889482259754
  - 3.089098072052002
  - 3.604058688879013
  - 2.98731946349144
  - 2.923836100101471
  - 2.9933656215667725
  - 3.0377653717994693
  - 2.975517356395722
  - 2.9731473594903948
  - 2.9616692990064624
  - 2.966334539651871
  - 2.9642701596021652
  - 2.9447069078683854
  - 3.0531310319900515
  - 2.9689574956893923
  - 2.9952525675296786
  - 5.0556240558624275
  - 3.951665306091309
  - 3.546706306934357
  - 3.6040596038103105
  - 3.2017140805721285
  - 3.633440947532654
  - 3.3609608650207523
  - 3.064775574207306
  - 2.8985813140869143
  - 2.9372569501399997
  - 3.103819739818573
  - 3.0219884634017946
  - 2.9979368448257446
  - 2.949805861711502
  - 3.0701993972063066
  - 2.998896762728691
  - 3.0114083111286165
  - 3.011175322532654
  - 2.9990983545780185
  - 3.046419382095337
  - 2.9460774183273317
  - 2.9285086512565615
  - 3.0199030339717865
  - 3.0506143957376484
  - 3.097657084465027
  - 3.0004312098026276
  - 3.004024809598923
  - 3.1085781395435337
  - 3.1056962549686435
  - 2.9836047768592837
  - 2.9095991373062136
  validation_losses:
  - 0.4481043219566345
  - 0.38290271162986755
  - 0.38920241594314575
  - 0.42942503094673157
  - 0.39670372009277344
  - 0.39542099833488464
  - 0.3972266614437103
  - 0.3944903016090393
  - 0.3966570198535919
  - 0.39599648118019104
  - 0.4099494516849518
  - 0.3999161720275879
  - 0.41198599338531494
  - 0.3951708972454071
  - 0.39608725905418396
  - 0.3960834741592407
  - 0.3961470127105713
  - 0.4458790123462677
  - 0.4021725654602051
  - 0.41402551531791687
  - 0.40294522047042847
  - 0.39806652069091797
  - 0.39579829573631287
  - 0.4115767478942871
  - 0.403560072183609
  - 0.3952156901359558
  - 0.39232611656188965
  - 0.4295445382595062
  - 0.4039149284362793
  - 0.39510172605514526
  - 0.39451295137405396
  - 0.44079506397247314
  - 0.3967607319355011
  - 0.40183955430984497
  - 0.39752110838890076
  - 0.39628422260284424
  - 0.4102960228919983
  - 0.39979130029678345
  - 0.39438164234161377
  - 0.3993831276893616
  - 0.45639652013778687
  - 0.39517074823379517
  - 0.39906612038612366
  - 0.4062883257865906
  - 0.4083305597305298
  - 0.44142946600914
  - 0.395302951335907
  - 0.39597272872924805
  - 0.3980015218257904
  - 0.4031640291213989
  - 0.3977496325969696
  - 0.39896097779273987
loss_records_fold4:
  train_losses:
  - 2.959917986392975
  - 2.911372968554497
  - 3.040872445702553
  - 3.024623987078667
  - 3.0705059230327607
  - 3.0062549352645878
  - 2.980767378211022
  - 3.025018683075905
  - 3.043127492070198
  - 3.1415504395961764
  - 2.9629317700862887
  - 2.953175738453865
  - 2.915101701021195
  - 3.0363613665103912
  - 2.9951326668262483
  validation_losses:
  - 0.38979777693748474
  - 0.3916628360748291
  - 0.48972994089126587
  - 0.39328062534332275
  - 0.41818541288375854
  - 0.393119752407074
  - 0.390024334192276
  - 0.3943806290626526
  - 0.44671863317489624
  - 0.39069050550460815
  - 0.3982410430908203
  - 0.3905860483646393
  - 0.3961278796195984
  - 0.3922024667263031
  - 0.39861735701560974
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 96 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 52 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8610634648370498, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.024096385542168676, 0.0, 0.0]'
  mean_eval_accuracy: 0.8589568118453549
  mean_f1_accuracy: 0.004819277108433735
  total_train_time: '0:19:45.452537'
