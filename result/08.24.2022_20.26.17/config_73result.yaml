config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:08:16.935437'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_73fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_73fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_73fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_73fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_73fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_73fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_73fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_73fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_73fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_73fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 47.19820086359978
  - 6.83590488433838
  - 6.384066820144653
  - 11.620300281047822
  - 13.059180614352227
  - 5.643130856752396
  - 6.3571220099926
  - 5.369015336036682
  - 7.692739540338517
  - 10.963965892791748
  - 7.286884272098542
  - 5.654636257886887
  - 4.138060188293458
  - 9.040873187780381
  - 8.834252992272377
  - 9.834636652469635
  - 6.772306933999062
  - 5.5874233692884445
  - 3.541756954789162
  - 3.364417159557343
  - 4.425828170776367
  - 3.6737798333168032
  - 4.137123259902
  - 6.368683153390885
  - 4.2908352524042135
  - 3.19558941423893
  - 4.405040693283081
  - 4.29103307723999
  - 3.9123788088560105
  - 3.0922504693269732
  - 3.361213690042496
  - 3.303625220060349
  - 3.805673205852509
  - 3.901485061645508
  validation_losses:
  - 0.5365484356880188
  - 1.0348402261734009
  - 0.511707067489624
  - 0.6736170053482056
  - 0.5358552932739258
  - 1.1044410467147827
  - 0.482679545879364
  - 0.4006741940975189
  - 0.48845064640045166
  - 0.443703293800354
  - 1.874293565750122
  - 0.48400378227233887
  - 0.44347265362739563
  - 0.5696229934692383
  - 0.9896548390388489
  - 0.4161727726459503
  - 0.3903633952140808
  - 0.39698758721351624
  - 0.388427734375
  - 0.49409446120262146
  - 0.4103772044181824
  - 0.4001423716545105
  - 0.4614761471748352
  - 0.386369913816452
  - 0.38777998089790344
  - 0.39393410086631775
  - 0.3856894075870514
  - 0.40711912512779236
  - 0.38672325015068054
  - 0.386156290769577
  - 0.3838210105895996
  - 0.3901667296886444
  - 0.3831294775009155
  - 0.38790810108184814
loss_records_fold1:
  train_losses:
  - 3.6481287628412247
  - 4.1428365230560305
  - 3.6559942901134495
  - 3.4415741056203846
  - 3.3329551935195925
  - 3.440060552954674
  - 3.049274587631226
  - 4.119608807563782
  - 3.2649099349975588
  - 3.357332992553711
  - 3.6756574124097825
  - 2.901458489894867
  - 3.26425017118454
  - 3.7869362056255342
  - 3.1358816742897035
  - 3.6831932544708255
  - 3.015829983353615
  - 2.957872197031975
  - 3.868456342816353
  - 4.802346032857895
  - 3.368529307842255
  - 3.285358661413193
  - 3.0581862509250644
  - 3.0734901994466783
  - 3.0746691823005676
  validation_losses:
  - 0.4122093617916107
  - 0.4035676121711731
  - 0.4023701548576355
  - 0.39958813786506653
  - 0.40051648020744324
  - 0.44549044966697693
  - 0.3986004590988159
  - 0.4137684404850006
  - 0.39861780405044556
  - 0.41117149591445923
  - 0.4061117470264435
  - 0.4037017822265625
  - 0.5397164225578308
  - 0.4150003492832184
  - 0.40533024072647095
  - 0.40291446447372437
  - 0.40788084268569946
  - 0.40294620394706726
  - 0.4807265102863312
  - 0.4669942855834961
  - 0.4447943866252899
  - 0.42973384261131287
  - 0.4049130380153656
  - 0.4051600992679596
  - 0.41500043869018555
loss_records_fold2:
  train_losses:
  - 3.117923226952553
  - 3.1749839901924135
  - 3.0464392542839054
  - 3.0042471438646317
  - 3.0657270491123203
  - 3.043741273880005
  - 3.2151464462280277
  - 3.218713027238846
  - 3.1551891356706623
  - 3.107730370759964
  - 3.244668018817902
  - 3.03327793776989
  - 3.071115601062775
  validation_losses:
  - 0.3886619508266449
  - 0.3819420635700226
  - 0.38111767172813416
  - 0.40127819776535034
  - 0.3833392560482025
  - 0.3836172819137573
  - 0.39725109934806824
  - 0.39329200983047485
  - 0.398506760597229
  - 0.4001539647579193
  - 0.38318148255348206
  - 0.39117345213890076
  - 0.38267314434051514
loss_records_fold3:
  train_losses:
  - 2.9337788939476015
  - 2.9667803555727006
  - 2.974062830209732
  - 3.012974733114243
  - 2.966862100362778
  - 3.0606160640716555
  - 3.101002663373947
  - 2.9386476218700412
  - 3.013439750671387
  - 2.8813302516937256
  - 2.9788287401199343
  - 2.9395669519901277
  - 3.033785343170166
  - 2.987068822979927
  - 2.996021601557732
  - 2.9746045947074893
  - 2.952637615799904
  - 3.119518452882767
  - 3.00054053068161
  - 3.0629152685403827
  - 2.9410289347171785
  - 2.9458440750837327
  - 2.977639812231064
  - 2.9847094088792803
  - 2.9875778377056124
  - 2.917352855205536
  - 2.9529039919376374
  - 2.9149955749511722
  - 2.911309200525284
  - 2.9689373791217806
  - 3.107535129785538
  - 3.086876529455185
  - 2.981920444965363
  - 2.933278089761734
  - 2.953271108865738
  validation_losses:
  - 0.4106156527996063
  - 0.39674896001815796
  - 0.42922094464302063
  - 0.41371840238571167
  - 0.44282597303390503
  - 0.4027649760246277
  - 0.39888912439346313
  - 0.41616585850715637
  - 0.4280047118663788
  - 0.40989553928375244
  - 0.39945968985557556
  - 0.3979043662548065
  - 0.40668246150016785
  - 0.4615311026573181
  - 0.41553404927253723
  - 0.4022732973098755
  - 0.4615553021430969
  - 0.4516337811946869
  - 0.4080336391925812
  - 0.3964310884475708
  - 0.40431347489356995
  - 0.39831167459487915
  - 0.4089541435241699
  - 0.4052252769470215
  - 0.4028371572494507
  - 0.4030091166496277
  - 0.819368839263916
  - 0.3971029818058014
  - 0.4217403829097748
  - 0.40627560019493103
  - 0.3977088928222656
  - 0.3962862193584442
  - 0.39911752939224243
  - 0.4010838270187378
  - 0.41090795397758484
loss_records_fold4:
  train_losses:
  - 2.943750709295273
  - 2.943769332766533
  - 2.9205714732408525
  - 2.940436673164368
  - 2.9809207171201706
  - 2.959877622127533
  - 2.9752587854862216
  - 2.913883775472641
  - 2.973782992362976
  - 2.978804498910904
  - 2.9412325084209443
  - 3.085270953178406
  - 3.1303554594516756
  - 3.01441690325737
  - 2.976359242200852
  - 3.015683794021607
  - 3.066125535964966
  - 3.0457576096057895
  - 2.935740399360657
  - 3.0243477880954743
  - 3.025034523010254
  - 3.162783256173134
  - 3.141998511552811
  - 2.9702540785074234
  - 3.173819559812546
  - 3.142243278026581
  - 2.9583861112594607
  - 2.955406403541565
  - 3.1167224526405337
  - 3.244531658291817
  - 3.0815716058015825
  - 3.7703174352645874
  - 3.1958779007196427
  - 3.0426683217287067
  - 2.9876477211713794
  - 3.456562036275864
  - 3.201423996686936
  - 2.9812219560146334
  - 2.956134939193726
  - 2.9658445477485658
  - 3.217791569232941
  - 3.379457706212998
  - 3.0911290377378466
  - 3.1078047990798954
  - 3.0557710707187655
  - 3.5399507105350496
  - 3.932989948987961
  - 3.668262642621994
  - 3.678762748837471
  - 3.3136995017528537
  - 3.3240071505308153
  - 3.036778184771538
  validation_losses:
  - 0.38953131437301636
  - 0.39003315567970276
  - 0.392819344997406
  - 0.40138697624206543
  - 0.3910432457923889
  - 0.4014896750450134
  - 0.3961764872074127
  - 0.39042729139328003
  - 0.4129391610622406
  - 0.39158862829208374
  - 0.3896178901195526
  - 0.40593966841697693
  - 0.41969066858291626
  - 0.3914043605327606
  - 0.39894282817840576
  - 0.4293617308139801
  - 0.38999760150909424
  - 0.3963613212108612
  - 0.39648759365081787
  - 0.3916110694408417
  - 0.39346596598625183
  - 0.61835116147995
  - 0.40231841802597046
  - 0.39351174235343933
  - 0.4196946918964386
  - 0.4031255841255188
  - 0.4013707935810089
  - 0.3929464817047119
  - 0.39587047696113586
  - 0.45216208696365356
  - 0.39089635014533997
  - 0.4115833342075348
  - 0.38870078325271606
  - 0.396126925945282
  - 0.456938773393631
  - 0.6582526564598083
  - 0.39395368099212646
  - 0.3965967893600464
  - 0.4140198826789856
  - 0.38978004455566406
  - 0.39087605476379395
  - 0.39258500933647156
  - 0.3954179286956787
  - 0.3926845192909241
  - 0.40577033162117004
  - 7.209286689758301
  - 0.642892599105835
  - 0.39827096462249756
  - 0.39697256684303284
  - 0.3924531638622284
  - 0.3927314281463623
  - 0.39525485038757324
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 35 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 52 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8542024013722127, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8575845991523876
  mean_f1_accuracy: 0.0
  total_train_time: '0:14:45.431659'
