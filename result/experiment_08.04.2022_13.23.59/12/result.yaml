config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 14:42:30.443688'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/12/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/12/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/12/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/12/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/12/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/12/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/12/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/12/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/12/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/12/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 7.705162882804871
  - 7.623155027627945
  - 7.909406304359436
  - 7.468823283910751
  - 7.678528875112534
  - 7.78061106801033
  - 7.628766044974327
  - 7.933355897665024
  - 7.738442629575729
  - 7.809485197067261
  - 7.3723622262477875
  - 7.946581810712814
  - 7.695248335599899
  - 7.545247629284859
  - 7.742167055606842
  - 7.622187823057175
  - 7.944939941167831
  - 7.5753704607486725
  - 7.6290551126003265
  - 7.844161778688431
  - 7.851312965154648
  - 7.62321150302887
  - 7.587931424379349
  - 7.637993976473808
  - 7.6907393634319305
  - 7.672824829816818
  - 7.964571923017502
  - 7.52052740752697
  - 7.483335971832275
  - 7.592323035001755
  - 7.673234194517136
  - 7.439377501606941
  - 7.9929408729076385
  - 7.9176343977451324
  - 7.8603737354278564
  - 7.824777364730835
  - 7.490735441446304
  - 7.754423826932907
  - 7.964633285999298
  - 7.571603238582611
  - 7.832166135311127
  - 7.518294632434845
  - 7.795310705900192
  - 7.7558804750442505
  - 7.4763873517513275
  - 7.7489504516124725
  - 7.790958672761917
  - 7.82498887181282
  - 7.578880771994591
  - 7.701728463172913
  - 7.723125904798508
  - 7.780474424362183
  - 7.8044310212135315
  - 7.692243367433548
  - 7.465533763170242
  - 7.743277192115784
  - 7.663292735815048
  - 7.610495418310165
  - 7.767349183559418
  - 7.723889738321304
  - 7.6408655643463135
  - 7.670324236154556
  - 7.9164097011089325
  - 7.343062847852707
  - 7.525321751832962
  - 7.630229204893112
  - 7.833888113498688
  - 8.031253784894943
  - 7.814416468143463
  - 7.787708580493927
  - 7.509026974439621
  - 7.770850658416748
  - 7.6859340369701385
  - 7.496772423386574
  - 7.573681592941284
  - 7.667206317186356
  - 7.884990364313126
  - 7.637281537055969
  - 7.7863984405994415
  - 7.863507151603699
  - 7.47925865650177
  - 7.635795474052429
  - 7.915126532316208
  - 7.525813937187195
  - 7.934752285480499
  - 7.5532925724983215
  - 7.658712446689606
  - 7.806416600942612
  - 7.621139645576477
  - 7.828419923782349
  - 7.640809655189514
  - 7.531926780939102
  - 7.688318103551865
  - 7.901862144470215
  - 7.548767298460007
  - 7.50024488568306
  - 7.655868083238602
  - 7.726628333330154
  - 7.5881913006305695
  - 7.821108549833298
  validation_losses:
  - 0.38632768392562866
  - 0.38767701387405396
  - 0.4019939601421356
  - 0.3902435004711151
  - 0.3883081376552582
  - 0.3954363763332367
  - 0.4024547338485718
  - 0.4146272838115692
  - 0.4023072421550751
  - 0.38956350088119507
  - 0.3952619135379791
  - 0.41435152292251587
  - 0.4046265780925751
  - 0.39165934920310974
  - 0.3917291462421417
  - 0.5111702084541321
  - 0.4234749376773834
  - 0.390286922454834
  - 0.39634063839912415
  - 0.40858352184295654
  - 0.41144800186157227
  - 0.39077937602996826
  - 0.46658939123153687
  - 0.42216363549232483
  - 0.44542554020881653
  - 0.7663779854774475
  - 0.47040894627571106
  - 0.3922216594219208
  - 0.4015725553035736
  - 0.5909427404403687
  - 0.5572925806045532
  - 0.5693153142929077
  - 0.40331119298934937
  - 0.38881102204322815
  - 0.40100711584091187
  - 0.39403119683265686
  - 0.39481276273727417
  - 0.4234503507614136
  - 0.4067009687423706
  - 0.3996647000312805
  - 0.41309288144111633
  - 0.40135055780410767
  - 0.4238679111003876
  - 0.39697062969207764
  - 0.38475102186203003
  - 0.3983776867389679
  - 0.4075329899787903
  - 0.40320542454719543
  - 0.3918811082839966
  - 0.398528128862381
  - 0.4207850396633148
  - 0.3914119303226471
  - 0.4009990394115448
  - 0.4532056152820587
  - 0.42940929532051086
  - 0.3913777768611908
  - 0.40031298995018005
  - 0.48711809515953064
  - 0.45819950103759766
  - 0.40394747257232666
  - 0.3936523199081421
  - 0.40649738907814026
  - 0.4227951765060425
  - 0.38158923387527466
  - 0.45201554894447327
  - 0.5327685475349426
  - 0.5593661069869995
  - 0.39612293243408203
  - 0.39965176582336426
  - 0.39668893814086914
  - 0.4120376706123352
  - 0.40597739815711975
  - 0.42679280042648315
  - 0.40143194794654846
  - 0.4137359857559204
  - 0.41932010650634766
  - 0.41558799147605896
  - 0.4569222927093506
  - 0.4422551989555359
  - 0.4536300301551819
  - 0.4322584569454193
  - 0.5229540467262268
  - 0.4393702745437622
  - 0.7292323112487793
  - 0.41969412565231323
  - 0.3865380585193634
  - 0.3855585753917694
  - 0.4187943637371063
  - 0.4125139117240906
  - 0.8846098780632019
  - 0.7909713387489319
  - 0.5149486660957336
  - 0.8506357073783875
  - 0.7821773290634155
  - 0.5065869688987732
  - 0.5702969431877136
  - 0.3960493803024292
  - 1.0753898620605469
  - 0.7239362001419067
  - 0.721208930015564
loss_records_fold3:
  train_losses:
  - 7.719286322593689
  - 7.712710320949554
  - 7.647115230560303
  - 7.626457720994949
  - 7.366471886634827
  - 7.610737502574921
  - 7.592097073793411
  - 7.709185719490051
  - 7.5945190489292145
  - 7.725416451692581
  - 7.653509199619293
  - 7.4918540716171265
  - 7.302468329668045
  - 7.870646893978119
  - 7.5555040538311005
  - 7.440306708216667
  - 7.478393644094467
  - 7.383024334907532
  - 7.588105797767639
  - 7.447316527366638
  - 7.529210925102234
  - 7.6243909895420074
  - 7.398017883300781
  - 7.6944355964660645
  - 7.494753569364548
  - 7.514725476503372
  - 7.667344480752945
  - 7.569242537021637
  - 7.499877482652664
  - 7.4419801235198975
  - 7.628053277730942
  - 7.539630442857742
  - 7.400425046682358
  - 7.531076639890671
  - 7.681634187698364
  - 7.475268200039864
  - 7.776248127222061
  - 7.626583486795425
  - 7.591699659824371
  - 7.3081682324409485
  - 7.331049084663391
  - 7.4607400596141815
  - 7.782897531986237
  - 7.584125071763992
  - 7.855339378118515
  - 7.4831682741642
  - 7.486962646245956
  - 7.612301975488663
  - 7.49273943901062
  - 7.494481474161148
  - 7.447087794542313
  - 7.4866963326931
  - 7.53179857134819
  - 7.5729508101940155
  - 7.819661200046539
  - 7.637083888053894
  - 7.4226361364126205
  - 7.6586123406887054
  - 7.638921171426773
  - 7.671929895877838
  - 7.429744005203247
  - 7.336010962724686
  - 7.473480463027954
  - 7.461239665746689
  - 7.4922429621219635
  - 7.375472724437714
  - 7.720807641744614
  - 7.422899678349495
  - 7.44268000125885
  - 7.907293289899826
  - 7.205978482961655
  - 7.695648789405823
  - 7.253239929676056
  - 7.369292110204697
  - 7.412220925092697
  - 7.470103025436401
  - 7.463481366634369
  - 7.2380311191082
  - 7.395041733980179
  - 7.529244214296341
  - 7.281177848577499
  - 7.369690239429474
  - 7.457122653722763
  - 7.647705614566803
  - 7.201849609613419
  - 7.4144651889801025
  - 7.537069708108902
  - 7.367642670869827
  - 7.528139889240265
  - 7.566298395395279
  - 7.270555853843689
  - 7.677719324827194
  - 7.3283644914627075
  - 7.75314062833786
  - 7.454571604728699
  - 7.339525520801544
  - 7.603405743837357
  - 7.742411017417908
  - 7.381835877895355
  - 7.372735887765884
  validation_losses:
  - 0.4130304753780365
  - 0.4052332043647766
  - 0.46444353461265564
  - 0.497404545545578
  - 0.7852359414100647
  - 0.9038680791854858
  - 0.3993861675262451
  - 0.601233720779419
  - 0.36981499195098877
  - 0.37792980670928955
  - 0.392364501953125
  - 0.3659478724002838
  - 0.3683679699897766
  - 0.3980747163295746
  - 0.3758111596107483
  - 0.37458178400993347
  - 0.3931114375591278
  - 0.3959085941314697
  - 0.5096063613891602
  - 0.89277583360672
  - 1.2808746099472046
  - 0.41708338260650635
  - 0.39525240659713745
  - 1.0812673568725586
  - 1.0183687210083008
  - 1.166878581047058
  - 1.4397542476654053
  - 0.6535731554031372
  - 1.1270198822021484
  - 0.46485018730163574
  - 0.5915815234184265
  - 0.4155677258968353
  - 0.525672197341919
  - 0.9954321384429932
  - 0.9140002131462097
  - 0.37120750546455383
  - 0.5106402039527893
  - 0.3878291845321655
  - 0.38036561012268066
  - 0.42711636424064636
  - 1.0004196166992188
  - 0.79306960105896
  - 0.5666429400444031
  - 0.4342963397502899
  - 0.5670109391212463
  - 0.46478623151779175
  - 0.44972750544548035
  - 0.4201098084449768
  - 0.43939512968063354
  - 0.4329557418823242
  - 0.48559871315956116
  - 0.6328800320625305
  - 0.7366604804992676
  - 1.1199804544448853
  - 1.0340485572814941
  - 0.9298095703125
  - 0.3895992040634155
  - 0.38420403003692627
  - 0.5888718962669373
  - 0.5438607335090637
  - 0.6427227854728699
  - 0.7057842016220093
  - 1.400865912437439
  - 1.1545169353485107
  - 0.3976127505302429
  - 0.8722521066665649
  - 1.0302315950393677
  - 0.592395544052124
  - 0.3966684937477112
  - 0.39762821793556213
  - 1.3359712362289429
  - 0.5837302803993225
  - 0.9402353167533875
  - 1.7047152519226074
  - 2.2476699352264404
  - 5.624707221984863
  - 5.345407009124756
  - 5.558561325073242
  - 6.009001731872559
  - 1.7304344177246094
  - 3.0059762001037598
  - 0.6412072777748108
  - 0.4007015526294708
  - 0.7722940444946289
  - 0.4325655996799469
  - 1.3468722105026245
  - 0.7783463597297668
  - 1.3435434103012085
  - 0.5267534852027893
  - 0.6110253930091858
  - 1.1742465496063232
  - 0.5201974511146545
  - 0.39769068360328674
  - 0.5151597857475281
  - 0.6666078567504883
  - 0.6555967330932617
  - 0.9699220657348633
  - 1.182740569114685
  - 0.37552735209465027
  - 1.3761191368103027
loss_records_fold4:
  train_losses:
  - 7.333951383829117
  - 7.547741949558258
  - 7.396204113960266
  - 7.708899840712547
  - 7.295896142721176
  - 7.600410521030426
  - 7.449798196554184
  - 7.3559411615133286
  - 7.52301561832428
  - 7.2399300038814545
  - 7.243565335869789
  - 7.651886910200119
  - 7.504728436470032
  - 7.406959146261215
  - 7.4901895225048065
  - 7.199643582105637
  - 7.369438052177429
  - 7.386012136936188
  - 7.331437706947327
  - 7.577513158321381
  - 7.497955918312073
  - 7.52547225356102
  - 7.170971930027008
  - 7.0826855301856995
  - 7.361065000295639
  - 7.18761619925499
  - 7.383336961269379
  - 7.510261207818985
  - 7.867949813604355
  - 7.239297419786453
  - 7.44011664390564
  - 7.11891271173954
  - 7.43833714723587
  - 7.726531684398651
  - 7.409835934638977
  - 7.617953419685364
  - 7.465463548898697
  - 7.244124382734299
  - 7.447428464889526
  - 7.1904809176921844
  - 7.536627560853958
  - 7.470428109169006
  - 7.478473037481308
  - 7.303562104701996
  - 7.0458617359399796
  - 7.315111994743347
  - 7.397802233695984
  - 7.351527988910675
  - 7.295810550451279
  - 7.212788015604019
  - 7.27972212433815
  - 7.48027178645134
  - 7.48411163687706
  - 7.459088444709778
  - 7.340162575244904
  - 7.364245116710663
  - 7.285217493772507
  - 7.421949237585068
  - 7.695377737283707
  - 7.556261777877808
  - 7.217306584119797
  - 7.467725962400436
  - 7.39137664437294
  - 7.349292516708374
  - 7.263758063316345
  - 7.236468702554703
  - 7.3751658499240875
  - 7.341404229402542
  - 7.221021890640259
  - 7.626324445009232
  - 7.468104124069214
  - 7.464892089366913
  - 7.5908085107803345
  - 7.312165081501007
  - 7.0341203808784485
  - 7.468085020780563
  - 7.149082779884338
  - 7.26862958073616
  - 7.27886164188385
  - 7.058085814118385
  - 7.448422759771347
  - 7.556987226009369
  - 7.220864951610565
  - 7.717404872179031
  - 7.670867562294006
  - 7.781192511320114
  - 7.908235251903534
  - 7.779176354408264
  - 7.549004346132278
  - 7.725815683603287
  - 7.438020706176758
  - 7.260069698095322
  - 7.27428337931633
  - 7.176657736301422
  - 7.624227583408356
  - 7.354578226804733
  - 7.309502393007278
  - 7.378512114286423
  - 7.307969376444817
  - 7.341519743204117
  validation_losses:
  - 0.36645346879959106
  - 0.4276023209095001
  - 0.4299868047237396
  - 0.3854652941226959
  - 0.4642810821533203
  - 0.4957389831542969
  - 0.39286258816719055
  - 0.43240466713905334
  - 0.4521920680999756
  - 0.49932485818862915
  - 1.6475707292556763
  - 0.36438804864883423
  - 0.37428128719329834
  - 0.3714506924152374
  - 0.40386030077934265
  - 0.41094985604286194
  - 0.3895843029022217
  - 0.5441721081733704
  - 0.39579659700393677
  - 0.36244237422943115
  - 0.36867931485176086
  - 0.3640938103199005
  - 0.3769332766532898
  - 0.4453127086162567
  - 0.6390610933303833
  - 0.5860843062400818
  - 0.7315821051597595
  - 0.582221508026123
  - 0.4644120931625366
  - 0.44714000821113586
  - 0.4322196841239929
  - 0.5288063883781433
  - 0.4393503963947296
  - 0.559540867805481
  - 0.36552950739860535
  - 0.3807314336299896
  - 1.2681347131729126
  - 0.9473167061805725
  - 1.6865440607070923
  - 0.7001268863677979
  - 0.36739176511764526
  - 0.3621424734592438
  - 0.6909627318382263
  - 0.47653713822364807
  - 0.3650181293487549
  - 0.794071614742279
  - 0.3623470067977905
  - 0.365720272064209
  - 0.38140860199928284
  - 1.1513720750808716
  - 0.46063244342803955
  - 0.5652292966842651
  - 0.36724603176116943
  - 0.3674962520599365
  - 0.4353143572807312
  - 0.5788093209266663
  - 0.6905738711357117
  - 0.5194370150566101
  - 0.3979126811027527
  - 0.3642861843109131
  - 0.4207274615764618
  - 0.36969074606895447
  - 0.36641398072242737
  - 0.3527049124240875
  - 0.8392782211303711
  - 0.621941328048706
  - 0.7721640467643738
  - 0.7015820741653442
  - 0.5408231019973755
  - 0.42151913046836853
  - 0.5090234875679016
  - 0.6429511904716492
  - 0.6524875164031982
  - 0.3695671260356903
  - 0.3734281063079834
  - 0.3947463035583496
  - 0.37562403082847595
  - 0.842510461807251
  - 0.7932092547416687
  - 1.0355181694030762
  - 0.5961664915084839
  - 0.8371591567993164
  - 0.7939077019691467
  - 0.5439409017562866
  - 0.4238278567790985
  - 0.3860616981983185
  - 0.3811047375202179
  - 0.5905173420906067
  - 0.5727762579917908
  - 0.4959644079208374
  - 0.3701431453227997
  - 0.3641306757926941
  - 0.6729545593261719
  - 0.477901816368103
  - 0.5004977583885193
  - 0.6332459449768066
  - 0.5230298042297363
  - 0.6049689650535583
  - 0.39098528027534485
  - 0.3642081916332245
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 35 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 87 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8542024013722127, 0.8593481989708405, 0.8593481989708405,
    0.852233676975945]'
  fold_eval_f1: '[0.0, 0.02298850574712644, 0.0, 0.0, 0.04444444444444444]'
  mean_eval_accuracy: 0.856553081878894
  mean_f1_accuracy: 0.013486590038314174
  total_train_time: '0:06:28.367109'
