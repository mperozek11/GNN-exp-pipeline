config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:46:44.573891'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_59fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_59fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_59fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_59fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_59fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_59fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_59fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_59fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_59fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_59fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 19.969682693481445
  - 7.656984424591065
  - 7.647199892997742
  - 5.9274607539176944
  - 5.156362676620484
  - 2.4142337441444397
  - 3.1982569575309756
  - 1.7798415422439575
  - 1.3070544809103013
  - 1.4466990530490875
  - 1.2072122156620027
  - 0.9605183601379395
  - 0.9384055197238923
  - 0.8937134683132172
  - 1.0837510108947754
  - 0.876132071018219
  - 0.8930586278438568
  - 1.1187681913375855
  - 0.9928802847862244
  - 1.1685763478279114
  - 1.0321828067302705
  - 1.0722781836986541
  - 5.2684531033039095
  - 5.068677246570587
  - 2.1628772020339966
  - 1.3837163329124451
  - 1.3348057240247728
  - 1.3563080906867981
  - 1.443673086166382
  - 1.0483782887458801
  - 1.0138773977756501
  - 3.5617461800575256
  - 2.376974749565125
  - 1.7391438007354738
  - 1.3190793335437776
  - 2.252449154853821
  - 1.9616703689098358
  - 2.2816470324993134
  - 1.164294245839119
  - 0.95285764336586
  - 1.0145044028759003
  - 1.3033334910869598
  - 5.22431093454361
  - 0.8827810466289521
  - 2.5887901306152346
  - 1.054400372505188
  - 0.9389799475669861
  - 1.0081980168819429
  - 1.0561971068382263
  - 0.8797793924808502
  - 0.9370373010635377
  - 0.8532741248607636
  - 0.8262718617916107
  - 0.7798539936542511
  - 0.8554280161857606
  - 1.0816103875637055
  - 0.9153496384620667
  - 0.842268866300583
  - 0.802748477458954
  - 0.7987544178962708
  - 0.7527026861906052
  - 0.9404402017593384
  - 0.777331429719925
  - 0.9286648154258729
  - 1.0885789811611175
  - 1.0184528291225434
  - 1.1704033493995667
  - 1.1286696314811706
  - 2.587162095308304
  - 1.6069997310638429
  - 1.1881157517433167
  - 1.3125826239585878
  - 3.4026518166065216
  - 1.6339371860027314
  - 0.9231641113758088
  - 1.2444193422794343
  - 0.9806908488273621
  validation_losses:
  - 4.201284408569336
  - 2.2131834030151367
  - 3.7128405570983887
  - 2.1486313343048096
  - 3.445805072784424
  - 0.5796961784362793
  - 1.064968228340149
  - 0.5217570066452026
  - 0.5195022821426392
  - 0.4903583526611328
  - 0.5111740827560425
  - 0.4818025231361389
  - 0.5382562279701233
  - 0.44085443019866943
  - 0.41532015800476074
  - 0.38456594944000244
  - 0.40655967593193054
  - 0.7190061211585999
  - 1.0830069780349731
  - 0.5985434055328369
  - 1.0942860841751099
  - 0.414957195520401
  - 0.8258394598960876
  - 0.48803335428237915
  - 0.6133103966712952
  - 0.5491697788238525
  - 0.5123208165168762
  - 1.2159208059310913
  - 0.7697590589523315
  - 0.4626011550426483
  - 0.5897083878517151
  - 0.5877805948257446
  - 1.6055330038070679
  - 0.5450134873390198
  - 0.5827224254608154
  - 0.9326481819152832
  - 0.4337255656719208
  - 0.4191051721572876
  - 0.40109866857528687
  - 0.3918803036212921
  - 0.4035543203353882
  - 0.5052626729011536
  - 0.48260217905044556
  - 0.40217483043670654
  - 0.3938091993331909
  - 0.40483757853507996
  - 0.41150811314582825
  - 0.4416157603263855
  - 0.41383424401283264
  - 0.4054904580116272
  - 0.4184931516647339
  - 0.4186737537384033
  - 0.48234519362449646
  - 0.3784661889076233
  - 0.4117882549762726
  - 0.40432408452033997
  - 0.5440869927406311
  - 0.4227736294269562
  - 0.40346211194992065
  - 0.3884773552417755
  - 0.3850380480289459
  - 0.3900902569293976
  - 0.46901735663414
  - 0.40550220012664795
  - 0.8556243181228638
  - 0.4218546748161316
  - 0.8915866017341614
  - 0.4219125509262085
  - 0.45572397112846375
  - 0.4428943991661072
  - 0.62775719165802
  - 0.43793678283691406
  - 0.4405577480792999
  - 0.4097082316875458
  - 0.3984416127204895
  - 0.4007819592952728
  - 0.39080819487571716
loss_records_fold1:
  train_losses:
  - 1.255467438697815
  - 0.8028078675270081
  - 0.7926371395587921
  - 3.3163070261478427
  - 0.8098795056343079
  - 0.8741408348083497
  - 0.8379923403263092
  - 0.8170965373516084
  - 0.8118863165378571
  - 0.9174046099185944
  - 0.8151550948619843
  - 0.8744720160961151
  - 1.0025777995586396
  - 1.2519948303699495
  - 0.9491267025470734
  - 0.8118025600910187
  - 0.808997392654419
  - 0.8540603578090669
  - 0.7537183046340943
  - 0.8329037249088288
  - 0.8137059271335603
  - 0.8090827465057373
  - 3.352641999721527
  - 3.3460223317146305
  - 1.1380922079086304
  - 1.0051444232463838
  - 1.074068981409073
  - 0.9811619222164154
  - 0.8052825748920441
  - 1.2373382389545442
  - 1.0014378488063813
  validation_losses:
  - 0.4848421514034271
  - 0.4313272535800934
  - 0.4161370098590851
  - 0.4171549379825592
  - 0.4113316237926483
  - 0.4382043778896332
  - 0.4602254629135132
  - 0.3989599645137787
  - 0.42005839943885803
  - 0.43381863832473755
  - 0.42960044741630554
  - 0.4014359712600708
  - 0.4160838723182678
  - 0.5777409076690674
  - 0.41563692688941956
  - 0.4071306586265564
  - 0.4027458429336548
  - 0.4467182159423828
  - 0.40221503376960754
  - 0.40045469999313354
  - 0.39501771330833435
  - 0.4234941303730011
  - 0.9518073797225952
  - 0.4359413683414459
  - 0.5047576427459717
  - 0.42004284262657166
  - 0.411733478307724
  - 0.4125381410121918
  - 0.41168272495269775
  - 0.40328970551490784
  - 0.3969767987728119
loss_records_fold2:
  train_losses:
  - 0.8355912148952485
  - 2.3897684514522552
  - 1.6922312080860138
  - 1.6600654900074006
  - 0.9893609404563904
  - 1.120717704296112
  - 0.883404940366745
  - 0.931284636259079
  - 0.8505733937025071
  - 0.9793247669935227
  - 0.9825866997241974
  - 0.8680429458618164
  - 1.0591751098632813
  - 0.9305766761302948
  - 0.8151702761650086
  - 0.8328024327754975
  - 0.9608828246593476
  - 0.7769196629524231
  - 0.8653914570808411
  - 0.7762266159057618
  - 0.9103473544120789
  - 0.8198231399059296
  - 0.799730521440506
  - 0.748606351017952
  - 2.1464229404926303
  - 1.1874858766794205
  - 0.9066040039062501
  - 0.9064701318740845
  validation_losses:
  - 0.38268300890922546
  - 0.418867826461792
  - 0.585108757019043
  - 0.4457484483718872
  - 0.5632882118225098
  - 0.4214872419834137
  - 0.37990447878837585
  - 0.3889363706111908
  - 0.4055744409561157
  - 0.3884912133216858
  - 0.38812196254730225
  - 0.3811424672603607
  - 0.3786129653453827
  - 0.476927787065506
  - 0.37426701188087463
  - 0.38400012254714966
  - 0.3996520936489105
  - 0.3881635367870331
  - 0.38130390644073486
  - 0.3768022656440735
  - 0.37766388058662415
  - 0.39102041721343994
  - 0.37947046756744385
  - 0.37841588258743286
  - 0.38007745146751404
  - 0.37965914607048035
  - 0.3738768994808197
  - 0.3827861547470093
loss_records_fold3:
  train_losses:
  - 0.8491584777832032
  - 0.8653986871242524
  - 0.82627632021904
  - 0.926961100101471
  - 0.845739233493805
  - 4.175846600532532
  - 1.8921827137470246
  - 0.9297331929206849
  - 0.9920084595680237
  - 0.8732942640781403
  - 0.8945755958557129
  - 3.2976433575153354
  - 0.9392988502979279
  - 0.8501694142818451
  - 0.8951314568519593
  - 0.887346613407135
  - 1.1807013094425203
  - 0.9814271152019501
  - 0.8106724381446839
  - 0.9076785922050477
  - 0.8823665082454681
  - 0.9931291043758392
  - 0.8223477900028229
  - 0.9839516162872315
  - 0.7821979224681854
  - 0.8806983232498169
  - 0.7794985592365266
  validation_losses:
  - 0.3798001706600189
  - 0.3779830038547516
  - 0.37863120436668396
  - 0.3927801251411438
  - 0.4196348488330841
  - 0.47206732630729675
  - 0.4005718231201172
  - 0.3793586194515228
  - 0.3893183469772339
  - 0.444670706987381
  - 0.40142038464546204
  - 0.38075801730155945
  - 0.3813033998012543
  - 0.38166937232017517
  - 0.3822154402732849
  - 0.4439256489276886
  - 0.38663002848625183
  - 0.44170066714286804
  - 0.38937434554100037
  - 0.3880555033683777
  - 0.4001580774784088
  - 0.38226446509361267
  - 0.39198559522628784
  - 0.3846433460712433
  - 0.38667675852775574
  - 0.3866206407546997
  - 0.38527604937553406
loss_records_fold4:
  train_losses:
  - 0.7469790756702424
  - 0.7681932747364044
  - 0.7517204433679581
  - 0.7672223389148712
  - 0.8296548426151276
  - 0.8065132081508637
  - 0.7695139586925507
  - 0.7866928517818451
  - 0.7867322564125061
  - 0.7618840396404267
  - 2.0354551792144777
  - 1.0824987649917603
  - 0.7539754450321198
  - 0.8940181195735932
  - 0.8956019818782807
  - 0.777009904384613
  - 0.7513682544231415
  - 0.8051373720169068
  - 0.8346813380718232
  - 0.7351876020431519
  - 0.7742909789085388
  - 0.8662934303283691
  - 0.7564005672931672
  - 0.7693310379981995
  - 2.322459489107132
  - 0.7612458884716035
  - 0.7641237378120422
  - 0.9048769056797028
  - 0.7441903322935105
  - 0.761633461713791
  - 0.7884809970855713
  - 0.8247791290283204
  - 0.8249832332134247
  - 0.8655474066734314
  - 0.7525065213441849
  - 0.8173346757888794
  validation_losses:
  - 0.40488094091415405
  - 0.39690372347831726
  - 0.39313364028930664
  - 0.4100278317928314
  - 0.40081876516342163
  - 0.4027140438556671
  - 0.3973693251609802
  - 0.42382362484931946
  - 0.38972803950309753
  - 0.3909219801425934
  - 0.39894577860832214
  - 0.39283549785614014
  - 0.41279035806655884
  - 0.4863828718662262
  - 0.42634451389312744
  - 0.4107062816619873
  - 0.4223655164241791
  - 0.4252912104129791
  - 0.4307478070259094
  - 0.4023274779319763
  - 0.44003742933273315
  - 0.4127437472343445
  - 0.41016846895217896
  - 0.40634685754776
  - 0.4501378536224365
  - 0.4175914525985718
  - 0.4091877043247223
  - 0.4076867997646332
  - 0.40036725997924805
  - 0.423218697309494
  - 0.40307340025901794
  - 0.389987051486969
  - 0.3933970630168915
  - 0.3980546295642853
  - 0.4079561233520508
  - 0.401623398065567
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 77 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 31 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 36 epochs
training_metrics:
  fold_eval_accs: '[0.8507718696397941, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8572415459791456
  mean_f1_accuracy: 0.0
  total_train_time: '0:16:46.274531'
