config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:30:13.099642'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_88fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_88fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_88fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_88fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_88fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_88fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_88fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_88fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_88fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_88fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 40.20582907497883
  - 31.293847620487213
  - 24.21107180118561
  - 19.497189298272133
  - 26.591244786977768
  - 14.8482320740819
  - 13.957564818859101
  - 13.48713811337948
  - 23.776519613713027
  - 18.720338755846026
  - 7.917148050665856
  - 9.022166503965854
  - 10.912965932488442
  - 10.254601567983627
  - 9.068683648109436
  - 7.9041562378406525
  - 7.198473027348519
  - 6.34733225107193
  - 6.922383055090904
  - 6.1691332295537
  - 6.394815576076508
  - 6.231054526567459
  - 6.3561422497034075
  - 6.010452789068222
  - 7.018681904673577
  - 6.276973554491997
  - 6.618135812878609
  - 6.380690369009972
  - 6.538946187496186
  - 5.849550551176072
  - 6.026992413401604
  - 5.838982462882996
  - 5.782283225655556
  - 6.058448047935963
  - 6.331309187412263
  - 6.423291024565697
  - 6.1076478049159055
  - 5.957155546545983
  - 6.295359246432781
  - 6.012139615416527
  - 5.938817536830903
  - 6.0221131756901745
  - 5.863559517264367
  - 5.8544421881437305
  - 5.8879952222108844
  - 5.998448887467385
  - 5.927953410148621
  - 6.087472200393677
  - 5.801422449946404
  - 5.801241040229797
  - 5.737244120240212
  - 6.2473267912864685
  - 5.910291954874992
  - 5.938394469022751
  - 5.9246768519282345
  - 6.181241124868393
  - 6.014362704753876
  - 5.990720829367638
  - 6.044114877283573
  - 6.136079579591751
  - 6.074540942907333
  - 5.985667783021928
  - 6.017190656065941
  - 5.799268066883087
  - 5.990840557217599
  - 5.831677249073983
  - 5.890877071022988
  - 5.882250440120697
  - 6.101847222447396
  - 6.0856379717588425
  - 6.1610893458127975
  - 5.963163650035859
  - 8.5116773635149
  - 10.488239920139314
  - 6.541760419309139
  - 6.667089021205903
  - 7.4102277070283895
  - 6.05045791566372
  - 6.176830050349236
  - 6.320203593373299
  - 5.925833827257157
  - 5.927081659436226
  - 5.977118155360222
  - 6.011174869537354
  - 6.016535750031472
  validation_losses:
  - 0.6458637714385986
  - 1.632716178894043
  - 0.5433220863342285
  - 0.7974482178688049
  - 0.5084704756736755
  - 0.3956722617149353
  - 0.7669406533241272
  - 0.4969746470451355
  - 0.7643346786499023
  - 0.3969413638114929
  - 0.38695648312568665
  - 0.39690566062927246
  - 0.39875221252441406
  - 0.40996915102005005
  - 0.4006126821041107
  - 0.8029066920280457
  - 0.43472573161125183
  - 0.41839316487312317
  - 0.40298226475715637
  - 0.3837197422981262
  - 0.38468822836875916
  - 0.42836493253707886
  - 0.4066128730773926
  - 0.4145878851413727
  - 0.39981526136398315
  - 0.3913609981536865
  - 0.4023231267929077
  - 0.3961750566959381
  - 0.41218674182891846
  - 0.39702025055885315
  - 0.4445323050022125
  - 0.38416045904159546
  - 0.38762718439102173
  - 0.4190821945667267
  - 0.382999449968338
  - 0.394091933965683
  - 0.4175790548324585
  - 0.3860740065574646
  - 0.41500329971313477
  - 0.4045741558074951
  - 0.3744677007198334
  - 0.39862868189811707
  - 0.3999086320400238
  - 0.38026168942451477
  - 0.3791114091873169
  - 0.40936532616615295
  - 0.3891134560108185
  - 0.5878313779830933
  - 0.3818710148334503
  - 0.39755040407180786
  - 0.41394665837287903
  - 0.40500131249427795
  - 0.5855472683906555
  - 0.645528256893158
  - 0.3965403735637665
  - 0.39675578474998474
  - 0.3933306038379669
  - 0.4470834732055664
  - 0.4187110364437103
  - 0.5053289532661438
  - 0.47362449765205383
  - 0.4049108624458313
  - 0.434844970703125
  - 0.39907848834991455
  - 0.39072856307029724
  - 0.4084261357784271
  - 0.39165401458740234
  - 0.4067356288433075
  - 2.3182694911956787
  - 0.389272004365921
  - 0.39993220567703247
  - 0.3955439031124115
  - 0.5179829001426697
  - 0.3922967314720154
  - 0.42374518513679504
  - 0.4095097482204437
  - 0.39752498269081116
  - 0.38900840282440186
  - 0.6136202812194824
  - 0.41647401452064514
  - 0.3906061351299286
  - 0.3974299728870392
  - 0.39568084478378296
  - 0.3903612196445465
  - 0.3919352889060974
loss_records_fold1:
  train_losses:
  - 6.046615278720856
  - 5.944230857491494
  - 5.995855364203454
  - 5.8243611946702005
  - 6.014028929173946
  - 5.922229045629502
  - 5.7919451057910925
  - 5.896908232569695
  - 5.894902864098549
  - 5.845163473486901
  - 5.961661306023598
  - 5.987370678782463
  - 6.0692815482616425
  - 5.96824289560318
  - 5.846460273861886
  - 6.042104211449623
  - 5.884294363856316
  - 5.904217556118965
  - 5.829517096281052
  - 5.896118339896202
  - 5.84636422097683
  - 5.832178768515587
  - 5.809629501402378
  - 5.801957461237908
  - 5.9062574237585075
  - 5.899417096376419
  - 5.902232828736306
  - 5.8430821180343635
  - 6.391422158479691
  - 6.255384021997452
  - 5.971566355228425
  - 6.026138719916344
  - 6.08337094783783
  - 5.911818370223045
  - 5.942210906744004
  - 6.206000694632531
  - 5.930605584383011
  validation_losses:
  - 0.5555272698402405
  - 0.42266544699668884
  - 0.4201633632183075
  - 0.4611469507217407
  - 0.4069312512874603
  - 0.4318374693393707
  - 0.4534452557563782
  - 0.4172350764274597
  - 0.406354159116745
  - 0.5163403153419495
  - 0.4015747010707855
  - 0.4093448519706726
  - 0.4763786792755127
  - 0.43838366866111755
  - 0.40717872977256775
  - 0.4158625900745392
  - 0.46301501989364624
  - 0.42702755331993103
  - 0.40300866961479187
  - 0.4088327884674072
  - 0.407492995262146
  - 0.409621924161911
  - 0.4594770669937134
  - 0.40806469321250916
  - 0.48398879170417786
  - 0.4043942987918854
  - 0.42102542519569397
  - 0.4085391163825989
  - 0.40264102816581726
  - 0.4032643139362335
  - 0.46958136558532715
  - 0.43270227313041687
  - 0.42123860120773315
  - 0.4273200035095215
  - 0.40889471769332886
  - 0.41727691888809204
  - 0.4213593602180481
loss_records_fold2:
  train_losses:
  - 5.947221475839616
  - 5.8872354328632355
  - 6.014821809530258
  - 5.915503853559494
  - 6.000833797454835
  - 5.916896316409112
  - 6.126235312223435
  - 5.976360085606576
  - 6.005208846926689
  - 5.9548647701740265
  - 5.910525095462799
  - 5.863899701833725
  - 6.020073622465134
  - 5.935936614871025
  - 6.064974570274353
  - 6.052418157458305
  - 6.276359134912491
  - 5.9184237241745
  - 5.975140583515167
  - 6.1634111434221275
  - 5.910612040758133
  - 5.881873840093613
  - 6.070706397294998
  - 6.0296274572610855
  - 5.981199690699578
  validation_losses:
  - 0.3846786916255951
  - 0.38711830973625183
  - 0.412506103515625
  - 1066.279296875
  - 3459651.75
  - 26487678976.0
  - 0.39585089683532715
  - 0.3846896290779114
  - 0.38592851161956787
  - 0.4026663303375244
  - 0.3851974904537201
  - 0.38423651456832886
  - 0.38627752661705017
  - 0.3940279483795166
  - 0.42879387736320496
  - 0.39135631918907166
  - 0.3851912021636963
  - 0.3848121166229248
  - 0.43280914425849915
  - 0.39809951186180115
  - 0.3922779858112335
  - 0.39120855927467346
  - 0.38486582040786743
  - 0.3837982714176178
  - 0.3924061059951782
loss_records_fold3:
  train_losses:
  - 5.894591569900513
  - 5.869182634353638
  - 5.930288061499596
  - 5.940084561705589
  - 5.941527217626572
  - 5.844798433780671
  - 5.9129920423030855
  - 6.424820223450661
  - 6.110733592510224
  - 6.138854986429215
  - 6.026353824138642
  - 5.89936330318451
  - 5.8994975060224535
  - 5.915087443590164
  - 5.974133375287057
  - 6.010485085844994
  - 5.981905987858773
  - 5.905810859799385
  - 6.025983229279518
  - 5.985114634037018
  - 6.287184935808182
  - 5.928500771522522
  - 5.845992857217789
  - 6.007747185230255
  - 5.919053751230241
  - 6.03797357082367
  - 6.418904092907906
  - 5.97143919467926
  - 5.861245661973953
  - 5.892099967598916
  - 5.949701434373856
  - 5.915313622355462
  - 5.795923763513565
  - 5.890816098451615
  - 5.967541624605656
  - 6.364065647125244
  - 5.886155837774277
  - 5.897639328241349
  - 6.078771314024926
  - 5.819489267468453
  - 5.92033922970295
  - 5.821582862734795
  - 5.8875514507293705
  - 5.981107729673386
  - 6.013226503133774
  - 5.828932991623879
  - 5.950491052865982
  - 5.89071456193924
  - 6.02635013461113
  - 5.953194010257722
  - 5.877269557118416
  - 6.0677516013383865
  - 5.834020994603634
  - 5.910801714658738
  - 5.943339595198632
  - 5.875481152534485
  - 6.0278599798679355
  - 6.080134394764901
  - 5.997018319368363
  - 5.887936282157899
  - 5.909124991297722
  - 5.93769837319851
  - 5.909065589308739
  - 6.445317520201207
  - 5.915097105503083
  - 6.0303423583507545
  - 5.935773703455926
  - 5.871146458387376
  - 5.864517697691918
  - 5.92767895758152
  - 5.903199675679208
  - 5.900109297037125
  - 5.861025425791741
  - 6.029988545179368
  - 5.984882634878159
  - 5.905675396323204
  - 5.935408180952073
  - 6.810841619968414
  - 6.069963905215264
  - 5.944731488823891
  - 6.026543021202087
  - 6.142586609721184
  - 5.772249156236649
  - 5.988326467573643
  - 5.816284969449043
  - 6.033838713169098
  - 6.010486966371537
  - 6.232755367457867
  - 6.373301959037781
  - 5.973156812787057
  - 6.04271414577961
  - 6.1515811532735825
  - 6.00934708416462
  - 6.1400707304477695
  - 6.000450333952904
  - 6.026538839936257
  - 6.074515499174595
  - 6.031195843219757
  - 5.932019227743149
  - 6.03836000263691
  validation_losses:
  - 0.3997473120689392
  - 0.40759748220443726
  - 0.40514686703681946
  - 0.398908793926239
  - 0.40345117449760437
  - 0.4522843360900879
  - 0.39903002977371216
  - 0.4417531192302704
  - 0.4043627977371216
  - 0.4595091640949249
  - 0.40221062302589417
  - 0.40379399061203003
  - 0.4141595959663391
  - 0.4021405577659607
  - 0.421538382768631
  - 0.4021945893764496
  - 0.4003125727176666
  - 0.45141711831092834
  - 0.4049092233181
  - 0.3991852104663849
  - 0.4470900595188141
  - 0.4098760485649109
  - 0.4513898491859436
  - 0.39952099323272705
  - 0.3982996344566345
  - 0.39918360114097595
  - 0.4145733416080475
  - 0.39759254455566406
  - 0.4003655016422272
  - 0.3990967273712158
  - 0.42520850896835327
  - 0.4035038352012634
  - 0.414974570274353
  - 0.39895352721214294
  - 0.42371830344200134
  - 0.4026665687561035
  - 0.39835280179977417
  - 0.41541171073913574
  - 0.4508318603038788
  - 0.3986121416091919
  - 0.3978554904460907
  - 0.3990042507648468
  - 0.40064865350723267
  - 0.4408528506755829
  - 0.40050560235977173
  - 0.42829737067222595
  - 0.3985229432582855
  - 0.40074872970581055
  - 0.4290277063846588
  - 0.40053990483283997
  - 0.3999442458152771
  - 0.39823299646377563
  - 0.4339774250984192
  - 0.41075319051742554
  - 0.4240630567073822
  - 0.39833855628967285
  - 0.3989224433898926
  - 0.4711246192455292
  - 0.40950584411621094
  - 0.3989199101924896
  - 0.40585264563560486
  - 0.3989461660385132
  - 0.39886710047721863
  - 0.41653764247894287
  - 0.41260191798210144
  - 0.3993712365627289
  - 0.39862963557243347
  - 0.4261666238307953
  - 0.39973002672195435
  - 0.4178963303565979
  - 0.44931158423423767
  - 0.4084630608558655
  - 0.39918458461761475
  - 0.47280439734458923
  - 0.40153664350509644
  - 0.3976200222969055
  - 23934228480.0
  - 0.3950403332710266
  - 6600.20068359375
  - 134518.875
  - 37250.203125
  - 38682.50390625
  - 27541.97265625
  - 38222.49609375
  - 43134.6015625
  - 4884.7509765625
  - 30725.671875
  - 3883.5986328125
  - 162.9357452392578
  - 163.79690551757812
  - 55.049827575683594
  - 338.2884826660156
  - 48.3003044128418
  - 189.34158325195312
  - 420.9696960449219
  - 91.94715881347656
  - 123.45781707763672
  - 54.40370559692383
  - 190.2088165283203
  - 91.00238800048828
loss_records_fold4:
  train_losses:
  - 6.0263902157545095
  - 6.029189559817315
  - 5.97538854777813
  - 5.908732637763023
  - 6.115651074051858
  - 6.2478503614664085
  - 6.00652494430542
  - 5.987718257308007
  - 6.154688057303429
  - 5.878637063503266
  - 5.862880268692971
  - 5.973643127083779
  - 8.45731166601181
  - 6.58216792345047
  - 5.870074477791786
  - 5.967127750813962
  - 6.096762266755104
  - 7.878586027026177
  - 8.225658018887044
  - 6.613284707069397
  - 6.303298690915108
  - 6.12367888391018
  - 6.027667748928071
  - 5.951664453744889
  - 6.270758849382401
  - 6.360894402861596
  - 6.184468603134156
  - 6.937764874100686
  - 6.334216991066933
  - 6.007135826349259
  - 5.997099500894547
  - 6.09433460533619
  - 5.997202911973
  - 5.932773065567017
  - 5.92854433208704
  - 5.86662372648716
  - 6.380556827783585
  - 5.759796479344368
  - 6.119235333800316
  - 7.152485942840577
  - 6.0395586639642715
  - 6.07474879026413
  - 6.170705628395081
  - 6.549715897440911
  - 5.9016386568546295
  - 5.998699828982353
  - 5.8437205970287325
  - 5.84451209306717
  - 5.9155599057674415
  - 5.964943623542786
  - 5.925910928845406
  - 5.794491332769394
  - 6.064008328318597
  validation_losses:
  - 71.53311157226562
  - 134.52450561523438
  - 96.2347640991211
  - 159.42886352539062
  - 76.89404296875
  - 255.61404418945312
  - 37.71738815307617
  - 51.87820053100586
  - 129.83645629882812
  - 4.234122276306152
  - 67.45018768310547
  - 0.4088153839111328
  - 0.4056044816970825
  - 0.4088800549507141
  - 0.390892893075943
  - 0.42867618799209595
  - 0.4284473955631256
  - 7.858348846435547
  - 0.3968420922756195
  - 0.42722535133361816
  - 0.39453238248825073
  - 0.3951818645000458
  - 0.39201387763023376
  - 0.40894272923469543
  - 0.3912806212902069
  - 0.5165699124336243
  - 0.4019780457019806
  - 0.41311153769493103
  - 0.38987988233566284
  - 0.40053918957710266
  - 0.39029237627983093
  - 0.39118897914886475
  - 0.4008338749408722
  - 0.39274895191192627
  - 0.41185641288757324
  - 0.41106176376342773
  - 0.3902818560600281
  - 0.4145602881908417
  - 0.396651953458786
  - 0.3927430510520935
  - 0.39088329672813416
  - 0.40943729877471924
  - 0.4015529155731201
  - 0.3919786214828491
  - 0.41072729229927063
  - 0.39170634746551514
  - 0.402055561542511
  - 0.3998472988605499
  - 0.4053114056587219
  - 0.39060455560684204
  - 0.3982172906398773
  - 0.3940800726413727
  - 0.3921870291233063
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 85 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 37 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 53 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8576329331046312,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:31:15.313975'
