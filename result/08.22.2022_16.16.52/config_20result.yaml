config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:33:54.142012'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_20fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_20fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_20fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_20fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_20fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_20fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_20fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_20fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_20fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_20fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.949650225043297
  - 6.4737510114908225
  - 6.4292752832174305
  - 6.390698459744454
  - 6.238344311714172
  - 6.1910284698009495
  - 6.370418044924737
  - 6.257779499888421
  - 6.328946429491044
  - 6.070892906188965
  - 6.200768917798996
  - 6.187176549434662
  - 6.042159625887871
  - 5.834382054209709
  - 6.099500319361687
  - 5.986049249768257
  - 5.957975918054581
  - 6.098952507972718
  - 5.90866032242775
  - 6.1048143237829215
  - 6.015121096372605
  - 6.093670439720154
  - 6.045698916912079
  - 6.077025875449181
  - 6.028479078412056
  - 5.9702181160449985
  - 5.909128817915917
  - 6.005542519688607
  validation_losses:
  - 0.40841251611709595
  - 0.40167558193206787
  - 0.3921251893043518
  - 0.4161170423030853
  - 0.42120224237442017
  - 0.39013954997062683
  - 0.40540531277656555
  - 0.4262998104095459
  - 0.3969837725162506
  - 0.44610723853111267
  - 0.39225152134895325
  - 0.3999774158000946
  - 0.4113224744796753
  - 0.39558184146881104
  - 0.4144904315471649
  - 0.3971371650695801
  - 0.39958712458610535
  - 0.39030182361602783
  - 0.41742849349975586
  - 0.40072667598724365
  - 0.40030965209007263
  - 0.4126451909542084
  - 0.39633411169052124
  - 0.4001675546169281
  - 0.39672303199768066
  - 0.4054163992404938
  - 0.39879322052001953
  - 0.39704176783561707
loss_records_fold1:
  train_losses:
  - 5.964137646555901
  - 5.9939751595258715
  - 6.125759693980218
  - 6.162743395566941
  - 6.255419853329659
  - 6.078643843531609
  - 5.905846229195595
  - 6.01526874601841
  - 6.054744970798493
  - 5.862777808308602
  - 6.088875898718834
  - 6.104365062713623
  - 5.953013971447945
  - 5.981994172930718
  - 6.07319355905056
  - 6.0321913242340095
  - 5.997121754288674
  - 6.0935011386871345
  - 5.965693533420563
  - 6.117427587509155
  - 5.920310157537461
  - 5.949717152118684
  - 5.879478138685227
  - 5.942444863915444
  - 5.8753085970878605
  - 6.083994928002358
  - 6.031538197398186
  - 6.034872108697892
  - 6.013082638382912
  - 6.038614490628243
  - 5.938933017849923
  - 6.025834584236145
  - 6.0737582594156265
  - 5.952297905087471
  - 6.054396313428879
  - 5.8383272111415865
  - 5.861253499984741
  - 5.8753565639257435
  - 5.8490220516920095
  - 5.912024182081223
  - 5.852021855115891
  - 5.850283852219582
  - 6.1077130258083345
  - 5.839270907640458
  - 5.8636113762855535
  - 5.863162204623222
  - 5.9189362108707435
  - 5.960731881856919
  - 5.899262538552285
  - 6.049033609032631
  - 5.767195960879326
  - 5.890971633791924
  - 5.8359136343002325
  - 5.9842216879129415
  - 5.78964920938015
  - 5.882936492562294
  - 5.785649311542511
  - 5.962851294875145
  - 5.896494016051292
  - 5.875628578662873
  - 5.827333024144173
  - 5.664831387996674
  - 5.934563493728638
  - 6.036365216970444
  - 5.838464269042015
  - 5.948035109043122
  - 5.898822754621506
  - 5.810146203637124
  - 5.792308151721954
  - 5.918077042698861
  - 5.920282825827599
  - 5.9229446917772295
  - 5.8113151609897615
  - 5.901349744200707
  - 5.867638835310936
  - 5.7800480216741565
  - 5.885478815436364
  - 5.865728732943535
  - 5.90410880446434
  - 5.920624706149102
  - 5.8654665708541875
  - 5.872796174883843
  - 5.880907225608826
  - 5.924754628539086
  - 5.72172478735447
  - 5.82455157339573
  - 5.767920199036599
  - 5.685101476311684
  - 5.86545040011406
  - 5.852815952897072
  - 6.044666421413422
  - 5.786946740746498
  - 5.897480148077012
  - 6.002056524157524
  - 5.904153034090996
  - 5.836597657203675
  - 5.812424489855767
  - 5.835103490948677
  - 5.889556401968003
  - 5.873365196585656
  validation_losses:
  - 0.38761067390441895
  - 0.38516920804977417
  - 0.40022966265678406
  - 0.4011954665184021
  - 0.3935701847076416
  - 0.3922872841358185
  - 0.40643587708473206
  - 0.399625301361084
  - 0.3976592421531677
  - 0.40536612272262573
  - 0.39316532015800476
  - 0.411727637052536
  - 0.424858957529068
  - 0.4011446237564087
  - 0.3982836902141571
  - 0.4085156321525574
  - 0.49237167835235596
  - 0.3895353078842163
  - 0.40007972717285156
  - 0.3890776038169861
  - 0.3873365521430969
  - 0.4365958571434021
  - 0.46553492546081543
  - 0.4680887758731842
  - 0.4357838034629822
  - 0.4392624497413635
  - 0.4824449121952057
  - 0.9379050731658936
  - 0.3912396728992462
  - 1.0346521139144897
  - 0.3935353755950928
  - 0.45284348726272583
  - 0.49781927466392517
  - 0.3986896574497223
  - 0.38519445061683655
  - 0.42698532342910767
  - 0.39816224575042725
  - 0.42305493354797363
  - 0.7336481213569641
  - 0.6772807836532593
  - 0.40492814779281616
  - 0.7570576071739197
  - 0.4286939203739166
  - 0.4042118191719055
  - 0.41174185276031494
  - 0.40822330117225647
  - 0.4798312485218048
  - 0.47042304277420044
  - 0.8639209270477295
  - 0.6659879684448242
  - 0.6379475593566895
  - 0.5269405841827393
  - 0.5249264240264893
  - 0.44842615723609924
  - 0.4658079743385315
  - 0.7352098226547241
  - 0.7305707335472107
  - 0.7516142725944519
  - 0.7007841467857361
  - 0.8163470029830933
  - 1.0133119821548462
  - 1.010846495628357
  - 0.7027407884597778
  - 0.5402178764343262
  - 0.8785973191261292
  - 1.3279391527175903
  - 1.1065860986709595
  - 0.5847824811935425
  - 0.47194617986679077
  - 1.0329880714416504
  - 0.3917950987815857
  - 0.6281852722167969
  - 0.8232313990592957
  - 0.39343446493148804
  - 0.751807451248169
  - 0.6597901582717896
  - 0.39475175738334656
  - 0.5833200216293335
  - 0.6255603432655334
  - 0.389011412858963
  - 0.5377795696258545
  - 0.5263983011245728
  - 0.45544126629829407
  - 0.5447902083396912
  - 0.43461430072784424
  - 0.49653586745262146
  - 0.49616488814353943
  - 0.6674805879592896
  - 0.46595463156700134
  - 0.5372657179832458
  - 0.4518866539001465
  - 1.1190476417541504
  - 0.9673930406570435
  - 0.5017321705818176
  - 0.6788364052772522
  - 0.44397875666618347
  - 0.6196072101593018
  - 0.8355985879898071
  - 0.47038137912750244
  - 0.49481138586997986
loss_records_fold2:
  train_losses:
  - 5.7837946921587
  - 5.822077524662018
  - 5.870522499084473
  - 5.889770498871804
  - 5.758364436030388
  - 5.757708442211151
  - 5.855926564335824
  - 5.79346067905426
  - 5.774246335029602
  - 5.879876893758774
  - 5.805576100945473
  - 5.808751460909844
  - 5.692475286126137
  - 5.823442158102989
  - 5.908965000510216
  - 5.79280803501606
  - 5.803632858395577
  - 5.855637168884278
  - 5.812862303853035
  - 5.901710441708565
  - 5.963786220550538
  - 5.755902329087258
  - 5.890871843695641
  - 5.91423456966877
  - 5.954552075266839
  - 5.839074873924256
  - 5.692202234268189
  - 5.974173134565354
  - 5.9459494143724445
  - 5.900059267878532
  - 5.841573375463486
  - 5.834711721539498
  - 5.778773710131645
  - 5.874030268192292
  - 5.889918994903565
  - 5.921570309996605
  - 5.927714323997498
  - 5.9560152202844625
  - 5.761643263697625
  - 5.955405706167221
  - 5.77316726744175
  - 5.876777645945549
  - 5.952179563045502
  - 5.808364209532738
  - 5.99246635735035
  - 5.858876451849937
  - 5.9510400861501696
  - 5.81200924217701
  - 5.762301027774811
  - 5.728685051202774
  - 5.7419931381940845
  - 5.947958546876908
  - 5.766025573015213
  - 5.631919547915459
  - 5.8541770637035375
  - 6.035386303067208
  - 5.761071613430977
  - 5.9214691638946535
  - 5.790200638771058
  - 5.775066617131234
  - 5.762301790714265
  - 5.854541027545929
  - 5.900144693255425
  - 5.675253027677536
  - 5.956659641861916
  - 5.906573694944382
  - 5.842878323793411
  - 5.774113431572914
  - 5.967775619029999
  - 5.8658284664154055
  - 5.948488345742226
  - 5.812540158629417
  - 5.914251747727395
  - 5.766243800520897
  - 5.889736834168435
  - 5.813409760594368
  - 5.777628737688065
  - 5.910032430291176
  - 5.685631653666497
  - 5.805490210652351
  - 5.97550810277462
  - 5.98365658223629
  - 5.6635377466678625
  - 5.823980194330216
  - 5.700842171907425
  - 5.839414814114571
  - 5.772814685106278
  - 5.574487879872322
  - 5.788059514760971
  - 5.801437991857529
  - 5.612667754292488
  - 5.863211485743523
  - 5.8757220089435584
  - 5.691415402293206
  - 5.7419772982597355
  - 5.795807042717934
  - 5.859041777253151
  - 5.8283506542444234
  - 5.681914672255516
  - 5.83674145936966
  validation_losses:
  - 0.3914628326892853
  - 0.6991285085678101
  - 0.4302121102809906
  - 0.4393513798713684
  - 0.7995786070823669
  - 0.4401983320713043
  - 0.4090847969055176
  - 0.4372411072254181
  - 0.46548113226890564
  - 0.5564742684364319
  - 0.9902893900871277
  - 0.7151776552200317
  - 0.7692771553993225
  - 0.6534790396690369
  - 1.1386525630950928
  - 0.4256460964679718
  - 0.4198291003704071
  - 0.5676889419555664
  - 0.5247654318809509
  - 1.1219463348388672
  - 0.6122167706489563
  - 0.6046992540359497
  - 0.41981908679008484
  - 0.8653892278671265
  - 0.620830237865448
  - 0.8851932883262634
  - 0.43060219287872314
  - 0.4042840600013733
  - 0.5386425256729126
  - 0.394389808177948
  - 0.5132911205291748
  - 0.4350883960723877
  - 0.4709550142288208
  - 0.7138393521308899
  - 0.3798714578151703
  - 0.4610900580883026
  - 0.46747875213623047
  - 0.42555224895477295
  - 0.5040497183799744
  - 0.46269240975379944
  - 0.5041648745536804
  - 1.1088732481002808
  - 0.4347928762435913
  - 0.45612233877182007
  - 0.4443296194076538
  - 0.38929304480552673
  - 0.4560289978981018
  - 0.40603241324424744
  - 0.3852131962776184
  - 0.5990298986434937
  - 0.40239065885543823
  - 0.4751860201358795
  - 0.6684027314186096
  - 0.5478367209434509
  - 0.569333016872406
  - 0.42298951745033264
  - 0.40377098321914673
  - 0.41509881615638733
  - 0.4559943974018097
  - 0.4355928599834442
  - 0.42759329080581665
  - 0.4680624008178711
  - 0.4371597170829773
  - 0.40328916907310486
  - 0.468353807926178
  - 0.531592607498169
  - 0.42982131242752075
  - 0.5544523596763611
  - 0.39183083176612854
  - 0.45245614647865295
  - 0.4246666729450226
  - 0.41551005840301514
  - 0.46857360005378723
  - 0.4839614927768707
  - 0.5316973328590393
  - 0.5324981212615967
  - 1.622668743133545
  - 0.38601744174957275
  - 0.44405123591423035
  - 0.48010703921318054
  - 0.49038398265838623
  - 0.4891672730445862
  - 0.4547194540500641
  - 0.43492406606674194
  - 0.4609624743461609
  - 0.4275646209716797
  - 0.5815338492393494
  - 0.46609440445899963
  - 0.9026131629943848
  - 0.4415161907672882
  - 0.4454423189163208
  - 0.4103791415691376
  - 0.4129396378993988
  - 0.42819899320602417
  - 0.48296818137168884
  - 0.5484891533851624
  - 0.3873993456363678
  - 0.5410854816436768
  - 0.6462940573692322
  - 0.4368245005607605
loss_records_fold3:
  train_losses:
  - 5.860368913412095
  - 5.9012201011180885
  - 5.775245067477226
  - 6.107979196310044
  - 5.814053511619568
  - 5.877341043949127
  - 5.816779488325119
  - 5.832529601454735
  - 5.864974427223206
  - 5.907786047458649
  - 5.839244523644448
  - 5.834530344605446
  - 5.75277508199215
  - 5.872517007589341
  - 5.8300783932209015
  - 5.722694146633149
  - 5.853841570019722
  - 6.020892152190209
  - 5.665786334872246
  - 5.987912645936013
  - 5.7921922475099565
  - 5.84367353618145
  - 5.701475670933724
  - 5.851582172513009
  - 5.769411966204643
  - 5.890395370125771
  - 5.893076759576798
  - 5.730926859378815
  - 5.733417922258377
  - 5.74664989411831
  - 5.862909007072449
  - 5.650528407096863
  - 5.981374543905258
  - 5.740781378746033
  - 5.837111184000969
  - 5.848709380626679
  - 5.967967823147774
  - 5.774399995803833
  - 5.84451297223568
  - 5.736091330647469
  - 5.749285042285919
  - 5.997855073213578
  - 5.88537464439869
  - 5.7978337675333025
  - 5.795476666092873
  - 5.767146626114846
  - 5.662116664648057
  - 5.78941935300827
  - 5.793908315896989
  - 6.0210767000913625
  - 5.848196193575859
  - 5.769561249017716
  - 5.800558295845986
  - 5.883537685871125
  validation_losses:
  - 0.46349775791168213
  - 0.6572967171669006
  - 0.5675899982452393
  - 0.5840551853179932
  - 0.4791249930858612
  - 0.434004008769989
  - 0.4173610806465149
  - 0.5319332480430603
  - 0.49038583040237427
  - 0.4695586562156677
  - 0.4148097038269043
  - 0.43903404474258423
  - 0.40445417165756226
  - 0.4813421070575714
  - 0.44691988825798035
  - 0.47623759508132935
  - 0.3983536958694458
  - 0.39538508653640747
  - 0.40088099241256714
  - 0.4449373483657837
  - 0.43624380230903625
  - 0.440176397562027
  - 0.5061472654342651
  - 0.4535426199436188
  - 0.41988974809646606
  - 0.554277241230011
  - 0.4743193984031677
  - 0.5959092974662781
  - 0.5162937045097351
  - 0.543347954750061
  - 0.6600500345230103
  - 0.5245302319526672
  - 0.5833583474159241
  - 0.6039500832557678
  - 0.583667516708374
  - 0.6894844174385071
  - 0.5364462733268738
  - 0.5158690214157104
  - 0.6366671323776245
  - 0.547171413898468
  - 0.5161677598953247
  - 0.5829440951347351
  - 0.46299979090690613
  - 0.6145837306976318
  - 0.4464084208011627
  - 0.4332290589809418
  - 0.5884577631950378
  - 0.8193556666374207
  - 0.5454068183898926
  - 0.5242136716842651
  - 0.5105258822441101
  - 0.4586130380630493
  - 0.44341927766799927
  - 0.4027121067047119
loss_records_fold4:
  train_losses:
  - 5.84026145040989
  - 5.759151509404183
  - 5.792429688572884
  - 5.897922697663308
  - 5.871661354601383
  - 5.811423674225807
  - 5.803624987602234
  - 5.760258907079697
  - 5.925146842002869
  - 5.991081643104554
  - 5.795439398288727
  - 5.921053668856621
  - 5.792059510946274
  - 5.791963624954224
  - 5.7892203569412235
  - 5.798755460977555
  - 5.717056688666344
  - 5.972119683027268
  - 5.834657084941864
  - 5.80849995315075
  - 5.769412630796433
  - 5.817088237404824
  - 5.89783907532692
  - 5.9290890991687775
  - 5.882795658707619
  - 5.766661822795868
  - 5.845835670828819
  - 5.870501294732094
  - 5.736117938160897
  - 5.883010959625245
  - 5.746462532877922
  - 5.7365813016891485
  - 5.7639291256666185
  - 5.824688702821732
  - 5.957798850536347
  - 5.817806363105774
  - 5.995081427693368
  - 5.754640567302705
  - 5.697487273812294
  - 5.770723313093185
  - 5.948247742652893
  - 5.811244928836823
  - 5.81680760383606
  - 5.835836338996888
  - 5.756259635090828
  - 5.760044473409653
  - 5.742025527358056
  - 5.900521466135979
  - 5.932011368870736
  - 5.7004918396472934
  - 5.877515363693238
  - 5.817233461141587
  - 5.8385403722524645
  - 5.632605153322221
  - 5.700046110153199
  - 5.6973213285207756
  - 5.759593689441681
  - 5.912802878022195
  - 5.6548689842224125
  - 5.713842976093293
  - 5.766322106122971
  - 5.693550023436547
  - 5.7602568000555046
  - 5.686986383795738
  - 5.747257339954377
  - 5.725980266928673
  - 5.702802151441574
  - 5.788721454143524
  - 5.808902928233147
  - 5.848139992356301
  - 5.823757421970368
  - 5.793987089395523
  - 5.691532048583031
  - 5.86634356379509
  - 5.883758157491684
  - 5.7884676426649095
  - 5.749222591519356
  - 5.624210485816002
  - 5.773869332671166
  - 5.847044321894646
  - 5.73650019466877
  - 5.822826927900315
  - 5.992281812429429
  - 5.9106971472501755
  - 5.765854334831238
  - 5.650191137194634
  - 5.775821879506111
  - 5.963557225465775
  - 5.779050463438034
  - 5.835984888672829
  - 5.860080692172051
  - 6.066775462031365
  - 5.86797790825367
  - 5.806003811955453
  - 5.741598120331765
  - 5.886872100830079
  - 5.8649406969547275
  - 5.750292012095452
  - 5.849573278427124
  - 5.696664172410966
  validation_losses:
  - 0.39848241209983826
  - 0.4406954348087311
  - 0.4600256383419037
  - 0.4153367578983307
  - 0.4020065367221832
  - 0.4133247435092926
  - 0.3883903920650482
  - 0.41381484270095825
  - 0.40713590383529663
  - 0.4348270297050476
  - 0.4812639057636261
  - 0.4310383200645447
  - 0.42932426929473877
  - 0.47878557443618774
  - 0.43511706590652466
  - 0.39110803604125977
  - 0.41086310148239136
  - 0.4572417438030243
  - 0.40620699524879456
  - 0.40644922852516174
  - 0.4260968863964081
  - 0.5352575182914734
  - 0.4505631625652313
  - 0.4820922613143921
  - 0.4254677891731262
  - 0.8725578188896179
  - 0.41351616382598877
  - 0.41361790895462036
  - 0.4734079837799072
  - 0.44829845428466797
  - 0.3899271786212921
  - 0.41645142436027527
  - 0.44744768738746643
  - 0.40181219577789307
  - 0.42267411947250366
  - 0.43218502402305603
  - 0.41293758153915405
  - 0.4118632376194
  - 0.4187275469303131
  - 0.4392976760864258
  - 0.40112656354904175
  - 0.6449869275093079
  - 0.4281345307826996
  - 0.5519987940788269
  - 0.48486411571502686
  - 0.47366029024124146
  - 0.43895402550697327
  - 0.4779087007045746
  - 0.41830480098724365
  - 0.498933881521225
  - 0.43564245104789734
  - 0.418800950050354
  - 0.42594096064567566
  - 0.5023632049560547
  - 0.44882532954216003
  - 0.41517892479896545
  - 0.42370980978012085
  - 0.4241476058959961
  - 0.4525892734527588
  - 0.44021230936050415
  - 0.3732075095176697
  - 0.47431764006614685
  - 0.4735812246799469
  - 0.5301081538200378
  - 0.5425888895988464
  - 0.7474128603935242
  - 0.5874785780906677
  - 0.5177457332611084
  - 0.47491005063056946
  - 0.4504939019680023
  - 0.42340660095214844
  - 0.49067988991737366
  - 0.44252684712409973
  - 0.42685651779174805
  - 0.44143953919410706
  - 0.45574480295181274
  - 0.48672372102737427
  - 0.5080429911613464
  - 0.4339713752269745
  - 0.4766008257865906
  - 0.43028682470321655
  - 0.45107346773147583
  - 0.43310683965682983
  - 0.45268726348876953
  - 0.4172937572002411
  - 0.4533761739730835
  - 0.4444253444671631
  - 0.466359406709671
  - 0.5042740702629089
  - 0.44602563977241516
  - 0.490017294883728
  - 0.4144546091556549
  - 0.4233825206756592
  - 0.4226315915584564
  - 0.4521826207637787
  - 0.42045220732688904
  - 0.3805631101131439
  - 0.4252285957336426
  - 0.448425829410553
  - 0.48463302850723267
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 54 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8507718696397941, 0.8387650085763293, 0.8524871355060034,
    0.8298969072164949]'
  fold_eval_f1: '[0.0, 0.02247191011235955, 0.1896551724137931, 0.04444444444444444,
    0.1680672268907563]'
  mean_eval_accuracy: 0.8459107708086506
  mean_f1_accuracy: 0.08492775077227069
  total_train_time: '0:37:50.556556'
