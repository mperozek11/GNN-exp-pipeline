config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:56:33.620703'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_24fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_24fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_24fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_24fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_24fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_24fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_24fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_24fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_24fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_24fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 58.05051298290491
  - 22.957548284530642
  - 20.48816968500614
  - 17.49918624460697
  - 12.773103778064252
  - 12.772026759386064
  - 13.346623238921167
  - 11.592326295375825
  - 12.354921263456346
  - 13.96191856712103
  - 17.637715756893158
  - 11.634520015120508
  - 9.878937104344368
  - 8.94273745417595
  - 7.052929966151715
  - 9.774307167530061
  - 8.549885019659996
  - 10.508145993947984
  - 10.0492632240057
  - 7.510325330495835
  - 8.611914567649364
  - 7.346130892634392
  - 7.077506846189499
  - 7.037855711579323
  - 6.6136254370212555
  - 6.593802601099014
  - 6.6955310851335526
  - 6.884933832287789
  - 6.252935913205147
  - 7.816719150543213
  - 5.819459906220437
  - 6.625267454981804
  - 6.458423084020615
  - 6.791239500045776
  - 6.1809123158454895
  - 6.704895618557931
  - 6.5548275411129
  - 6.026993362605572
  - 5.988694041967392
  - 5.966303065419197
  - 5.874928559362889
  - 5.738815808296204
  - 5.860695087909699
  - 6.001915255188942
  - 5.847604510188103
  - 5.680095049738885
  - 5.825003603100777
  - 5.9093974232673645
  - 5.834499633312226
  - 5.765386204421521
  - 5.828964802622796
  - 5.8068414747715
  - 5.6587128996849065
  - 5.786096134781838
  - 5.908972936868668
  - 5.906571260094643
  - 5.746109741926194
  - 5.706763792037965
  - 5.83044026196003
  - 6.031515979766846
  - 6.040661825239659
  - 5.823591831326485
  - 5.618670013546944
  - 5.622931104898453
  - 5.713290888071061
  - 5.786552441120148
  - 5.719977790117264
  - 5.84908601641655
  - 5.86549514234066
  - 5.961283481121064
  - 5.912537959218025
  - 5.811223310232163
  - 5.772990173101426
  - 5.723971989750862
  - 5.760933780670166
  - 7.137608429789544
  - 7.315058946609497
  - 5.9152528047561646
  - 5.896420630812646
  - 5.659026701748371
  - 5.750549885630608
  - 5.862009704113007
  - 5.60952174961567
  - 5.698293614387513
  - 5.652458874881268
  - 5.651729336380959
  - 5.761978515982628
  - 5.79634322822094
  - 5.681345540285111
  - 5.8374325245618826
  - 5.809763884544373
  - 5.770946103334428
  - 6.0028716981410986
  - 6.429929551482201
  - 5.920733377337456
  - 5.6677202761173255
  - 5.981148824095726
  - 5.677042374014855
  - 5.922617331147194
  - 6.088904866576195
  validation_losses:
  - 5.25659704208374
  - 0.7486122250556946
  - 0.6717761754989624
  - 0.5406744480133057
  - 3.4405622482299805
  - 0.4108404219150543
  - 1.5041650533676147
  - 0.4195757806301117
  - 0.40757322311401367
  - 0.8724263906478882
  - 0.42832687497138977
  - 0.3793571889400482
  - 0.39214178919792175
  - 0.40608078241348267
  - 0.381880521774292
  - 0.46549150347709656
  - 0.5633028149604797
  - 0.4367086589336395
  - 0.4152541160583496
  - 0.38463065028190613
  - 0.4565904140472412
  - 0.39725202322006226
  - 0.3970754146575928
  - 0.4230532646179199
  - 0.46849748492240906
  - 0.4167988896369934
  - 0.4038960933685303
  - 0.46928319334983826
  - 0.38362056016921997
  - 0.3833141624927521
  - 0.38338565826416016
  - 0.4111383259296417
  - 0.4016304612159729
  - 0.4247214198112488
  - 0.37832725048065186
  - 0.38247376680374146
  - 0.5620735287666321
  - 0.38317131996154785
  - 0.507087767124176
  - 0.3862631320953369
  - 0.38717031478881836
  - 1.5282272100448608
  - 1.0716521739959717
  - 0.5995162129402161
  - 0.9615349769592285
  - 0.3916320502758026
  - 0.40519005060195923
  - 0.4805096983909607
  - 0.3829832971096039
  - 0.38292598724365234
  - 0.5134060382843018
  - 0.6530982851982117
  - 0.397627055644989
  - 0.44061651825904846
  - 1.8466269969940186
  - 0.41182124614715576
  - 0.37955236434936523
  - 0.42155691981315613
  - 0.4212736189365387
  - 0.3872241973876953
  - 0.4398433268070221
  - 0.3830223083496094
  - 0.3949345052242279
  - 0.38699519634246826
  - 0.37928491830825806
  - 0.3925020694732666
  - 0.3804461658000946
  - 0.5408884286880493
  - 0.7330533266067505
  - 0.6091344356536865
  - 0.39186570048332214
  - 0.38240349292755127
  - 0.3777555823326111
  - 0.41459891200065613
  - 0.3774604797363281
  - 0.6972018480300903
  - 0.38793128728866577
  - 0.3954736292362213
  - 0.39074039459228516
  - 0.3797222971916199
  - 0.43561992049217224
  - 0.38085535168647766
  - 0.456374853849411
  - 0.39605432748794556
  - 0.3746900260448456
  - 0.38042914867401123
  - 0.3921719789505005
  - 0.39194047451019287
  - 0.4236351251602173
  - 0.38158902525901794
  - 0.3968086242675781
  - 0.385576456785202
  - 0.39061057567596436
  - 0.44763174653053284
  - 0.3861866593360901
  - 0.3958832919597626
  - 0.4206828474998474
  - 0.38327130675315857
  - 0.432323694229126
  - 0.3861807882785797
loss_records_fold1:
  train_losses:
  - 5.725376099348068
  - 5.856641209125519
  - 5.631458324193955
  - 5.714344987273217
  - 5.699616703391076
  - 5.679184165596962
  - 5.865411823987961
  - 5.727278652787209
  - 5.8707900963723665
  - 5.932105639576912
  - 5.711208030581474
  - 5.699521669745446
  - 5.645265698432922
  - 5.7295759409666065
  - 5.61784249842167
  - 5.635886761546136
  validation_losses:
  - 0.39655786752700806
  - 0.38988274335861206
  - 0.4049004018306732
  - 0.4123055338859558
  - 0.47983428835868835
  - 0.3907293379306793
  - 0.4032890200614929
  - 0.3968657851219177
  - 0.39934808015823364
  - 0.43614789843559265
  - 0.3959830701351166
  - 0.39422041177749634
  - 0.4027976393699646
  - 0.391694575548172
  - 0.3945484757423401
  - 0.39227449893951416
loss_records_fold2:
  train_losses:
  - 5.837047019600869
  - 5.97319276034832
  - 5.957960397005081
  - 5.991500487923623
  - 5.592638546228409
  - 5.65120130777359
  - 5.945651260018349
  - 5.788285540044308
  - 5.818981963396073
  - 5.724595692753792
  - 5.93834020793438
  - 6.103826183080674
  - 5.845499584078789
  - 5.8728791087865835
  - 5.842357526719571
  - 5.7646962538361555
  - 5.9057396233081825
  - 5.731353217363358
  - 5.76938850581646
  - 6.056201764941216
  - 5.869135019183159
  - 6.082741272449494
  - 6.177630315721036
  - 5.919770988821984
  - 5.996170324087143
  - 5.882612183690071
  - 6.041182121634484
  - 5.948401185870171
  - 6.250176858901978
  - 5.965250231325626
  - 5.970882195234299
  - 6.379858958721162
  - 6.05196257531643
  - 6.059816166758537
  - 5.965978515148163
  - 5.986195847392082
  - 5.911496818065643
  - 5.879597321152687
  - 5.881300628185272
  - 5.98603113591671
  - 6.067685782909393
  - 5.9594478815794
  - 6.0234439998865135
  - 5.937955829501153
  - 5.995371842384339
  - 5.9670705735683445
  - 5.9850777894258504
  - 5.938293415307999
  - 6.000955992937088
  - 6.043927145004273
  - 6.0808062136173255
  - 6.118759864568711
  - 6.166752600669861
  - 5.962395995855331
  validation_losses:
  - 0.4074482023715973
  - 0.3775521218776703
  - 0.4613629877567291
  - 0.40830451250076294
  - 0.4767063558101654
  - 0.38230183720588684
  - 0.3687731623649597
  - 0.389773428440094
  - 0.37861737608909607
  - 0.44525304436683655
  - 0.38116344809532166
  - 0.382436603307724
  - 0.3894871175289154
  - 0.40676918625831604
  - 0.37286022305488586
  - 0.3684070110321045
  - 0.38649997115135193
  - 0.37776610255241394
  - 0.36831390857696533
  - 1.289322853088379
  - 365046.09375
  - 4579564032.0
  - 0.4025651216506958
  - 0.3844166100025177
  - 0.4491150677204132
  - 0.39717036485671997
  - 0.3849983513355255
  - 0.42829421162605286
  - 0.38984882831573486
  - 0.3940185606479645
  - 0.39435556530952454
  - 0.41947123408317566
  - 0.3896736204624176
  - 0.38611605763435364
  - 0.3848036527633667
  - 0.38442131876945496
  - 0.38724735379219055
  - 0.39850330352783203
  - 0.38974106311798096
  - 0.4019901752471924
  - 0.3839447796344757
  - 0.38393762707710266
  - 0.39129096269607544
  - 0.40556231141090393
  - 0.42158666253089905
  - 0.3995576500892639
  - 0.38542771339416504
  - 0.3977217674255371
  - 0.39066281914711
  - 0.3873223662376404
  - 0.38965508341789246
  - 0.385437935590744
  - 0.38398534059524536
  - 0.3841249346733093
loss_records_fold3:
  train_losses:
  - 5.9226140767335895
  - 5.920366327464581
  - 5.952735076844693
  - 5.944916185736656
  - 5.887171319127083
  - 5.983955046534539
  - 6.1126614063978195
  - 5.892901834845543
  - 5.831631898880005
  - 5.932285785675049
  - 6.1888781815767295
  - 6.117824795842171
  - 5.860317835211754
  - 6.1295250266790395
  - 6.033641824126244
  - 5.878425481915475
  - 5.869279491901398
  - 5.970761579275131
  - 5.906310862302781
  - 6.127662706375123
  - 5.82366852760315
  - 5.9261912971735
  - 6.1703490853309635
  - 5.996362027525902
  - 5.846618244051934
  - 5.928116658329964
  - 5.9118370622396474
  - 6.038430342078209
  - 5.918239030241967
  - 5.958016949892045
  - 5.8423476159572605
  - 6.103072252869606
  - 5.798818889260293
  - 6.045279616117478
  - 5.968599708378315
  - 6.085789476335049
  - 6.024686996638775
  - 5.836355143785477
  - 5.882363498210907
  - 5.986735777556897
  - 5.901902008056641
  - 5.81837372481823
  - 6.242873793840409
  - 6.124174559116364
  - 6.100373834371567
  - 5.873648530244828
  - 6.03670089840889
  - 5.8242082253098495
  - 5.889717096090317
  - 5.844511222839356
  - 5.961990684270859
  - 5.814532625675202
  - 5.980153733491898
  - 6.03157272040844
  validation_losses:
  - 0.39830905199050903
  - 0.3996275067329407
  - 0.39984387159347534
  - 0.41690748929977417
  - 0.4805830717086792
  - 0.4139513671398163
  - 0.409839004278183
  - 0.42881524562835693
  - 0.403097540140152
  - 0.43522340059280396
  - 0.40441638231277466
  - 0.39832085371017456
  - 0.4273005723953247
  - 0.43684065341949463
  - 0.3999234139919281
  - 0.39870157837867737
  - 0.40184885263442993
  - 0.4296402037143707
  - 0.40973806381225586
  - 0.3991558849811554
  - 1351775360.0
  - 0.39895159006118774
  - 0.45209935307502747
  - 0.4001273512840271
  - 0.41145679354667664
  - 0.39846524596214294
  - 0.4035590887069702
  - 0.4167225956916809
  - 0.4030364453792572
  - 0.44605591893196106
  - 0.45371317863464355
  - 0.4231799244880676
  - 0.4077393412590027
  - 0.4348689019680023
  - 0.46476247906684875
  - 0.4199119508266449
  - 0.408214271068573
  - 0.407260537147522
  - 0.41593286395072937
  - 0.4290483891963959
  - 0.39771488308906555
  - 0.42341819405555725
  - 0.46503451466560364
  - 0.469383180141449
  - 0.3990882635116577
  - 0.4351852834224701
  - 0.39829209446907043
  - 0.4220989942550659
  - 0.41100677847862244
  - 0.4023974537849426
  - 0.3982984125614166
  - 0.3992709517478943
  - 0.40815696120262146
  - 0.39826080203056335
loss_records_fold4:
  train_losses:
  - 5.9622433766722684
  - 6.034571239352227
  - 5.864607173204423
  - 6.006828346848488
  - 5.942920923233032
  - 5.957820540666581
  - 6.034701278805733
  - 5.91586643755436
  - 6.121793442964554
  - 5.8548012867569925
  - 6.157975116372109
  - 5.84563314318657
  - 6.099340716004372
  - 6.191322618722916
  - 5.841416731476784
  - 5.955490067601204
  - 5.927227568626404
  - 6.4865072011947635
  - 6.114307072758675
  - 5.897038897871972
  - 5.8392000019550325
  - 5.9003143876791
  - 6.164425158500672
  - 6.277447058260441
  - 5.919733262062073
  - 6.0829852208495145
  - 6.102522614598275
  - 6.044604226946831
  - 5.806668812036515
  validation_losses:
  - 0.40223702788352966
  - 0.40583765506744385
  - 0.3995673358440399
  - 674392832.0
  - 0.39239948987960815
  - 0.39741799235343933
  - 0.39625099301338196
  - 0.394887775182724
  - 186892544.0
  - 0.40913793444633484
  - 0.4018149971961975
  - 0.39242175221443176
  - 0.4139425754547119
  - 0.4452492594718933
  - 0.39455756545066833
  - 0.3967059254646301
  - 0.41375625133514404
  - 0.3992028534412384
  - 0.39107605814933777
  - 0.3988034427165985
  - 0.3910170793533325
  - 0.3922213613986969
  - 0.44189685583114624
  - 0.3902597427368164
  - 0.3989883363246918
  - 0.3918946385383606
  - 0.390931636095047
  - 0.3920254111289978
  - 0.40184396505355835
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 54 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 54 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:25:03.248689'
