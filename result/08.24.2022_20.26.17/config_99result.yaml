config:
  aggregation: mean
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:39:38.947497'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_99fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_99fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_99fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_99fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_99fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_99fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_99fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_99fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_99fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_99fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.037133651971817
  - 0.8156528353691102
  - 0.809565681219101
  - 0.7822002887725831
  - 0.9458207666873932
  - 0.8174305140972138
  - 0.794856607913971
  - 0.7436515897512437
  - 0.7651894092559814
  - 0.7528793275356294
  - 0.7701101064682008
  validation_losses:
  - 0.500747561454773
  - 0.40573444962501526
  - 0.40074023604393005
  - 0.42836877703666687
  - 0.40586134791374207
  - 0.39666274189949036
  - 0.39974573254585266
  - 0.39310410618782043
  - 0.3854402005672455
  - 0.38565564155578613
  - 0.3939184248447418
loss_records_fold1:
  train_losses:
  - 0.76068115234375
  - 0.7699783086776734
  - 0.8026872098445893
  - 0.7619656622409821
  - 0.7315272033214569
  - 0.7450699985027314
  - 0.8440600335597992
  - 0.7496689677238465
  - 0.8003644168376923
  - 0.8025233387947083
  - 0.8396811544895173
  - 0.7956616997718812
  - 0.8448545336723328
  - 0.7456654906272888
  - 0.7632883846759797
  - 0.7790043830871582
  - 0.7648942530155183
  validation_losses:
  - 0.38489341735839844
  - 0.39066004753112793
  - 0.3892033100128174
  - 0.3859933614730835
  - 0.38611212372779846
  - 0.397085040807724
  - 0.39390939474105835
  - 0.3972717821598053
  - 0.39809954166412354
  - 0.39013558626174927
  - 0.40477028489112854
  - 0.3986208736896515
  - 0.39320504665374756
  - 0.4008122682571411
  - 0.401792049407959
  - 0.38567274808883667
  - 0.3872522711753845
loss_records_fold2:
  train_losses:
  - 0.7631896138191223
  - 0.7757463037967682
  - 0.781936925649643
  - 0.7777515709400178
  - 0.7874758124351502
  - 0.7955378651618958
  - 0.8032680094242096
  - 0.7800489842891694
  - 0.7478546798229218
  - 0.731557959318161
  - 0.7949813961982728
  - 0.7285401701927186
  - 0.7557757019996644
  - 0.7396221160888672
  - 0.749940848350525
  - 0.7961977303028107
  - 0.8043493986129762
  - 0.759793758392334
  - 0.7613167762756348
  - 0.7514373600482941
  - 0.7562857687473298
  - 0.7544260561466217
  - 0.7601784050464631
  - 0.7463498532772065
  - 0.7381106615066528
  - 0.7429925501346588
  - 0.8080942392349244
  - 0.752196091413498
  - 0.7464177072048188
  - 0.7452845454216004
  - 0.7702395260334015
  - 0.764033579826355
  - 0.75985746383667
  - 0.7503352880477906
  - 0.7642074584960938
  - 0.7728872776031495
  - 0.8606982529163361
  - 0.7772965133190155
  - 0.763439691066742
  - 0.8102973461151124
  - 0.747569876909256
  - 0.8514617085456848
  - 0.8275318741798401
  - 0.7571600496768952
  - 0.7870923519134522
  - 0.7407422244548798
  - 0.7384277433156967
  - 0.8088602900505066
  - 0.7461309969425202
  - 0.7673505008220673
  - 0.7473831593990327
  - 0.7626953542232514
  - 0.7012211158871651
  - 0.7692205727100373
  - 0.7921456098556519
  - 0.8069457113742828
  - 0.7463759362697602
  - 0.7878175735473634
  - 0.7552327930927277
  - 0.764327871799469
  - 0.7152596086263657
  - 0.7512949883937836
  - 0.7552297651767731
  - 0.7352533042430878
  - 0.7599215984344483
  - 0.7601801037788392
  - 0.7553094565868378
  - 0.7284510254859925
  - 0.7580224931240083
  - 0.7607253849506379
  - 0.72176952958107
  - 0.7347472906112671
  - 0.7101655751466751
  - 0.7591847240924836
  - 0.7262792050838471
  - 0.7561893522739411
  - 0.7656584441661836
  - 0.7446259140968323
  - 0.7466274201869965
  - 0.7840513348579408
  - 0.7717302322387696
  - 0.7380453765392304
  - 0.7224110424518586
  - 0.8002044916152955
  - 0.7497741520404816
  - 0.8004900634288789
  - 0.7726407587528229
  - 0.7290298700332642
  - 0.7494641900062562
  - 0.7452593326568604
  - 0.7519899308681488
  - 0.7353400528430939
  - 0.7215683698654175
  - 0.7276842951774598
  - 0.7913806796073914
  - 0.8324768245220184
  - 0.758082389831543
  - 0.799837338924408
  - 0.9601376950740814
  - 0.7823182225227356
  validation_losses:
  - 0.4621671140193939
  - 0.4044492244720459
  - 0.37865906953811646
  - 0.4017620384693146
  - 0.3886457085609436
  - 0.3818039000034332
  - 0.40979161858558655
  - 0.37799206376075745
  - 0.38542768359184265
  - 0.38722240924835205
  - 0.48540937900543213
  - 0.3892773687839508
  - 0.3822382986545563
  - 0.3923107087612152
  - 0.3876996636390686
  - 0.3750156760215759
  - 0.40458592772483826
  - 0.3909408748149872
  - 0.4092409014701843
  - 0.3860422372817993
  - 0.38823747634887695
  - 0.38587629795074463
  - 0.39082005620002747
  - 0.39585691690444946
  - 0.4275430738925934
  - 0.40469810366630554
  - 0.41432130336761475
  - 0.3795303702354431
  - 0.3789425492286682
  - 0.38168400526046753
  - 0.39755189418792725
  - 0.393532395362854
  - 0.43283551931381226
  - 0.42706334590911865
  - 0.5857715606689453
  - 0.3793862760066986
  - 0.4301695227622986
  - 0.5232552289962769
  - 0.3794534206390381
  - 0.636548638343811
  - 0.41677501797676086
  - 1.1682084798812866
  - 0.3821049928665161
  - 0.40119341015815735
  - 0.4077613949775696
  - 0.37918856739997864
  - 0.38322317600250244
  - 0.3816358149051666
  - 0.3925374746322632
  - 0.3804912865161896
  - 0.38001012802124023
  - 0.43137356638908386
  - 0.38670510053634644
  - 0.4351246654987335
  - 0.38363170623779297
  - 0.7404812574386597
  - 0.39487722516059875
  - 1.0659154653549194
  - 0.37956398725509644
  - 0.5129498839378357
  - 0.3755887448787689
  - 0.8086217641830444
  - 0.7674882411956787
  - 1.0878385305404663
  - 0.38142576813697815
  - 0.38828903436660767
  - 1.3079750537872314
  - 0.6887965202331543
  - 0.4116370677947998
  - 0.42600730061531067
  - 0.3809436559677124
  - 1.4281537532806396
  - 1.7683974504470825
  - 0.3839988708496094
  - 0.4926221966743469
  - 1.082334280014038
  - 1.2703334093093872
  - 0.38116568326950073
  - 0.3854248523712158
  - 0.3938380181789398
  - 0.435699462890625
  - 0.3847973346710205
  - 0.3811473250389099
  - 0.40455394983291626
  - 0.37531566619873047
  - 0.4111078679561615
  - 0.38819125294685364
  - 0.41013845801353455
  - 0.446478933095932
  - 0.4311975836753845
  - 0.39010488986968994
  - 0.41325899958610535
  - 0.3940991461277008
  - 0.6579687595367432
  - 0.7269895672798157
  - 0.38856276869773865
  - 0.500494122505188
  - 0.6850972175598145
  - 0.5719536542892456
  - 0.6638750433921814
loss_records_fold3:
  train_losses:
  - 0.8060684025287629
  - 0.7571344256401062
  - 0.7402920782566071
  - 0.727685895562172
  - 0.7946712136268617
  - 0.7647790610790253
  - 0.7774665355682373
  - 0.7459315836429596
  - 0.7353921651840211
  - 0.743609642982483
  - 0.7434885680675507
  - 0.7839993655681611
  - 0.7679933726787568
  - 0.7399334847927094
  - 0.7349840700626373
  - 0.7385350048542023
  - 0.7210958302021027
  - 0.7729218900203705
  - 0.7392789125442505
  - 0.7587765157222748
  - 0.7762620091438294
  - 0.7458257198333741
  - 0.8089361250400544
  - 0.7552702605724335
  - 0.7824541926383972
  - 0.7488800168037415
  - 0.8405614197254181
  - 0.8120916664600373
  - 0.8227726340293885
  - 0.7444521427154541
  - 0.7550203323364258
  - 0.780826610326767
  - 0.7671128630638123
  - 0.7510541439056397
  - 0.756033056974411
  - 0.7727924168109894
  - 0.7731169700622559
  - 0.8229725897312165
  - 0.9105907082557678
  - 0.7604920268058777
  - 0.7521196126937867
  - 0.7152728736400604
  - 0.7705781400203705
  - 0.8101548969745637
  - 0.7444235980510712
  validation_losses:
  - 1.865574836730957
  - 0.6297634840011597
  - 1.317103385925293
  - 0.3766527771949768
  - 0.3744705021381378
  - 0.36441683769226074
  - 0.3841898441314697
  - 0.41826364398002625
  - 0.3653920292854309
  - 0.4220687448978424
  - 0.3763735890388489
  - 0.4749031662940979
  - 0.37111905217170715
  - 0.5997438430786133
  - 0.5970565676689148
  - 0.6583285331726074
  - 1.1094716787338257
  - 1.976370096206665
  - 0.3641408383846283
  - 0.38278087973594666
  - 0.43420150876045227
  - 0.38631510734558105
  - 0.39279866218566895
  - 0.36906370520591736
  - 0.379362016916275
  - 0.3716544806957245
  - 0.37052643299102783
  - 0.4049464464187622
  - 0.3638744354248047
  - 0.3750380873680115
  - 0.39007803797721863
  - 0.37487703561782837
  - 0.38039132952690125
  - 0.43832311034202576
  - 1.0769851207733154
  - 0.8917677402496338
  - 0.40119844675064087
  - 0.3748028874397278
  - 0.3856985569000244
  - 0.3821353614330292
  - 0.3731022775173187
  - 0.37470147013664246
  - 0.38083818554878235
  - 0.3765942454338074
  - 0.3746262490749359
loss_records_fold4:
  train_losses:
  - 0.7286755055189134
  - 0.7660284638404846
  - 0.8027927994728089
  - 0.727758875489235
  - 0.8103030145168305
  - 0.7979042172431946
  - 0.7713976383209229
  - 0.72817422747612
  - 0.7597017228603363
  - 0.7291904866695404
  - 0.7433982193470001
  validation_losses:
  - 0.3729371428489685
  - 0.37521985173225403
  - 0.37374961376190186
  - 0.3696606457233429
  - 0.3794666528701782
  - 0.3701292872428894
  - 0.3741915225982666
  - 0.3719014525413513
  - 0.3695521950721741
  - 0.3697376549243927
  - 0.37969115376472473
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8610634648370498, 0.8593481989708405,
    0.8556701030927835]'
  fold_eval_f1: '[0.0, 0.0, 0.047058823529411764, 0.0, 0.04545454545454545]'
  mean_eval_accuracy: 0.8582695266219872
  mean_f1_accuracy: 0.018502673796791443
  total_train_time: '0:15:40.141602'
