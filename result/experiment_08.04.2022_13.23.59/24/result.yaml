config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 15:58:07.006404'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/24/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/24/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/24/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/24/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/24/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/24/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/24/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/24/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/24/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/24/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 389.41594494879246
  - 129.09716035425663
  - 140.5611079633236
  - 132.96396243572235
  - 111.62850526720285
  - 101.31084580719471
  - 115.42489968240261
  - 131.16832569241524
  - 66.56753325462341
  - 67.26201263070107
  - 72.21595636755228
  - 63.77228707075119
  - 45.58779427409172
  - 51.18861463665962
  - 53.8292698264122
  - 42.65725611150265
  - 41.009749472141266
  - 66.34697888791561
  - 60.1143599152565
  - 53.46161448955536
  - 42.745146349072456
  - 48.02246215939522
  - 38.20736691355705
  - 41.209691882133484
  - 37.89893677830696
  - 41.41394546627998
  - 39.573710501194
  - 35.9414167702198
  - 37.207142151892185
  - 35.964380875229836
  - 31.548602625727654
  - 37.42818382382393
  - 36.23609736561775
  - 33.227573439478874
  - 32.18430307507515
  - 32.98982071876526
  - 35.34753733873367
  - 33.59385131299496
  - 32.95672155916691
  - 33.16117185354233
  - 33.76575754582882
  - 33.236309468746185
  - 32.876970052719116
  - 33.85084915161133
  - 33.036142334342
  - 32.72486202418804
  - 32.731976076960564
  - 32.051762729883194
  - 36.56368640065193
  - 33.26538400352001
  - 33.37295611202717
  - 33.86230991780758
  - 34.07802717387676
  - 33.369067430496216
  - 31.50355353951454
  - 32.472409561276436
  - 32.04509249329567
  - 32.72032314538956
  - 32.750722482800484
  - 32.162746638059616
  - 31.851645603775978
  - 32.68658721446991
  - 31.692232325673103
  - 33.16435648500919
  - 32.44498006999493
  - 32.69652971625328
  - 31.63239476084709
  - 31.431215345859528
  - 31.899867236614227
  - 33.888285890221596
  - 32.67072378098965
  - 32.30363088846207
  - 32.958372086286545
  - 35.393832594156265
  - 32.08863854408264
  - 39.82292501628399
  - 70.14041735231876
  - 45.803400844335556
  - 34.76610401272774
  - 34.74439962208271
  - 32.989952847361565
  - 35.185739919543266
  - 31.171295523643494
  - 34.07407255470753
  - 32.473895356059074
  - 32.5924691259861
  - 33.36582870781422
  - 33.448287114501
  - 33.683640480041504
  - 34.403031408786774
  - 32.40815721452236
  - 33.065282717347145
  - 31.799070864915848
  - 33.00810149312019
  - 32.50279065966606
  - 32.76970027387142
  - 33.424188390374184
  - 33.19502617418766
  - 33.204901456832886
  - 32.622364938259125
  validation_losses:
  - 3.9778623580932617
  - 0.7190407514572144
  - 0.4456329643726349
  - 0.5263619422912598
  - 0.4453471004962921
  - 0.5303645133972168
  - 0.4684043228626251
  - 0.8182520866394043
  - 0.4250493049621582
  - 0.42510804533958435
  - 0.416120320558548
  - 0.4223354458808899
  - 0.41518884897232056
  - 0.5414727330207825
  - 0.4156845211982727
  - 0.4121227264404297
  - 0.5980676412582397
  - 0.48997390270233154
  - 0.5234581232070923
  - 0.41163918375968933
  - 0.42768076062202454
  - 0.4625183939933777
  - 0.3924545347690582
  - 0.40654614567756653
  - 0.7026313543319702
  - 0.40250617265701294
  - 0.39869871735572815
  - 0.4388792812824249
  - 0.41502127051353455
  - 0.41156452894210815
  - 0.40410852432250977
  - 0.4234113395214081
  - 0.43774279952049255
  - 0.4094598591327667
  - 0.4058054983615875
  - 0.42766764760017395
  - 0.4066309332847595
  - 0.441069632768631
  - 0.45528364181518555
  - 0.40973037481307983
  - 0.4029841721057892
  - 0.44548651576042175
  - 0.415901243686676
  - 0.40592312812805176
  - 0.42836660146713257
  - 0.43790802359580994
  - 0.41947710514068604
  - 0.41164350509643555
  - 0.6198546886444092
  - 0.400027334690094
  - 0.4004029631614685
  - 0.44205179810523987
  - 0.41287386417388916
  - 0.4570987820625305
  - 0.4385208189487457
  - 0.40892162919044495
  - 0.40263453125953674
  - 0.4327637255191803
  - 0.4492453634738922
  - 0.4232577383518219
  - 0.426877498626709
  - 0.40664568543434143
  - 0.40310433506965637
  - 0.43183422088623047
  - 0.4014669954776764
  - 0.43119460344314575
  - 0.40919333696365356
  - 0.4661369025707245
  - 0.41145360469818115
  - 0.4751311242580414
  - 0.40689191222190857
  - 6.257887840270996
  - 0.4266539216041565
  - 0.39898204803466797
  - 0.4170791804790497
  - 0.5827347636222839
  - 0.4015475809574127
  - 0.41961225867271423
  - 0.4178478419780731
  - 0.4225815236568451
  - 0.45776650309562683
  - 0.4006804823875427
  - 0.5064206123352051
  - 0.39886415004730225
  - 0.4157370924949646
  - 0.4109324514865875
  - 0.4116600453853607
  - 0.4082590937614441
  - 0.42146891355514526
  - 0.4491841197013855
  - 0.4638567268848419
  - 0.4322599172592163
  - 0.4707619845867157
  - 0.4810280203819275
  - 0.40928250551223755
  - 0.4344336986541748
  - 0.46919935941696167
  - 0.4048040211200714
  - 0.4640222489833832
  - 0.40560272336006165
loss_records_fold2:
  train_losses:
  - 31.94127517938614
  - 32.28871329128742
  - 31.195846676826477
  - 33.99782055616379
  - 33.01264674961567
  - 33.790434807538986
  - 32.84458030760288
  - 31.522048071026802
  - 32.85858663916588
  - 33.318275675177574
  - 32.28806434571743
  - 32.58689588308334
  - 31.909438088536263
  - 32.57912527769804
  - 32.72224733233452
  - 32.29025726020336
  - 31.880682423710823
  - 33.30721840262413
  - 33.42392100393772
  - 32.718607664108276
  - 32.45734645426273
  - 33.30369555950165
  - 32.75804217159748
  - 32.75001223385334
  - 33.9370027333498
  - 32.52913026511669
  - 32.23129850625992
  - 31.951433837413788
  - 32.21513846516609
  - 31.531065940856934
  - 33.765510216355324
  - 32.93639698624611
  - 32.41541090607643
  - 35.101827293634415
  - 33.00301565229893
  - 33.669639110565186
  - 33.09992387890816
  - 32.7071727514267
  - 33.0023652613163
  - 31.74517749249935
  - 31.338427186012268
  - 33.811641946434975
  - 34.142847165465355
  - 32.02003452181816
  - 32.942223221063614
  - 32.40076783299446
  - 32.22695833444595
  - 32.74783080816269
  - 34.62505868077278
  - 32.77644094824791
  - 34.69240686297417
  - 33.5479167252779
  - 34.32368069887161
  - 32.29366834461689
  - 32.781027272343636
  - 32.8688390403986
  - 33.331599324941635
  - 32.45802743732929
  - 33.872647523880005
  - 32.840153977274895
  - 32.20573250949383
  - 31.798314332962036
  - 32.634034648537636
  - 32.724305510520935
  - 32.06550708413124
  - 33.642268508672714
  - 33.48417356610298
  - 32.03723283112049
  - 33.60807250440121
  - 35.84578327834606
  - 34.20879901945591
  - 32.140369191765785
  - 32.34693916141987
  - 35.59797519445419
  - 32.4689189940691
  - 32.43926829099655
  - 32.90636149048805
  - 31.478111118078232
  - 33.53169287741184
  - 33.30925418436527
  - 34.61627386510372
  - 32.405912563204765
  - 33.252117961645126
  - 32.28846776485443
  - 33.186500281095505
  - 32.1841886639595
  - 32.650030478835106
  - 31.705801382660866
  - 34.12646110355854
  - 32.68415769934654
  - 32.638547480106354
  - 32.590007558465004
  - 31.685410737991333
  - 32.381774604320526
  - 33.349843829870224
  - 33.149168372154236
  - 31.938799425959587
  - 32.7171548306942
  - 32.19956476986408
  - 31.841263309121132
  validation_losses:
  - 0.40238410234451294
  - 0.3942178189754486
  - 0.44387876987457275
  - 0.40904319286346436
  - 0.4224012494087219
  - 0.41234689950942993
  - 0.4068892300128937
  - 0.4139023423194885
  - 0.4046613276004791
  - 0.4057844281196594
  - 371628.21875
  - 0.3993268609046936
  - 0.3947058320045471
  - 0.40830758213996887
  - 0.4737006723880768
  - 0.4219515025615692
  - 0.43309393525123596
  - 0.4110507071018219
  - 101013.4921875
  - 1508399872.0
  - 0.4128112494945526
  - 0.4064105749130249
  - 0.4668081998825073
  - 0.4157646894454956
  - 0.4469902515411377
  - 0.4091952443122864
  - 0.5001137256622314
  - 0.4108385741710663
  - 0.406872034072876
  - 0.41578930616378784
  - 0.41438883543014526
  - 0.4253729581832886
  - 0.39950841665267944
  - 0.4320928454399109
  - 0.4138372540473938
  - 0.40190187096595764
  - 0.42180135846138
  - 0.4184623956680298
  - 0.3971453011035919
  - 0.4318040907382965
  - 0.4567011594772339
  - 0.42667344212532043
  - 0.46495798230171204
  - 0.4314398467540741
  - 0.3921339809894562
  - 0.4022780656814575
  - 0.4030471742153168
  - 0.4374101161956787
  - 0.4497314393520355
  - 0.4045353829860687
  - 0.4020734131336212
  - 0.42306411266326904
  - 0.4722491204738617
  - 0.4092094898223877
  - 0.3907538950443268
  - 0.4138357937335968
  - 0.40442538261413574
  - 0.42375218868255615
  - 0.4026225507259369
  - 0.4081880450248718
  - 0.4192066192626953
  - 0.3975870907306671
  - 0.4331927001476288
  - 0.40756115317344666
  - 0.39644843339920044
  - 0.45785847306251526
  - 0.4030883014202118
  - 0.4267100393772125
  - 0.45229586958885193
  - 0.4908839464187622
  - 0.4335838258266449
  - 0.4100625813007355
  - 0.4602888524532318
  - 0.4059115946292877
  - 0.41300058364868164
  - 0.49291008710861206
  - 0.40140300989151
  - 0.49229544401168823
  - 0.42740434408187866
  - 0.4391566514968872
  - 0.40152764320373535
  - 0.42625826597213745
  - 0.40405991673469543
  - 0.41856813430786133
  - 0.41075751185417175
  - 0.42746850848197937
  - 0.4202009439468384
  - 0.40015560388565063
  - 0.46293920278549194
  - 0.4626632034778595
  - 0.4315968453884125
  - 0.4012258052825928
  - 0.4258098602294922
  - 0.43434175848960876
  - 0.412382036447525
  - 0.42768019437789917
  - 0.41550737619400024
  - 0.44509822130203247
  - 0.3917445242404938
  - 0.39777809381484985
loss_records_fold4:
  train_losses:
  - 33.057597041130066
  - 33.14013855159283
  - 32.229665607213974
  - 33.80203188955784
  - 32.338896095752716
  - 33.28365486860275
  - 32.39168645441532
  - 31.778961703181267
  - 32.066653326153755
  - 33.70796497166157
  - 32.28042668104172
  - 32.43932431936264
  - 32.620132356882095
  - 33.235470443964005
  - 33.21452347934246
  - 31.702772833406925
  - 32.269233375787735
  - 33.32057046890259
  - 35.84758508205414
  - 33.83955396711826
  - 32.708816945552826
  - 31.6527299284935
  - 32.82851296663284
  - 33.86378316581249
  - 33.96786414086819
  - 33.92631033062935
  - 32.785356909036636
  - 31.9678306132555
  - 32.136433854699135
  - 33.87261617183685
  - 34.02337473630905
  - 32.33284977078438
  - 33.31650039553642
  - 32.05835671722889
  - 32.76087876409292
  - 31.949827551841736
  - 32.48179855942726
  - 32.612999491393566
  - 32.450260013341904
  - 32.83461306989193
  - 33.10447758436203
  - 32.43857914209366
  - 32.11264047026634
  - 35.13190358877182
  - 32.877720192074776
  - 33.52315600216389
  - 32.698917120695114
  - 32.14173810184002
  - 32.231544464826584
  - 31.151796102523804
  - 31.877294272184372
  - 32.28330980241299
  - 33.018100678920746
  - 32.71730613708496
  - 31.647548377513885
  - 32.218207493424416
  - 53.1980786472559
  - 36.220006823539734
  - 32.14815965294838
  - 33.4081339687109
  - 31.462970837950706
  - 34.92330880463123
  - 33.68302194774151
  - 35.984137803316116
  - 33.302827790379524
  - 32.15483516454697
  - 33.56638163328171
  - 33.55804359912872
  - 33.336642771959305
  - 33.13330150395632
  - 34.54413978755474
  - 32.470407247543335
  - 33.13643632829189
  - 34.92105619609356
  - 32.02917155623436
  - 32.24921257793903
  - 32.15784489363432
  - 31.6102474629879
  - 33.139191552996635
  - 33.67310585081577
  - 32.2972754240036
  - 31.980513677001
  - 31.92194612324238
  - 32.55020332336426
  - 32.15723703801632
  - 32.504629492759705
  - 32.45493507385254
  - 32.59510861337185
  - 35.3077447861433
  - 31.9835018068552
  - 31.862614154815674
  - 34.865519270300865
  - 31.73271232843399
  - 32.994752049446106
  - 33.44493665546179
  - 32.56890808045864
  - 32.79439739882946
  - 33.12150056660175
  - 33.00459325313568
  - 34.03145657479763
  validation_losses:
  - 0.4360605776309967
  - 0.455352783203125
  - 0.509155809879303
  - 0.3931179940700531
  - 0.41651278734207153
  - 0.4323115944862366
  - 0.42043668031692505
  - 0.40287840366363525
  - 0.4173724949359894
  - 0.4061551094055176
  - 0.40687859058380127
  - 0.41303324699401855
  - 0.40070492029190063
  - 33256402944.0
  - 0.4289119243621826
  - 0.42478376626968384
  - 0.40387091040611267
  - 0.4138067066669464
  - 380509120.0
  - 0.41194024682044983
  - 12645796864.0
  - 52599361536.0
  - 0.40955349802970886
  - 0.41968008875846863
  - 0.41851744055747986
  - 2840318208.0
  - 0.4047512412071228
  - 0.39956334233283997
  - 0.4003869295120239
  - 16587867136.0
  - 0.4097001552581787
  - 24771792896.0
  - 0.43344828486442566
  - 0.4174099266529083
  - 0.43571361899375916
  - 0.43629565834999084
  - 0.42265331745147705
  - 0.4106544554233551
  - 2801531.75
  - 0.4670414328575134
  - 0.4231258034706116
  - 13475290112.0
  - 5026715648.0
  - 0.43196552991867065
  - 0.4639323055744171
  - 21144.021484375
  - 548538.3125
  - 173254864.0
  - 30990506.0
  - 32344078.0
  - 24284990.0
  - 28631294.0
  - 3604112896.0
  - 27783574.0
  - 26981548.0
  - 26043572.0
  - 0.4421279728412628
  - 71.3252182006836
  - 19.340749740600586
  - 0.398120254278183
  - 0.4411219358444214
  - 0.48095065355300903
  - 4.621589660644531
  - 39.857608795166016
  - 1030.95263671875
  - 198.28973388671875
  - 37.78012466430664
  - 1432.528564453125
  - 876.4161376953125
  - 322.69873046875
  - 1043.957275390625
  - 2784.594970703125
  - 856.5812377929688
  - 1587.0081787109375
  - 1422.635498046875
  - 1257.458984375
  - 1307.2010498046875
  - 51.18399429321289
  - 134.72726440429688
  - 283.0736389160156
  - 9160.845703125
  - 2089.321533203125
  - 14773.4091796875
  - 22.70819854736328
  - 74.89119720458984
  - 1254.8662109375
  - 67.13172912597656
  - 228.13365173339844
  - 274.8652648925781
  - 1182.9776611328125
  - 1206.5859375
  - 7128.8173828125
  - 14649.1484375
  - 47.772396087646484
  - 24.952733993530273
  - 244.47079467773438
  - 12.582877159118652
  - 788.3495483398438
  - 5545.2490234375
  - 14271.27734375
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 72 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:15:47.749968'
