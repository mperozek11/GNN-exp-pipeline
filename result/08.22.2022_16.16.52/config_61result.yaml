config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:27:04.015347'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_61fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_61fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_61fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_61fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_61fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_61fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_61fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_61fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_61fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_61fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 60.59833421707154
  - 24.859630113840105
  - 19.164367282390597
  - 15.732115072011949
  - 17.072519779205322
  - 11.787242627143861
  - 10.586857903003693
  - 5.041086328029633
  - 6.452000173926354
  - 6.449494224786759
  - 5.749675077199936
  - 6.804421669244767
  - 16.03395499587059
  - 12.385583230853081
  - 9.81333248615265
  - 10.082266569137573
  - 16.189757496118546
  - 11.447340828180314
  - 9.392740309238434
  - 7.5999935388565065
  - 6.617378595471383
  - 8.413743901252747
  - 7.588446933031083
  - 5.2937057495117195
  - 9.996446150541306
  - 10.069838178157807
  - 8.491380372643471
  - 11.5792622089386
  - 9.155020010471345
  - 6.016941744089127
  - 6.552117568254471
  - 5.481830337643624
  - 5.758233016729355
  - 7.22653461098671
  - 6.375029581785203
  - 5.327358973026276
  - 5.0484009206295015
  - 5.232212853431702
  - 5.1145002245903015
  - 3.624324080348015
  - 5.805877637863159
  - 6.020995062589646
  - 5.35999163389206
  - 4.442210787534714
  - 3.549934220314026
  - 6.616073215007782
  - 7.010712498426438
  - 7.36731930077076
  - 6.282880401611329
  - 6.692239850759506
  - 4.61738520860672
  - 4.530180060863495
  - 5.076014131307602
  - 4.991437584161758
  - 4.249414688348771
  - 4.920310580730439
  - 4.94720742404461
  - 3.5398450046777725
  - 8.079802572727203
  - 3.968821495771408
  - 6.156988626718522
  - 4.263690626621247
  - 4.177201789617539
  - 5.031446570158005
  - 5.445001149177552
  - 5.8858796715736394
  - 4.118895697593689
  - 3.819888037443161
  - 3.9182997941970825
  - 3.3395901322364807
  - 3.5601740896701815
  - 4.638552811741829
  - 4.329806703329086
  - 4.643599745631218
  - 4.537606146931648
  - 4.763062283396721
  - 5.273710960149765
  - 4.205804806947708
  - 3.9830946028232574
  - 3.639447194337845
  - 4.079829072952271
  - 3.8303541928529743
  - 3.635113197565079
  validation_losses:
  - 3.5570385456085205
  - 4.135897636413574
  - 1.5958307981491089
  - 0.9108882546424866
  - 0.6331613063812256
  - 0.5379619002342224
  - 0.521037757396698
  - 0.5171725749969482
  - 0.49485427141189575
  - 0.4440409243106842
  - 0.4809722304344177
  - 9.666479110717773
  - 2.798915386199951
  - 0.5767041444778442
  - 0.6610173583030701
  - 0.5814264416694641
  - 0.6354215741157532
  - 0.7099946737289429
  - 0.7655476331710815
  - 0.40695706009864807
  - 0.4621536135673523
  - 0.4117356538772583
  - 0.42379793524742126
  - 0.4617096185684204
  - 0.7982336282730103
  - 1.2314784526824951
  - 0.7216233015060425
  - 1.9789338111877441
  - 0.4892946779727936
  - 0.6921334266662598
  - 0.8014068603515625
  - 0.448764443397522
  - 0.4182431697845459
  - 0.8789280652999878
  - 0.42502346634864807
  - 0.4283800721168518
  - 0.6379318237304688
  - 0.5338931679725647
  - 0.4766139090061188
  - 0.3799506723880768
  - 0.5733261704444885
  - 0.4164506793022156
  - 0.4288114905357361
  - 0.44955745339393616
  - 0.4416341185569763
  - 0.7065390348434448
  - 0.4882166087627411
  - 0.40851789712905884
  - 0.429730087518692
  - 0.4180920422077179
  - 0.4214872717857361
  - 0.44128912687301636
  - 0.42446017265319824
  - 0.40902188420295715
  - 0.42658206820487976
  - 0.4246135354042053
  - 0.4438379406929016
  - 0.4170568585395813
  - 0.5904389023780823
  - 0.39266061782836914
  - 0.3996627628803253
  - 0.4925358295440674
  - 0.5884625911712646
  - 0.5097289681434631
  - 0.4281820058822632
  - 0.5306540727615356
  - 0.39410722255706787
  - 0.44665926694869995
  - 0.4093932807445526
  - 0.4007551074028015
  - 0.42604705691337585
  - 0.44841891527175903
  - 0.5191009044647217
  - 0.39921438694000244
  - 0.39241930842399597
  - 0.4592011570930481
  - 0.5322719812393188
  - 0.4319830536842346
  - 0.4265155792236328
  - 0.41313984990119934
  - 0.4156184196472168
  - 0.39277103543281555
  - 0.39659908413887024
loss_records_fold1:
  train_losses:
  - 3.817436802387238
  - 3.1272080302238465
  - 4.0510417282581335
  - 3.6838903307914737
  - 3.38133459687233
  - 3.2783728003501893
  - 3.7942486196756366
  - 3.329848176240921
  - 3.146738421916962
  - 3.2691322654485706
  - 4.196469342708588
  - 3.8019520819187167
  - 3.431299197673798
  - 3.442000162601471
  - 3.8738356262445452
  - 4.258882910013199
  - 4.048108008503914
  - 3.5751547515392303
  - 3.2674151897430423
  - 3.2298315405845646
  - 3.277907454967499
  - 3.2563839435577395
  - 3.338619959354401
  - 3.509997671842575
  - 3.3493481516838077
  - 3.464972841739655
  - 3.504790422320366
  - 3.500571379065514
  - 3.665618434548378
  - 3.383087527751923
  - 3.5838057100772858
  - 3.4140896916389467
  - 3.1238301753997804
  - 3.1147276163101196
  - 3.1754831969738007
  - 3.170208579301834
  - 3.1338927119970323
  - 3.1310214400291443
  - 3.7379290372133256
  - 3.151137560606003
  - 3.4464305818080905
  - 3.1773530662059786
  - 3.1979369044303896
  - 3.299769023060799
  - 3.5071988582611087
  - 3.6431449472904207
  - 3.1814305245876313
  - 3.5092211425304414
  - 3.2345366120338443
  - 3.152481859922409
  - 3.0860555052757266
  - 3.035328111052513
  - 3.2202925741672517
  - 3.097324937582016
  - 3.2460137486457827
  - 3.0551385223865513
  - 3.4770143747329714
  - 3.395717698335648
  - 3.402713894844055
  - 3.484110575914383
  - 3.469249075651169
  - 3.1746420919895173
  - 3.4802998483181002
  - 3.3360365986824037
  - 3.0873389422893527
  - 3.37920737862587
  - 3.2360455572605136
  - 3.0569224536418917
  - 3.015973886847496
  - 2.9990723848342897
  - 3.027380827069283
  - 3.3391961574554445
  - 3.8686259329319004
  - 3.841873133182526
  - 3.228194510936737
  - 3.4377951622009277
  - 3.254588001966477
  - 3.1586561024188997
  - 3.232218360900879
  - 3.294486790895462
  - 3.473841810226441
  - 3.214068925380707
  - 3.183697384595871
  - 3.237482398748398
  - 3.6080214858055117
  - 3.2039434254169468
  - 4.0461511552333835
  - 3.266433930397034
  - 3.130770641565323
  - 3.146579033136368
  - 3.1687416434288025
  - 3.241235899925232
  - 3.1818963766098025
  - 3.2222346484661104
  - 3.393088269233704
  - 3.351511982083321
  - 3.2648990869522097
  - 3.116817826032639
  - 3.3977771252393723
  - 3.1152698665857317
  validation_losses:
  - 0.5346822738647461
  - 0.4017775356769562
  - 0.4344698488712311
  - 0.41874176263809204
  - 0.42821621894836426
  - 0.4966775178909302
  - 0.4032449424266815
  - 0.43080610036849976
  - 0.45162299275398254
  - 0.4138183891773224
  - 0.4443363547325134
  - 0.46530336141586304
  - 0.458750456571579
  - 0.4201483428478241
  - 0.4204200506210327
  - 0.49690890312194824
  - 0.43759530782699585
  - 0.4137653410434723
  - 0.4400186836719513
  - 0.4018896222114563
  - 0.4040207266807556
  - 0.4386005997657776
  - 0.4687320590019226
  - 0.4268415570259094
  - 0.4297737777233124
  - 0.44439321756362915
  - 0.407321959733963
  - 0.4553350508213043
  - 0.39301684498786926
  - 0.41361910104751587
  - 0.519997239112854
  - 0.45252418518066406
  - 0.40223532915115356
  - 0.428870290517807
  - 0.40107595920562744
  - 0.3930526077747345
  - 0.43241503834724426
  - 0.4020331799983978
  - 0.4427575170993805
  - 0.40893125534057617
  - 0.4461304247379303
  - 0.40772202610969543
  - 0.4225058853626251
  - 0.43955546617507935
  - 0.5128226280212402
  - 0.39773815870285034
  - 0.4040590524673462
  - 0.43150296807289124
  - 0.5551363825798035
  - 0.44866955280303955
  - 0.4211573898792267
  - 0.40887027978897095
  - 0.4350675046443939
  - 0.41843241453170776
  - 0.411710262298584
  - 0.4309118390083313
  - 0.4435707628726959
  - 0.4124561846256256
  - 0.4087650775909424
  - 0.43447768688201904
  - 0.4235893785953522
  - 0.4208051860332489
  - 0.557464063167572
  - 0.4088916778564453
  - 0.41002193093299866
  - 0.40171995759010315
  - 0.41289955377578735
  - 0.40394827723503113
  - 0.4055638015270233
  - 0.40842562913894653
  - 0.40495234727859497
  - 0.4101199209690094
  - 0.46045660972595215
  - 0.420162558555603
  - 0.4106552302837372
  - 0.412765234708786
  - 0.41737157106399536
  - 0.434047669172287
  - 0.4103030860424042
  - 0.45139211416244507
  - 0.4025692045688629
  - 0.49443796277046204
  - 0.41574370861053467
  - 0.44489631056785583
  - 0.41896024346351624
  - 0.5024732351303101
  - 0.4333411157131195
  - 0.4054090678691864
  - 0.4211130738258362
  - 0.42246437072753906
  - 0.4209741950035095
  - 0.43058302998542786
  - 1.1669296026229858
  - 0.41930991411209106
  - 0.485723078250885
  - 0.4314805567264557
  - 1.42007315158844
  - 0.449624240398407
  - 0.4401681423187256
  - 0.41182029247283936
loss_records_fold2:
  train_losses:
  - 3.181879433989525
  - 3.377707588672638
  - 3.3100069761276245
  - 3.2290279209613804
  - 3.11901319026947
  - 3.2196717441082003
  - 3.107931160926819
  - 3.155711090564728
  - 3.3579776614904406
  - 3.275182396173477
  - 3.2633056640625
  validation_losses:
  - 0.44710248708724976
  - 0.4122871458530426
  - 0.40606656670570374
  - 0.4062814712524414
  - 0.40938201546669006
  - 0.4018027186393738
  - 0.4013618528842926
  - 0.411283940076828
  - 0.4065123200416565
  - 0.39748090505599976
  - 0.40323877334594727
loss_records_fold3:
  train_losses:
  - 3.361030834913254
  - 3.1217957317829135
  - 3.1908633887767794
  - 3.1221581399440765
  - 3.1303908824920654
  - 3.126998692750931
  - 3.0942927181720736
  - 3.146890938282013
  - 3.2496487677097323
  - 3.252670705318451
  - 3.0501024544239046
  - 3.3324911862611772
  - 3.1980176687240602
  - 3.1458830267190936
  - 3.211855518817902
  - 3.215422582626343
  - 3.0587701469659807
  - 3.2103717982769013
  - 3.5663093447685243
  - 5.028337511420251
  - 3.413557577133179
  - 3.4949495732784275
  - 3.436328035593033
  - 3.5537986755371094
  - 3.211129128932953
  - 3.1491092622280124
  - 3.137207189202309
  validation_losses:
  - 0.41555336117744446
  - 0.4062153100967407
  - 0.4026367962360382
  - 0.41295161843299866
  - 0.406526654958725
  - 0.4051303565502167
  - 0.4040623903274536
  - 0.42475804686546326
  - 0.40620559453964233
  - 0.42017579078674316
  - 0.4348335862159729
  - 0.4448118805885315
  - 0.41943788528442383
  - 0.4065510630607605
  - 0.4391491115093231
  - 7.271642208099365
  - 0.42519211769104004
  - 0.4089398980140686
  - 1.5307996273040771
  - 0.44943755865097046
  - 0.616582989692688
  - 0.6151535511016846
  - 0.46190664172172546
  - 0.4349789619445801
  - 0.42241522669792175
  - 0.42170649766921997
  - 0.4296732246875763
loss_records_fold4:
  train_losses:
  - 3.098685985803604
  - 3.208326989412308
  - 3.184611523151398
  - 3.3527757078409195
  - 3.559532278776169
  - 3.537462991476059
  - 3.147694754600525
  - 3.466138550639153
  - 3.156579923629761
  - 3.1235841512680054
  - 3.3124558448791506
  - 3.1619458854198457
  - 3.11253776550293
  - 3.0002672225236893
  - 3.215195256471634
  - 3.2185202777385715
  - 3.204422280192375
  - 3.2298778653144837
  - 3.21098849773407
  - 3.3435988247394564
  - 3.312409752607346
  - 3.2389468014240266
  - 3.176926463842392
  - 3.2414781212806703
  - 3.1469001293182375
  - 3.3301979899406433
  - 3.2659455239772797
  - 3.3042796432971957
  - 3.1400711596012116
  - 3.2035809189081195
  - 3.1846593022346497
  - 3.148988807201386
  - 3.213441950082779
  - 3.251810199022293
  - 3.25047544836998
  - 3.2140462279319766
  - 3.173655790090561
  - 3.139307820796967
  - 3.254874783754349
  - 3.1485631406307224
  - 3.25294828414917
  - 3.1473844021558763
  - 3.1280269205570224
  - 3.1736869335174562
  - 3.2008171021938328
  - 3.153458589315415
  - 3.2738024294376373
  - 3.10829821228981
  - 3.2699740767478946
  - 3.1501569330692294
  - 3.099118518829346
  - 3.0793451160192493
  - 3.146751266717911
  - 3.2612573862075807
  - 3.1820400744676594
  - 3.1865435361862184
  - 3.1774757117033006
  - 4.975500819087029
  - 4.13928112089634
  - 4.001123464107514
  - 3.6790700614452363
  - 3.6336506545543674
  - 3.45680795609951
  - 3.885653680562973
  - 3.218924576044083
  - 3.441875070333481
  - 3.2214957535266877
  - 3.2256198912858967
  - 3.556093853712082
  - 3.461770135164261
  - 3.2941491305828094
  - 3.2843831181526184
  - 3.3075025737285615
  - 3.231645154953003
  - 3.357292461395264
  - 3.5201127231121063
  - 3.3490750283002857
  - 3.2504621684551243
  - 3.262498539686203
  - 3.4250522911548615
  - 3.4842632651329044
  - 3.4483369648456574
  - 3.210115480422974
  - 3.472583770751953
  - 3.3094217717647556
  - 3.481300300359726
  - 3.6466756701469425
  - 3.12776218354702
  - 3.254664745926857
  - 3.2128770411014558
  - 3.253145134449005
  - 3.2213410556316378
  - 3.3781269758939745
  - 3.307736736536026
  - 3.1281257271766663
  validation_losses:
  - 0.4017375409603119
  - 0.4008824825286865
  - 0.4086197018623352
  - 0.5395572781562805
  - 0.44691210985183716
  - 150.39254760742188
  - 294.30987548828125
  - 0.43747076392173767
  - 0.40402379631996155
  - 0.39157697558403015
  - 0.4207455813884735
  - 0.3951491415500641
  - 0.39846885204315186
  - 0.40150678157806396
  - 0.4015510380268097
  - 0.42417171597480774
  - 0.3984331488609314
  - 0.3914034366607666
  - 0.40695393085479736
  - 0.42191818356513977
  - 2.8637917041778564
  - 27.78785514831543
  - 0.4062888026237488
  - 0.3930177390575409
  - 0.4165351390838623
  - 36.26245880126953
  - 0.4204905033111572
  - 0.41094157099723816
  - 55.0145263671875
  - 171.93385314941406
  - 536.7529907226562
  - 0.3981817364692688
  - 0.4164949059486389
  - 0.39816662669181824
  - 0.41828128695487976
  - 0.39314520359039307
  - 0.3996158540248871
  - 0.38502344489097595
  - 0.403144508600235
  - 595.0944213867188
  - 0.44336384534835815
  - 0.40222129225730896
  - 0.4073302447795868
  - 0.4033454358577728
  - 0.4176537096500397
  - 2102.206787109375
  - 1924.736083984375
  - 2652.43359375
  - 1758.2242431640625
  - 0.3900276720523834
  - 0.40375080704689026
  - 0.40716084837913513
  - 0.40468961000442505
  - 0.40260010957717896
  - 0.4210970401763916
  - 77024424.0
  - 80.89746856689453
  - 0.47547268867492676
  - 0.5061226487159729
  - 0.4254477024078369
  - 0.4624382555484772
  - 0.40792375802993774
  - 0.4291194975376129
  - 0.41042137145996094
  - 0.4354962110519409
  - 0.4137836694717407
  - 0.40835583209991455
  - 0.4108274281024933
  - 0.41815441846847534
  - 0.4387420415878296
  - 0.4396171569824219
  - 0.4075779914855957
  - 0.4196036458015442
  - 0.3959358036518097
  - 0.4212329387664795
  - 0.4085569679737091
  - 0.42329123616218567
  - 0.4217255711555481
  - 0.40841037034988403
  - 0.4078412652015686
  - 0.4083915948867798
  - 0.3941628038883209
  - 0.40533876419067383
  - 0.43430033326148987
  - 0.40397757291793823
  - 0.4558805525302887
  - 0.4043937027454376
  - 0.4050559401512146
  - 0.4172356426715851
  - 0.41479647159576416
  - 0.40755003690719604
  - 0.41453981399536133
  - 0.41688671708106995
  - 0.4107882082462311
  - 0.40133199095726013
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 83 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 95 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:30:02.320668'
