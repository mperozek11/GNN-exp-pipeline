config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:38:52.906856'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_24fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_24fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_24fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_24fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_24fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_24fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_24fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_24fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_24fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_24fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 55.58628362864256
  - 20.832076966762543
  - 20.62334535866976
  - 18.895805242657662
  - 16.827407547831537
  - 12.73040240108967
  - 15.67923358976841
  - 12.45000788718462
  - 11.856666842103005
  - 8.91624092012644
  - 13.733273601531984
  - 8.905333581566811
  - 7.540398767590523
  - 9.514966103434563
  - 11.011338073015214
  - 9.475631004571914
  - 9.259064477682115
  - 7.819689425826073
  - 9.027629771828652
  - 7.3118850052356725
  - 8.095168167352677
  - 7.868494355678559
  - 7.4796119928359985
  - 7.12541530430317
  - 5.9465046495199205
  - 6.222401428222657
  - 6.398999467492104
  - 6.397904759645463
  - 6.370082733035088
  - 7.1261307269334795
  - 5.811412832140923
  validation_losses:
  - 0.5571834444999695
  - 1.0886662006378174
  - 0.5943068861961365
  - 0.5269655585289001
  - 0.8990627527236938
  - 0.447564959526062
  - 0.542186975479126
  - 0.6963874101638794
  - 0.7125458121299744
  - 0.41064903140068054
  - 0.43461543321609497
  - 0.37881800532341003
  - 0.44810065627098083
  - 0.4094659686088562
  - 0.43889132142066956
  - 0.38141173124313354
  - 0.4086739122867584
  - 0.3850537836551666
  - 0.3830934464931488
  - 0.38530734181404114
  - 0.47835829854011536
  - 0.41256484389305115
  - 0.4217304289340973
  - 0.3923509120941162
  - 0.5387181639671326
  - 0.39275193214416504
  - 0.3906054198741913
  - 0.3858679234981537
  - 0.384769469499588
  - 0.37942156195640564
  - 0.38294097781181335
loss_records_fold1:
  train_losses:
  - 6.1628834277391435
  - 6.018903547525406
  - 6.486527451872826
  - 7.418450635671616
  - 6.266726228594781
  - 6.096197557449341
  - 5.930723553895951
  - 6.029726654291153
  - 6.26252510547638
  - 6.094866126775742
  - 6.423367939889431
  - 6.232407426834107
  - 5.879889813065529
  - 6.053438144922257
  validation_losses:
  - 0.41425272822380066
  - 0.39624470472335815
  - 0.8953489065170288
  - 0.6378345489501953
  - 0.40273016691207886
  - 0.46435606479644775
  - 0.4136814773082733
  - 0.464211106300354
  - 0.4157968759536743
  - 0.4026302099227905
  - 0.40926119685173035
  - 0.41381725668907166
  - 0.4011131525039673
  - 0.4072360098361969
loss_records_fold2:
  train_losses:
  - 6.159292754530907
  - 5.915383261442185
  - 5.888686037063599
  - 5.875987252593041
  - 5.875157022476197
  - 5.997298312187195
  - 6.1092676550149925
  - 5.792949733138085
  - 5.815273869037629
  - 5.889393746852875
  - 5.90002425312996
  - 6.577666501700879
  - 5.941877847909928
  - 5.940645065903664
  - 5.90850260257721
  - 5.763085752725601
  - 5.995472475886345
  - 5.890060231089592
  - 6.03622833788395
  - 6.052045753598214
  - 6.037834891676903
  - 5.976357716321946
  - 5.902137959003449
  - 5.733402812480927
  - 6.020565786957741
  - 5.916008940339089
  - 5.836050808429718
  - 5.816959738731384
  - 5.767806068062782
  - 5.88192258477211
  - 6.450622794032097
  - 5.8238311320543295
  - 5.899037349224091
  - 5.940636605024338
  - 6.717567959427834
  - 11.235242151468993
  - 6.905084645748139
  - 6.415472903847695
  - 6.085222777724266
  - 6.062207224965096
  - 6.403639504313469
  - 6.5004347473382955
  - 6.412969282269478
  - 6.063166376948357
  - 5.952547341585159
  - 6.478292828798295
  - 5.760430671274662
  - 6.029509864747524
  - 6.05927406847477
  - 6.146366146206856
  - 5.932530945539475
  - 5.779583469033241
  - 5.684056946635247
  - 5.745311680436135
  - 5.916114754974842
  - 6.052254834771157
  - 5.754605528712273
  - 6.1179677784442905
  - 6.044279807806015
  - 6.195688530802727
  - 5.710407400131226
  - 5.983402165770531
  - 5.880010616779328
  - 5.843618848919869
  - 5.804169419407845
  - 5.730842812359334
  - 5.745362657308579
  - 5.725864011049271
  - 5.736654549837112
  - 5.630964240431786
  - 6.049861335754395
  - 5.872546005249024
  - 6.261074438691139
  - 5.88529501259327
  - 5.877101609110833
  - 5.627980223298073
  - 5.642698886990548
  - 6.026399838924409
  - 5.767171819508076
  - 6.170025506615639
  - 5.802732315659523
  - 5.949042737483978
  - 5.903019037842751
  - 5.909222677350044
  - 5.8686307072639465
  - 5.69665199816227
  - 5.707274402678014
  - 5.784115171432496
  - 5.718552067875862
  - 5.763572034239769
  - 5.9677970767021185
  - 5.687576246261597
  - 5.928890883922577
  - 5.970792432129383
  - 5.626738065481186
  validation_losses:
  - 0.3845968544483185
  - 0.3795118033885956
  - 0.37637239694595337
  - 0.39041951298713684
  - 2.711383104324341
  - 0.3840503692626953
  - 0.5026370882987976
  - 0.3875792920589447
  - 0.7682405114173889
  - 0.3985960781574249
  - 0.5373237729072571
  - 0.5915641784667969
  - 0.9661396741867065
  - 0.5225175023078918
  - 0.38775479793548584
  - 0.5035975575447083
  - 0.3779309391975403
  - 0.4377801716327667
  - 0.5307222604751587
  - 0.3762595057487488
  - 0.4042607247829437
  - 0.415801078081131
  - 0.3924354314804077
  - 0.3842380940914154
  - 0.439973920583725
  - 0.4782628118991852
  - 0.7915545105934143
  - 0.5135517716407776
  - 0.47192707657814026
  - 1.091588020324707
  - 0.4136790931224823
  - 0.3815755248069763
  - 0.40278929471969604
  - 0.38782644271850586
  - 1.2953381538391113
  - 0.4161374568939209
  - 0.3800765872001648
  - 0.43068501353263855
  - 0.45472627878189087
  - 0.37478891015052795
  - 0.4140743017196655
  - 0.3799365758895874
  - 0.4066019356250763
  - 0.4018478989601135
  - 0.37078699469566345
  - 0.43801942467689514
  - 0.3806236982345581
  - 0.49245086312294006
  - 0.3929464817047119
  - 0.3785834014415741
  - 0.37029004096984863
  - 0.36945411562919617
  - 0.3745296895503998
  - 0.43657800555229187
  - 0.380310982465744
  - 0.4057122468948364
  - 0.3867728114128113
  - 0.43037155270576477
  - 0.387134850025177
  - 0.37249183654785156
  - 0.40932387113571167
  - 0.4978083372116089
  - 0.37824687361717224
  - 0.3691917955875397
  - 0.37144386768341064
  - 0.36867836117744446
  - 0.39324307441711426
  - 0.37872815132141113
  - 0.37349990010261536
  - 0.3915966749191284
  - 0.36897391080856323
  - 0.42874690890312195
  - 0.397580087184906
  - 0.6078327298164368
  - 0.39533841609954834
  - 0.36832666397094727
  - 0.3694002032279968
  - 0.3698984384536743
  - 0.38397449254989624
  - 0.38325390219688416
  - 0.4345090687274933
  - 0.3676552176475525
  - 0.38267895579338074
  - 0.39556849002838135
  - 0.40459713339805603
  - 0.3675822615623474
  - 0.37437912821769714
  - 0.393093466758728
  - 0.40567323565483093
  - 0.36904579401016235
  - 0.3772599995136261
  - 0.37870505452156067
  - 0.3682194948196411
  - 0.3690386712551117
  - 0.37380632758140564
loss_records_fold3:
  train_losses:
  - 6.079214829206467
  - 5.706068632006645
  - 5.670447382330895
  - 5.929778924584389
  - 6.051159834861756
  - 5.794837728142738
  - 5.590766444802284
  - 6.152622726559639
  - 6.04943650662899
  - 5.6703773766756065
  - 5.658751720190049
  - 5.7852608948946
  - 6.1956474691629415
  - 5.98704141676426
  - 5.688856381177903
  - 5.729924219846726
  - 5.661028352379799
  - 5.7246125698089605
  - 5.8023080170154575
  - 5.773802545666695
  - 5.78186309337616
  - 5.7761038661003115
  - 6.639729416370392
  - 6.042487972974778
  - 5.68209040760994
  - 5.7061498016119
  - 5.957808989286423
  - 5.6710216283798225
  - 5.642572212219239
  - 5.990460249781609
  - 5.728669598698616
  - 5.771373528242112
  - 5.773238089680672
  - 5.784286466240883
  - 5.730839297175407
  - 5.823647612333298
  - 5.989736558496952
  - 5.7468507289886475
  - 5.6439569950103765
  - 6.019820648431779
  - 6.178108939528466
  - 6.093692329525948
  - 5.911969059705735
  - 6.102241951227189
  - 6.036771053075791
  - 5.870268335938454
  - 5.879536953568459
  - 5.969468340277672
  - 5.9071211397647865
  - 6.124774098396301
  - 5.823408925533295
  - 5.925773179531098
  - 6.169103166460991
  - 5.9951395958662035
  - 5.846209794282913
  - 5.928713157773018
  - 5.911076444387437
  - 6.037886852025986
  - 5.91842914223671
  - 5.95817204117775
  - 5.842482101917267
  - 6.102468198537827
  - 5.7987905770540245
  - 6.044825097918511
  - 5.968258601427078
  - 6.085068020224572
  - 6.024228896200658
  - 5.836271384358406
  - 5.882191759347916
  - 5.986456972360611
  - 5.9018163055181505
  - 5.818310967087746
  - 6.242437767982484
  - 6.12402136027813
  - 6.10060111284256
  - 5.873560616374016
  - 6.036858561635018
  - 5.824222837388516
  - 5.889654257893563
  - 5.844470301270485
  - 5.9619443178176885
  - 5.8145142018795015
  - 5.980062219500542
  - 6.031554108858109
  validation_losses:
  - 0.39443835616111755
  - 0.38054752349853516
  - 0.41843557357788086
  - 0.5304552912712097
  - 0.3769470155239105
  - 0.3839159607887268
  - 0.37505027651786804
  - 0.40772145986557007
  - 0.39273175597190857
  - 0.41621851921081543
  - 0.3785220682621002
  - 0.42164546251296997
  - 0.5740804672241211
  - 0.41514864563941956
  - 0.3953741490840912
  - 0.6115769147872925
  - 0.4335032105445862
  - 0.37603557109832764
  - 0.44992250204086304
  - 0.39785897731781006
  - 0.3832024931907654
  - 0.3772982954978943
  - 0.385635644197464
  - 0.3731901943683624
  - 0.38605329394340515
  - 0.37499910593032837
  - 0.37671908736228943
  - 0.38315385580062866
  - 0.39340677857398987
  - 0.37450480461120605
  - 0.3827139437198639
  - 0.3734011650085449
  - 0.37258052825927734
  - 0.39036768674850464
  - 0.4229675829410553
  - 0.3787391185760498
  - 0.37870094180107117
  - 0.43271538615226746
  - 0.4209872782230377
  - 544.4052734375
  - 0.4031819701194763
  - 0.3951040208339691
  - 0.42476028203964233
  - 2.286525011062622
  - 0.39852023124694824
  - 1.0486254692077637
  - 0.4018300473690033
  - 0.4296990931034088
  - 0.4097403883934021
  - 0.39907121658325195
  - 0.40898603200912476
  - 0.3989938497543335
  - 0.4518699049949646
  - 0.4000970423221588
  - 0.4115973114967346
  - 0.398516982793808
  - 0.40351706743240356
  - 0.4166440963745117
  - 0.40310749411582947
  - 0.4462900161743164
  - 0.4537716507911682
  - 0.4231345057487488
  - 0.40764057636260986
  - 0.43503475189208984
  - 0.46463635563850403
  - 0.4198510944843292
  - 0.40818196535110474
  - 0.40720662474632263
  - 0.4158884286880493
  - 0.4290579557418823
  - 0.39771565794944763
  - 0.4234006702899933
  - 0.4649803340435028
  - 0.4693484306335449
  - 0.3990878164768219
  - 0.43518733978271484
  - 0.3982928395271301
  - 0.4220716655254364
  - 0.4109938442707062
  - 0.4024031460285187
  - 0.3982986509799957
  - 0.399271160364151
  - 0.4081576466560364
  - 0.39825984835624695
loss_records_fold4:
  train_losses:
  - 5.96217692643404
  - 6.034538853168488
  - 5.864574775099754
  - 6.006813606619835
  - 5.94290284216404
  - 5.957761466503143
  - 6.034715542197228
  - 5.915868455171585
  - 6.121759700775147
  - 5.8547901943326
  - 6.1579244345426565
  - 5.84561739563942
  - 6.099306374788284
  - 6.1912733763456345
  - 5.841409131884575
  - 5.95549659729004
  - 5.9272312223911285
  - 6.181727367639542
  - 6.0044955044984825
  - 5.908447101712227
  - 5.863774389028549
  - 5.940802830457688
  - 6.055959713459015
  - 6.314736264944077
  - 5.927916231751443
  - 6.1278390973806385
  - 6.139417004585266
  - 6.071527960896493
  - 5.840920579433441
  - 5.984824866056442
  - 5.912405222654343
  - 5.900326228141785
  - 5.9074860453605655
  - 5.950725331902504
  - 6.005775138735771
  - 6.035050629079342
  - 5.969486376643181
  - 6.039634385704995
  - 6.041099879145623
  - 5.954894226789475
  - 5.940502664446831
  - 5.927517561614514
  - 6.2038469687104225
  - 6.118500536680222
  - 6.031093734502793
  - 5.848156729340554
  - 5.91603616476059
  - 6.032395663857461
  - 5.927530831098557
  - 6.296414107084274
  - 5.928718586266041
  - 5.88074223548174
  - 5.932862320542336
  - 5.931966617703438
  - 6.122019404172898
  - 6.079696306586266
  - 5.91416651904583
  - 5.834598875045777
  - 5.967265525460244
  - 6.089093583822251
  validation_losses:
  - 0.4022318422794342
  - 0.40583860874176025
  - 0.3995661437511444
  - 228533927936.0
  - 0.3924024701118469
  - 0.39741477370262146
  - 0.3962586522102356
  - 5341729.0
  - 582130401280.0
  - 0.40914052724838257
  - 0.4018089175224304
  - 0.392422080039978
  - 0.4139339327812195
  - 0.4452440142631531
  - 0.3945578336715698
  - 0.3967028260231018
  - 0.4137568771839142
  - 0.3928215503692627
  - 0.39213141798973083
  - 0.3980132043361664
  - 0.3919987678527832
  - 0.3919256925582886
  - 0.43248260021209717
  - 0.3921032249927521
  - 0.39919206500053406
  - 0.3936675786972046
  - 1325615546368.0
  - 0.39416566491127014
  - 178844958720.0
  - 0.3932186961174011
  - 0.3946593999862671
  - 0.39454343914985657
  - 0.42059263586997986
  - 0.39190179109573364
  - 0.4188065230846405
  - 0.3911977708339691
  - 0.39166101813316345
  - 0.4433342218399048
  - 0.4309629499912262
  - 0.39154335856437683
  - 0.3912436366081238
  - 1796582932480.0
  - 0.4512738287448883
  - 0.42856210470199585
  - 0.3923726975917816
  - 0.39093664288520813
  - 0.41036075353622437
  - 0.39176705479621887
  - 0.39581283926963806
  - 492046221312.0
  - 0.4157426953315735
  - 0.40514764189720154
  - 0.3986636996269226
  - 0.4463486969470978
  - 0.40244612097740173
  - 0.39513009786605835
  - 0.39215320348739624
  - 0.3943336009979248
  - 0.39890021085739136
  - 0.39404967427253723
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 31 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 95 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 84 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 60 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:28:09.764054'
