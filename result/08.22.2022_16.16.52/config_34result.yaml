config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:54:35.402711'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_34fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_34fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_34fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_34fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_34fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_34fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_34fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_34fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_34fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_34fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.7297630965709687
  - 1.5189754247665406
  - 1.6078018724918366
  - 1.5474577963352205
  - 1.514007568359375
  - 1.5254102587699891
  - 1.5977183043956757
  - 1.5031952619552613
  - 1.4710932642221453
  - 1.5120752215385438
  - 1.5239546537399293
  - 1.5105928480625153
  - 1.5202879250049592
  - 1.5252480864524842
  - 1.4562226206064226
  - 1.49172700047493
  - 1.5508312523365022
  - 1.4725430548191072
  - 1.473489809036255
  - 1.4718194901943207
  - 1.5193627059459687
  - 1.5267903506755829
  - 1.5237749874591828
  - 1.5358425438404084
  - 1.5436326563358307
  - 1.4794071704149248
  - 1.4216629385948183
  - 1.482154405117035
  - 1.5510361433029176
  - 1.474782419204712
  - 1.4407694846391679
  - 1.4838702857494355
  - 1.4561304092407228
  - 1.4996577680110932
  - 1.4495802223682404
  - 1.4744850754737855
  - 1.4833688914775849
  - 1.5033314049243929
  - 1.4270914554595948
  - 1.4579125225543976
  - 1.503863936662674
  - 1.4438703060150146
  - 1.438149702548981
  - 1.412521243095398
  - 1.4406255900859835
  - 1.4648699760437012
  - 1.4592018902301789
  - 1.4147057920694353
  - 1.4216852426528932
  - 1.4328377544879913
  - 1.4622653484344483
  validation_losses:
  - 0.4142642319202423
  - 0.4175029397010803
  - 0.4325235188007355
  - 0.42153650522232056
  - 0.39766258001327515
  - 0.39922189712524414
  - 0.4290461838245392
  - 0.40144652128219604
  - 0.38918501138687134
  - 0.44610199332237244
  - 0.4498782753944397
  - 0.3843640387058258
  - 0.3991753160953522
  - 0.40139076113700867
  - 0.3908659815788269
  - 0.4112468957901001
  - 0.3945724368095398
  - 0.38254255056381226
  - 0.3930659592151642
  - 0.48922041058540344
  - 0.44318056106567383
  - 0.38443171977996826
  - 0.3833687901496887
  - 0.39826062321662903
  - 0.46010622382164
  - 0.4029337167739868
  - 0.3955211043357849
  - 0.46427544951438904
  - 0.6408163905143738
  - 0.3835062086582184
  - 0.3849964737892151
  - 0.3975084722042084
  - 0.38553547859191895
  - 0.39095014333724976
  - 0.38209739327430725
  - 0.3792864680290222
  - 0.4103184938430786
  - 0.3954121470451355
  - 0.3887971043586731
  - 0.38441556692123413
  - 0.37956327199935913
  - 0.3996277153491974
  - 0.37849852442741394
  - 0.37663203477859497
  - 0.9349805116653442
  - 0.43121853470802307
  - 0.38186943531036377
  - 0.38083764910697937
  - 0.3792957365512848
  - 0.38827383518218994
  - 0.3755876421928406
loss_records_fold1:
  train_losses:
  - 1.4080519556999207
  - 1.428233027458191
  - 1.4015246391296388
  - 1.418862843513489
  - 1.4116198360919954
  - 1.4199877828359604
  - 1.441148865222931
  - 1.4413439750671388
  - 1.450224530696869
  - 1.435258248448372
  - 1.413046282529831
  - 1.4216054022312166
  - 1.3996409058570862
  - 1.4119130432605744
  - 1.4814154326915743
  - 1.4071171939373017
  - 1.4565126836299898
  - 1.4308111906051637
  - 1.4305708587169648
  - 1.428575497865677
  - 1.435687530040741
  - 1.391726088523865
  - 1.4530706048011781
  - 1.4436059653759004
  - 1.4432691037654877
  - 1.4796342998743057
  - 1.5024611830711365
  - 1.4246151566505434
  - 1.4092185974121094
  - 1.5026001155376436
  - 1.67899187207222
  - 1.643236976861954
  - 1.5101787567138674
  - 1.4068061351776124
  - 1.4407146751880646
  - 1.4865627110004427
  - 1.4011191189289094
  - 1.4150154232978822
  - 1.4408448457717897
  validation_losses:
  - 0.3907753825187683
  - 0.40240219235420227
  - 0.43162310123443604
  - 0.5778548717498779
  - 0.38770338892936707
  - 0.3833100199699402
  - 0.3859701454639435
  - 0.3847810924053192
  - 0.3872376084327698
  - 0.4073403775691986
  - 0.4215683937072754
  - 0.4180855453014374
  - 0.4387279748916626
  - 0.5789124965667725
  - 0.43738308548927307
  - 0.47127479314804077
  - 0.4094313681125641
  - 0.5297614932060242
  - 0.38827258348464966
  - 0.38522225618362427
  - 0.38532572984695435
  - 0.38604679703712463
  - 0.38342154026031494
  - 0.396683931350708
  - 0.38516074419021606
  - 0.38420021533966064
  - 0.38712722063064575
  - 0.38625937700271606
  - 0.3860626518726349
  - 0.40581873059272766
  - 0.43736186623573303
  - 0.39854806661605835
  - 0.4450150728225708
  - 0.4015306532382965
  - 0.40473446249961853
  - 0.4068967401981354
  - 0.3861808478832245
  - 0.39382562041282654
  - 0.3902086317539215
loss_records_fold2:
  train_losses:
  - 1.40237375497818
  - 1.4129818737506867
  - 1.4418548285961152
  - 1.394367164373398
  - 1.4299892485141754
  - 1.4121853470802308
  - 1.4167525291442873
  - 1.3960215091705324
  - 1.4193388223648071
  - 1.4288255274295807
  - 1.3939365923404694
  validation_losses:
  - 0.4267962872982025
  - 0.38499709963798523
  - 0.38456401228904724
  - 0.3800569474697113
  - 0.3956237733364105
  - 0.37501609325408936
  - 0.37650665640830994
  - 0.3791273236274719
  - 0.3803478479385376
  - 0.3796743154525757
  - 0.38061270117759705
loss_records_fold3:
  train_losses:
  - 1.4037207752466203
  - 1.4380759298801422
  - 1.4301690816879273
  - 1.4027721703052523
  - 1.4171380221843721
  - 1.4487373888492585
  - 1.4369108259677887
  - 1.4100989699363708
  - 1.4181806325912476
  - 1.409963446855545
  - 1.4159606873989106
  - 1.4154660880565644
  - 1.4035180270671845
  - 1.4223028600215912
  - 1.4169418811798096
  - 1.3771342039108276
  - 1.3917945325374603
  - 1.3939467787742617
  - 1.4393252074718477
  - 1.4074957072734833
  - 1.4336031258106232
  - 1.411361861228943
  - 1.401790660619736
  - 1.4070547819137573
  - 1.3910235822200776
  - 1.4094567239284517
  - 1.382725140452385
  - 1.3768652588129044
  - 1.375622120499611
  - 1.4521573901176454
  - 1.411825954914093
  - 1.4227805137634277
  - 1.4160107135772706
  - 1.4043731629848482
  - 1.4193999588489534
  - 1.419081234931946
  - 1.409940904378891
  - 1.3960527539253236
  - 1.421318316459656
  - 1.4283100247383118
  - 1.5098289251327515
  - 1.4731778621673586
  - 1.429388052225113
  - 1.4210400104522707
  - 1.4047884523868561
  validation_losses:
  - 0.3585153818130493
  - 0.36512210965156555
  - 0.36281517148017883
  - 0.37727323174476624
  - 0.36912253499031067
  - 0.4050566852092743
  - 0.6329749226570129
  - 0.846950352191925
  - 0.7505884170532227
  - 1.0157614946365356
  - 0.6695053577423096
  - 0.37692224979400635
  - 0.3934003710746765
  - 0.3732558488845825
  - 0.5525293946266174
  - 0.40905094146728516
  - 0.5347158312797546
  - 0.3630928099155426
  - 0.37417587637901306
  - 0.4005522131919861
  - 0.4265386760234833
  - 0.4785556197166443
  - 0.802907407283783
  - 0.379482626914978
  - 0.5926238298416138
  - 0.6764187216758728
  - 0.43616580963134766
  - 0.8626235127449036
  - 0.37121233344078064
  - 0.40716636180877686
  - 0.38760340213775635
  - 0.36945146322250366
  - 0.39271116256713867
  - 0.4643513858318329
  - 0.37042105197906494
  - 0.5371556282043457
  - 0.3842910826206207
  - 0.49352964758872986
  - 0.7386251091957092
  - 0.3705514967441559
  - 0.37870192527770996
  - 0.3819974958896637
  - 0.3748909831047058
  - 0.369666188955307
  - 0.373757928609848
loss_records_fold4:
  train_losses:
  - 1.386298304796219
  - 1.4191326022148134
  - 1.427799540758133
  - 1.4305976271629335
  - 1.4239940643310547
  - 1.4019017696380616
  - 1.4206291317939759
  - 1.408285415172577
  - 1.3848799526691438
  - 1.38233842253685
  - 1.4035395741462708
  - 1.41380552649498
  - 1.4052160918712617
  - 1.4027753412723543
  - 1.3749820828437807
  - 1.3998400926589967
  - 1.4071009457111359
  - 1.3538853645324709
  - 1.4035023987293245
  - 1.3902327716350555
  - 1.406957644224167
  - 1.3908132731914522
  - 1.4167047798633576
  - 1.3676519960165026
  - 1.4219519257545472
  - 1.4136039078235627
  - 1.3904491901397706
  - 1.432032823562622
  - 1.373458516597748
  - 1.3663956046104433
  - 1.4647158324718477
  - 1.412525498867035
  - 1.3982644379138947
  - 1.3851599365472795
  - 1.3925887465476992
  - 1.4047797977924348
  - 1.3558250665664673
  - 1.3852358520030976
  - 1.4148155510425569
  - 1.4090421795845032
  - 1.3788293600082397
  - 1.4080853819847108
  - 1.3656989723443986
  - 1.3937781631946564
  - 1.4225502371788026
  - 1.3705439031124116
  - 1.3775758355855943
  - 1.4095001220703125
  - 1.420067095756531
  - 1.4262953937053682
  - 1.403912377357483
  - 1.3660819232463837
  - 1.3903158426284792
  - 1.3689910650253296
  - 1.3468449234962465
  - 1.3897349536418915
  - 1.40030135512352
  - 1.4265255212783814
  - 1.4342783749103547
  - 1.3593799591064455
  - 1.3443320840597153
  - 1.3861068665981293
  - 1.3663220405578613
  - 1.3825249135494233
  - 1.362717965245247
  - 1.3946945786476137
  - 1.4074679076671601
  - 1.402558195590973
  - 1.4500353753566744
  - 1.3812127888202668
  - 1.416539627313614
  - 1.478858745098114
  - 1.438027447462082
  - 1.4293702185153963
  - 1.3935590833425522
  - 1.3806333601474763
  - 1.4082374334335328
  validation_losses:
  - 0.36228108406066895
  - 0.3878765404224396
  - 0.4060523211956024
  - 0.3677138388156891
  - 0.3767520785331726
  - 0.3703930974006653
  - 0.3891337215900421
  - 0.4384734630584717
  - 0.3786335289478302
  - 0.39429911971092224
  - 0.3642870783805847
  - 0.41510292887687683
  - 0.4631807506084442
  - 0.47735512256622314
  - 0.37267249822616577
  - 0.36456283926963806
  - 0.4215831160545349
  - 0.37173306941986084
  - 0.3900991678237915
  - 0.3778921961784363
  - 0.4066436290740967
  - 0.35986754298210144
  - 0.3932096064090729
  - 0.4524986147880554
  - 0.37652507424354553
  - 0.375172883272171
  - 0.4247041940689087
  - 0.36566081643104553
  - 0.3665671944618225
  - 0.36047518253326416
  - 0.41074541211128235
  - 0.3740190267562866
  - 0.37407687306404114
  - 0.3713947832584381
  - 0.3617932200431824
  - 0.4234527051448822
  - 0.3642134964466095
  - 0.36720824241638184
  - 0.37616899609565735
  - 0.42140868306159973
  - 0.36423560976982117
  - 0.3756104111671448
  - 0.3623262941837311
  - 0.36114799976348877
  - 0.4137325882911682
  - 0.4009400010108948
  - 0.42077845335006714
  - 0.36306893825531006
  - 0.4097616672515869
  - 0.4025990068912506
  - 0.40558093786239624
  - 0.4904804229736328
  - 0.39372992515563965
  - 0.3917867839336395
  - 0.3645419776439667
  - 0.4350883364677429
  - 0.36977770924568176
  - 0.361317902803421
  - 0.4467586278915405
  - 0.48836058378219604
  - 0.42423850297927856
  - 0.48839259147644043
  - 0.39831453561782837
  - 0.3656979501247406
  - 0.38646355271339417
  - 0.3792027235031128
  - 0.39683470129966736
  - 0.369308739900589
  - 0.3753354251384735
  - 0.37385889887809753
  - 0.407413512468338
  - 0.37511441111564636
  - 0.37639158964157104
  - 0.3711004853248596
  - 0.3728119730949402
  - 0.36668670177459717
  - 0.3742368519306183
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 77 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8490566037735849, 0.8576329331046312, 0.855917667238422,
    0.8539518900343642]'
  fold_eval_f1: '[0.0, 0.0, 0.023529411764705882, 0.04545454545454545, 0.10526315789473685]'
  mean_eval_accuracy: 0.8544953522778849
  mean_f1_accuracy: 0.03484942302279763
  total_train_time: '0:19:14.707450'
