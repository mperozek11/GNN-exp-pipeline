config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:38:15.476249'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_95fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_95fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_95fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_95fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_95fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_95fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_95fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_95fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_95fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_95fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 11.112405347824097
  - 5.985051393508911
  - 4.525398433208466
  - 3.0412973880767824
  - 4.1137104511260985
  - 1.8501637101173403
  - 2.345371866226196
  - 1.4875050485134125
  - 1.9176506459712983
  - 2.6473868966102603
  - 2.4707293748855594
  - 1.7989791452884676
  - 1.1621230363845825
  - 1.3263716459274293
  - 1.4726879060268403
  - 10.765375363826752
  - 4.357247567176819
  - 1.9953145503997805
  - 1.1124332010746003
  - 1.3921374022960664
  - 2.6448517680168155
  - 2.1135425925254823
  - 1.3493880331516266
  - 1.1196336925029755
  - 1.024937665462494
  - 1.0142829716205597
  - 1.4056752026081085
  - 1.3313393950462342
  - 1.0361229360103608
  - 0.9291024386882782
  - 1.4401766121387483
  - 1.6481797575950623
  - 1.2172667145729066
  - 2.6309084177017215
  - 1.1008009374141694
  - 1.402015745639801
  - 1.2098368883132935
  - 1.2196468472480775
  - 1.2481888175010682
  - 1.4597927868366243
  - 1.721098554134369
  - 0.8949903726577759
  - 1.083501398563385
  - 1.3016847610473634
  - 2.1703216254711153
  - 0.9209639608860016
  - 1.1296169579029083
  - 1.5564908683300018
  - 2.0419614732265474
  - 1.7669433116912843
  - 0.9714865386486053
  - 1.0869886577129364
  - 1.7248471260070801
  - 0.9962208032608033
  - 1.415998125076294
  - 1.2635152816772461
  - 1.584212750196457
  - 1.8332914948463441
  - 1.5939706146717072
  - 0.999200052022934
  - 2.0664647579193116
  - 4.518588185310364
  - 3.4243043541908267
  - 1.383315521478653
  - 2.1046913743019107
  - 1.5337326884269715
  - 1.0744017362594604
  - 1.1146845638751983
  - 0.9349390268325806
  - 1.0863045811653138
  - 1.2236421763896943
  - 0.9433046281337738
  - 6.703450977802277
  - 1.2968380928039551
  - 3.421080982685089
  - 1.7278792798519136
  - 1.2002827644348146
  - 1.1053191274404526
  - 0.9925208449363709
  - 1.066071105003357
  - 0.8836537182331086
  - 0.8939112663269043
  - 1.0853651940822602
  - 0.949570244550705
  - 1.2731793940067293
  - 1.434249097108841
  - 4.813618934154511
  - 2.7055028438568116
  - 1.5754072546958924
  - 1.1260469824075698
  - 1.0822961747646331
  - 0.9848333060741425
  - 0.9747129619121552
  - 0.9709436893463135
  - 0.8350607782602311
  - 1.0739228367805482
  - 0.9242691516876221
  - 0.9033787846565247
  - 1.1921626329421997
  - 0.854265320301056
  validation_losses:
  - 2.14595103263855
  - 2.659405469894409
  - 4.845823764801025
  - 1.0179929733276367
  - 0.936557412147522
  - 1.2452969551086426
  - 0.7880688905715942
  - 0.6159214973449707
  - 1.672351360321045
  - 0.7274070978164673
  - 0.6590094566345215
  - 0.5586301684379578
  - 0.5264766216278076
  - 0.7293199300765991
  - 0.5283099412918091
  - 0.5398663878440857
  - 0.5729113817214966
  - 0.5980799794197083
  - 0.5305678248405457
  - 1.6580355167388916
  - 1.0945042371749878
  - 0.5148756504058838
  - 0.5204182863235474
  - 0.45993760228157043
  - 0.5128534436225891
  - 0.542350172996521
  - 0.42401382327079773
  - 0.41408994793891907
  - 0.42446088790893555
  - 0.4929271638393402
  - 0.49769720435142517
  - 0.7736285328865051
  - 0.4559105336666107
  - 0.7880098819732666
  - 0.4762526750564575
  - 0.5917132496833801
  - 0.4865112900733948
  - 0.44339630007743835
  - 0.44711393117904663
  - 0.5360727310180664
  - 0.41828015446662903
  - 0.4403644800186157
  - 0.4506210386753082
  - 0.4319564402103424
  - 0.5465685725212097
  - 0.4333687126636505
  - 0.40646401047706604
  - 0.42847007513046265
  - 0.48292627930641174
  - 0.39593827724456787
  - 0.4155382215976715
  - 0.428466796875
  - 0.4123058021068573
  - 0.4477592706680298
  - 0.4044434428215027
  - 0.4214000701904297
  - 0.40115663409233093
  - 0.41835176944732666
  - 0.40272092819213867
  - 0.4230644106864929
  - 0.39835160970687866
  - 0.4609511196613312
  - 0.7381058931350708
  - 0.46784454584121704
  - 0.5070269107818604
  - 0.45222383737564087
  - 0.4932441711425781
  - 0.39100193977355957
  - 0.4420815706253052
  - 0.424129456281662
  - 0.414568692445755
  - 0.4281977713108063
  - 0.4558246433734894
  - 0.4578327238559723
  - 0.49044570326805115
  - 0.5094925165176392
  - 0.43505358695983887
  - 0.497518926858902
  - 0.41682496666908264
  - 0.402689665555954
  - 0.4024352431297302
  - 0.4150238633155823
  - 0.4092966318130493
  - 0.4173642694950104
  - 0.40650084614753723
  - 0.4259669780731201
  - 0.4914519488811493
  - 0.4247981011867523
  - 0.437198668718338
  - 0.4497445821762085
  - 0.43367111682891846
  - 0.39692744612693787
  - 0.3989146947860718
  - 0.43135446310043335
  - 0.4019995331764221
  - 0.4091067910194397
  - 0.4247167706489563
  - 0.399056077003479
  - 0.39881372451782227
  - 0.3994676172733307
loss_records_fold1:
  train_losses:
  - 1.0084699869155884
  - 0.9201873242855072
  - 0.8222530841827393
  - 0.956476503610611
  - 0.9096166014671326
  - 1.181261283159256
  - 0.8331845343112946
  - 0.893388730287552
  - 0.9804437160491943
  - 0.9125647366046906
  - 0.8233815431594849
  - 0.8476704895496369
  - 0.8770701706409455
  - 0.8724927127361298
  - 6.712145960330964
  - 1.1129586100578308
  - 1.1624973952770234
  - 0.8769201040267944
  - 0.8473969101905823
  - 1.0000036418437959
  - 0.866340982913971
  - 0.8759602904319763
  validation_losses:
  - 0.41129252314567566
  - 0.402784526348114
  - 0.3987228572368622
  - 0.40694761276245117
  - 0.41278618574142456
  - 0.42137256264686584
  - 0.4364762008190155
  - 0.44158580899238586
  - 0.4331364631652832
  - 0.41794610023498535
  - 0.41168856620788574
  - 0.4234451651573181
  - 0.4079817831516266
  - 0.4224729537963867
  - 0.40697574615478516
  - 0.46229231357574463
  - 0.42388778924942017
  - 0.41084057092666626
  - 0.4053780138492584
  - 0.4081301689147949
  - 0.4156661331653595
  - 0.42458778619766235
loss_records_fold2:
  train_losses:
  - 0.8761404097080231
  - 1.365110355615616
  - 0.8775972545146943
  - 0.864984804391861
  - 0.8325995564460755
  - 0.8496617555618287
  - 0.8342455387115479
  - 0.8952613592147828
  - 0.8647615432739258
  - 1.8166251301765444
  - 1.6538293719291688
  validation_losses:
  - 0.38811415433883667
  - 0.37694793939590454
  - 0.39053502678871155
  - 0.39552760124206543
  - 0.39780041575431824
  - 0.3837783932685852
  - 0.382353812456131
  - 0.39224323630332947
  - 0.3935839533805847
  - 0.38069143891334534
  - 0.37915003299713135
loss_records_fold3:
  train_losses:
  - 1.0868299901485443
  - 0.947069001197815
  - 0.9009132891893388
  - 0.885253405570984
  - 0.8685571789741516
  - 0.7945139735937119
  - 7.349463045597076
  - 1.0076781630516052
  - 1.085189539194107
  - 0.8644996285438538
  - 1.4577707767486574
  - 2.9503503978252414
  - 0.9183591663837434
  - 1.3135287046432496
  - 0.9125956058502198
  - 0.8473707497119904
  - 0.8314271748065949
  - 0.8875937283039094
  - 0.890953427553177
  - 0.8899057626724244
  - 1.1001601159572603
  - 0.915756344795227
  - 0.8920577883720399
  - 0.8621599793434144
  - 0.84011510014534
  - 0.9117268443107606
  - 0.8605686187744142
  - 0.8269826591014863
  validation_losses:
  - 0.49755409359931946
  - 0.45918765664100647
  - 0.4047755300998688
  - 0.4055330753326416
  - 0.3958541452884674
  - 0.39214062690734863
  - 0.39871838688850403
  - 0.8651021718978882
  - 0.42918866872787476
  - 0.4305366575717926
  - 0.4274376332759857
  - 0.3977615535259247
  - 0.431032657623291
  - 0.4434567391872406
  - 0.43271583318710327
  - 0.4211675226688385
  - 0.422385036945343
  - 0.4410916566848755
  - 0.4804989695549011
  - 0.40839460492134094
  - 0.4302191436290741
  - 0.45626381039619446
  - 0.455535888671875
  - 0.40828433632850647
  - 0.3937799632549286
  - 0.3957563042640686
  - 0.39384809136390686
  - 0.3973609507083893
loss_records_fold4:
  train_losses:
  - 0.8422026753425599
  - 0.9052508294582368
  - 0.8780221939086914
  - 0.835731315612793
  - 0.864584767818451
  - 0.856591272354126
  - 0.8381309807300568
  - 0.8761190950870514
  - 0.8552002310752869
  - 0.8863055229187012
  - 0.8458919286727906
  - 0.8783296167850495
  - 0.833561646938324
  - 0.794432145357132
  - 0.8579653918743134
  - 0.8403537392616273
  - 0.8141380965709687
  - 0.8068202793598176
  - 0.8308807373046876
  - 1.0625966608524322
  - 0.929837989807129
  - 0.890344649553299
  - 0.8722671985626221
  - 0.8194155305624009
  - 1.40416476726532
  validation_losses:
  - 0.4019683301448822
  - 0.3993327021598816
  - 0.4382140040397644
  - 0.421648234128952
  - 0.4195808470249176
  - 0.3983863294124603
  - 0.4267813265323639
  - 0.42287909984588623
  - 0.3984191417694092
  - 0.43481042981147766
  - 0.4277051091194153
  - 0.41211798787117004
  - 0.4201931953430176
  - 0.39792799949645996
  - 0.39871200919151306
  - 0.4308430850505829
  - 0.38414695858955383
  - 0.42141249775886536
  - 0.4838162958621979
  - 0.4538382291793823
  - 0.4523324966430664
  - 0.4109608232975006
  - 0.4160301983356476
  - 0.4107629358768463
  - 0.39361146092414856
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:15:35.903962'
