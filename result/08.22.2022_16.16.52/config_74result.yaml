config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:52:33.653375'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_74fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_74fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_74fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_74fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_74fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_74fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_74fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_74fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_74fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_74fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 24.44664454460144
  - 5.1670789539814
  - 4.510483002662659
  - 3.2896278679370883
  - 2.8261416375637056
  - 3.7326694607734683
  - 3.7374026775360107
  - 2.7081393852829936
  - 4.156719815731049
  - 2.672516417503357
  - 2.3182481050491335
  - 2.6109834909439087
  - 2.957818800210953
  - 2.8029542088508608
  - 3.299970805644989
  - 5.226941275596619
  - 10.703720915317536
  - 6.745302510261536
  - 2.1987433165311816
  - 1.9647821247577668
  - 2.4986512899398807
  validation_losses:
  - 6.160743713378906
  - 1.586081624031067
  - 0.9341301321983337
  - 0.46412232518196106
  - 1.422404170036316
  - 0.5811880826950073
  - 0.9196034073829651
  - 0.6878546476364136
  - 1.4137554168701172
  - 1.1386696100234985
  - 0.43403324484825134
  - 0.7506065964698792
  - 0.43897148966789246
  - 0.5493610501289368
  - 0.7311203479766846
  - 0.691828727722168
  - 0.5755101442337036
  - 0.5152565836906433
  - 0.48011165857315063
  - 0.41337066888809204
  - 0.4076288044452667
loss_records_fold1:
  train_losses:
  - 4.726110750436783
  - 2.408750292658806
  - 1.7170880436897278
  - 2.7168065667152406
  - 1.8937792241573335
  - 1.9049984186887743
  - 1.7743360519409181
  - 2.3926072239875795
  - 4.479103368520737
  - 2.6141027688980105
  - 1.6760732352733614
  - 1.74673507809639
  - 1.6695185363292695
  - 2.828402578830719
  - 1.6018809735774995
  - 1.9203375160694123
  - 3.776868587732315
  - 1.978271383047104
  - 2.2147743582725528
  - 1.6929006457328797
  - 1.7153961122035981
  - 1.5347584187984467
  - 1.5252376675605774
  validation_losses:
  - 0.5180696845054626
  - 0.49437618255615234
  - 0.477666437625885
  - 0.6029195189476013
  - 0.4562806189060211
  - 0.4159705638885498
  - 1.3833032846450806
  - 0.39935988187789917
  - 0.47379356622695923
  - 0.46268001198768616
  - 0.44100049138069153
  - 0.5082224607467651
  - 0.5983773469924927
  - 0.44209057092666626
  - 0.398542195558548
  - 0.4628773331642151
  - 0.5155863761901855
  - 0.45128118991851807
  - 0.4035068452358246
  - 0.4054186940193176
  - 0.4034990966320038
  - 0.39785078167915344
  - 0.4042372703552246
loss_records_fold2:
  train_losses:
  - 1.5541441321372986
  - 2.404558235406876
  - 2.239062058925629
  - 1.6187933087348938
  - 1.6918521106243134
  - 1.7750236034393312
  - 2.42071715593338
  - 1.5169258177280427
  - 1.644728744029999
  - 1.8500834465026856
  - 1.7506791770458223
  - 1.8730772584676743
  - 2.2566273093223574
  - 1.6175449311733248
  - 1.5999732464551926
  - 1.6899360060691835
  - 1.5785950779914857
  - 1.9814471721649172
  - 2.1625020861625672
  - 2.7229216516017916
  - 2.7768036961555485
  - 1.6613556623458863
  - 6.533486932516098
  - 2.627215600013733
  - 3.0033425033092502
  - 2.8940276443958286
  - 2.253853887319565
  - 1.5507521986961366
  - 2.008750927448273
  - 1.8148795127868653
  - 1.5337263107299806
  - 1.5289160549640657
  - 1.4922767877578735
  - 1.4931937336921692
  - 1.6071872651576997
  - 1.6909258902072908
  - 1.495822834968567
  - 1.4622919201850892
  - 1.503966784477234
  - 1.6654668927192688
  - 1.6821446239948274
  - 1.4979873418807985
  - 1.5115338504314424
  - 1.488043400645256
  - 1.504401034116745
  - 1.4811943531036378
  - 1.4294686794281006
  validation_losses:
  - 0.39654025435447693
  - 0.3934445083141327
  - 0.39796727895736694
  - 0.42694559693336487
  - 0.3843356668949127
  - 0.3902709484100342
  - 0.378574401140213
  - 0.3789365291595459
  - 0.37707433104515076
  - 0.38339316844940186
  - 0.4008277952671051
  - 0.8827539086341858
  - 0.3932262063026428
  - 0.37829115986824036
  - 0.3788430392742157
  - 0.3819813132286072
  - 0.403470516204834
  - 0.3783377707004547
  - 0.3786587119102478
  - 0.52475905418396
  - 0.4055349826812744
  - 0.40875759720802307
  - 0.39103832840919495
  - 1.1809587478637695
  - 0.38002505898475647
  - 0.37634366750717163
  - 0.37467217445373535
  - 0.3821381628513336
  - 0.3945099413394928
  - 0.44100624322891235
  - 0.4514414668083191
  - 0.3974946141242981
  - 0.4203248620033264
  - 0.3793987035751343
  - 0.38730883598327637
  - 0.4238049387931824
  - 0.4291616976261139
  - 0.4315226972103119
  - 0.37555474042892456
  - 0.43696361780166626
  - 0.48097744584083557
  - 0.4116324782371521
  - 0.4021592438220978
  - 0.37688592076301575
  - 0.37716272473335266
  - 0.3805823028087616
  - 0.3747796416282654
loss_records_fold3:
  train_losses:
  - 1.4493639916181564
  - 1.5176184713840486
  - 2.288348764181137
  - 1.5030497193336487
  - 2.1733649373054504
  - 2.753698021173477
  - 1.525472927093506
  - 1.896891102194786
  - 1.5888496667146683
  - 1.9098061800003052
  - 1.5095701634883882
  - 1.834394520521164
  - 1.9218190491199494
  - 2.0149756610393523
  - 1.7046732485294342
  - 1.5153572022914887
  - 1.4892122507095338
  - 1.436862224340439
  - 1.497742396593094
  - 1.996159064769745
  - 1.7157299995422364
  - 1.7140233278274537
  - 1.5192685574293137
  - 1.7091383934020996
  - 1.5665902495384216
  - 1.5204676687717438
  - 1.478732505440712
  - 1.5181543111801148
  - 1.4610906779766084
  - 1.6372857987880707
  - 1.4962150275707247
  - 1.490150809288025
  - 1.4417264640331269
  - 1.4973155736923218
  - 1.4279409527778626
  - 1.4412716150283815
  - 1.5959083855152132
  - 1.434177652001381
  - 1.44676274061203
  - 1.5794538855552673
  - 1.543625384569168
  validation_losses:
  - 0.38501015305519104
  - 0.6021662354469299
  - 0.3915066123008728
  - 0.3846591114997864
  - 0.44090500473976135
  - 0.3929460346698761
  - 0.3742750287055969
  - 0.38803526759147644
  - 0.3768444359302521
  - 0.3909991681575775
  - 0.43669381737709045
  - 0.3814953863620758
  - 0.3909892439842224
  - 0.3756503462791443
  - 0.3985781669616699
  - 0.3813613951206207
  - 0.37294691801071167
  - 0.3931707441806793
  - 0.3843640089035034
  - 0.3782118558883667
  - 0.4486910104751587
  - 0.43971121311187744
  - 0.3869993984699249
  - 0.41030314564704895
  - 0.3927668631076813
  - 0.5751546025276184
  - 0.3870369493961334
  - 0.4657556116580963
  - 0.5807914733886719
  - 0.3773382306098938
  - 0.37215468287467957
  - 0.3725299835205078
  - 0.3777162432670593
  - 0.3695816397666931
  - 0.4635797142982483
  - 0.392106831073761
  - 0.3765797019004822
  - 0.38441184163093567
  - 0.3718981444835663
  - 0.37762144207954407
  - 0.37592971324920654
loss_records_fold4:
  train_losses:
  - 1.413042199611664
  - 1.4870589315891267
  - 1.4388330638408662
  - 1.5790692389011385
  - 1.5206483900547028
  - 1.4995185732841492
  - 1.441686499118805
  - 1.468727993965149
  - 1.4430069446563722
  - 1.5037989675998689
  - 1.4918033242225648
  - 1.419850143790245
  - 1.446364164352417
  - 1.4163814544677735
  - 1.4950106203556062
  - 1.5609357416629792
  - 1.4719728529453278
  - 1.4505830824375154
  - 1.456235581636429
  - 1.4792162001132967
  - 1.4578060746192933
  - 1.4420459210872651
  - 1.4141722291707994
  - 1.5048596560955048
  - 1.4231639415025712
  - 1.4485583066940309
  - 1.5080667316913605
  - 1.5691539108753205
  - 1.6420951962471009
  - 1.539262080192566
  - 1.4905402719974519
  - 1.4484434127807617
  - 1.4532488346099854
  - 1.4296415209770204
  - 1.4356704473495485
  - 1.4404750883579256
  - 1.4611784636974336
  - 1.5829577863216402
  - 1.4981479793787003
  - 1.4613358855247498
  - 1.4559985220432283
  - 1.4377034425735475
  - 1.3995875239372255
  - 1.4368709564208986
  - 1.429293090105057
  - 1.4054971933364868
  - 1.4424512922763826
  - 1.4721978068351746
  - 1.4681075036525728
  - 1.4855042040348054
  - 1.5076909333467485
  - 1.4484881937503815
  - 1.418831092119217
  - 1.6077402353286745
  - 1.5177592515945435
  - 1.482980901002884
  - 1.560351848602295
  - 1.6439859688282015
  - 1.4652484327554705
  - 1.5199360728263855
  - 1.4418281853199006
  - 1.4301973044872285
  - 1.407254683971405
  - 1.4434710800647736
  - 1.4298146724700929
  - 1.4310954868793488
  - 1.4512797176837922
  - 1.421426147222519
  - 1.4815733909606934
  - 1.4448347330093385
  - 1.4729592472314836
  - 1.5533933997154237
  - 1.4428902685642244
  - 1.4767834186553956
  - 1.4980327785015106
  - 1.6687057077884675
  - 1.5613242983818054
  - 1.4714448213577271
  - 1.6423335433006288
  - 1.5245085448026658
  - 1.410759621858597
  - 1.4678282737731934
  - 1.4321722745895387
  - 1.4635988712310792
  - 1.551411283016205
  - 1.5858224153518679
  - 1.5054747849702836
  - 1.4988612473011018
  - 1.5678664922714234
  - 1.5967084228992463
  - 1.4935791850090028
  - 1.4816537320613863
  - 1.3987157583236696
  - 1.4342270255088807
  - 1.4148003935813904
  - 1.4196010649204256
  - 1.4462636888027192
  validation_losses:
  - 0.38802072405815125
  - 0.387234628200531
  - 0.3801487684249878
  - 0.3868325352668762
  - 0.4139939844608307
  - 0.388762503862381
  - 0.4358123242855072
  - 0.4213073253631592
  - 0.5108226537704468
  - 0.3682822585105896
  - 0.36904874444007874
  - 0.38839685916900635
  - 0.3766278624534607
  - 0.372829407453537
  - 0.38068634271621704
  - 0.4266885221004486
  - 0.8961901068687439
  - 0.40303289890289307
  - 0.3760664165019989
  - 0.43018245697021484
  - 0.3675667345523834
  - 0.5839764475822449
  - 0.45566320419311523
  - 0.7297664284706116
  - 0.42221105098724365
  - 0.41533371806144714
  - 0.4433254599571228
  - 0.4516294300556183
  - 0.38129833340644836
  - 0.3978065848350525
  - 0.37305551767349243
  - 0.38926756381988525
  - 0.37200623750686646
  - 0.36979034543037415
  - 0.36711522936820984
  - 0.37049269676208496
  - 0.4121980667114258
  - 0.3990418612957001
  - 0.3731795847415924
  - 0.36817097663879395
  - 0.3749721944332123
  - 0.3690202236175537
  - 0.39692288637161255
  - 0.38305068016052246
  - 0.3740571141242981
  - 0.3777897357940674
  - 0.40902766585350037
  - 0.3718356788158417
  - 0.43749716877937317
  - 0.9168906807899475
  - 0.3806278109550476
  - 0.3799964487552643
  - 0.3770211935043335
  - 0.4343373775482178
  - 0.38277438282966614
  - 0.3871293365955353
  - 0.4206852614879608
  - 0.3786090016365051
  - 0.4196169674396515
  - 0.3747425973415375
  - 0.4077797532081604
  - 0.3693518340587616
  - 0.3692837655544281
  - 0.4153425097465515
  - 0.4027891457080841
  - 0.38958385586738586
  - 0.36946362257003784
  - 0.36964699625968933
  - 0.38062721490859985
  - 0.4280005693435669
  - 0.5274704694747925
  - 0.37833186984062195
  - 0.4233562648296356
  - 0.3921963572502136
  - 0.41123124957084656
  - 0.36858025193214417
  - 0.3821116089820862
  - 0.37941867113113403
  - 0.3721450865268707
  - 0.4842926859855652
  - 0.3943595290184021
  - 0.3692997992038727
  - 0.46692797541618347
  - 0.3748018443584442
  - 0.36874693632125854
  - 0.3905194103717804
  - 0.3717634677886963
  - 0.41697293519973755
  - 0.38986915349960327
  - 0.3660995662212372
  - 0.3833889961242676
  - 0.36759981513023376
  - 0.3686310350894928
  - 0.37532779574394226
  - 0.3678361773490906
  - 0.36198559403419495
  - 0.36751025915145874
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 47 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 41 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 97 epochs
training_metrics:
  fold_eval_accs: '[0.8490566037735849, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8608247422680413]'
  fold_eval_f1: '[0.022222222222222223, 0.0, 0.0, 0.0, 0.024096385542168676]'
  mean_eval_accuracy: 0.8572421354175876
  mean_f1_accuracy: 0.00926372155287818
  total_train_time: '0:19:35.208853'
