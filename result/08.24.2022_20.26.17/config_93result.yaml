config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:33:23.364299'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_93fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_93fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_93fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_93fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_93fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_93fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_93fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_93fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_93fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_93fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 34.944087660312654
  - 18.70999550819397
  - 8.320294433832169
  - 16.326634618639947
  - 22.157901006937028
  - 7.900722020864487
  - 6.811037307977677
  - 7.037494885921479
  - 12.846504706144334
  - 9.841484391689301
  - 7.514424830675125
  - 8.526405677199364
  - 5.606035655736924
  - 7.896809136867524
  - 10.99505478143692
  - 11.826490944623949
  - 10.238001227378845
  - 8.748624843358995
  - 14.479875981807709
  - 5.03462341427803
  - 5.25471488237381
  - 3.557186418771744
  - 4.956799033284188
  - 5.717369878292084
  - 8.175535178184509
  - 3.757151958346367
  - 4.7379749655723575
  - 4.625931748747826
  - 6.2833574473857885
  - 3.7159548103809357
  - 3.807391941547394
  - 3.722799384593964
  - 6.160558235645294
  - 4.703336173295975
  - 4.478950917720795
  - 3.414328247308731
  - 5.539096760749818
  - 3.6073386609554294
  - 3.2861197471618655
  validation_losses:
  - 8.447729110717773
  - 0.8448055982589722
  - 0.689232349395752
  - 2.106659173965454
  - 0.7606465220451355
  - 0.6329646110534668
  - 0.44073787331581116
  - 0.6300568580627441
  - 0.5570514798164368
  - 0.41744688153266907
  - 0.45881766080856323
  - 0.5466208457946777
  - 0.4378891885280609
  - 0.5132710337638855
  - 0.5755672454833984
  - 0.41051337122917175
  - 0.3968374729156494
  - 8.62177848815918
  - 1.2474201917648315
  - 0.47426438331604004
  - 0.3991892337799072
  - 0.4161897301673889
  - 0.5120974183082581
  - 0.4007363021373749
  - 0.38947999477386475
  - 0.43858057260513306
  - 0.4266239106655121
  - 0.4498603045940399
  - 0.4396994709968567
  - 0.400340735912323
  - 0.39321377873420715
  - 0.4085071384906769
  - 0.4446417987346649
  - 0.40748131275177
  - 0.40740880370140076
  - 0.41154828667640686
  - 0.4157220423221588
  - 0.3928067982196808
  - 0.402557909488678
loss_records_fold1:
  train_losses:
  - 3.9074484884738925
  - 3.37931205034256
  - 4.45577164888382
  - 3.8077646732330326
  - 3.515428102016449
  - 3.935671103000641
  - 3.6324827015399936
  - 3.971480578184128
  - 3.4544972836971284
  - 3.1559574961662293
  - 6.929524475336075
  - 5.521913677453995
  - 3.4012772858142855
  - 3.583452954888344
  - 5.320545089244843
  - 3.7244368582963947
  - 3.4617444694042208
  - 3.8577102899551394
  - 3.5513086318969727
  - 3.5916434645652773
  - 3.3773078680038453
  - 3.2820010840892793
  - 3.3167830109596252
  - 3.4390833973884583
  - 3.3735593736171725
  - 3.4995814621448518
  - 3.3300880789756775
  - 3.1318814694881443
  - 3.3939200758934023
  - 3.2882138550281526
  - 3.3649993240833282
  - 3.1610148787498478
  - 3.361768874526024
  - 3.2469042479991916
  - 3.1401928007602695
  - 3.1013252943754197
  - 3.097106382250786
  - 3.1502522945404055
  - 3.175545978546143
  - 3.303493314981461
  - 3.163721713423729
  - 3.388306343555451
  - 3.41688933968544
  - 3.4011780619621277
  - 3.246919083595276
  - 3.234463852643967
  - 3.280857759714127
  - 3.363696569204331
  - 3.236906385421753
  - 3.1766253083944322
  - 3.2564422428607944
  - 3.112404590845108
  - 3.230546921491623
  - 3.4301100909709934
  - 3.271520358324051
  - 3.195339035987854
  - 3.209792983531952
  - 3.1962888062000276
  - 3.18040024638176
  - 3.2204316794872287
  - 3.166962453722954
  - 3.1908228754997254
  - 3.1388062596321107
  - 3.1898281544446947
  - 3.273162788152695
  validation_losses:
  - 0.423048198223114
  - 0.4298185408115387
  - 0.45983752608299255
  - 0.6254817247390747
  - 0.4256272614002228
  - 0.5454478859901428
  - 0.4152253270149231
  - 0.4912472665309906
  - 0.4017848074436188
  - 0.40383678674697876
  - 0.5715551972389221
  - 0.425999253988266
  - 0.41817614436149597
  - 0.43115490674972534
  - 0.5417795181274414
  - 0.4538037180900574
  - 0.40998655557632446
  - 0.41335245966911316
  - 0.40968945622444153
  - 0.4304061532020569
  - 0.47163158655166626
  - 0.41325637698173523
  - 0.41309499740600586
  - 0.4468157887458801
  - 0.4558025896549225
  - 0.4099816679954529
  - 0.4146042764186859
  - 0.41829201579093933
  - 0.410895437002182
  - 0.4322083294391632
  - 0.4382033050060272
  - 0.4159010648727417
  - 0.458474338054657
  - 0.4319755733013153
  - 0.41588321328163147
  - 0.4673706889152527
  - 0.4145421087741852
  - 0.4386788606643677
  - 1.9480937719345093
  - 0.44004809856414795
  - 0.42909491062164307
  - 0.42087405920028687
  - 0.41845008730888367
  - 0.4149647057056427
  - 0.442333459854126
  - 0.42719706892967224
  - 0.45421579480171204
  - 0.41767311096191406
  - 0.41284966468811035
  - 0.5339735746383667
  - 0.414974570274353
  - 0.43606606125831604
  - 0.491816908121109
  - 0.42337262630462646
  - 0.41464951634407043
  - 0.44909030199050903
  - 0.43088671565055847
  - 0.4294970631599426
  - 0.46922969818115234
  - 0.44273868203163147
  - 0.41579851508140564
  - 0.4199199378490448
  - 0.42719772458076477
  - 0.4190674424171448
  - 0.4168832302093506
loss_records_fold2:
  train_losses:
  - 3.4583780169487
  - 3.527733224630356
  - 3.212434083223343
  - 3.269044089317322
  - 3.2756061762571336
  - 3.5946575313806535
  - 3.28368513584137
  - 3.411275190114975
  - 3.2741998195648194
  - 3.612747257947922
  - 4.180694246292115
  - 3.246905589103699
  - 3.3541090905666353
  - 3.727265748381615
  - 3.7593482434749603
  - 3.3858360767364504
  - 3.6147002756595614
  - 3.5212898552417755
  - 3.3102203726768495
  - 3.334998619556427
  - 3.3791242599487306
  - 3.4259166538715364
  - 3.2631211638450623
  - 3.123312294483185
  - 3.134451794624329
  - 3.184190744161606
  - 3.3298985779285433
  - 3.2781127095222473
  - 3.3499554216861727
  - 3.2455623745918274
  - 3.187163317203522
  - 3.2060625493526462
  - 3.217729070782662
  - 3.233931088447571
  - 3.262398147583008
  - 3.282470536231995
  - 3.3484642148017887
  - 3.278053951263428
  - 3.147879844903946
  validation_losses:
  - 0.4028381109237671
  - 109.42269897460938
  - 5290.9453125
  - 14788.353515625
  - 20.801618576049805
  - 0.40321609377861023
  - 1.443451166152954
  - 0.47881171107292175
  - 0.38780897855758667
  - 0.3999699354171753
  - 0.395844429731369
  - 0.38980674743652344
  - 0.8511598110198975
  - 0.46946075558662415
  - 0.386804461479187
  - 0.41055819392204285
  - 0.41547897458076477
  - 0.39308321475982666
  - 0.38784733414649963
  - 0.4019683301448822
  - 0.47218796610832214
  - 0.39988771080970764
  - 0.4080078601837158
  - 0.4023442566394806
  - 0.42466866970062256
  - 0.3958349823951721
  - 0.41657114028930664
  - 0.4503015875816345
  - 0.3878539502620697
  - 0.41623350977897644
  - 0.4023699462413788
  - 0.39608198404312134
  - 0.42894238233566284
  - 0.41699641942977905
  - 0.40545111894607544
  - 0.399931937456131
  - 0.39530858397483826
  - 0.3981911540031433
  - 0.3943275511264801
loss_records_fold3:
  train_losses:
  - 3.1907141506671906
  - 3.173195254802704
  - 3.1218453586101536
  - 3.2958180487155917
  - 3.2228617131710053
  - 3.184704595804215
  - 3.2017001807689667
  - 3.223152405023575
  - 3.2428385198116305
  - 3.2570227771997455
  - 3.247323101758957
  - 3.2471643447875977
  - 3.2126163959503176
  - 3.084844541549683
  - 3.1323862910270694
  - 3.1952370762825013
  - 3.2899977743625644
  - 3.311803475022316
  - 3.2500642389059067
  - 3.3547274053096774
  - 3.215502715110779
  - 3.1301314115524295
  - 3.2484835863113406
  - 3.255763164162636
  - 4.17720091342926
  - 4.577281177043915
  - 3.4942547321319584
  - 3.6734019339084627
  - 3.3387421011924747
  - 3.336604291200638
  - 3.2670350193977358
  - 3.813738465309143
  - 3.253827965259552
  - 3.2988147258758547
  - 3.2310630768537525
  - 3.182120227813721
  - 3.181089699268341
  - 3.416591063141823
  - 3.152627855539322
  - 3.216095501184464
  - 3.3375444531440737
  - 3.1674537092447284
  - 3.199730595946312
  - 3.1040231466293338
  - 3.2278331130743028
  - 3.2459838807582857
  - 3.2788888931274416
  - 3.2758935749530793
  - 3.4282364368438722
  - 3.2465888500213627
  - 3.2507220566272736
  - 3.2365167975425724
  - 3.133939164876938
  - 3.2456141114234924
  - 3.1668634057044986
  - 3.231660443544388
  - 3.159896120429039
  - 3.1453479468822483
  - 3.142557156085968
  - 3.16986637711525
  - 3.9159302711486816
  - 3.4558400332927706
  - 3.3162819981575016
  - 3.2733970224857334
  - 3.26219579577446
  - 3.3057383775711062
  - 3.218357610702515
  - 3.256333583593369
  - 3.215789330005646
  - 4.674272179603577
  validation_losses:
  - 0.4046444892883301
  - 0.40302443504333496
  - 0.46484020352363586
  - 0.4133923351764679
  - 0.4025340974330902
  - 0.3975429832935333
  - 0.4169173836708069
  - 0.40167102217674255
  - 0.4384844899177551
  - 0.39814049005508423
  - 0.4329582154750824
  - 0.41110464930534363
  - 0.41929566860198975
  - 0.4282089173793793
  - 0.4200955033302307
  - 0.410632461309433
  - 0.43310439586639404
  - 0.42100459337234497
  - 0.4379745125770569
  - 0.4077478349208832
  - 0.4295133054256439
  - 0.4054088592529297
  - 3.275318145751953
  - 0.4860340654850006
  - 0.8013266921043396
  - 0.42508143186569214
  - 0.41868358850479126
  - 0.4096219539642334
  - 0.4401400089263916
  - 0.39927372336387634
  - 23.58799171447754
  - 1.378951907157898
  - 6745.12060546875
  - 1443690.75
  - 713339.9375
  - 65531.30078125
  - 30313.880859375
  - 45828.66015625
  - 12311.66796875
  - 5023.193359375
  - 17182.3828125
  - 38119.078125
  - 15176.357421875
  - 27702.515625
  - 5173.09130859375
  - 22427.611328125
  - 21248.689453125
  - 35664.515625
  - 582.02587890625
  - 0.46983659267425537
  - 31.93531036376953
  - 53.86494445800781
  - 837.0083618164062
  - 6320.5712890625
  - 3453.519287109375
  - 4728.6708984375
  - 19281.53125
  - 21953.01953125
  - 13464.0576171875
  - 248.80126953125
  - 0.4112979471683502
  - 261214.09375
  - 5585750.5
  - 98761536.0
  - 19177648.0
  - 613411.0625
  - 27761.23828125
  - 1783.02587890625
  - 1760.1522216796875
  - 0.46814438700675964
loss_records_fold4:
  train_losses:
  - 3.8743935972452164
  - 4.337262591719628
  - 3.4018432259559632
  - 3.5768017381429673
  - 5.3811178565025335
  - 4.50070887207985
  - 3.275792914628983
  - 3.463193112611771
  - 3.272032630443573
  - 3.209005850553513
  - 3.1999255776405335
  - 3.3047857016325
  - 3.4630647599697113
  - 3.423362755775452
  - 3.6712197422981263
  - 3.4542554318904877
  - 3.28646537065506
  - 3.1725487232208254
  - 3.1995539307594303
  - 3.178723651170731
  - 3.158064252138138
  - 3.2332742869853974
  - 3.205349969863892
  - 4.672963201999664
  - 3.3212337255477906
  - 3.159440964460373
  - 3.2517849266529084
  - 3.179729121923447
  - 3.258835768699646
  - 3.279809567332268
  - 4.489510822296142
  - 6.061222323775292
  - 5.932578676939011
  - 4.385435378551484
  - 5.41268100142479
  - 4.845827305316925
  - 3.443915677070618
  - 4.058611708879471
  - 3.543353056907654
  - 3.6491258919239047
  - 3.229570370912552
  - 3.360604786872864
  - 5.006460356712342
  - 3.4787282764911653
  - 3.3234503269195557
  - 3.1680774599313737
  - 3.142478394508362
  - 3.417426633834839
  - 3.231507360935211
  - 3.237492519617081
  - 3.369100993871689
  - 3.2391451537609104
  validation_losses:
  - 0.44566014409065247
  - 0.4374135434627533
  - 0.4517309367656708
  - 0.4212783873081207
  - 0.43784797191619873
  - 0.4130145311355591
  - 0.3993450403213501
  - 0.40912511944770813
  - 0.40108656883239746
  - 0.4088524281978607
  - 9.266327857971191
  - 0.41212981939315796
  - 0.4133428931236267
  - 0.4071449041366577
  - 0.3983759880065918
  - 0.4348292648792267
  - 0.40360602736473083
  - 0.40125423669815063
  - 0.4015001952648163
  - 5.705111980438232
  - 7.3629255294799805
  - 0.423117458820343
  - 88.04607391357422
  - 0.41783300042152405
  - 0.4031829237937927
  - 0.40038779377937317
  - 4.862584590911865
  - 197.5105438232422
  - 393.95941162109375
  - 492.29376220703125
  - 5043.33203125
  - 0.7795054912567139
  - 1.6649149656295776
  - 0.6196794509887695
  - 0.5283298492431641
  - 0.47658905386924744
  - 0.4880172312259674
  - 0.48180675506591797
  - 0.46140071749687195
  - 0.4409647583961487
  - 0.4034733772277832
  - 0.43683791160583496
  - 0.4390852451324463
  - 0.4021662473678589
  - 0.4033443331718445
  - 0.42936837673187256
  - 0.41557782888412476
  - 0.4154531955718994
  - 0.4220159351825714
  - 0.400656133890152
  - 0.4061027765274048
  - 0.4063992500305176
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 65 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 70 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 52 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:24:38.494423'
