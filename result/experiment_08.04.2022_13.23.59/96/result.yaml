config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 00:16:35.935636'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/96/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/96/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/96/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/96/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/96/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/96/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/96/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/96/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/96/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/96/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 28.00957590341568
  - 28.056340664625168
  - 27.978295281529427
  - 28.16399545967579
  - 28.605769887566566
  - 27.943103045225143
  - 28.383514255285263
  - 28.17479257285595
  - 28.279288426041603
  - 27.716631025075912
  - 27.924388125538826
  - 27.66701665520668
  - 27.649103477597237
  - 27.579514995217323
  - 27.588610261678696
  - 28.26618355512619
  - 28.034030817449093
  - 27.84171923995018
  - 27.625207111239433
  - 28.047415018081665
  - 27.343090564012527
  - 27.58837702870369
  - 27.433066800236702
  - 27.573201537132263
  - 27.553283169865608
  - 27.64220567047596
  - 27.38525128364563
  - 27.41154534369707
  - 27.694585248827934
  - 27.54233257472515
  - 27.40089812874794
  - 27.88784059882164
  - 27.489391028881073
  - 27.54164530336857
  - 27.991446658968925
  - 27.90529753267765
  - 27.740172505378723
  - 27.821724355220795
  - 27.71371552348137
  - 27.366085305809975
  - 27.610215812921524
  - 28.27322019636631
  - 28.048609778285027
  - 27.73804458230734
  - 27.975867241621017
  - 28.245585665106773
  - 28.063743203878403
  - 27.307393014431
  - 27.982620388269424
  - 27.73959320783615
  - 27.32426466047764
  - 27.980672597885132
  - 28.62735216319561
  - 27.64788718521595
  - 27.557551845908165
  - 27.638663426041603
  - 27.65185710787773
  - 27.696299374103546
  - 27.578218922019005
  - 27.64575830101967
  - 27.25701481103897
  - 27.534006521105766
  - 27.35077641904354
  - 27.783001586794853
  - 27.55232010781765
  - 27.45599241554737
  - 27.531666040420532
  - 27.219573453068733
  - 27.16974426805973
  - 27.7830860465765
  - 27.111872732639313
  - 27.76665686070919
  - 27.393874257802963
  - 27.693424955010414
  - 27.320345103740692
  - 27.149771600961685
  - 27.14251485466957
  - 27.203008860349655
  - 27.433005526661873
  - 27.32934232056141
  - 27.299841955304146
  - 27.46616420149803
  - 27.5289558917284
  - 27.323550418019295
  - 27.40006497502327
  - 27.458655565977097
  - 27.320466443896294
  - 27.493106096982956
  - 27.515524834394455
  - 26.88166879117489
  - 26.771536722779274
  - 26.846524879336357
  - 27.28935295343399
  - 27.549538493156433
  - 27.601168990135193
  - 27.164438232779503
  - 27.5139599442482
  - 27.221732139587402
  - 27.42872481048107
  - 27.238451436161995
  validation_losses:
  - 0.3859423100948334
  - 0.4883134663105011
  - 0.39551299810409546
  - 0.38944175839424133
  - 0.38871249556541443
  - 0.3999072015285492
  - 0.392180472612381
  - 0.39081230759620667
  - 0.3883291780948639
  - 0.4001193940639496
  - 0.39758285880088806
  - 0.46230772137641907
  - 0.38534027338027954
  - 0.4127825200557709
  - 1.0490385293960571
  - 0.3837743401527405
  - 0.3864322602748871
  - 0.4246709942817688
  - 0.3874649405479431
  - 0.38301733136177063
  - 0.4742085337638855
  - 0.4737507104873657
  - 0.41395652294158936
  - 0.4729841649532318
  - 0.5023151636123657
  - 0.5758514404296875
  - 0.43719154596328735
  - 0.4600559175014496
  - 0.5798476338386536
  - 0.4711970090866089
  - 0.40477851033210754
  - 0.8374456167221069
  - 0.8587548732757568
  - 0.48799455165863037
  - 0.3837284445762634
  - 0.49981826543807983
  - 0.4548710584640503
  - 0.43550387024879456
  - 0.9828049540519714
  - 0.5294709205627441
  - 0.4233766496181488
  - 0.389956533908844
  - 0.40788304805755615
  - 0.4306320548057556
  - 0.391727477312088
  - 0.41329002380371094
  - 0.43193483352661133
  - 0.42966189980506897
  - 0.42113950848579407
  - 0.4738277196884155
  - 0.473859578371048
  - 0.38471683859825134
  - 0.4129626154899597
  - 0.40818995237350464
  - 0.43488606810569763
  - 0.41437599062919617
  - 0.43324795365333557
  - 0.43667882680892944
  - 0.44377511739730835
  - 0.4492659866809845
  - 0.5237993001937866
  - 0.40636783838272095
  - 0.42111095786094666
  - 0.4926251173019409
  - 0.6270466446876526
  - 0.6783897876739502
  - 0.47023239731788635
  - 0.4890422224998474
  - 0.5501179099082947
  - 0.621853232383728
  - 0.6367610692977905
  - 0.4842178225517273
  - 0.6348274350166321
  - 0.7860859036445618
  - 0.6405606865882874
  - 0.48362866044044495
  - 0.7004748582839966
  - 0.5408283472061157
  - 0.5739754438400269
  - 0.4924694001674652
  - 0.5001282095909119
  - 0.40977969765663147
  - 0.46639278531074524
  - 0.489200234413147
  - 0.486588716506958
  - 0.4766964614391327
  - 0.5182689428329468
  - 0.5062127113342285
  - 0.6254742741584778
  - 0.5073492527008057
  - 0.6135876774787903
  - 0.6432713270187378
  - 0.6504401564598083
  - 0.4168033003807068
  - 0.6075476408004761
  - 0.49225959181785583
  - 0.5438142418861389
  - 0.4579867720603943
  - 0.560386598110199
  - 0.4617781937122345
loss_records_fold2:
  train_losses:
  - 27.391255363821983
  - 27.156120374798775
  - 27.07979892194271
  - 26.965593457221985
  - 27.81179128587246
  - 26.955212131142616
  - 27.26471382379532
  - 27.581902727484703
  - 27.42182846367359
  - 27.27505861222744
  - 27.518935471773148
  - 28.01951466500759
  - 27.21044221520424
  - 27.123818919062614
  - 27.024987690150738
  - 27.413584038615227
  - 27.27275910973549
  - 26.994738429784775
  - 27.90200550854206
  - 27.282070830464363
  - 27.41563531756401
  - 27.0776856392622
  - 27.49476060271263
  - 27.562427178025246
  - 27.109336346387863
  - 26.63940140604973
  - 27.103896737098694
  - 27.439777612686157
  - 27.123143956065178
  - 26.702540330588818
  - 27.451601341366768
  - 27.410271510481834
  - 27.38789190351963
  - 27.7910308688879
  - 27.60895825922489
  - 27.454277768731117
  - 26.804159715771675
  - 27.334118008613586
  - 27.6117662191391
  - 27.216165110468864
  - 26.92320980131626
  - 27.004221200942993
  - 26.918466687202454
  - 27.207904905080795
  - 27.056091994047165
  - 27.009582549333572
  - 27.15789583325386
  - 27.058280512690544
  - 27.422474920749664
  - 27.035363227128983
  - 27.102123513817787
  - 27.11582562327385
  - 27.269392102956772
  - 27.099641904234886
  - 27.17331077158451
  - 27.522640615701675
  - 27.64777436852455
  - 27.647353172302246
  - 27.84902849793434
  - 26.8116288036108
  - 27.174229338765144
  - 26.97876589000225
  - 26.79358507692814
  - 27.321821451187134
  - 27.004017755389214
  - 27.064144626259804
  - 27.71496742218733
  - 27.258143559098244
  - 26.958376869559288
  - 27.435041524469852
  - 27.184274286031723
  - 27.056449070572853
  - 27.12953731417656
  - 26.700180500745773
  - 27.258113376796246
  - 26.61539851129055
  - 27.383508920669556
  - 26.80121909081936
  - 27.2702509611845
  - 27.057381123304367
  - 27.02206064760685
  - 27.4897141456604
  - 26.793233528733253
  - 27.494071781635284
  - 27.02593784034252
  - 26.94073837995529
  - 27.12436829507351
  - 27.110121339559555
  - 26.963319405913353
  - 26.903054028749466
  - 27.631447464227676
  - 27.343362376093864
  - 27.05929373204708
  - 26.8951927870512
  - 27.22218357026577
  - 26.959736466407776
  - 26.588681429624557
  - 27.38066801428795
  - 26.734658643603325
  - 27.401901751756668
  validation_losses:
  - 0.4781322479248047
  - 0.38128766417503357
  - 0.38355740904808044
  - 0.5247910618782043
  - 0.5054137110710144
  - 0.4710961580276489
  - 0.43795013427734375
  - 0.39824557304382324
  - 0.39959651231765747
  - 0.4987799823284149
  - 0.38853296637535095
  - 0.4387683570384979
  - 0.4178180992603302
  - 0.47131484746932983
  - 0.5997114777565002
  - 0.45943036675453186
  - 0.4455695152282715
  - 0.48194417357444763
  - 0.4830265939235687
  - 0.47860032320022583
  - 0.46456706523895264
  - 0.4906631112098694
  - 0.41634422540664673
  - 0.46154001355171204
  - 0.4498888850212097
  - 0.5007296204566956
  - 0.5351462960243225
  - 0.4567151963710785
  - 0.44152677059173584
  - 0.5336686968803406
  - 0.47440439462661743
  - 0.40534889698028564
  - 0.48194625973701477
  - 0.5862566232681274
  - 0.47839561104774475
  - 0.5156784653663635
  - 0.6239480376243591
  - 0.48266932368278503
  - 0.509174108505249
  - 0.5380350947380066
  - 0.7101497054100037
  - 0.7282555103302002
  - 0.5660203695297241
  - 0.41680970788002014
  - 0.5038251876831055
  - 0.4295043349266052
  - 0.5384175181388855
  - 0.5069690942764282
  - 0.4588856101036072
  - 0.5824583172798157
  - 0.47218242287635803
  - 0.46241188049316406
  - 0.46596264839172363
  - 0.5575622916221619
  - 0.43789854645729065
  - 0.43930280208587646
  - 0.4132412374019623
  - 0.40950098633766174
  - 0.42794111371040344
  - 0.6870459914207458
  - 0.4853460192680359
  - 0.4285799264907837
  - 0.4901584982872009
  - 0.459764301776886
  - 0.41122862696647644
  - 0.49229001998901367
  - 0.47227245569229126
  - 0.49936944246292114
  - 0.47143033146858215
  - 0.4125083386898041
  - 0.39557352662086487
  - 0.4215380847454071
  - 0.38569286465644836
  - 0.4223972260951996
  - 0.5096082091331482
  - 0.4728357493877411
  - 0.4635370671749115
  - 0.4593372642993927
  - 0.46561580896377563
  - 0.43564966320991516
  - 0.4504251182079315
  - 0.4295555353164673
  - 0.4078052043914795
  - 0.49470198154449463
  - 0.524253249168396
  - 0.4512518644332886
  - 0.48812469840049744
  - 0.49416857957839966
  - 0.42579004168510437
  - 0.4512227177619934
  - 0.41833335161209106
  - 0.4039284884929657
  - 0.4947957992553711
  - 0.4757562279701233
  - 0.405910849571228
  - 0.4378279447555542
  - 0.4699593484401703
  - 0.9233505725860596
  - 0.620904266834259
  - 0.6860863566398621
loss_records_fold3:
  train_losses:
  - 27.4660934060812
  - 27.632580369710922
  - 27.83991800248623
  - 27.57665827870369
  - 27.183583989739418
  - 27.062464848160744
  - 27.098629489541054
  - 27.13657769560814
  - 27.322948798537254
  - 27.08791470527649
  - 27.095748275518417
  - 27.30448743700981
  - 27.010043740272522
  - 27.308983370661736
  - 27.256332859396935
  - 27.737187638878822
  - 27.413828134536743
  - 26.908472269773483
  - 27.36439999938011
  - 27.80036886036396
  - 27.075438484549522
  - 27.794500783085823
  - 27.3934358805418
  - 27.147948771715164
  - 27.59293732047081
  - 27.226508498191833
  - 27.700214073061943
  - 27.025820940732956
  - 27.429760217666626
  - 27.634623289108276
  - 27.616998732089996
  - 27.836297869682312
  - 27.252279102802277
  - 27.37228935956955
  - 27.085142269730568
  - 27.318384021520615
  - 27.460737198591232
  - 27.214252039790154
  - 27.153570383787155
  - 27.651786029338837
  - 27.25916700065136
  - 27.540650323033333
  - 27.33381812274456
  - 27.248545587062836
  - 27.193524330854416
  - 27.599641263484955
  - 27.135344587266445
  - 27.303345501422882
  - 27.816907554864883
  - 27.395345509052277
  - 26.95546880364418
  - 27.160778522491455
  - 27.19691763818264
  - 27.66199654340744
  - 27.185888841748238
  - 27.1929033100605
  - 26.886850118637085
  - 27.29738900065422
  - 27.72709919512272
  - 27.168605759739876
  - 27.153733044862747
  - 27.113188818097115
  - 27.458140715956688
  - 27.246881902217865
  - 26.981936007738113
  - 27.19125084578991
  - 27.01840677857399
  - 27.433565765619278
  - 27.467601791024208
  - 27.30609054863453
  - 27.300172366201878
  - 27.13307872414589
  - 27.34861223399639
  - 27.341555386781693
  - 27.246394619345665
  - 27.227823682129383
  - 27.31267848610878
  - 27.550864458084106
  - 27.316045492887497
  - 26.87434820830822
  - 27.315430775284767
  - 27.12781611084938
  - 27.25827780365944
  - 27.138403609395027
  - 27.910285882651806
  - 27.126091241836548
  - 27.289276868104935
  - 27.134770080447197
  - 27.39456307888031
  - 26.929474025964737
  - 27.297283828258514
  - 27.20487979054451
  - 27.138821840286255
  - 27.25921329855919
  - 27.12332531809807
  - 27.11423671245575
  - 27.301465034484863
  - 27.13304267823696
  - 27.31292013823986
  - 26.831953898072243
  validation_losses:
  - 1.4754524230957031
  - 0.6213767528533936
  - 0.881923496723175
  - 1.0329447984695435
  - 0.5351126790046692
  - 0.7213462591171265
  - 0.6818679571151733
  - 0.6696871519088745
  - 0.4323340952396393
  - 0.836016833782196
  - 0.6457975506782532
  - 0.5713925361633301
  - 0.4275059401988983
  - 0.473337858915329
  - 0.5144954323768616
  - 0.4960678517818451
  - 0.46319466829299927
  - 0.5016339421272278
  - 1.275356650352478
  - 0.5676019787788391
  - 0.426969975233078
  - 0.3861009478569031
  - 0.5058332085609436
  - 0.4650992751121521
  - 0.5030542016029358
  - 0.45510053634643555
  - 0.4565555453300476
  - 0.44860735535621643
  - 0.4815162122249603
  - 0.5517550110816956
  - 0.4894121289253235
  - 0.7292346358299255
  - 0.45805472135543823
  - 0.6275922060012817
  - 0.5336577892303467
  - 0.8189994692802429
  - 0.597855269908905
  - 0.6025032997131348
  - 0.6240660548210144
  - 0.561604917049408
  - 0.49459919333457947
  - 0.4952341914176941
  - 0.5302459597587585
  - 0.5467259287834167
  - 0.4821617305278778
  - 0.5323416590690613
  - 0.5006349682807922
  - 0.47781902551651
  - 0.7683348655700684
  - 0.634179413318634
  - 0.4836743175983429
  - 0.634863018989563
  - 0.9952159523963928
  - 0.44502514600753784
  - 0.9498448371887207
  - 0.6366490721702576
  - 0.5474684834480286
  - 0.5994002223014832
  - 0.42771852016448975
  - 0.534912645816803
  - 0.5715941786766052
  - 0.4052893817424774
  - 0.42643049359321594
  - 0.4680326282978058
  - 0.6033021211624146
  - 0.5367500185966492
  - 0.5766800045967102
  - 0.47855904698371887
  - 0.48534253239631653
  - 2.0109145641326904
  - 1.6547532081604004
  - 0.9414711594581604
  - 0.954372763633728
  - 1.021836757659912
  - 1.242574691772461
  - 0.7880071401596069
  - 0.45161765813827515
  - 0.4557381570339203
  - 0.5283219814300537
  - 0.5217574834823608
  - 0.449738085269928
  - 0.4288473129272461
  - 0.7563306093215942
  - 0.5057860016822815
  - 0.43876591324806213
  - 0.6226410865783691
  - 0.5592314004898071
  - 0.5277926921844482
  - 0.4895098805427551
  - 0.5146430730819702
  - 0.5515720248222351
  - 0.4346114993095398
  - 0.7111136317253113
  - 0.6466428637504578
  - 0.6799960732460022
  - 0.5423343181610107
  - 0.5584394931793213
  - 0.5088917016983032
  - 0.5584537386894226
  - 0.4446471929550171
loss_records_fold4:
  train_losses:
  - 27.464398100972176
  - 27.05795206129551
  - 27.533780097961426
  - 27.551588907837868
  - 27.35805617272854
  - 27.252441108226776
  - 27.20206780731678
  - 27.11616311967373
  - 26.9044399112463
  - 27.03262810409069
  - 27.80284045636654
  - 27.234139174222946
  - 27.234240010380745
  - 27.124605923891068
  - 27.490113079547882
  - 27.359748661518097
  - 27.452817603945732
  - 27.631523475050926
  - 27.31875589489937
  - 27.15579031407833
  - 27.28438176214695
  - 27.30014345049858
  - 28.031051635742188
  - 27.738017067313194
  - 27.365560859441757
  - 27.21058774739504
  - 27.25169676542282
  - 27.37778927385807
  - 27.259605810046196
  - 27.13883239030838
  - 27.409730970859528
  - 27.26678629219532
  - 27.441314339637756
  - 27.587367966771126
  - 27.245850816369057
  - 27.001854494214058
  - 27.025758132338524
  - 27.050931677222252
  - 26.891668394207954
  - 27.65995167195797
  - 27.988596349954605
  - 27.126090183854103
  - 27.35625049471855
  - 27.492973819375038
  - 27.36935691535473
  - 26.99872976541519
  - 27.209773808717728
  - 26.9828424975276
  - 27.52035741508007
  - 27.23570543527603
  - 26.83262449502945
  - 26.79656471312046
  - 27.30303266644478
  - 26.848266750574112
  - 27.515676200389862
  - 27.42017801105976
  - 27.01099480688572
  - 27.40797382593155
  - 27.078139141201973
  - 27.32803437113762
  - 27.141999259591103
  - 27.298249438405037
  - 26.71354115009308
  - 27.152052626013756
  - 26.822679445147514
  - 26.84919974207878
  - 27.094775140285492
  - 26.932368114590645
  - 26.735649898648262
  - 27.140205547213554
  - 27.295911729335785
  - 27.023660749197006
  - 27.05723786354065
  - 27.049719735980034
  - 26.747793659567833
  - 26.996629998087883
  - 27.808525532484055
  - 27.332173392176628
  - 26.841181620955467
  - 27.030263796448708
  - 27.023130103945732
  - 27.172698214650154
  - 27.221441194415092
  - 26.950282350182533
  - 26.696808993816376
  - 26.767316967248917
  - 26.74652463197708
  - 27.026771917939186
  - 26.968403577804565
  - 27.143577739596367
  - 27.19525156915188
  - 27.00629858672619
  - 26.586113661527634
  - 27.157432466745377
  - 26.941262513399124
  - 27.234460175037384
  - 26.86202935129404
  - 26.95008048415184
  - 27.032740265130997
  - 27.35965885221958
  validation_losses:
  - 0.42923954129219055
  - 0.46077266335487366
  - 0.4496268630027771
  - 0.4214838147163391
  - 0.4348580837249756
  - 0.4856063425540924
  - 0.4173642694950104
  - 0.4312538206577301
  - 0.5915845632553101
  - 0.4644894003868103
  - 0.4113784432411194
  - 0.46436721086502075
  - 0.3926909863948822
  - 0.47408705949783325
  - 0.4401824176311493
  - 0.4745247960090637
  - 0.4749095141887665
  - 0.5145241022109985
  - 0.5371180772781372
  - 0.5322606563568115
  - 0.4836164712905884
  - 0.5128476023674011
  - 0.4137228727340698
  - 0.5241965055465698
  - 0.6070294380187988
  - 0.6912717223167419
  - 0.5457768440246582
  - 0.534735381603241
  - 0.5785068273544312
  - 0.466476708650589
  - 0.5479040145874023
  - 0.5544064044952393
  - 0.7456741333007812
  - 0.5184634327888489
  - 0.4224352538585663
  - 0.5129303932189941
  - 0.47992703318595886
  - 0.541416347026825
  - 0.5921673774719238
  - 0.4744888246059418
  - 0.47687262296676636
  - 0.4440072774887085
  - 0.46469661593437195
  - 0.403084397315979
  - 0.44759732484817505
  - 0.562188446521759
  - 0.6001389026641846
  - 0.5680719017982483
  - 0.52138352394104
  - 0.5384380221366882
  - 0.4992215037345886
  - 0.5058278441429138
  - 0.46944692730903625
  - 0.5484439730644226
  - 0.5427918434143066
  - 0.648631751537323
  - 0.5560664534568787
  - 0.5167219638824463
  - 0.6449332237243652
  - 0.6804217100143433
  - 0.7191879749298096
  - 0.572470486164093
  - 0.7316185832023621
  - 1.110990285873413
  - 1.3635036945343018
  - 0.6536603569984436
  - 0.6770000457763672
  - 1.6869724988937378
  - 0.9377873539924622
  - 0.5949379801750183
  - 0.48675301671028137
  - 0.5363471508026123
  - 0.5672594308853149
  - 0.5366503596305847
  - 0.4476046860218048
  - 0.4083649814128876
  - 0.43055206537246704
  - 0.7005518078804016
  - 0.48135676980018616
  - 0.6748319864273071
  - 0.5614849925041199
  - 0.6240021586418152
  - 0.6113499402999878
  - 0.38744235038757324
  - 0.5022350549697876
  - 0.5528237223625183
  - 0.6415920257568359
  - 0.6645413041114807
  - 0.7322347164154053
  - 0.5907538533210754
  - 0.6149099469184875
  - 0.6201196312904358
  - 0.7190258502960205
  - 0.786707878112793
  - 1.0021090507507324
  - 0.8458977341651917
  - 0.8282857537269592
  - 0.7409906387329102
  - 0.7430046796798706
  - 0.7251346707344055
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8456260720411664, 0.8353344768439108, 0.8473413379073756,
    0.7508591065292096]'
  fold_eval_f1: '[0.0, 0.09999999999999998, 0.07692307692307693, 0.043010752688172046,
    0.2328042328042328]'
  mean_eval_accuracy: 0.8273587852852586
  mean_f1_accuracy: 0.09054761248309635
  total_train_time: '0:11:48.103561'
