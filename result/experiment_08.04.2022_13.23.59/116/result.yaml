config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 03:16:57.580992'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/116/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/116/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/116/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/116/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/116/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/116/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/116/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/116/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/116/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/116/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 151.79529559612274
  - 54.97968631982803
  - 37.70441845059395
  - 21.21476697921753
  - 12.474773079156876
  - 12.077677100896835
  - 18.155254274606705
  - 14.95531889796257
  - 13.950552389025688
  - 10.301616936922073
  - 10.216406732797623
  - 8.731217175722122
  - 8.988155245780945
  - 12.029936045408249
  - 8.736891210079193
  - 10.044631779193878
  - 16.48302413523197
  - 10.895924165844917
  - 9.974838823080063
  - 12.991836071014404
  - 8.035540789365768
  - 14.347580164670944
  - 23.920824140310287
  - 53.4539400935173
  - 14.309606850147247
  - 10.59422191977501
  - 11.738423481583595
  - 14.6542409658432
  - 10.170935153961182
  - 9.000105381011963
  - 9.20446391403675
  - 10.671874970197678
  - 12.351050704717636
  - 16.872278541326523
  - 11.206496715545654
  - 8.00806599855423
  - 8.910597652196884
  - 11.412434756755829
  - 9.757180660963058
  - 8.434297531843185
  - 16.34563499689102
  - 8.308374881744385
  - 11.126863032579422
  - 11.1688192486763
  - 10.18025016784668
  - 13.195083677768707
  - 8.917163342237473
  - 7.615529030561447
  - 8.778454512357712
  - 8.165302991867065
  - 8.28792718052864
  - 8.001714676618576
  - 7.548587650060654
  - 7.854748785495758
  - 8.026222497224808
  - 9.210281878709793
  - 9.52165749669075
  - 8.104240387678146
  - 8.600003629922867
  - 8.52199524641037
  - 7.72363144159317
  - 8.821500837802887
  - 8.797513782978058
  - 9.666260242462158
  - 9.631773889064789
  - 8.512651771306992
  - 8.551253348588943
  - 7.380273222923279
  - 10.194310754537582
  - 8.603918612003326
  - 9.424775034189224
  - 19.802849620580673
  - 13.993095129728317
  - 13.087678581476212
  - 7.553486227989197
  - 13.614870801568031
  - 8.130174696445465
  - 7.424263149499893
  - 7.338630557060242
  - 7.679936200380325
  - 8.974698185920715
  - 9.344272762537003
  - 8.299636602401733
  - 7.78888002038002
  - 9.16458797454834
  - 7.665741324424744
  - 7.458348602056503
  - 7.437552750110626
  - 9.28769114613533
  - 7.714380264282227
  - 8.053865998983383
  - 7.754849523305893
  - 22.87976783514023
  - 9.424085527658463
  - 8.005050271749496
  - 9.44627320766449
  - 9.083489760756493
  - 7.67516627907753
  - 7.484241157770157
  - 7.36847859621048
  validation_losses:
  - 1.8678929805755615
  - 1.0114411115646362
  - 0.7573708295822144
  - 0.9251338839530945
  - 0.4515603482723236
  - 0.5058289766311646
  - 0.583724856376648
  - 0.5188121795654297
  - 0.5353390574455261
  - 0.39064109325408936
  - 0.4914604425430298
  - 0.38679444789886475
  - 0.5584967136383057
  - 0.44050779938697815
  - 0.39483317732810974
  - 0.3871268928050995
  - 0.4614953100681305
  - 0.4341864287853241
  - 0.40477803349494934
  - 0.40175387263298035
  - 0.38304513692855835
  - 0.3924826979637146
  - 0.4084925651550293
  - 0.5922521352767944
  - 0.43663784861564636
  - 0.38139086961746216
  - 0.4276363253593445
  - 0.4676341116428375
  - 0.43582627177238464
  - 0.40345874428749084
  - 0.42192429304122925
  - 0.4388120472431183
  - 0.48382067680358887
  - 0.5965569615364075
  - 0.3997291028499603
  - 0.5501468181610107
  - 0.47333815693855286
  - 0.6795036196708679
  - 0.407275527715683
  - 0.3791632652282715
  - 0.3980022668838501
  - 0.380114883184433
  - 0.44226396083831787
  - 0.5041947960853577
  - 0.38112232089042664
  - 0.40058526396751404
  - 0.4288393557071686
  - 0.3961012661457062
  - 0.3855953812599182
  - 0.39067530632019043
  - 0.5726356506347656
  - 0.38323089480400085
  - 0.47426682710647583
  - 0.3836858868598938
  - 0.3977982997894287
  - 0.422394335269928
  - 0.4434104859828949
  - 0.42492735385894775
  - 0.4138997793197632
  - 0.3888294994831085
  - 0.3832549750804901
  - 0.44782736897468567
  - 0.44063615798950195
  - 0.409837007522583
  - 0.3918885588645935
  - 0.44901391863822937
  - 0.37712225317955017
  - 0.6135079264640808
  - 0.39386284351348877
  - 0.3856445252895355
  - 0.6005494594573975
  - 0.3871970474720001
  - 0.721280574798584
  - 0.3782283365726471
  - 0.3796018362045288
  - 0.4621691107749939
  - 0.3840882182121277
  - 0.37097394466400146
  - 0.3771696090698242
  - 0.38884374499320984
  - 0.37409427762031555
  - 0.3783092200756073
  - 0.5116079449653625
  - 0.38106468319892883
  - 0.5177255272865295
  - 0.4173023998737335
  - 0.43968120217323303
  - 0.38662657141685486
  - 0.4445462226867676
  - 0.3922877013683319
  - 0.38334518671035767
  - 0.390184611082077
  - 0.6163511276245117
  - 0.378589928150177
  - 0.39766278862953186
  - 0.40811023116111755
  - 0.40981459617614746
  - 0.39409518241882324
  - 0.37861311435699463
  - 0.38329797983169556
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 86 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.023529411764705882]'
  mean_eval_accuracy: 0.8582701160604291
  mean_f1_accuracy: 0.004705882352941176
  total_train_time: '0:03:57.019512'
