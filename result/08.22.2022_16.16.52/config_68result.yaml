config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:48:01.830807'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_68fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_68fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_68fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_68fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_68fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_68fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_68fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_68fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_68fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_68fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.981868797540665
  - 6.95343936085701
  - 6.4072031617164615
  - 6.229817965626717
  - 7.098302271962166
  - 6.490777260065079
  - 6.218473529815674
  - 6.448759251832962
  - 6.465622624754906
  - 6.2878393024206165
  - 6.2012254893779755
  - 6.293309000134468
  - 6.050268727540971
  - 6.166251572966576
  - 5.986381459236146
  - 6.10364537537098
  - 6.134700042009354
  - 6.116045263409615
  - 6.030723142623902
  - 5.982932907342911
  - 5.992675095796585
  validation_losses:
  - 0.42433780431747437
  - 0.4400462210178375
  - 0.4010741114616394
  - 0.4017975330352783
  - 0.4082026779651642
  - 0.4279800057411194
  - 0.4054328203201294
  - 0.38987165689468384
  - 0.4072267413139343
  - 0.39771196246147156
  - 0.49376624822616577
  - 0.4069557785987854
  - 0.39718765020370483
  - 0.3854577839374542
  - 0.40185052156448364
  - 0.3976585865020752
  - 0.39589807391166687
  - 0.3994458019733429
  - 0.39096006751060486
  - 0.39039868116378784
  - 0.3930920660495758
loss_records_fold1:
  train_losses:
  - 5.9790800541639335
  - 6.155589675903321
  - 6.032625851035118
  - 5.949200296401978
  - 6.0879216432571415
  - 5.931804043054581
  - 5.993546789884568
  - 6.000556001067162
  - 5.959032022953034
  - 6.090756639838219
  - 6.015996426343918
  - 5.9146512925624855
  - 5.883419573307037
  - 6.076506739854813
  - 5.878290918469429
  - 6.026017186045647
  - 5.944754466414452
  - 5.954825812578202
  validation_losses:
  - 0.4098936915397644
  - 0.3997361660003662
  - 0.4236533045768738
  - 0.3955078721046448
  - 0.4050646722316742
  - 0.4074254333972931
  - 0.40038442611694336
  - 0.4218756854534149
  - 0.40171054005622864
  - 0.4033202826976776
  - 0.3889811038970947
  - 0.4086253046989441
  - 0.3976036310195923
  - 0.3951793313026428
  - 0.3898990750312805
  - 0.39624378085136414
  - 0.4007905125617981
  - 0.40982556343078613
loss_records_fold2:
  train_losses:
  - 6.005271643400192
  - 5.928364488482476
  - 5.89387486577034
  - 5.9451339751482015
  - 5.817303630709649
  - 6.02045302093029
  - 6.0091527014970785
  - 5.990057936310769
  - 6.017971688508988
  - 5.927519062161446
  - 6.033452552556992
  validation_losses:
  - 0.3845948576927185
  - 0.3978813588619232
  - 0.400573194026947
  - 0.39266958832740784
  - 0.3976055681705475
  - 0.3934338688850403
  - 0.39074018597602844
  - 0.3942713737487793
  - 0.39121437072753906
  - 0.3881419599056244
  - 0.38501858711242676
loss_records_fold3:
  train_losses:
  - 6.015695163607598
  - 6.061182057857514
  - 6.136025166511536
  - 6.051504611968994
  - 5.933256912231446
  - 5.899916332960129
  - 6.131900823116303
  - 5.971918159723282
  - 6.017464867234231
  - 5.967416542768479
  - 6.017217585444451
  validation_losses:
  - 1.0329318046569824
  - 0.37922796607017517
  - 0.3743167817592621
  - 0.3845677077770233
  - 0.4118868112564087
  - 0.378984659910202
  - 0.3738679885864258
  - 0.38163718581199646
  - 0.3739960491657257
  - 0.3773302733898163
  - 0.3820075988769531
loss_records_fold4:
  train_losses:
  - 6.032673931121827
  - 5.892600962519646
  - 6.139825165271759
  - 6.03587935268879
  - 5.9194429397583015
  - 6.078731563687325
  - 5.94161223769188
  - 6.036860013008118
  - 6.057237654924393
  - 5.982391545176506
  - 5.932292422652245
  - 6.111892613768578
  - 5.889305731654168
  - 6.0486008614301685
  - 6.014832621812821
  - 5.903044609725476
  - 6.085539656877518
  - 6.010651165246964
  - 5.86119658946991
  - 5.882663759589196
  - 5.985770761966705
  - 5.851633098721504
  - 5.991760155558587
  - 6.01709150671959
  - 5.932165125012398
  - 5.913117212057114
  - 6.041379150748253
  - 5.898483404517174
  - 5.971119129657746
  - 6.048235046863557
  - 6.040350860357285
  - 5.790513095259667
  - 6.070375600457192
  - 5.936975148320198
  - 5.988689833879471
  - 5.859912493824959
  - 5.9004969403147705
  - 6.118056297302246
  - 5.949718189239502
  - 6.014748734235764
  - 5.900140687823296
  - 6.034257435798645
  - 6.037893408536911
  - 5.832771489024163
  validation_losses:
  - 0.37369504570961
  - 0.3788605332374573
  - 0.37514492869377136
  - 0.39540329575538635
  - 0.3789055645465851
  - 0.3831532895565033
  - 0.38941311836242676
  - 0.3769059479236603
  - 0.375243216753006
  - 0.3757561147212982
  - 0.38909608125686646
  - 0.37580740451812744
  - 0.37007009983062744
  - 0.38312989473342896
  - 0.3799908757209778
  - 0.3741062879562378
  - 0.382697731256485
  - 0.38726145029067993
  - 0.3821317255496979
  - 0.45804157853126526
  - 0.39365118741989136
  - 0.37191489338874817
  - 0.3747420012950897
  - 0.4131533205509186
  - 0.3780467212200165
  - 0.37605977058410645
  - 0.37575623393058777
  - 0.4057621955871582
  - 0.3745511770248413
  - 0.38233333826065063
  - 0.3786582946777344
  - 0.3783431351184845
  - 0.4211790859699249
  - 0.39518535137176514
  - 0.4069509208202362
  - 0.3842715620994568
  - 0.3862927258014679
  - 0.42093324661254883
  - 0.3845627009868622
  - 0.3790765106678009
  - 0.3832389712333679
  - 0.3837907016277313
  - 0.38849177956581116
  - 0.38367512822151184
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 44 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8542024013722127, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.0
  total_train_time: '0:11:01.817629'
