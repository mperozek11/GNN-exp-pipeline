config:
  aggregation: mean
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:15:25.430232'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_119fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_119fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_119fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_119fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_119fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_119fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_119fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_119fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_119fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_119fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.1469032049179078
  - 0.9247833788394928
  - 0.9227919757366181
  - 0.8835923075675964
  - 0.9990450918674469
  - 0.8948425233364106
  - 0.8753163278102876
  - 0.8343241572380067
  - 0.8519048869609833
  - 0.8175070703029633
  - 0.8165703773498536
  - 0.8252332329750062
  - 0.8197499573230744
  - 0.9631052136421204
  validation_losses:
  - 0.4232846796512604
  - 0.4127614498138428
  - 0.4198901057243347
  - 0.40940651297569275
  - 0.4331730604171753
  - 0.4891982972621918
  - 0.39505869150161743
  - 0.40891167521476746
  - 0.4039897620677948
  - 0.4028988480567932
  - 0.39617615938186646
  - 0.3927718698978424
  - 0.38687217235565186
  - 0.3936764597892761
loss_records_fold1:
  train_losses:
  - 0.8482261061668397
  - 0.7872261673212052
  - 0.8085041999816895
  - 0.9288596868515016
  - 0.7914075791835785
  - 0.8601566135883332
  - 0.8763372063636781
  - 0.871559751033783
  - 0.8258790671825409
  - 0.9034958481788635
  - 0.7780720174312592
  - 0.8188111126422882
  - 0.7998335480690003
  - 0.8178439557552338
  - 0.873344910144806
  - 0.8550313532352448
  - 0.8258237719535828
  validation_losses:
  - 0.39873290061950684
  - 0.39169880747795105
  - 0.39872732758522034
  - 0.40687668323516846
  - 0.39710232615470886
  - 0.4155981242656708
  - 0.41033825278282166
  - 0.4049800932407379
  - 0.39346712827682495
  - 0.39150092005729675
  - 0.4118897318840027
  - 0.3992742598056793
  - 0.40197110176086426
  - 0.3974473774433136
  - 0.39596879482269287
  - 0.3889217972755432
  - 0.39292070269584656
loss_records_fold2:
  train_losses:
  - 0.8627040147781373
  - 0.8241165339946748
  - 0.8343495905399323
  - 0.8347423315048218
  - 0.8382006406784058
  - 0.7949129819869996
  - 0.7743785917758942
  - 0.8457632601261139
  - 0.7862884640693665
  - 0.8038445293903351
  - 0.8008800089359284
  - 0.8112141847610475
  - 0.8406347155570985
  - 0.8394200086593628
  - 0.7848548114299775
  - 0.7939837992191315
  - 0.784958627820015
  - 0.8017778277397156
  - 0.813963931798935
  - 0.7970416307449342
  - 0.7920889675617219
  - 0.7946393311023713
  - 0.7962235987186432
  - 0.8774047195911407
  - 0.8103614807128907
  - 0.7895472645759583
  - 0.7907741367816925
  - 0.7950657546520233
  - 0.7906502664089203
  - 0.8255205094814301
  - 0.8066632330417634
  - 0.8210393905639649
  - 0.8043464958667755
  - 0.925519472360611
  - 0.8192595660686494
  - 0.8143370151519775
  - 0.8515257000923158
  - 0.7975968599319458
  - 0.9470183491706848
  - 0.8408219397068024
  - 0.8194204628467561
  - 0.8277330458164216
  - 0.8596963882446289
  - 0.8318574607372284
  - 0.8923278510570527
  - 0.8017877459526063
  - 0.8406510174274445
  - 0.7878409981727601
  - 0.8043967604637147
  - 0.7626866608858109
  - 0.8064301371574403
  - 0.7973649919033051
  - 0.8566172063350678
  validation_losses:
  - 0.39532792568206787
  - 0.40231597423553467
  - 0.39456790685653687
  - 0.401858925819397
  - 0.40306127071380615
  - 0.4005970358848572
  - 0.39776214957237244
  - 0.42068570852279663
  - 0.3949461579322815
  - 0.4004083275794983
  - 0.390156090259552
  - 0.3854657709598541
  - 0.3880541920661926
  - 0.39951232075691223
  - 0.41444632411003113
  - 0.41977542638778687
  - 0.3933596611022949
  - 0.4094249904155731
  - 0.4013945460319519
  - 0.3964329957962036
  - 0.3910219669342041
  - 0.4026065468788147
  - 0.3994590938091278
  - 0.46442368626594543
  - 0.38947731256484985
  - 0.3993051052093506
  - 0.38481593132019043
  - 0.3988019824028015
  - 0.4140029549598694
  - 0.44386860728263855
  - 0.47475600242614746
  - 0.44785499572753906
  - 0.5166259407997131
  - 0.46379104256629944
  - 0.39014431834220886
  - 0.39264073967933655
  - 0.5602610111236572
  - 0.5063244700431824
  - 1.5530931949615479
  - 1.3585073947906494
  - 2.009248733520508
  - 1.0258839130401611
  - 0.5304994583129883
  - 0.4032474458217621
  - 0.4058172404766083
  - 0.39389923214912415
  - 0.4054562747478485
  - 0.3892674148082733
  - 0.3958473205566406
  - 0.3870016038417816
  - 0.3936246335506439
  - 0.38977816700935364
  - 0.3975975811481476
loss_records_fold3:
  train_losses:
  - 0.7840713560581207
  - 0.8619699716567993
  - 0.8016553819179535
  - 0.8349868297576905
  - 0.89287029504776
  - 0.9195010125637055
  - 0.8507109344005586
  - 0.826183307170868
  - 1.1157967448234558
  - 0.8258358955383301
  - 0.7849621266126633
  - 0.8309370279312134
  - 0.8154374778270722
  - 0.7807173728942871
  - 0.8163381636142731
  - 0.851460474729538
  - 0.7929084420204163
  - 0.8001543760299683
  - 0.7989799141883851
  - 0.8290355980396271
  - 0.7832573413848878
  - 0.7688122421503067
  - 0.7708171993494034
  - 0.7679760754108429
  - 0.7762339532375336
  - 0.8775486409664155
  - 0.8113501161336899
  - 0.8041452884674073
  - 0.8284221947193147
  - 0.8217701673507691
  - 0.8182392776012422
  - 0.8422550559043884
  - 0.7872751891613007
  - 0.8210194170475007
  - 0.8104317128658295
  validation_losses:
  - 0.37750908732414246
  - 0.42925596237182617
  - 0.4074406921863556
  - 0.4075576364994049
  - 0.3707817494869232
  - 0.368230402469635
  - 0.3741972744464874
  - 0.3867201805114746
  - 0.3847823739051819
  - 0.378689706325531
  - 0.39131414890289307
  - 0.3912229835987091
  - 0.38468801975250244
  - 0.3748876750469208
  - 0.3813108801841736
  - 0.37420299649238586
  - 0.38470324873924255
  - 0.3768237829208374
  - 0.37601587176322937
  - 0.37395188212394714
  - 0.38409924507141113
  - 0.386462926864624
  - 0.39797869324684143
  - 0.39631199836730957
  - 0.38974687457084656
  - 0.4187316596508026
  - 0.36695557832717896
  - 0.38752785325050354
  - 0.44627001881599426
  - 0.4038867652416229
  - 0.39696601033210754
  - 0.3862031400203705
  - 0.387251079082489
  - 0.39070767164230347
  - 0.3857254087924957
loss_records_fold4:
  train_losses:
  - 0.8116564810276032
  - 0.8338546335697175
  - 0.8121809720993043
  - 0.8113077819347382
  - 0.7633125066757203
  - 0.8113428175449372
  - 0.7850217819213867
  - 0.7963720321655274
  - 0.8002041876316071
  - 0.8265053629875183
  - 0.7813887655735017
  - 0.8067736089229585
  - 0.7970206260681153
  - 0.7939423024654388
  - 0.8026544809341432
  - 0.7939158856868744
  - 0.7573693543672562
  - 0.836938089132309
  - 0.8216398060321808
  - 0.7828721851110458
  - 0.8197197496891022
  - 0.8075838029384613
  - 0.7736896276473999
  - 0.7984314203262329
  - 0.7882262229919434
  - 0.8138862907886506
  - 0.7871849358081818
  - 0.8010118126869202
  - 0.7955531001091004
  - 0.8218146800994873
  - 0.7405836135149002
  - 0.8790829241275788
  - 0.8561302542686463
  - 0.7786860466003418
  - 0.7990412175655366
  - 0.8135948240756989
  - 0.8199244499206544
  - 0.830996960401535
  - 0.765674924850464
  - 0.7895353496074677
  - 0.796601015329361
  - 0.7986254870891571
  - 0.7674559116363526
  - 0.7877459108829499
  - 0.7972398340702057
  - 0.8040049552917481
  - 0.8006411969661713
  - 0.7978064298629761
  - 0.7778884112834931
  - 0.7835011959075928
  - 0.7852598786354066
  - 0.7849116265773773
  - 0.8189894139766694
  - 0.8113088667392732
  - 0.7494217783212662
  - 0.8005647361278534
  - 0.8370882332324983
  - 0.7788890480995179
  - 0.848163253068924
  validation_losses:
  - 0.37813252210617065
  - 0.41083216667175293
  - 0.38852378726005554
  - 0.3900168836116791
  - 0.3810599446296692
  - 0.39557328820228577
  - 0.3914087116718292
  - 0.3874635398387909
  - 0.378131628036499
  - 0.3923870027065277
  - 0.380937784910202
  - 0.3983413279056549
  - 0.38629433512687683
  - 0.40287959575653076
  - 0.5352441072463989
  - 0.3836461007595062
  - 0.3773360550403595
  - 0.38095518946647644
  - 0.42286068201065063
  - 0.3943847119808197
  - 0.39548954367637634
  - 0.3897612392902374
  - 0.3796054720878601
  - 0.4179430603981018
  - 0.37975409626960754
  - 0.38719305396080017
  - 0.38868990540504456
  - 0.37949809432029724
  - 0.3818686604499817
  - 0.3952326774597168
  - 0.39476948976516724
  - 0.38562071323394775
  - 0.468580961227417
  - 0.41147616505622864
  - 0.6827632188796997
  - 0.40088316798210144
  - 0.38764825463294983
  - 0.37443017959594727
  - 0.3859880566596985
  - 0.3828398287296295
  - 0.3911201059818268
  - 0.37865111231803894
  - 0.39206138253211975
  - 0.3888368308544159
  - 0.40486934781074524
  - 0.39294448494911194
  - 0.38467738032341003
  - 0.39832624793052673
  - 0.470580130815506
  - 0.4229363799095154
  - 0.3797294497489929
  - 0.38427335023880005
  - 0.39805230498313904
  - 0.38738584518432617
  - 0.3890334963798523
  - 0.3975129723548889
  - 0.3939312994480133
  - 0.3686974048614502
  - 0.37795501947402954
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 53 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 35 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 59 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:14:30.612849'
