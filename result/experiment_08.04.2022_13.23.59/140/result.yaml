config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 06:37:43.580718'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/140/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/140/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/140/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/140/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/140/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/140/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/140/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/140/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/140/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/140/fold_4_state_dict.pt
loss_records_fold3:
  train_losses:
  - 8.117230325937271
  - 7.649094179272652
  - 8.002194583415985
  - 7.9848175048828125
  - 7.711696118116379
  - 7.92118826508522
  - 7.974595129489899
  - 7.865689992904663
  - 7.957221820950508
  - 8.164097309112549
  - 7.901191681623459
  - 7.972087413072586
  - 7.855220705270767
  - 7.568335324525833
  - 7.823749780654907
  - 7.8149018585681915
  - 7.701855570077896
  - 7.708605915307999
  - 7.822975039482117
  - 7.748384580016136
  - 7.770390063524246
  - 7.65524822473526
  - 7.612399905920029
  - 7.62218651175499
  - 7.7786449790000916
  - 7.975903034210205
  - 7.946916341781616
  - 7.947712868452072
  - 7.772623747587204
  - 7.762671232223511
  - 7.740159064531326
  - 7.504438012838364
  - 8.156469255685806
  - 7.983145505189896
  - 7.695207297801971
  - 8.068290174007416
  - 8.063962608575821
  - 7.634491801261902
  - 7.946896076202393
  - 7.607702940702438
  - 8.097777098417282
  - 7.677801877260208
  - 7.9570790231227875
  - 7.991099506616592
  - 8.081015437841415
  - 7.725282549858093
  - 7.730564445257187
  - 7.51492840051651
  - 7.836224764585495
  - 7.673223555088043
  - 7.649224877357483
  - 7.981524020433426
  - 7.922721982002258
  - 7.813554584980011
  - 8.03409031033516
  - 7.973939776420593
  - 8.036218345165253
  - 7.673122018575668
  - 7.65808030962944
  - 7.844174176454544
  - 8.132922679185867
  - 7.74985334277153
  - 7.883725881576538
  - 7.6249799728393555
  - 7.856264501810074
  - 7.634577304124832
  - 7.582708775997162
  - 8.138284504413605
  - 8.060030579566956
  - 8.172292679548264
  - 7.636258780956268
  - 7.54597932100296
  - 7.733468532562256
  - 7.717269539833069
  - 7.893633991479874
  - 7.605576828122139
  - 7.730106294155121
  - 7.455007374286652
  - 7.772846221923828
  - 7.502160906791687
  - 7.686618745326996
  - 7.872625350952148
  - 7.608273893594742
  - 7.978267133235931
  - 7.625121682882309
  - 7.7755067050457
  - 7.3870871514081955
  - 7.6459488570690155
  - 8.053062826395035
  - 7.63360658288002
  - 7.696967452764511
  - 7.526163160800934
  - 7.826998800039291
  - 7.58077184855938
  - 7.607107847929001
  - 8.189689338207245
  - 7.441802203655243
  - 7.909378409385681
  - 7.47754892706871
  - 7.500498056411743
  validation_losses:
  - 0.38563525676727295
  - 0.37792009115219116
  - 0.40687528252601624
  - 0.3867567181587219
  - 0.3865683674812317
  - 0.3751164376735687
  - 0.3970118761062622
  - 0.39583683013916016
  - 0.38842907547950745
  - 0.38974788784980774
  - 0.3809794485569
  - 0.38974565267562866
  - 0.4302516281604767
  - 0.6680558323860168
  - 0.8359308838844299
  - 0.5783023238182068
  - 0.6377375721931458
  - 0.8413811922073364
  - 0.9093958139419556
  - 0.39242979884147644
  - 0.3878036439418793
  - 0.6266518831253052
  - 0.49382075667381287
  - 0.544052004814148
  - 0.5942496061325073
  - 0.8466543555259705
  - 0.49636465311050415
  - 1.0433868169784546
  - 0.7254887223243713
  - 1.0307791233062744
  - 1.1058892011642456
  - 0.8409805297851562
  - 0.4857248067855835
  - 0.3905787765979767
  - 0.39030876755714417
  - 0.45094943046569824
  - 0.41572532057762146
  - 0.3826020359992981
  - 0.4041297733783722
  - 0.40294933319091797
  - 0.5766348242759705
  - 0.487047016620636
  - 0.5099772214889526
  - 2.3469364643096924
  - 0.5690556168556213
  - 0.38160035014152527
  - 0.5023468732833862
  - 0.5870132446289062
  - 0.6129018664360046
  - 0.5637722015380859
  - 1.3676875829696655
  - 1.1914637088775635
  - 9.249090194702148
  - 0.3945237398147583
  - 0.38830047845840454
  - 0.4099738299846649
  - 0.3941948413848877
  - 0.3824995756149292
  - 0.3941313326358795
  - 0.3895720839500427
  - 0.3961432874202728
  - 0.3910711109638214
  - 0.3846067190170288
  - 0.37842246890068054
  - 0.4019586145877838
  - 0.3840186595916748
  - 0.412079393863678
  - 0.4085574746131897
  - 1.1686162948608398
  - 0.49833017587661743
  - 0.37975525856018066
  - 0.4710662066936493
  - 1.563514232635498
  - 0.7871397733688354
  - 1.0619632005691528
  - 0.7149476408958435
  - 0.8461363911628723
  - 9.40057373046875
  - 0.4981449246406555
  - 0.8851267099380493
  - 0.9118962287902832
  - 0.5539081692695618
  - 1.0033317804336548
  - 0.7380620241165161
  - 0.46055901050567627
  - 0.6222240328788757
  - 0.5221208333969116
  - 0.7122900485992432
  - 0.8456398248672485
  - 0.4099624752998352
  - 0.4598413407802582
  - 0.5873933434486389
  - 0.8048702478408813
  - 0.460838258266449
  - 0.7955629229545593
  - 1.396260380744934
  - 0.3913075923919678
  - 0.6121167540550232
  - 0.8686098456382751
  - 1.0485624074935913
loss_records_fold4:
  train_losses:
  - 7.8589330315589905
  - 7.721144467592239
  - 7.679712682962418
  - 7.375367522239685
  - 7.50526949763298
  - 7.863481014966965
  - 7.775820195674896
  - 7.479139298200607
  - 7.585809946060181
  - 7.833860695362091
  - 7.50667479634285
  - 7.630145758390427
  - 7.518295586109161
  - 7.540091544389725
  - 7.498410940170288
  - 7.748764127492905
  - 8.039525657892227
  - 7.541648238897324
  - 7.700315743684769
  - 7.445840656757355
  - 7.562468767166138
  - 7.510857939720154
  - 7.615281701087952
  - 7.4955222606658936
  - 7.418657153844833
  - 7.492451250553131
  - 7.58948478102684
  - 7.660734415054321
  - 7.474442660808563
  - 7.549900770187378
  - 7.483934998512268
  - 7.639940857887268
  - 7.326049223542213
  - 7.771026760339737
  - 7.68784499168396
  - 7.343758225440979
  - 7.512236028909683
  - 7.559916704893112
  - 7.659144729375839
  - 8.202889144420624
  - 7.936121374368668
  - 7.85368886590004
  - 7.347402945160866
  - 7.516176223754883
  - 7.555201202630997
  - 7.496410503983498
  - 7.44690266251564
  - 7.54771015048027
  - 7.6151847541332245
  - 7.723028540611267
  - 7.699466824531555
  - 7.512767493724823
  - 7.614958018064499
  - 7.691887050867081
  - 7.688721746206284
  - 7.521703839302063
  - 7.591282933950424
  - 7.4749709367752075
  - 7.526499003171921
  - 7.705421060323715
  - 7.39286807179451
  - 7.537395894527435
  - 7.709162682294846
  - 7.720674693584442
  - 7.717423468828201
  - 7.660257309675217
  - 7.5660103261470795
  - 7.571419775485992
  - 7.58972355723381
  - 7.301046743988991
  - 7.443785637617111
  - 7.359902024269104
  - 7.503640294075012
  - 7.21010285615921
  - 7.499841570854187
  - 7.444969415664673
  - 7.342783883213997
  - 7.77173113822937
  - 7.817028492689133
  - 8.086392730474472
  - 7.617315083742142
  - 7.643293976783752
  - 7.525235444307327
  - 7.429169237613678
  - 7.627838373184204
  - 7.396731734275818
  - 7.5793187618255615
  - 7.833700001239777
  - 7.264518737792969
  - 7.395613491535187
  - 7.479070395231247
  - 7.6073572635650635
  - 7.588823080062866
  - 7.6383056640625
  - 7.391352266073227
  - 7.745904296636581
  - 7.263162389397621
  - 7.6164379715919495
  - 7.291620522737503
  - 7.434969931840897
  validation_losses:
  - 0.4116378128528595
  - 0.401457279920578
  - 0.379589706659317
  - 0.4114869236946106
  - 0.3837638795375824
  - 0.40334659814834595
  - 0.4104676842689514
  - 0.450772225856781
  - 0.4885046184062958
  - 0.4942317605018616
  - 0.6037935614585876
  - 0.36692890524864197
  - 0.38974615931510925
  - 0.3838220238685608
  - 0.4267573952674866
  - 0.4107855260372162
  - 0.5832574963569641
  - 0.45655250549316406
  - 0.39473092555999756
  - 0.3902870714664459
  - 0.3841879665851593
  - 0.40439775586128235
  - 0.40184664726257324
  - 0.38003650307655334
  - 0.3938780128955841
  - 0.4560880959033966
  - 0.3891559839248657
  - 0.4619353711605072
  - 0.37286844849586487
  - 0.3833746016025543
  - 0.4364432692527771
  - 0.5619502663612366
  - 0.7892481684684753
  - 0.4164392054080963
  - 0.43226364254951477
  - 0.4272404909133911
  - 0.3820510804653168
  - 0.4941957890987396
  - 0.6059298515319824
  - 0.43574628233909607
  - 0.4377233684062958
  - 0.3870662450790405
  - 0.3709988594055176
  - 0.38543936610221863
  - 0.4308282434940338
  - 0.5527256727218628
  - 0.5198105573654175
  - 0.3691380023956299
  - 0.3775941729545593
  - 0.3841027617454529
  - 0.384330153465271
  - 0.40450435876846313
  - 0.3894907832145691
  - 0.41193288564682007
  - 0.3990061581134796
  - 0.3847399353981018
  - 0.38812321424484253
  - 0.39886555075645447
  - 0.4025045931339264
  - 0.38415002822875977
  - 0.396481454372406
  - 0.39484795928001404
  - 0.3736380934715271
  - 0.48459649085998535
  - 0.46663999557495117
  - 0.38223886489868164
  - 0.4023009240627289
  - 0.5529094934463501
  - 0.4312824308872223
  - 0.37756162881851196
  - 0.37892353534698486
  - 0.6044923067092896
  - 0.4260963797569275
  - 0.36173632740974426
  - 0.5707879662513733
  - 0.3769284784793854
  - 0.44577494263648987
  - 0.3905009627342224
  - 0.4013771414756775
  - 0.3762814700603485
  - 0.40410324931144714
  - 0.42347362637519836
  - 0.4590754210948944
  - 0.41553497314453125
  - 0.48061126470565796
  - 0.5488911867141724
  - 0.36302855610847473
  - 0.3766936659812927
  - 0.38862577080726624
  - 0.39006996154785156
  - 0.6633955240249634
  - 0.4122486114501953
  - 0.4201614558696747
  - 0.3852221965789795
  - 0.49273887276649475
  - 0.39404842257499695
  - 0.505744993686676
  - 0.4761521518230438
  - 0.37064528465270996
  - 0.5119971632957458
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 9 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8542024013722127,
    0.852233676975945]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.15686274509803924]'
  mean_eval_accuracy: 0.8558669755324102
  mean_f1_accuracy: 0.03137254901960785
  total_train_time: '0:02:40.219336'
