config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 18:53:23.593302'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/53/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/53/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/53/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/53/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/53/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/53/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/53/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/53/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/53/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/53/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 7.916274189949036
  - 8.426835238933563
  - 9.849788784980774
  - 8.109005585312843
  - 8.384263217449188
  - 8.32643011212349
  - 8.477386206388474
  - 8.36835315823555
  - 8.143643915653229
  - 7.904279246926308
  - 8.828110963106155
  - 13.736820936203003
  - 10.128854155540466
  - 8.015357702970505
  - 9.106948852539062
  - 8.217142254114151
  - 7.377628579735756
  - 8.21034699678421
  - 8.893431961536407
  - 7.879803568124771
  - 7.499764710664749
  - 8.7699775993824
  - 12.928175866603851
  - 10.084129452705383
  - 9.642265647649765
  - 8.142781347036362
  - 10.204091519117355
  - 8.092377036809921
  - 7.904616743326187
  - 8.630520761013031
  - 7.967749536037445
  - 7.916266053915024
  - 16.49673244357109
  - 15.670246094465256
  - 8.923059433698654
  - 7.475752085447311
  - 8.288926422595978
  - 8.028581321239471
  - 8.15029987692833
  - 7.812405675649643
  - 8.378260493278503
  - 7.736744195222855
  - 8.008203744888306
  - 7.957155108451843
  - 7.881813287734985
  - 7.481295943260193
  - 7.811866015195847
  - 14.355747193098068
  - 14.033399105072021
  - 8.596988826990128
  - 7.630788683891296
  - 7.81576132774353
  - 8.65557074546814
  - 8.299509853124619
  - 8.505596607923508
  - 10.577603608369827
  - 8.910330533981323
  - 8.088251411914825
  - 10.285580575466156
  - 9.998150080442429
  - 8.054946422576904
  - 7.5215403735637665
  - 7.666353285312653
  - 7.945777341723442
  - 7.832321137189865
  - 8.043712347745895
  - 7.63176903128624
  - 8.299768298864365
  - 9.04958513379097
  - 7.6205639243125916
  - 10.322939842939377
  - 9.057611763477325
  - 11.150018453598022
  - 8.358152747154236
  - 7.718565493822098
  - 8.723397821187973
  - 7.888838201761246
  - 8.71114131808281
  - 8.161741197109222
  - 8.001095712184906
  - 8.133321583271027
  - 8.089528977870941
  - 8.109132945537567
  - 9.188583701848984
  - 13.242632240056992
  - 16.017638504505157
  - 14.503397166728973
  - 70.2297485768795
  - 72.84747669100761
  - 13.492775559425354
  - 31.742993265390396
  - 15.690888345241547
  - 16.105713188648224
  - 8.575145602226257
  - 9.692771017551422
  - 8.491580486297607
  - 9.524174690246582
  - 9.189495146274567
  - 8.265489786863327
  - 8.097843736410141
  validation_losses:
  - 0.43060988187789917
  - 0.8268622159957886
  - 0.41329872608184814
  - 0.409542441368103
  - 0.4692438542842865
  - 0.42800983786582947
  - 0.4631267189979553
  - 1.9406156539916992
  - 5.392088413238525
  - 6.671830177307129
  - 8.847221374511719
  - 3.4988651275634766
  - 6.015058994293213
  - 6.515093803405762
  - 6.70850133895874
  - 5.840336799621582
  - 6.271880626678467
  - 7.713757038116455
  - 6.892292022705078
  - 7.187517166137695
  - 0.4032866656780243
  - 0.4072059988975525
  - 0.4334053099155426
  - 0.7057899236679077
  - 0.4466191232204437
  - 0.6151594519615173
  - 0.4872235655784607
  - 1.5979315042495728
  - 6.994662761688232
  - 4.327264308929443
  - 7.939486980438232
  - 10.11789608001709
  - 8.218937873840332
  - 6.023519515991211
  - 15.503469467163086
  - 5.039711952209473
  - 7.284554958343506
  - 7.7885518074035645
  - 7.342404365539551
  - 11.45079517364502
  - 6.608096122741699
  - 7.302597999572754
  - 9.349454879760742
  - 8.398151397705078
  - 7.791907787322998
  - 6.925200939178467
  - 2.1209073066711426
  - 3.124452829360962
  - 3.0775952339172363
  - 6.985010147094727
  - 8.630738258361816
  - 7.8993682861328125
  - 6.886255264282227
  - 5.314236164093018
  - 8.76872730255127
  - 7.6318254470825195
  - 4.904934883117676
  - 9.734612464904785
  - 7.760622024536133
  - 5.101253032684326
  - 6.2331976890563965
  - 6.896125316619873
  - 9.085826873779297
  - 5.146103382110596
  - 6.507212162017822
  - 9.087677955627441
  - 7.485467433929443
  - 6.6954569816589355
  - 8.156465530395508
  - 8.350981712341309
  - 4.3374176025390625
  - 8.766890525817871
  - 5.908210754394531
  - 10.11927318572998
  - 8.29742431640625
  - 6.318232536315918
  - 10.876070976257324
  - 8.501978874206543
  - 7.855004787445068
  - 0.4173946976661682
  - 0.47843050956726074
  - 0.44179394841194153
  - 0.5495397448539734
  - 0.7704339027404785
  - 0.4068896174430847
  - 0.5354325175285339
  - 0.680333137512207
  - 3.921243190765381
  - 2.2697689533233643
  - 6.040079593658447
  - 8.589051246643066
  - 4.48613977432251
  - 0.49120277166366577
  - 0.5629950761795044
  - 0.47797390818595886
  - 0.4679417014122009
  - 0.7808718085289001
  - 0.4660237431526184
  - 0.4198814332485199
  - 0.42068004608154297
loss_records_fold2:
  train_losses:
  - 7.908186167478561
  - 8.26236629486084
  - 11.581948399543762
  - 14.800892293453217
  - 12.960933893918991
  - 10.154045790433884
  - 10.869777888059616
  - 9.991281300783157
  - 9.576339364051819
  - 8.75551775097847
  - 7.716756731271744
  - 7.7084101140499115
  - 7.758209943771362
  - 14.779920518398285
  - 13.348380416631699
  - 9.66875621676445
  - 9.50821566581726
  - 8.601223677396774
  - 9.682927131652832
  - 10.695069760084152
  - 27.208918929100037
  - 8.21398788690567
  - 8.066851258277893
  - 7.996934533119202
  - 7.942191898822784
  - 7.729591369628906
  - 8.142575472593307
  - 8.497647613286972
  - 9.319504588842392
  - 8.155821144580841
  - 8.616162478923798
  - 7.832436263561249
  - 8.063095688819885
  - 7.934423208236694
  - 7.801833152770996
  - 8.11091360449791
  - 8.505000621080399
  - 11.749077588319778
  - 8.724834889173508
  - 8.251203954219818
  - 7.8587091863155365
  - 7.780056267976761
  - 8.960605561733246
  - 10.798426985740662
  - 11.855824023485184
  - 8.96320965886116
  - 8.797311216592789
  - 8.01917290687561
  - 8.456944733858109
  - 7.91489365696907
  - 9.51613599061966
  - 9.903549313545227
  - 8.16882860660553
  - 8.183790922164917
  - 8.606129229068756
  - 7.673749506473541
  - 9.253211945295334
  - 11.493080824613571
  - 8.227394759654999
  - 7.739629358053207
  - 8.753098756074905
  - 9.055341452360153
  - 8.188180416822433
  - 8.216511100530624
  - 8.106492519378662
  - 7.892611384391785
  - 7.752672292292118
  - 9.785566419363022
  - 8.078815698623657
  - 9.28015449643135
  - 8.017375200986862
  - 7.713001325726509
  - 7.8473080694675446
  - 7.867825448513031
  - 9.838741660118103
  - 9.955943018198013
  - 7.9540340304374695
  - 8.496689528226852
  - 14.889680862426758
  - 9.879517436027527
  - 8.182176113128662
  - 9.318398922681808
  - 11.363388121128082
  - 9.538698107004166
  - 8.017801761627197
  - 7.692748546600342
  - 8.833197444677353
  - 8.242994278669357
  - 9.990032523870468
  - 8.577636033296585
  - 10.152163088321686
  - 8.066722929477692
  - 10.849329143762589
  - 13.436979353427887
  - 13.451077498495579
  - 13.776191115379333
  - 14.149666607379913
  - 12.381102293729782
  - 10.579211235046387
  - 7.9161298125982285
  validation_losses:
  - 0.39650240540504456
  - 0.4130842089653015
  - 0.5311124324798584
  - 0.5054861903190613
  - 0.4385268986225128
  - 0.4557148814201355
  - 78850.46875
  - 32256.126953125
  - 25107.798828125
  - 0.39651814103126526
  - 0.4365251362323761
  - 0.3927186131477356
  - 0.48782649636268616
  - 1897.4769287109375
  - 0.43179789185523987
  - 0.5012202858924866
  - 0.41803857684135437
  - 0.3941675126552582
  - 0.6575344800949097
  - 0.6222589015960693
  - 3.897531032562256
  - 2.9642856121063232
  - 0.3905911147594452
  - 0.45254260301589966
  - 0.4160260260105133
  - 0.41136232018470764
  - 0.38809072971343994
  - 0.7933271527290344
  - 0.40795502066612244
  - 0.3869771659374237
  - 0.43021872639656067
  - 0.4154451787471771
  - 0.4266120493412018
  - 0.39188316464424133
  - 0.38442090153694153
  - 0.38860464096069336
  - 0.43867096304893494
  - 0.39408358931541443
  - 0.38618209958076477
  - 0.38846102356910706
  - 0.3875604569911957
  - 0.4403371512889862
  - 0.388873428106308
  - 0.6964379549026489
  - 0.4386969804763794
  - 2085829.25
  - 47035796.0
  - 22167872.0
  - 1.215731295322112e+17
  - 6.611221855695012e+17
  - 1.4145085149942907e+18
  - 9.195618439224361e+17
  - 2.3177998889281126e+17
  - 0.4120883047580719
  - 0.3838261365890503
  - 0.41021138429641724
  - 0.5757704973220825
  - 0.3867563009262085
  - 0.3957962095737457
  - 0.4561213254928589
  - 0.4819428324699402
  - 0.406385213136673
  - 0.39788323640823364
  - 0.3885689079761505
  - 0.39466631412506104
  - 0.39380186796188354
  - 0.4983406662940979
  - 0.4544721841812134
  - 0.6070566177368164
  - 0.39000916481018066
  - 22737806.0
  - 0.4058379828929901
  - 0.38355618715286255
  - 0.3987829387187958
  - 0.4848933517932892
  - 0.4152466654777527
  - 44478064.0
  - 128197256.0
  - 0.8672532439231873
  - 0.4124760031700134
  - 0.5366487503051758
  - 0.47531062364578247
  - 0.4715737998485565
  - 0.40427741408348083
  - 0.38691243529319763
  - 0.42473524808883667
  - 0.3953896760940552
  - 0.39360275864601135
  - 8610014.0
  - 2277586.5
  - 0.4162082076072693
  - 0.4073312282562256
  - 0.610101580619812
  - 1.0445268154144287
  - 0.8683053851127625
  - 0.39024391770362854
  - 0.7697310447692871
  - 0.7148725390434265
  - 11405806.0
  - 0.38710492849349976
loss_records_fold4:
  train_losses:
  - 9.251199334859848
  - 8.488383561372757
  - 8.088892161846161
  - 7.824430584907532
  - 8.086365312337875
  - 8.051481008529663
  - 8.495431780815125
  - 8.012195974588394
  - 7.706660717725754
  - 8.248821318149567
  - 9.797523736953735
  - 10.806635737419128
  - 10.351037234067917
  - 8.205406844615936
  - 8.874895825982094
  - 9.117674261331558
  - 15.179595425724983
  - 9.592421859502792
  - 9.165803879499435
  - 8.9470254778862
  - 7.753709942102432
  - 7.66703574359417
  - 10.291768193244934
  - 10.374577641487122
  - 8.94655466079712
  - 8.587951183319092
  - 11.071996361017227
  - 9.264055877923965
  - 9.305046498775482
  - 7.943693578243256
  - 7.541298002004623
  - 7.6808169186115265
  - 7.584379136562347
  - 8.167864829301834
  - 9.297177761793137
  - 7.803908944129944
  - 7.680116832256317
  - 7.9511191844940186
  - 7.724306754767895
  - 7.942379206418991
  - 7.8989715576171875
  - 8.12552472949028
  - 7.691284865140915
  - 8.039235010743141
  - 8.044915437698364
  - 7.919388711452484
  - 7.779904872179031
  - 9.355140596628189
  - 8.321223974227905
  - 13.123440206050873
  - 23.33900624513626
  - 16.86253198981285
  - 15.529960989952087
  - 9.526264369487762
  - 7.9462838768959045
  - 7.803518861532211
  - 8.09364864230156
  - 8.037023305892944
  - 8.13190370798111
  - 8.335012376308441
  - 12.55309671163559
  - 10.594260334968567
  - 9.921535909175873
  - 7.7715123295784
  - 7.814321577548981
  - 9.016936302185059
  - 7.969255805015564
  - 8.121625185012817
  - 7.889430522918701
  - 9.055588364601135
  - 7.64774090051651
  - 8.293412059545517
  - 8.295125931501389
  - 8.160436630249023
  - 9.547835558652878
  - 9.741196542978287
  - 7.825878351926804
  - 7.9326012134552
  - 8.39239090681076
  - 12.034371435642242
  - 8.701029479503632
  - 7.803683966398239
  - 9.152340561151505
  - 8.582457453012466
  - 8.092261612415314
  - 7.931197822093964
  - 8.993597716093063
  - 8.288218826055527
  - 8.331959992647171
  - 10.528434991836548
  - 9.155529141426086
  - 12.743045799434185
  - 17.017648607492447
  - 12.16664245724678
  - 11.033034324645996
  - 7.874360114336014
  - 11.095579236745834
  - 12.200356394052505
  - 7.82561519742012
  - 7.619537487626076
  validation_losses:
  - 0.3964785039424896
  - 0.533340334892273
  - 0.39172446727752686
  - 0.4233286678791046
  - 0.44031354784965515
  - 0.39434200525283813
  - 0.4085898697376251
  - 0.4533577263355255
  - 0.39769861102104187
  - 0.39237064123153687
  - 0.5325263738632202
  - 0.457351416349411
  - 0.40624088048934937
  - 0.40401336550712585
  - 0.4059179723262787
  - 35768572.0
  - 132606608.0
  - 260027424.0
  - 1087690752.0
  - 0.39214059710502625
  - 0.41764920949935913
  - 0.469314306974411
  - 0.5989230275154114
  - 0.4243844151496887
  - 0.43940168619155884
  - 0.5347926616668701
  - 0.43352243304252625
  - 0.4528405964374542
  - 0.45008814334869385
  - 0.3967795968055725
  - 11861092.0
  - 57936712.0
  - 0.4329826235771179
  - 0.533439576625824
  - 0.415848970413208
  - 0.4138793349266052
  - 0.4806142747402191
  - 0.39424481987953186
  - 0.43290945887565613
  - 0.3911271095275879
  - 0.3986123502254486
  - 0.39135169982910156
  - 0.4176168441772461
  - 11046139.0
  - 107860304.0
  - 175766928.0
  - 325712160.0
  - 699325248.0
  - 1337930624.0
  - 1029330624.0
  - 0.5446209907531738
  - 0.4197005033493042
  - 0.4920710325241089
  - 0.39147573709487915
  - 0.4137025475502014
  - 0.40651896595954895
  - 20583812.0
  - 90119256.0
  - 235591424.0
  - 0.7995337843894958
  - 0.6593083739280701
  - 0.403431236743927
  - 0.3980337381362915
  - 0.39561665058135986
  - 0.4782034158706665
  - 0.4009697735309601
  - 0.4111207127571106
  - 0.3957973122596741
  - 25064022.0
  - 122484504.0
  - 0.508213222026825
  - 0.40487873554229736
  - 0.39459851384162903
  - 0.39300715923309326
  - 2446033.75
  - 81727880.0
  - 134309568.0
  - 0.3952927589416504
  - 0.4335467517375946
  - 0.48147815465927124
  - 0.4009383022785187
  - 0.5355395674705505
  - 0.4992089569568634
  - 0.42458561062812805
  - 42970404.0
  - 121322144.0
  - 314713888.0
  - 0.3923952281475067
  - 0.3935423195362091
  - 0.39998534321784973
  - 0.9129770994186401
  - 0.8559759855270386
  - 1.0507032871246338
  - 5262503.0
  - 167224016.0
  - 0.40089207887649536
  - 0.617093563079834
  - 0.40918782353401184
  - 0.39469948410987854
  - 0.3971860706806183
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 80 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:05:31.453983'
