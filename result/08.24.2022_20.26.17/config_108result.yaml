config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:54:33.757682'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_108fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_108fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_108fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_108fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_108fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_108fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_108fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_108fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_108fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_108fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 99.35352959483862
  - 76.2797970533371
  - 58.3712282627821
  - 82.5763083383441
  - 42.79575783610344
  - 44.82995193898678
  - 25.096446809172633
  - 25.778563645482066
  - 36.36252116262913
  - 43.733654296398164
  - 25.41900230050087
  - 19.219231945276263
  - 32.315086647868156
  - 16.016996464133264
  - 20.428366550803187
  - 16.803623563051225
  - 13.268493393063546
  - 13.42863439321518
  - 12.895850881934166
  - 10.038456827402115
  - 8.7652941852808
  - 9.438846382498742
  - 7.933075216412544
  - 9.85298957526684
  - 9.098020768165588
  - 7.6923410758376125
  - 7.6439706742763525
  - 7.7009197473526
  - 7.496416199207307
  - 8.368339902162552
  - 8.704328265786172
  - 8.679136577248574
  - 7.965812066197396
  - 9.060112303495407
  - 7.791346371173859
  - 7.131212750077248
  - 7.048450246453285
  - 7.4333172857761385
  - 7.115552210807801
  - 9.626654690504076
  - 6.864377593994141
  - 6.61559636592865
  - 6.474815383553505
  - 6.326784077286721
  - 6.538308262825012
  - 6.318155759572983
  - 6.536983591318131
  - 6.317873403429985
  - 6.534286600351334
  - 6.394238656759263
  - 6.566764238476754
  - 6.507196468114853
  - 7.067288130521774
  - 6.4612237304449085
  - 6.315301647782326
  validation_losses:
  - 4.278247356414795
  - 3.0795633792877197
  - 1.1253825426101685
  - 1.0927655696868896
  - 0.44518786668777466
  - 0.4903155267238617
  - 0.4510802924633026
  - 0.657464861869812
  - 0.433287650346756
  - 0.4679911136627197
  - 0.42818766832351685
  - 0.4056050479412079
  - 0.40616077184677124
  - 0.40654435753822327
  - 0.46082937717437744
  - 0.4238184094429016
  - 0.40332460403442383
  - 0.41394490003585815
  - 0.434323787689209
  - 0.4083375036716461
  - 0.4935861825942993
  - 0.47607046365737915
  - 0.40012127161026
  - 0.39840689301490784
  - 0.40117916464805603
  - 0.42118608951568604
  - 0.4457912743091583
  - 0.5565323829650879
  - 0.42154911160469055
  - 0.3944151699542999
  - 0.4299944043159485
  - 0.4122548997402191
  - 1.5518701076507568
  - 0.423353374004364
  - 0.4880012571811676
  - 0.39654484391212463
  - 0.5924173593521118
  - 0.4161415696144104
  - 0.48515668511390686
  - 0.4094908535480499
  - 0.45067381858825684
  - 0.4055185317993164
  - 0.4298444986343384
  - 0.41042542457580566
  - 0.4041648507118225
  - 0.43012940883636475
  - 0.39977940917015076
  - 0.42274197936058044
  - 0.47295212745666504
  - 0.43514999747276306
  - 0.4040045738220215
  - 0.4108530282974243
  - 0.4133933186531067
  - 0.41978392004966736
  - 0.42489996552467346
loss_records_fold1:
  train_losses:
  - 6.470261147618294
  - 6.311417734622956
  - 6.264569392800332
  - 6.594931709766389
  - 6.561292198300362
  - 6.854383373260498
  - 6.666899970173836
  - 6.195258295536042
  - 6.637678287923336
  - 6.409595933556557
  - 6.592764392495155
  - 6.511203172802926
  - 6.302407237887383
  - 6.516493093967438
  - 6.543496751785279
  - 6.617601430416108
  - 6.766507196426392
  - 6.821593797206879
  - 6.473112326860428
  - 6.962956109642983
  - 6.422250771522522
  - 9.129288867115974
  - 8.749963414669038
  - 9.218694990873336
  - 7.71759638786316
  - 7.17431366443634
  - 6.627562767267228
  - 6.611718255281449
  - 6.331149035692215
  - 6.3999081581830985
  - 6.677127784490586
  - 6.45857025384903
  - 6.597642654180527
  - 6.5079278528690345
  - 6.094695955514908
  - 6.418479418754578
  - 6.660800245404244
  - 7.082599577307701
  - 6.374547553062439
  - 6.520248600840569
  - 6.7848266661167145
  - 6.533375105261803
  - 6.478138855099679
  - 6.460249227285385
  - 6.268665716052055
  - 6.307065829634666
  - 6.199388271570206
  - 6.432553528249264
  - 6.512519425153733
  - 6.807355454564095
  - 7.379868951439858
  - 7.314516922831536
  - 6.362474489212037
  - 6.801946492493153
  - 6.332452237606049
  - 6.3954209923744205
  - 6.679922825098038
  - 6.547340399026871
  - 6.398519533872605
  - 6.583608046174049
  - 6.6830094590783125
  - 6.225467431545258
  - 6.310034868121147
  - 6.431368881464005
  - 8.763702061772348
  - 7.290586245059967
  - 6.307873851060868
  - 6.337336173653603
  - 6.525889745354653
  - 6.25391281247139
  validation_losses:
  - 0.43722212314605713
  - 0.4559321105480194
  - 0.4350142180919647
  - 0.4277503490447998
  - 0.4468679130077362
  - 0.4713199734687805
  - 0.4477302134037018
  - 0.47166356444358826
  - 0.47004398703575134
  - 0.4306008815765381
  - 0.44164177775382996
  - 0.42051199078559875
  - 0.5266996026039124
  - 0.4815607964992523
  - 0.4202069044113159
  - 1.6128937005996704
  - 0.42505863308906555
  - 0.41515544056892395
  - 0.8580984473228455
  - 0.7149903178215027
  - 77.14679718017578
  - 0.946931779384613
  - 0.4481075704097748
  - 0.4161075949668884
  - 0.4195403456687927
  - 0.4153243899345398
  - 0.5171055197715759
  - 1.0427778959274292
  - 0.4191955626010895
  - 0.4112521708011627
  - 0.43344566226005554
  - 0.455387145280838
  - 0.41900113224983215
  - 0.41605305671691895
  - 0.42008131742477417
  - 0.4726284146308899
  - 0.47673431038856506
  - 0.4074070155620575
  - 0.4098990857601166
  - 0.4429476261138916
  - 0.4704209566116333
  - 0.43332257866859436
  - 0.4102650582790375
  - 0.4232189953327179
  - 0.4077950716018677
  - 0.4143419861793518
  - 0.42930346727371216
  - 109.1263656616211
  - 841.8861694335938
  - 32.34434127807617
  - 24.40645408630371
  - 0.41697943210601807
  - 784.4339599609375
  - 289809.625
  - 718655.1875
  - 124914.0625
  - 4068167.25
  - 6220638.5
  - 649618.0625
  - 392515.1875
  - 49895964.0
  - 64055.3828125
  - 18890.271484375
  - 162932.609375
  - 0.9343841671943665
  - 0.46757203340530396
  - 0.46703997254371643
  - 0.47308456897735596
  - 0.4421672224998474
  - 0.42626717686653137
loss_records_fold2:
  train_losses:
  - 6.52326107621193
  - 6.519679355621339
  - 6.704048445820809
  - 6.9169494152069095
  - 9.09970431625843
  - 8.204033213853837
  - 7.576426294445992
  - 6.9654540687799456
  - 6.723623126745224
  - 6.949399313330651
  - 7.468842938542366
  - 6.510732227563858
  - 8.051797908544541
  - 6.918262884020805
  - 6.52199830710888
  - 6.406822609901429
  - 6.743938153982163
  - 6.9437679052352905
  - 6.240949028730393
  - 6.795226904749871
  - 6.727032351493836
  - 6.4234505057334905
  - 6.782553017139435
  - 6.323485594987869
  - 6.436642137169838
  - 6.491249835491181
  - 6.439637231826783
  - 6.405563566088677
  - 6.5513026237487795
  - 6.676877421140671
  - 6.231271275877953
  - 6.40770511329174
  - 6.474206456542015
  - 6.376274372637273
  - 6.77718999683857
  - 6.623992079496384
  - 6.3404270082712175
  - 6.198241317272187
  - 6.556738594174385
  - 6.468315303325653
  - 6.61472675204277
  - 6.327528941631318
  - 6.353729256987572
  - 6.420174872875214
  - 6.512605854868889
  - 6.465416240692139
  - 6.242678952217102
  - 6.31822994351387
  - 6.481091684103013
  - 6.6713970601558685
  - 6.661786201596261
  - 6.584022669494153
  - 6.541927990317345
  - 6.618550342321396
  - 6.322677311301232
  - 6.383931857347489
  - 6.372091892361642
  - 6.450142607092857
  - 6.367835310101509
  - 6.199338284134865
  - 6.5564135134220125
  - 6.633665001392365
  - 6.400173464417458
  - 6.508717688918114
  - 6.5061376899480825
  - 6.5273181140422825
  - 6.2696253657341
  - 6.5293539524078374
  - 6.52705836892128
  - 6.595303100347519
  - 6.587592855095863
  - 6.528396698832513
  - 6.336216759681702
  - 6.439871329069138
  - 6.476648980379105
  - 6.497411152720452
  - 6.619790643453598
  - 7.006458580493927
  - 6.746763083338738
  - 6.248490890860558
  - 6.4593424797058105
  - 6.333189728856087
  - 6.361916241049767
  - 6.385749444365501
  - 6.755171355605126
  - 6.748964503407478
  - 6.365881225466729
  - 6.210382755100728
  - 6.457322791218758
  - 6.5990122109651566
  - 6.42232620716095
  - 6.263945147395134
  - 6.6409326016902925
  - 6.396176582574845
  - 6.40487365424633
  - 6.3490001469850545
  - 6.546840944886208
  - 6.794766211509705
  - 6.950583636760712
  - 6.350083407759667
  validation_losses:
  - 0.4008983373641968
  - 0.3881324827671051
  - 0.39985620975494385
  - 11161.974609375
  - 0.5618667602539062
  - 0.44044163823127747
  - 0.40653106570243835
  - 0.40068918466567993
  - 0.3918232023715973
  - 0.5406379103660583
  - 1174.9202880859375
  - 21.39698028564453
  - 0.4134346842765808
  - 0.5017367601394653
  - 3.8002822399139404
  - 681.51513671875
  - 313.8322448730469
  - 1027.44384765625
  - 544.841064453125
  - 665.7926635742188
  - 6271.888671875
  - 3336.177734375
  - 4727.7529296875
  - 3705.3115234375
  - 17891.93359375
  - 6279.087890625
  - 60965.0703125
  - 6812.0654296875
  - 34442.9921875
  - 21602.115234375
  - 4506.70849609375
  - 50779.03515625
  - 22323.923828125
  - 151417.328125
  - 8926.2265625
  - 13255.1337890625
  - 9475.357421875
  - 45755.2578125
  - 233852.546875
  - 15741.087890625
  - 2843.650146484375
  - 1919.0528564453125
  - 360.7572937011719
  - 4509.50634765625
  - 564.6799926757812
  - 5837.30126953125
  - 3655.7373046875
  - 2068.536865234375
  - 3505.077392578125
  - 7188.80126953125
  - 3274.859375
  - 34140.3125
  - 17564.013671875
  - 3566.572509765625
  - 83.45148468017578
  - 39.306861877441406
  - 0.886365532875061
  - 4.55092716217041
  - 9.736573219299316
  - 365.8927917480469
  - 41.167179107666016
  - 8.680031776428223
  - 82.62288665771484
  - 393.9136047363281
  - 53.71653747558594
  - 53.655052185058594
  - 20.905902862548828
  - 349.3027648925781
  - 74.77650451660156
  - 85.58049774169922
  - 440.13201904296875
  - 828.3292846679688
  - 0.4462664723396301
  - 5.403761863708496
  - 993.5676879882812
  - 109.10095977783203
  - 231.24534606933594
  - 73.43329620361328
  - 177.77142333984375
  - 110.64741516113281
  - 999.1751708984375
  - 395.9247131347656
  - 409.7886047363281
  - 2605.516845703125
  - 460.3652648925781
  - 244.5948028564453
  - 159.74368286132812
  - 2605.165283203125
  - 1319.5341796875
  - 740.9451293945312
  - 689.4085693359375
  - 1020.6892700195312
  - 133.6886749267578
  - 390.796630859375
  - 272.0732421875
  - 1580.5596923828125
  - 291.619140625
  - 71.49275207519531
  - 194.30331420898438
  - 298.7107849121094
loss_records_fold3:
  train_losses:
  - 6.544814839959145
  - 6.199919110536576
  - 6.414568442106248
  - 6.323032826185226
  - 6.179542097449303
  - 6.385424557328225
  - 6.615735250711442
  - 6.2840984821319585
  - 6.334953480958939
  - 6.528568410873413
  - 6.4799181699752815
  - 6.446616160869599
  - 6.807686227560044
  - 6.346199774742127
  - 6.451049834489822
  - 6.579976460337639
  - 6.613161998987199
  - 6.446282914280892
  - 6.238266134262085
  - 6.4130110263824465
  - 6.390197992324829
  - 6.326582297682762
  - 6.618627846240997
  - 6.8265143811702735
  - 6.4143325090408325
  - 6.293219354748726
  - 6.407453355193138
  - 6.516250947117806
  - 6.53210836648941
  - 6.514057877659798
  - 6.370331302285194
  - 6.337238696217537
  - 6.429624027013779
  - 6.343727952241898
  - 6.301434367895126
  - 6.348866254091263
  - 6.3985859036445625
  - 6.3846167623996735
  - 6.455090329051018
  - 6.301510149240494
  - 6.599748715758324
  - 6.383549505472184
  - 6.408010774850846
  - 6.360945671796799
  - 6.346583899855614
  - 6.330828133225442
  - 6.304942710697651
  - 6.233128851652146
  - 6.433204379677773
  - 6.323601320385933
  - 6.864895814657212
  - 6.383682522177697
  - 6.278160491585732
  - 6.429542312026024
  - 6.3547952473163605
  - 6.408081918954849
  - 6.475134009122849
  - 6.3020044952631
  - 6.4304019749164585
  - 6.16774009168148
  - 6.363747283816338
  - 6.632429438829423
  - 6.454539066553116
  - 6.240564173460007
  - 6.6462461233139045
  - 6.4208578944206245
  - 6.416194072365761
  - 6.647871440649033
  - 6.317602041363717
  - 6.53833444416523
  - 6.407628008723259
  - 6.565731352567673
  - 6.7168754070997245
  - 6.664658764004708
  - 6.4017063111066825
  - 6.372663933038712
  - 6.313573125004769
  - 6.438745015859604
  - 6.54501346051693
  - 6.306137412786484
  - 6.282227540016175
  - 6.759943583607674
  - 6.6233723938465126
  - 6.602000895142556
  - 6.380772116780282
  - 15.380103805661202
  - 7.180109813809395
  - 6.601080626249313
  - 6.66136541068554
  - 6.855802264809609
  - 6.697495597600938
  - 6.482532641291619
  - 6.342996275424958
  - 6.799846509099007
  - 6.244611436128617
  - 6.332601842284203
  - 6.270732331275941
  - 6.443580457568169
  - 6.539762860536576
  - 6.49822154045105
  validation_losses:
  - 1562.0516357421875
  - 8663.2626953125
  - 1593.993408203125
  - 210.86453247070312
  - 1349.4803466796875
  - 4073.436279296875
  - 464.4316101074219
  - 6229.3037109375
  - 1073.4998779296875
  - 2140.453857421875
  - 3058.27099609375
  - 2333.869384765625
  - 1816.305419921875
  - 378.7382507324219
  - 276.856201171875
  - 12930.9755859375
  - 879.8292846679688
  - 1677.23779296875
  - 1333.357177734375
  - 1006.131591796875
  - 7230.13037109375
  - 6137.0634765625
  - 3296.738525390625
  - 8510.044921875
  - 2515.1982421875
  - 1095.7176513671875
  - 2097.537109375
  - 8180.48583984375
  - 308.8228454589844
  - 2361.88427734375
  - 23525.0234375
  - 2707.3935546875
  - 11256.234375
  - 17105.61328125
  - 828.1626586914062
  - 14744.5869140625
  - 17402.1953125
  - 896.4464111328125
  - 24128.970703125
  - 1160.50537109375
  - 15257.7705078125
  - 5847.546875
  - 11095.9072265625
  - 6698.17919921875
  - 1027.0455322265625
  - 6336.9638671875
  - 1450.8021240234375
  - 25052.109375
  - 16360.638671875
  - 3029.89892578125
  - 17285.4375
  - 1108.661865234375
  - 3898.646728515625
  - 4005.763427734375
  - 13103.2041015625
  - 2200.040771484375
  - 57229.06640625
  - 3269.551025390625
  - 710.0285034179688
  - 2751.083251953125
  - 7786.775390625
  - 100596.640625
  - 518501.78125
  - 0.4022813141345978
  - 0.4358499050140381
  - 0.4319448471069336
  - 0.4290030300617218
  - 0.414619505405426
  - 0.4311607778072357
  - 0.40494200587272644
  - 0.4153159260749817
  - 0.4858103394508362
  - 0.43259939551353455
  - 0.42505505681037903
  - 0.40345704555511475
  - 0.4526631236076355
  - 0.4115004539489746
  - 0.4136068522930145
  - 0.42646631598472595
  - 0.42042267322540283
  - 0.40552133321762085
  - 0.4403930604457855
  - 0.40385702252388
  - 0.40406641364097595
  - 0.42494645714759827
  - 21536.0390625
  - 13040466.0
  - 16819040.0
  - 65659.578125
  - 208749104.0
  - 648745408.0
  - 8431287808.0
  - 7629158400.0
  - 276042304.0
  - 11225373.0
  - 263953328.0
  - 22143260672.0
  - 622398528.0
  - 23005562880.0
  - 1961714176.0
loss_records_fold4:
  train_losses:
  - 6.584126257896424
  - 6.781626254320145
  - 6.363837018609047
  - 6.559112983942033
  - 6.396801808476448
  - 6.599397659301758
  - 6.3267829567193985
  - 6.4053661450743675
  - 6.591406786441803
  - 6.3139890998601915
  - 6.522369095683098
  - 14.640032941102982
  - 9.924411341547966
  - 6.4727654516696935
  - 6.778103801608086
  - 6.631152334809304
  - 6.648738983273507
  - 6.970677191019059
  - 6.768391701579095
  - 6.7837119653821
  - 6.497930371761322
  - 6.2685583382844925
  - 6.50245375931263
  - 6.615264701843262
  - 6.185197910666466
  - 6.4435731917619705
  - 7.067168709635735
  - 6.436171653866769
  - 6.549547818303108
  - 6.42147136926651
  - 6.476077122986317
  - 6.300655090808869
  - 6.2176358789205555
  - 6.466800287365913
  - 6.39717692732811
  - 6.651626303792
  - 6.769134277105332
  - 6.4167387127876285
  - 6.362143659591675
  - 6.395157763361931
  - 6.4333002209663395
  - 6.635623449087143
  - 6.366218394041062
  - 6.595147728919983
  - 6.652510365843773
  - 6.374965637922287
  - 6.388488370180131
  - 6.281908380985261
  - 6.339097759127617
  - 6.328299245238305
  - 6.498247641324998
  - 6.284026008844376
  - 6.425666126608849
  - 6.262801450490952
  - 6.312815240025521
  - 6.48068825006485
  - 6.453952881693841
  - 6.444720575213433
  - 6.630930361151695
  - 6.447745150327683
  - 6.393136605620384
  - 6.549252665042878
  - 6.2071374535560615
  - 6.302912765741349
  - 6.325732208788395
  - 6.569896483421326
  - 6.326079815626144
  - 6.6361793160438545
  - 6.299546051025391
  - 6.431155019998551
  - 6.43180679678917
  - 6.206598329544068
  - 6.484652429819107
  - 6.449801200628281
  - 6.223738870024682
  - 6.408102726936341
  - 6.505706560611725
  - 6.855541601777077
  - 6.34440322816372
  - 6.435527738928795
  - 6.41432087123394
  - 6.535550460219383
  - 6.594434776902199
  - 6.469681286811829
  - 6.670782554149628
  - 6.634509837627411
  - 6.231874236464501
  - 6.422704385221005
  - 6.635915663838387
  - 6.520368441939354
  - 6.803359439969063
  - 6.235442635416985
  - 6.65161900818348
  - 6.321683633327485
  - 6.897104805707932
  - 6.345019525289536
  - 6.490746510028839
  - 6.304127702116967
  - 6.548748737573624
  - 6.6774500697851185
  validation_losses:
  - 276067072.0
  - 3550188544.0
  - 1874148224.0
  - 24201435136.0
  - 3236772352.0
  - 21831002112.0
  - 3408187136.0
  - 33347524608.0
  - 19119933440.0
  - 31356663808.0
  - 42673332224.0
  - 0.5644734501838684
  - 0.8598172664642334
  - 1.2753469944000244
  - 1.7157261371612549
  - 22.394258499145508
  - 272.6604919433594
  - 34.49657440185547
  - 609.6382446289062
  - 39.12042236328125
  - 2679.99658203125
  - 6.181456089019775
  - 0.5145339369773865
  - 0.41169440746307373
  - 0.5005597472190857
  - 0.4307529330253601
  - 0.4328237771987915
  - 0.4014281630516052
  - 0.40795662999153137
  - 0.41818922758102417
  - 4.762449741363525
  - 5603.19189453125
  - 64612.42578125
  - 145953.65625
  - 46427.48046875
  - 149845.90625
  - 37.88255310058594
  - 63.0268669128418
  - 16.401798248291016
  - 31.84041404724121
  - 225.83338928222656
  - 944.5115966796875
  - 1565.2791748046875
  - 491.52288818359375
  - 87.75996398925781
  - 4.032444953918457
  - 0.4075871407985687
  - 68.65522766113281
  - 315.6916809082031
  - 0.4078434109687805
  - 17.943857192993164
  - 82.5263900756836
  - 316.58447265625
  - 74.03462982177734
  - 0.39543288946151733
  - 0.4280703663825989
  - 32.31418991088867
  - 6.70820426940918
  - 0.49109533429145813
  - 0.43115049600601196
  - 146.2841339111328
  - 0.43096932768821716
  - 0.4164421558380127
  - 1.2781307697296143
  - 0.4010109007358551
  - 0.46067437529563904
  - 0.40520328283309937
  - 7.959475994110107
  - 139.2023162841797
  - 295.3280944824219
  - 0.42578357458114624
  - 5302.80517578125
  - 7640510.0
  - 510781408.0
  - 5182072553472.0
  - 19150107836416.0
  - 68912058204160.0
  - 33272174215168.0
  - 50253508116480.0
  - 8479138381824.0
  - 5630414290944.0
  - 65017533693952.0
  - 27359549849600.0
  - 5293541949440.0
  - 14145983348736.0
  - 5615035351040.0
  - 50686490312704.0
  - 22182469632.0
  - 6884282282803200.0
  - 0.43309205770492554
  - 0.39120951294898987
  - 0.3952297866344452
  - 0.4429513216018677
  - 0.40962910652160645
  - 0.4127858281135559
  - 0.4289282262325287
  - 0.4064975678920746
  - 0.40589097142219543
  - 0.42148932814598083
  - 0.3999069929122925
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 55 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 70 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8593481989708405, 0.14065180102915953,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.24661654135338348, 0.0]'
  mean_eval_accuracy: 0.7145314259105351
  mean_f1_accuracy: 0.0493233082706767
  total_train_time: '0:43:37.896940'
