config:
  aggregation: mean
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:51:12.878356'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_19fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_19fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_19fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_19fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_19fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_19fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_19fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_19fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_19fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_19fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.0350150167942047
  - 0.8126245141029358
  - 0.8153228163719177
  - 0.8024632751941682
  - 0.8107205510139466
  - 0.7621834456920624
  - 0.7671042203903199
  - 0.8060281336307527
  - 0.7562474489212037
  - 0.7402311861515045
  - 0.7452467918395996
  - 0.7538508653640748
  - 0.8128355860710145
  - 0.7658457338809967
  - 0.8019290208816529
  - 0.7737163722515107
  - 0.754922103881836
  - 0.7447443664073945
  - 0.7431589543819428
  - 0.7501491010189056
  - 0.7641620337963104
  - 0.7557185888290405
  - 0.7638376653194427
  - 0.7851464331150055
  - 0.8384072661399842
  - 0.8078301310539246
  - 0.8204504787921906
  - 0.767297238111496
  - 0.7836280226707459
  - 0.7363541513681412
  - 0.7321701020002366
  - 0.7501634776592255
  - 0.7820647656917572
  - 0.7552439630031587
  - 0.7699457406997681
  - 0.7437312126159669
  - 0.7733897507190705
  - 0.7542778313159944
  - 0.7434626400470734
  - 0.7642947673797608
  - 0.7437156736850739
  - 0.757373034954071
  - 0.7541738152503967
  - 0.7602008402347565
  - 0.7575538158416748
  - 0.7177356868982315
  - 0.7241710811853409
  - 0.7143338024616241
  - 0.7680881440639497
  - 0.7297629714012146
  - 0.7695633590221406
  - 0.7394310235977173
  - 0.7018594384193421
  - 0.7347804546356201
  validation_losses:
  - 0.4057290256023407
  - 0.40224573016166687
  - 0.39667433500289917
  - 0.3944700062274933
  - 0.3901861608028412
  - 0.3892766237258911
  - 0.4944128394126892
  - 0.38468748331069946
  - 0.3841771185398102
  - 0.5483716726303101
  - 0.9220340251922607
  - 0.39233970642089844
  - 0.390288382768631
  - 0.3792870342731476
  - 0.43892744183540344
  - 1.079676628112793
  - 1.1190158128738403
  - 0.38553380966186523
  - 0.3859134912490845
  - 0.38700026273727417
  - 0.9272799491882324
  - 1.7550415992736816
  - 0.8276898860931396
  - 1.2122070789337158
  - 1.3533995151519775
  - 0.4064473807811737
  - 0.37552735209465027
  - 0.4735274910926819
  - 0.4606552720069885
  - 0.5933858752250671
  - 0.3905974328517914
  - 0.38539645075798035
  - 0.4143601655960083
  - 0.6423431038856506
  - 1.821207880973816
  - 0.3747892677783966
  - 0.4704396426677704
  - 0.39660096168518066
  - 0.4941791594028473
  - 0.8007901906967163
  - 0.45222896337509155
  - 0.9584131240844727
  - 0.44197598099708557
  - 0.6087321043014526
  - 0.4907688796520233
  - 0.3964991569519043
  - 0.39963433146476746
  - 0.6557084918022156
  - 0.5099467635154724
  - 0.3745897114276886
  - 0.37912511825561523
  - 0.38376322388648987
  - 0.37614017724990845
  - 0.3843197822570801
loss_records_fold1:
  train_losses:
  - 0.7604377746582032
  - 0.7623306632041932
  - 0.7266879677772522
  - 0.7177128106355668
  - 0.7243150502443314
  - 0.7407797873020172
  - 0.7540801167488098
  - 0.8228638708591461
  - 0.7313933849334717
  - 0.7502282977104188
  - 0.749348020553589
  - 0.7277192175388336
  - 0.7238222599029541
  - 0.755401211977005
  - 0.7352347075939178
  - 0.7708430707454682
  - 0.7452748417854309
  - 0.7246027588844299
  - 0.7394184648990632
  - 0.7556177735328675
  - 0.732767677307129
  - 0.7501612484455109
  - 0.7148148983716965
  - 0.7622688055038452
  - 0.733628523349762
  - 0.7208094477653504
  - 0.7466742634773255
  - 0.786410927772522
  - 0.734109377861023
  - 0.7136377692222595
  - 0.7552529990673066
  - 0.7522889912128449
  - 0.7537526845932008
  - 0.7578143954277039
  - 0.7497881948947906
  - 0.7055854856967927
  - 0.7473703384399415
  - 0.7332569479942322
  - 0.7245505750179291
  - 0.7242981195449829
  - 0.7135946154594421
  - 0.7400100708007813
  - 0.716971480846405
  - 0.8117145955562592
  - 0.728838586807251
  - 0.7461403369903565
  - 0.7419866919517517
  - 0.8047658741474152
  - 0.8077391266822815
  - 0.7662278950214386
  - 0.7467282295227051
  - 0.7635833203792572
  - 0.7960935294628144
  - 0.7446984648704529
  - 0.7252521514892578
  - 0.735497659444809
  - 0.7332650244235993
  - 0.7935371160507203
  - 0.7254555761814118
  - 0.7709074854850769
  - 0.8178722858428955
  - 0.7316346049308777
  - 0.7684417605400086
  - 0.7702158451080323
  - 0.7275748014450074
  - 0.7476208806037903
  - 0.7184343636035919
  - 0.7417788147926331
  - 0.7663089394569398
  - 0.7215383708477021
  - 0.7397770762443543
  - 0.70435009598732
  - 0.7453048169612885
  - 0.7437621414661408
  - 0.7295198798179627
  - 0.7107564568519593
  - 0.7195582449436189
  - 0.7238093018531799
  - 0.7453121006488801
  - 0.7051331043243408
  - 0.7638890743255615
  - 0.7305720627307892
  - 0.7171763181686401
  - 0.7413490533828736
  - 0.7739171445369721
  - 0.7226121962070465
  - 0.7389268457889557
  - 0.7547806978225708
  - 0.7515135765075684
  - 0.7252848565578461
  - 0.7689737796783448
  - 0.7663386166095734
  - 0.7441544950008393
  - 0.789190411567688
  - 0.7366748452186584
  - 0.7089568495750428
  validation_losses:
  - 0.4106513559818268
  - 0.5854828357696533
  - 0.37919390201568604
  - 0.38278114795684814
  - 0.38269612193107605
  - 0.39973530173301697
  - 0.4053536355495453
  - 0.46493810415267944
  - 0.47908368706703186
  - 0.4012758433818817
  - 0.44151929020881653
  - 0.4945041835308075
  - 0.4511931538581848
  - 0.45452064275741577
  - 0.43129438161849976
  - 0.40323692560195923
  - 0.4263134002685547
  - 0.5953687429428101
  - 0.4448947310447693
  - 0.5129023790359497
  - 0.4345422089099884
  - 0.39162591099739075
  - 0.45148950815200806
  - 0.5369974970817566
  - 0.6454847455024719
  - 0.4881204664707184
  - 0.5556648373603821
  - 0.5095033645629883
  - 0.45795390009880066
  - 0.585895299911499
  - 0.46504026651382446
  - 0.4809773564338684
  - 0.38465288281440735
  - 0.4413146674633026
  - 0.45077547430992126
  - 0.572657585144043
  - 0.5830368399620056
  - 0.7204645872116089
  - 0.5809948444366455
  - 0.6112197637557983
  - 0.5565574169158936
  - 0.4433891773223877
  - 0.39556410908699036
  - 0.41221973299980164
  - 0.3866439759731293
  - 0.40899720788002014
  - 0.3826506435871124
  - 0.40862271189689636
  - 0.4077451825141907
  - 0.4221780002117157
  - 0.4337787926197052
  - 0.4405784606933594
  - 0.5747575759887695
  - 0.4044870138168335
  - 0.4042130410671234
  - 0.4173123836517334
  - 0.4332717955112457
  - 0.4034981429576874
  - 0.39628463983535767
  - 0.3950377106666565
  - 0.4179782569408417
  - 0.3913687467575073
  - 0.3907216489315033
  - 0.4054284691810608
  - 0.45221027731895447
  - 0.4190399944782257
  - 0.47394123673439026
  - 0.4684540629386902
  - 0.467243492603302
  - 0.4912950098514557
  - 0.5137104392051697
  - 0.48917868733406067
  - 0.3994297385215759
  - 0.41546496748924255
  - 0.4820508658885956
  - 0.43014344573020935
  - 0.5370811223983765
  - 0.4394109845161438
  - 0.5474470257759094
  - 0.6749019622802734
  - 0.5449288487434387
  - 0.4297615885734558
  - 0.46929267048835754
  - 0.45628249645233154
  - 0.5489212870597839
  - 0.4669147729873657
  - 0.5403192639350891
  - 0.5159314870834351
  - 0.39831483364105225
  - 0.4136221408843994
  - 0.40097880363464355
  - 0.3913484811782837
  - 0.4008709192276001
  - 0.38752561807632446
  - 0.38285699486732483
  - 0.38556623458862305
loss_records_fold2:
  train_losses:
  - 0.7215909838676453
  - 0.7513400912284851
  - 0.7338397443294525
  - 0.7125428676605225
  - 0.7477407097816468
  - 0.6963795095682145
  - 0.729581081867218
  - 0.7019214332103729
  - 0.7033730536699295
  - 0.7458692789077759
  - 0.7880809903144836
  - 0.713171410560608
  - 0.7357267856597901
  - 0.75242338180542
  - 0.7379138469696045
  - 0.6958186775445938
  - 0.7196206033229828
  - 0.7237187266349793
  - 0.7267609059810639
  - 0.7202526092529298
  - 0.7222408175468445
  - 0.7125824689865112
  - 0.7299910008907319
  - 0.708970981836319
  - 0.6903375715017319
  - 0.7503730952739716
  - 0.7362840831279756
  - 0.7172585725784302
  - 0.7814977943897248
  - 0.7301370918750764
  - 0.7368853867053986
  - 0.7505546748638153
  - 0.7470595061779023
  - 0.7718296051025391
  - 0.7383738160133362
  - 0.6972747087478638
  - 0.7156730592250824
  - 0.7240717470645905
  - 0.7140526056289673
  - 0.7171074748039246
  - 0.7398967921733857
  - 0.6960288912057877
  - 0.7025408387184143
  - 0.7558521389961244
  - 0.7156097829341889
  - 0.6968334436416627
  - 0.7140290796756745
  - 0.7486013054847718
  - 0.8270467817783356
  - 0.779607105255127
  - 0.7834466993808746
  - 0.7259986698627472
  - 0.7492227733135224
  - 0.8327492117881775
  - 0.7460674822330475
  validation_losses:
  - 0.3825812041759491
  - 0.4026632010936737
  - 0.3997264504432678
  - 0.40846681594848633
  - 0.42979148030281067
  - 0.4079648554325104
  - 0.47366392612457275
  - 0.4519515335559845
  - 0.4751880168914795
  - 0.5347920656204224
  - 0.5032442212104797
  - 0.5911401510238647
  - 0.5512383580207825
  - 0.654839038848877
  - 0.7117035388946533
  - 0.437715619802475
  - 0.5010185837745667
  - 0.4468752443790436
  - 0.5054864883422852
  - 0.46062973141670227
  - 0.47940918803215027
  - 0.4637974798679352
  - 0.7040019631385803
  - 0.5237804055213928
  - 0.9233742356300354
  - 0.637079119682312
  - 0.41370171308517456
  - 0.5078765749931335
  - 0.4367372691631317
  - 0.41946136951446533
  - 0.4921743869781494
  - 0.5431453585624695
  - 0.42707785964012146
  - 0.3967740535736084
  - 0.42077094316482544
  - 0.4692622721195221
  - 0.4875306189060211
  - 0.42027753591537476
  - 0.48571261763572693
  - 0.4220010042190552
  - 0.4636305570602417
  - 0.47856849431991577
  - 0.45162662863731384
  - 0.4475467801094055
  - 0.37622788548469543
  - 0.4971214532852173
  - 0.49978435039520264
  - 0.6594366431236267
  - 0.8986459374427795
  - 0.509506344795227
  - 0.5025877952575684
  - 0.40963175892829895
  - 0.40746673941612244
  - 0.4140320122241974
  - 0.4152543544769287
loss_records_fold3:
  train_losses:
  - 0.7945378720760345
  - 0.7579367160797119
  - 0.7487494468688966
  - 0.7654165327548981
  - 0.7831445813179017
  - 0.7480534076690675
  - 0.7084153652191163
  - 0.7547688305377961
  - 0.7563699185848236
  - 0.7655471444129944
  - 0.7678180754184724
  - 0.7770935535430908
  validation_losses:
  - 0.531254768371582
  - 0.4987852871417999
  - 0.4073279798030853
  - 0.43695080280303955
  - 0.3665207326412201
  - 0.3971744179725647
  - 0.3658261001110077
  - 0.36488938331604004
  - 0.3617405295372009
  - 0.36400550603866577
  - 0.3694659173488617
  - 0.36434224247932434
loss_records_fold4:
  train_losses:
  - 0.7388318777084351
  - 0.7941186785697938
  - 0.7835175752639771
  - 0.7418717682361603
  - 0.724719923734665
  - 0.7579533815383912
  - 0.7513576149940491
  - 0.7337699353694916
  - 0.7372795581817627
  - 0.7096350312232972
  - 0.7459430634975434
  - 0.7354992151260377
  - 0.7391684591770172
  - 0.7561389744281769
  - 0.7665958881378174
  - 0.7493787348270416
  - 0.7318774253129959
  - 0.7447591364383698
  validation_losses:
  - 0.39894402027130127
  - 0.4232538640499115
  - 0.4064231216907501
  - 0.3936205804347992
  - 0.36639097332954407
  - 0.3844880163669586
  - 0.354807049036026
  - 0.3518645465373993
  - 0.35500550270080566
  - 0.35255736112594604
  - 0.40939053893089294
  - 0.8166846036911011
  - 0.4804762601852417
  - 0.42447876930236816
  - 0.410688191652298
  - 0.39767971634864807
  - 0.38705533742904663
  - 0.375246524810791
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 54 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 96 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 55 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8524871355060034, 0.8490566037735849, 0.8593481989708405,
    0.8556701030927835]'
  fold_eval_f1: '[0.0, 0.0, 0.04347826086956522, 0.0, 0.023255813953488372]'
  mean_eval_accuracy: 0.8544959417163268
  mean_f1_accuracy: 0.01334681496461072
  total_train_time: '0:18:58.609799'
