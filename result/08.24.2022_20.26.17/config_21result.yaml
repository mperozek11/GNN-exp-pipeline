config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:52:18.454237'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_21fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_21fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_21fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_21fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_21fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_21fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_21fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_21fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_21fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_21fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.6876916468143466
  - 3.2028073549270633
  - 3.1857156813144685
  - 3.128571879863739
  - 3.2296864807605745
  - 3.1073822498321535
  - 3.1521350324153903
  - 3.2657800376415254
  - 3.180582672357559
  - 3.0929693937301637
  - 3.1018608629703524
  - 3.1182139754295353
  - 3.1059867382049564
  - 3.07443745136261
  - 3.1425406575202945
  - 3.1411033272743225
  - 3.0916910767555237
  - 3.180759292840958
  validation_losses:
  - 0.4303751587867737
  - 0.40376409888267517
  - 0.41098642349243164
  - 0.4031703770160675
  - 0.42388713359832764
  - 0.39525583386421204
  - 0.4306153655052185
  - 0.4050161838531494
  - 0.3925938308238983
  - 0.43119296431541443
  - 0.39056533575057983
  - 0.41917192935943604
  - 0.41074860095977783
  - 0.4009261429309845
  - 0.39911359548568726
  - 0.3964128792285919
  - 0.38821151852607727
  - 0.38891398906707764
loss_records_fold1:
  train_losses:
  - 2.9906184643507006
  - 2.9597021371126178
  - 3.1365789234638215
  - 3.126704639196396
  - 3.104171109199524
  - 3.0382534682750704
  - 3.0848194777965547
  - 3.058675229549408
  - 3.0025361239910127
  - 3.052146923542023
  - 2.996102583408356
  validation_losses:
  - 0.41555655002593994
  - 0.407185435295105
  - 0.39908522367477417
  - 0.4004344046115875
  - 0.40248173475265503
  - 0.3993162512779236
  - 0.40868598222732544
  - 0.4057864546775818
  - 0.39658865332603455
  - 0.3951537609100342
  - 0.38850080966949463
loss_records_fold2:
  train_losses:
  - 3.0484601289033892
  - 3.048662668466568
  - 2.9922304987907413
  - 3.097159004211426
  - 3.143482339382172
  - 3.121254575252533
  - 3.1245519787073137
  - 2.991445815563202
  - 3.038943421840668
  - 3.1126352429389956
  - 3.1195105850696567
  - 3.035308611392975
  - 3.0634216725826264
  - 3.1076958179473877
  - 3.0750270545482636
  - 3.0364509344100954
  - 3.012424647808075
  - 2.9922672629356386
  validation_losses:
  - 0.38713499903678894
  - 0.40389883518218994
  - 0.4126235842704773
  - 0.39441001415252686
  - 0.4007924795150757
  - 0.39078959822654724
  - 0.38734135031700134
  - 0.38804489374160767
  - 0.3941713869571686
  - 0.40698081254959106
  - 0.3949531018733978
  - 0.4077736437320709
  - 0.38633909821510315
  - 0.39120760560035706
  - 0.3929484486579895
  - 0.39218953251838684
  - 0.39305371046066284
  - 0.3928700089454651
loss_records_fold3:
  train_losses:
  - 2.9934227913618088
  - 2.9836878538131715
  - 3.0394517093896867
  - 3.0575560569763187
  - 3.0442567825317384
  - 3.1218292236328127
  - 3.008583623170853
  - 3.031706738471985
  - 3.143504595756531
  - 3.0138959228992466
  - 3.1008198618888856
  - 3.066586595773697
  - 3.0396485805511477
  - 2.9660839796066285
  - 2.96050922870636
  - 3.0221966385841372
  - 3.005338889360428
  - 3.0104890763759613
  - 3.0330741047859195
  - 3.0238168358802797
  - 3.036798483133316
  - 3.0253216147422792
  - 2.9262172818183902
  - 2.9586107313632968
  - 3.0596035897731784
  - 3.0233430445194247
  - 2.9344334423542024
  - 2.9387188047170643
  - 2.9616591513156894
  - 3.000061529874802
  - 3.037533462047577
  - 2.9902298092842106
  - 2.9468345761299135
  - 3.0114735662937164
  - 3.0371021866798404
  - 2.948241186141968
  - 2.982716122269631
  - 3.0512013256549837
  - 2.903252750635147
  - 2.9210264086723328
  - 3.0403090655803684
  - 3.089769572019577
  - 3.0399981111288072
  - 2.975491625070572
  - 3.003990882635117
  - 3.0141409039497375
  - 2.892683929204941
  - 2.9718054711818698
  - 3.0718685895204545
  - 3.0150919914245606
  - 2.9847500920295715
  - 2.953478527069092
  - 3.0422624349594116
  - 2.9679147601127625
  - 3.019382351636887
  - 3.0632963836193086
  - 2.9407602667808534
  - 3.0023626863956454
  - 2.9665787518024445
  - 3.0511420249938968
  - 3.019832867383957
  - 2.983546817302704
  - 2.9872203409671787
  - 2.958654046058655
  - 2.9227367818355563
  - 2.910290497541428
  - 2.8948653101921082
  - 2.8710532367229464
  - 2.962168735265732
  - 2.927406066656113
  - 2.9651217818260194
  - 2.952862560749054
  - 2.958998566865921
  - 2.9421204030513763
  - 2.9722803175449375
  - 2.980490452051163
  - 2.9801840782165527
  - 2.96503176689148
  - 2.932358738780022
  - 2.94273399412632
  - 2.9463449627161027
  - 2.892750632762909
  - 2.859086954593659
  - 2.9561879277229313
  - 2.9823968887329104
  - 2.9286957859992984
  - 2.8594033777713777
  - 2.9309766054153443
  - 2.908106726408005
  - 2.9245974600315097
  - 2.9511438071727754
  - 2.9049159049987794
  - 2.919296139478684
  - 2.9665362000465394
  - 2.96452594101429
  - 2.955800193548203
  - 2.9211311221122744
  - 2.9115191400051117
  - 2.928999584913254
  - 2.9351507008075717
  validation_losses:
  - 0.39365142583847046
  - 0.3925653100013733
  - 0.3974112868309021
  - 0.393501877784729
  - 0.5360922813415527
  - 0.471794456243515
  - 1.0083098411560059
  - 0.9266452789306641
  - 0.816813051700592
  - 1.390395164489746
  - 0.3781980872154236
  - 0.38844576478004456
  - 0.541398823261261
  - 0.3787105977535248
  - 0.9749770760536194
  - 2.0049917697906494
  - 0.7422540783882141
  - 0.9071907997131348
  - 1.4104492664337158
  - 0.9873255491256714
  - 0.6924258470535278
  - 0.7501557469367981
  - 0.3913717567920685
  - 0.7581659555435181
  - 0.4310207962989807
  - 0.5824295282363892
  - 1.1425793170928955
  - 0.9028523564338684
  - 1.3949618339538574
  - 0.5668121576309204
  - 0.6712138652801514
  - 0.565183162689209
  - 0.7732672691345215
  - 0.38146597146987915
  - 0.4136003255844116
  - 0.7969561815261841
  - 0.6087827682495117
  - 0.9263004660606384
  - 0.9371426105499268
  - 0.7570505738258362
  - 0.46115103363990784
  - 0.45356592535972595
  - 0.43548154830932617
  - 0.5093277096748352
  - 1.2213170528411865
  - 0.6054107546806335
  - 1.2339733839035034
  - 1.1301889419555664
  - 0.401941180229187
  - 0.7377167344093323
  - 0.5273579955101013
  - 0.41789039969444275
  - 0.46453744173049927
  - 0.41281017661094666
  - 0.4458245038986206
  - 0.4349713921546936
  - 0.42907440662384033
  - 0.45255246758461
  - 0.4337620437145233
  - 0.436065673828125
  - 0.42323553562164307
  - 0.7976052165031433
  - 0.5620064735412598
  - 0.4336562752723694
  - 0.48245716094970703
  - 0.48713961243629456
  - 0.5240685343742371
  - 0.5603674650192261
  - 0.42421361804008484
  - 0.5470583438873291
  - 0.5472173690795898
  - 0.5089370608329773
  - 0.5566771626472473
  - 0.5497792959213257
  - 0.48047176003456116
  - 1.217085599899292
  - 1.7154523134231567
  - 3.523343801498413
  - 1.2653727531433105
  - 0.4517582654953003
  - 0.7699311971664429
  - 1.389050841331482
  - 1.7610142230987549
  - 1.674826979637146
  - 0.5340116620063782
  - 0.5515705347061157
  - 0.8756609559059143
  - 0.5228509902954102
  - 0.4186604619026184
  - 0.5889434218406677
  - 0.9480862021446228
  - 0.5182897448539734
  - 0.44354841113090515
  - 0.4818892180919647
  - 0.798255205154419
  - 0.4873444736003876
  - 0.7866466045379639
  - 0.511553168296814
  - 0.5108315944671631
  - 0.4954592287540436
loss_records_fold4:
  train_losses:
  - 2.9875345289707185
  - 2.989174628257752
  - 2.8898466706275943
  - 2.966790309548378
  - 2.862270933389664
  - 3.016800665855408
  - 2.9438040584325793
  - 2.9908012449741364
  - 3.0224090218544006
  - 2.961280584335327
  - 2.8282661497592927
  - 2.9244269251823427
  - 2.9328012406826023
  - 2.903508478403092
  - 2.9631657719612123
  - 2.92103950381279
  - 2.93232256770134
  - 2.902636700868607
  - 2.9272897839546204
  - 2.8655042439699177
  - 2.9634557306766514
  - 2.9566492587327957
  - 2.9773531317710877
  - 2.9823415815830234
  - 2.9959708690643314
  - 2.900389164686203
  - 2.9487109005451204
  - 2.9303264498710635
  - 2.8852333009243014
  - 2.973415517807007
  - 2.892706435918808
  - 2.92242723107338
  - 2.9772901713848117
  - 2.956845480203629
  - 2.9758422821760178
  - 2.975384068489075
  - 2.8682038247585298
  - 2.9420067310333256
  - 2.9467539846897126
  - 2.976892626285553
  - 2.895948898792267
  - 2.8686064541339875
  - 2.9101888656616213
  - 2.95925555229187
  - 2.935878059267998
  - 2.909058052301407
  - 2.913425821065903
  - 2.98814714550972
  - 2.893716907501221
  - 2.8988570034503938
  - 2.916612297296524
  - 2.956232833862305
  - 2.9096726655960086
  - 2.9422762393951416
  - 2.9558449119329455
  - 2.944547984004021
  - 2.9647881865501406
  - 2.927468174695969
  - 2.8581160694360737
  - 2.903224352002144
  - 2.9159022092819216
  - 2.966060119867325
  - 2.9189749598503116
  - 2.9326765507459642
  - 2.937928229570389
  - 2.88415818810463
  - 2.920906525850296
  - 2.8908869415521625
  - 2.9286876648664477
  - 2.9639944940805436
  - 2.891880488395691
  - 2.96942897439003
  - 2.9630097031593325
  - 2.9834267079830172
  - 2.908361887931824
  - 2.932208240032196
  - 2.8584447324275972
  - 2.949219518899918
  - 2.9085150837898257
  - 2.8656294614076616
  - 2.9326993823051453
  - 2.897367918491364
  - 2.876797080039978
  - 2.9206079840660095
  - 2.881508702039719
  - 2.8486536145210266
  - 2.8089347392320634
  - 2.8344658702611927
  - 2.90656718313694
  - 2.8634332031011582
  - 2.9158387124538425
  - 2.9685094296932224
  - 2.9559807598590853
  - 2.9184901237487795
  - 2.8828720986843113
  - 2.9015786528587344
  - 2.8241536855697635
  - 2.908826017379761
  - 2.972693818807602
  - 2.962571102380753
  validation_losses:
  - 0.6737453937530518
  - 0.4409312307834625
  - 0.45292940735816956
  - 0.5712941884994507
  - 0.4877282679080963
  - 0.5128116011619568
  - 0.5275437235832214
  - 0.4726877212524414
  - 0.3849126398563385
  - 0.4588347375392914
  - 0.4067268967628479
  - 0.49311840534210205
  - 0.4685034453868866
  - 0.5090286135673523
  - 0.49718374013900757
  - 0.4127621650695801
  - 0.5200328826904297
  - 0.4527702331542969
  - 0.5475468039512634
  - 0.4756372272968292
  - 0.3791099190711975
  - 0.44820427894592285
  - 0.4073921740055084
  - 0.43535545468330383
  - 0.43954163789749146
  - 0.4924113154411316
  - 0.494295209646225
  - 0.4675729274749756
  - 0.49327361583709717
  - 0.37891075015068054
  - 0.4915033280849457
  - 0.4758460223674774
  - 0.4288348853588104
  - 0.4535568654537201
  - 0.5324879884719849
  - 0.44226011633872986
  - 0.43724459409713745
  - 0.5035145878791809
  - 0.45896655321121216
  - 0.41479116678237915
  - 0.47570011019706726
  - 0.4629836976528168
  - 0.4625092148780823
  - 1.3997852802276611
  - 0.42852044105529785
  - 0.398183137178421
  - 0.468670517206192
  - 0.4070354998111725
  - 0.5058313608169556
  - 0.4973747730255127
  - 0.4602315127849579
  - 0.43667072057724
  - 0.38341546058654785
  - 0.5654104948043823
  - 0.38943156599998474
  - 0.5168895721435547
  - 0.3986717462539673
  - 0.377836138010025
  - 0.5426328778266907
  - 0.4824710786342621
  - 0.4434264004230499
  - 0.43047088384628296
  - 0.4505632221698761
  - 0.4755789041519165
  - 0.4363051950931549
  - 0.528805136680603
  - 0.46722182631492615
  - 0.45546573400497437
  - 0.5035887360572815
  - 0.4637537896633148
  - 0.48226314783096313
  - 0.47076278924942017
  - 0.47641709446907043
  - 0.5058647394180298
  - 0.4680854380130768
  - 0.4698304831981659
  - 0.456636518239975
  - 0.5228074789047241
  - 0.5355857014656067
  - 0.477267861366272
  - 0.5340234637260437
  - 0.5712645053863525
  - 0.5441032648086548
  - 0.7615390419960022
  - 0.5360584855079651
  - 0.49542146921157837
  - 0.6024724841117859
  - 0.5260657668113708
  - 0.4861246943473816
  - 0.539232075214386
  - 0.3626689016819
  - 0.5907685160636902
  - 0.5291513204574585
  - 0.47978946566581726
  - 0.36994993686676025
  - 0.4646342098712921
  - 0.5575217604637146
  - 0.49272507429122925
  - 0.5053758025169373
  - 0.5427793860435486
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8353344768439108,
    0.7989690721649485]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.18644067796610173, 0.23529411764705882]'
  mean_eval_accuracy: 0.8417835228377923
  mean_f1_accuracy: 0.0843469591226321
  total_train_time: '0:22:08.029494'
