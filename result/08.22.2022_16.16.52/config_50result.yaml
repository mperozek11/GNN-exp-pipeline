config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:13:00.471130'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_50fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_50fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_50fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_50fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_50fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_50fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_50fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_50fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_50fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_50fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.7334449887275696
  - 1.539227479696274
  - 1.5987289547920227
  - 1.5035375893115999
  - 1.4627297937870027
  - 1.4977824330329896
  - 1.5185826122760773
  - 1.4596469432115555
  - 1.4392611116170884
  - 1.484648358821869
  - 1.5097173094749452
  - 1.4841485381126405
  - 1.5040332794189455
  - 1.4807160913944246
  - 1.4204851865768433
  - 1.4527896225452424
  - 1.5396578431129457
  - 1.4503381848335266
  - 1.4526160061359406
  - 1.4473831295967103
  - 1.4856392681598665
  - 1.4739787101745607
  - 1.461192923784256
  - 1.541502094268799
  - 1.4825055062770844
  - 1.4295568749308587
  - 1.4192972987890244
  - 1.4785355389118195
  - 1.5020865321159365
  - 1.4328594952821732
  - 1.4219836533069612
  - 1.463675493001938
  - 1.4510814845561981
  - 1.4723284959793093
  - 1.4329601585865022
  - 1.4663359344005586
  - 1.4477556943893433
  - 1.4794856786727906
  - 1.4137593746185304
  - 1.424298244714737
  - 1.4592710852622988
  - 1.4315986216068268
  - 1.4252159297466278
  - 1.4112734705209733
  - 1.4330420911312105
  - 1.4423985183238983
  - 1.4262890458106996
  - 1.4667265176773072
  - 1.4430562436580658
  - 1.444204252958298
  - 1.463879644870758
  validation_losses:
  - 0.4066357910633087
  - 0.4167843461036682
  - 0.39685118198394775
  - 0.395515114068985
  - 0.47542399168014526
  - 0.4083554148674011
  - 0.547207236289978
  - 0.41330063343048096
  - 0.3943943381309509
  - 0.3839394152164459
  - 0.40023842453956604
  - 0.38771677017211914
  - 0.3888110816478729
  - 0.3870368003845215
  - 0.38372933864593506
  - 0.3966151177883148
  - 0.3898732662200928
  - 0.3829042613506317
  - 0.3823152780532837
  - 0.39236050844192505
  - 0.4423372745513916
  - 0.4278901219367981
  - 0.38606375455856323
  - 0.3821483552455902
  - 0.3804672658443451
  - 0.41901201009750366
  - 0.5236811637878418
  - 0.38693881034851074
  - 0.3824319541454315
  - 0.3948034942150116
  - 0.39778199791908264
  - 0.38433608412742615
  - 0.6127849221229553
  - 0.3840450942516327
  - 0.3818102180957794
  - 0.37858259677886963
  - 0.380119264125824
  - 0.4064773619174957
  - 0.38520708680152893
  - 0.4096001088619232
  - 0.676816463470459
  - 0.37549057602882385
  - 0.3751397728919983
  - 0.3798452913761139
  - 1.021882176399231
  - 0.6091176867485046
  - 0.40487149357795715
  - 0.41202041506767273
  - 0.3867819309234619
  - 0.38179850578308105
  - 0.3778693377971649
loss_records_fold1:
  train_losses:
  - 1.4144075453281404
  - 1.4378018349409105
  - 1.4069636464118958
  - 1.4164153814315796
  - 1.4030569434165956
  - 1.4027064561843874
  - 1.446451222896576
  - 1.4309144735336305
  - 1.435768187046051
  - 1.4011043787002564
  - 1.4219915270805359
  - 1.4310743033885958
  - 1.4073093831539154
  - 1.441477930545807
  - 1.4775091290473938
  - 1.4188803136348724
  - 1.4385732948780061
  - 1.4301270067691805
  - 1.4214271903038025
  - 1.4225843489170076
  - 1.4323935687541962
  - 1.3838438749313355
  - 1.466290181875229
  - 1.414649260044098
  - 1.4483306288719178
  - 1.4780874609947205
  - 1.5031374514102938
  - 1.4115079939365387
  - 1.3970372080802917
  - 1.5373804032802583
  - 1.4682930648326875
  - 1.4194435834884644
  - 1.4538095891475677
  - 1.4037024199962618
  - 1.4149238407611848
  - 1.4659441113471985
  - 1.3937042206525803
  - 1.409818094968796
  - 1.4224902629852296
  - 1.4120188117027284
  - 1.3721288323402405
  - 1.3772254168987275
  - 1.3971084743738176
  - 1.3976336121559143
  - 1.381593042612076
  - 1.3794412255287172
  - 1.374109572172165
  - 1.4091671943664552
  - 1.4406481742858888
  - 1.424966323375702
  - 1.4458441317081452
  - 1.3739064693450929
  - 1.408337724208832
  - 1.413429617881775
  - 1.3826752960681916
  - 1.380761379003525
  - 1.4005207270383835
  - 1.3483321338891985
  - 1.378504377603531
  - 1.3487395524978638
  - 1.3976619660854341
  - 1.409729427099228
  - 1.4116562604904175
  - 1.3987600147724153
  - 1.3774359822273254
  - 1.3696440100669862
  - 1.3849237382411959
  - 1.4081253826618196
  - 1.3510978162288667
  - 1.34546617269516
  - 1.3347403585910798
  - 1.3257527202367783
  - 1.496661427617073
  - 1.3882771730422974
  - 1.3703476876020433
  - 1.3665023714303972
  - 1.4393400996923447
  - 1.3726237773895265
  - 1.354787251353264
  - 1.3611189424991608
  - 1.3582777082920074
  - 1.3398129463195803
  - 1.3616483867168427
  - 1.332857745885849
  - 1.3921875834465027
  - 1.3508653581142427
  - 1.3394010037183763
  - 1.3377351760864258
  - 1.370866060256958
  - 1.333539927005768
  - 1.3252711057662965
  - 1.373025941848755
  - 1.4286616921424866
  - 1.3679960072040558
  - 1.3480035603046419
  - 1.3651053309440613
  - 1.3609138280153275
  - 1.364208871126175
  - 1.3621680259704592
  - 1.377417540550232
  validation_losses:
  - 0.38257989287376404
  - 0.3852308988571167
  - 0.3841334581375122
  - 0.3848206400871277
  - 0.38194823265075684
  - 0.3831513524055481
  - 0.39825522899627686
  - 0.39850175380706787
  - 0.3867824673652649
  - 0.3972530961036682
  - 0.4125761091709137
  - 0.4020868241786957
  - 0.39259591698646545
  - 0.41869625449180603
  - 0.39694032073020935
  - 0.3796882927417755
  - 0.38754212856292725
  - 0.4124654531478882
  - 0.40567153692245483
  - 0.4142494797706604
  - 0.5104765892028809
  - 0.4134554862976074
  - 0.5910864472389221
  - 0.7298853993415833
  - 0.7393670678138733
  - 0.4216387867927551
  - 0.3968782424926758
  - 0.41323554515838623
  - 0.40089672803878784
  - 0.4304269552230835
  - 0.42994433641433716
  - 0.3943812847137451
  - 0.4903946816921234
  - 1.0600652694702148
  - 0.8729534149169922
  - 0.9961249828338623
  - 0.38578611612319946
  - 0.4635937511920929
  - 0.44545814394950867
  - 0.896308422088623
  - 0.7391949892044067
  - 0.4641004502773285
  - 0.8082154989242554
  - 0.5744594931602478
  - 0.6551515460014343
  - 0.5680471658706665
  - 0.6084233522415161
  - 0.6591248512268066
  - 0.6924282312393188
  - 0.4865572154521942
  - 0.538267970085144
  - 0.42072606086730957
  - 0.5351896286010742
  - 0.38391417264938354
  - 0.391880065202713
  - 0.399181067943573
  - 0.4679785370826721
  - 0.4327155649662018
  - 0.5258792042732239
  - 0.5758851170539856
  - 0.6015963554382324
  - 0.7856652736663818
  - 0.5641516447067261
  - 0.505643904209137
  - 0.5856828093528748
  - 0.6113065481185913
  - 0.6038058996200562
  - 0.4209567904472351
  - 0.5538667440414429
  - 0.7147529125213623
  - 0.508684515953064
  - 0.6846232414245605
  - 0.5833531618118286
  - 0.4613197445869446
  - 0.5078129172325134
  - 0.49216604232788086
  - 0.6113499999046326
  - 0.6770511865615845
  - 0.8307396769523621
  - 0.580061137676239
  - 0.5637581944465637
  - 0.4068577289581299
  - 0.40636759996414185
  - 0.5387757420539856
  - 0.5793565511703491
  - 0.5778730511665344
  - 0.5616265535354614
  - 0.5836520195007324
  - 0.45763909816741943
  - 0.5689978003501892
  - 0.5985084772109985
  - 0.5929058194160461
  - 0.8854271173477173
  - 1.1681740283966064
  - 0.6234447360038757
  - 0.5784649848937988
  - 0.6549320816993713
  - 0.5435348153114319
  - 1.07957124710083
  - 0.668439507484436
loss_records_fold2:
  train_losses:
  - 1.295194062590599
  - 1.339140060544014
  - 1.3619878351688386
  - 1.3760368168354036
  - 1.392642003297806
  - 1.367387589812279
  - 1.2842282593250276
  - 1.3131453454494477
  - 1.3405547797679902
  - 1.3166887611150742
  - 1.468865764141083
  - 1.422569340467453
  - 1.4349852979183197
  - 1.4352373719215394
  - 1.3764793574810028
  - 1.3666470795869827
  - 1.3536391496658327
  - 1.396109014749527
  - 1.3297361075878145
  - 1.3260125547647477
  - 1.325162822008133
  - 1.336610782146454
  - 1.3584429025650024
  - 1.3443689405918122
  - 1.3115833938121797
  - 1.3501198112964632
  - 1.344157522916794
  - 1.3180104434490205
  - 1.3041614532470704
  - 1.3385771244764328
  - 1.2804807305336
  - 1.311513066291809
  - 1.359361171722412
  - 1.4374088943004608
  - 1.4088105231523516
  - 1.4606181740760804
  - 1.4327468216419221
  - 1.471170723438263
  - 1.4093947887420655
  - 1.4196752130985262
  - 1.3944720804691315
  - 1.4385484397411348
  - 1.396631270647049
  - 1.4162576377391816
  - 1.3434675097465516
  - 1.3628774702548982
  - 1.3340485095977783
  - 1.3594703435897828
  - 1.335652369260788
  - 1.3622120797634125
  - 1.358677613735199
  - 1.3401744544506073
  - 1.3648654878139497
  - 1.3270934879779817
  - 1.3572652876377107
  - 1.328327476978302
  - 1.3142033338546755
  - 1.3244089365005494
  - 1.2871917217969895
  - 1.3149604916572573
  - 1.3154049485921862
  - 1.336200398206711
  - 1.2943283885717394
  - 1.3236862301826477
  - 1.3660881221294403
  - 1.3098022282123567
  - 1.3326275527477265
  - 1.3369365990161897
  - 1.3549355149269104
  - 1.3432398796081544
  - 1.2875947117805482
  - 1.3776955246925355
  - 1.2967827409505845
  - 1.3137426137924195
  - 1.3725752532482147
  - 1.3007878303527833
  - 1.3019102424383164
  - 1.2874932110309603
  - 1.2973935127258303
  - 1.3152015030384065
  - 1.3255177736282349
  - 1.3470368921756746
  - 1.3187149047851563
  - 1.377787557244301
  - 1.3166365146636965
  - 1.2831739842891694
  - 1.3166038155555726
  - 1.316388374567032
  - 1.309326621890068
  - 1.3093516111373902
  - 1.3047069370746613
  - 1.2817760705947876
  - 1.2657999873161316
  - 1.2962789297103883
  - 1.298168557882309
  - 1.3046727180480957
  - 1.269314557313919
  - 1.287230944633484
  - 1.285641372203827
  - 1.3027728736400606
  validation_losses:
  - 0.5320064425468445
  - 0.5076592564582825
  - 0.48409929871559143
  - 0.5257133841514587
  - 0.6190583109855652
  - 0.6492436528205872
  - 0.7913599610328674
  - 0.8468344211578369
  - 0.9811137914657593
  - 1.4300216436386108
  - 0.40346333384513855
  - 0.37873345613479614
  - 0.3767271637916565
  - 0.3723459541797638
  - 0.4177916646003723
  - 0.436665803194046
  - 0.4315906763076782
  - 0.43817138671875
  - 0.5007163882255554
  - 0.6537907123565674
  - 0.5125269889831543
  - 0.5307365655899048
  - 0.499039888381958
  - 0.7033476829528809
  - 0.6118834018707275
  - 0.45186787843704224
  - 0.5670015215873718
  - 0.6873143315315247
  - 0.5184656381607056
  - 0.6843841075897217
  - 0.6144251227378845
  - 0.6087360978126526
  - 0.4801267981529236
  - 0.4091223180294037
  - 0.5116004347801208
  - 0.39059019088745117
  - 0.389110803604126
  - 0.42675697803497314
  - 0.3854910135269165
  - 0.3867114782333374
  - 0.37304064631462097
  - 0.4096234142780304
  - 0.42659592628479004
  - 0.3799890875816345
  - 0.4077184796333313
  - 0.5321133136749268
  - 0.3981718122959137
  - 0.3996416926383972
  - 0.4577600657939911
  - 0.4867515563964844
  - 0.4008007347583771
  - 0.6051580309867859
  - 0.6316028833389282
  - 0.5243852734565735
  - 0.5704111456871033
  - 0.5189875364303589
  - 0.6023616194725037
  - 0.5648258328437805
  - 0.6836912631988525
  - 0.8342934846878052
  - 0.671389102935791
  - 0.7168244123458862
  - 0.667630672454834
  - 0.44600430130958557
  - 0.8066591024398804
  - 0.3903621733188629
  - 0.45125260949134827
  - 0.5161755681037903
  - 0.5395136475563049
  - 0.4656694531440735
  - 0.5873443484306335
  - 0.5968353152275085
  - 0.5633113980293274
  - 0.5745015144348145
  - 0.567307710647583
  - 0.575903594493866
  - 0.6258513331413269
  - 0.6163871884346008
  - 0.6451618075370789
  - 1.4259920120239258
  - 0.5399060249328613
  - 0.5858626365661621
  - 0.4615817964076996
  - 0.6111970543861389
  - 0.6213691830635071
  - 0.6078976392745972
  - 0.6418441534042358
  - 0.6065374612808228
  - 0.553231418132782
  - 0.5407554507255554
  - 0.5828573107719421
  - 0.5580271482467651
  - 0.593505322933197
  - 0.5876404047012329
  - 0.6030020713806152
  - 0.5616031289100647
  - 0.619260311126709
  - 0.616213858127594
  - 0.5979587435722351
  - 0.5714035034179688
loss_records_fold3:
  train_losses:
  - 1.3358047962188722
  - 1.2721652865409852
  - 1.3150503277778627
  - 1.3287317037582398
  - 1.3072932898998262
  - 1.371111297607422
  - 1.3289410114288331
  - 1.3328263580799105
  - 1.326414728164673
  - 1.3191184252500534
  - 1.336603432893753
  - 1.35013367831707
  - 1.3240031480789185
  - 1.337317991256714
  - 1.327818974852562
  - 1.2665261238813401
  - 1.3315218448638917
  - 1.3242782294750215
  - 1.2836855590343477
  - 1.3033768951892855
  - 1.3010445833206177
  - 1.3316052198410036
  - 1.315479013323784
  - 1.2837042331695558
  - 1.2744175702333451
  - 1.324932485818863
  - 1.2750858783721926
  - 1.2707519322633745
  - 1.293304580450058
  - 1.3281970262527467
  - 1.301155000925064
  - 1.310776174068451
  - 1.3530737459659576
  - 1.3118048906326294
  - 1.2961708515882493
  - 1.315555387735367
  - 1.3209380209445953
  - 1.2986059546470643
  - 1.3294576585292817
  - 1.2867240548133851
  - 1.2824454486370087
  - 1.3182128131389619
  - 1.3038867533206941
  - 1.3027643322944642
  - 1.266516235470772
  - 1.299206256866455
  - 1.3021744787693024
  - 1.3577501833438874
  - 1.3129178047180177
  - 1.2931990683078767
  - 1.3210418045520784
  - 1.2685201942920685
  - 1.3171339601278307
  - 1.2490879237651826
  - 1.2816582798957825
  - 1.2843529343605042
  - 1.264778146147728
  - 1.2690394401550293
  - 1.2727235555648804
  - 1.3822677195072175
  - 1.2850835233926774
  - 1.3124767482280733
  - 1.3159910321235657
  - 1.2935312926769258
  - 1.294821047782898
  - 1.2681280821561813
  - 1.3092704832553865
  - 1.2868065536022186
  - 1.2564442038536072
  - 1.2665157854557039
  - 1.2525526076555253
  - 1.3128603458404542
  - 1.3036954373121263
  - 1.2350671142339706
  - 1.3595197677612305
  - 1.2595182687044144
  - 1.2368257611989977
  - 1.2394385486841202
  - 1.3221049547195436
  - 1.2895538091659546
  - 1.3213602602481842
  - 1.3099566161632539
  - 1.3139729261398316
  - 1.2713621824979784
  - 1.2985622584819794
  - 1.2566673129796984
  - 1.249241054058075
  - 1.2624708801507951
  - 1.2304182946681976
  - 1.2677187502384186
  - 1.2562653481960298
  - 1.3143902570009232
  - 1.2341178953647614
  - 1.2698919713497163
  - 1.2720091938972473
  - 1.2578020036220552
  - 1.2676622927188874
  - 1.2439942061901093
  - 1.2384198129177095
  - 1.2588050425052644
  validation_losses:
  - 0.3620877265930176
  - 0.5125651955604553
  - 0.6209242343902588
  - 0.5096642374992371
  - 0.5014260411262512
  - 0.47532588243484497
  - 0.4757426679134369
  - 0.5026191473007202
  - 0.45418742299079895
  - 0.46951285004615784
  - 0.5280727744102478
  - 0.5197482109069824
  - 0.4951055347919464
  - 0.4129156768321991
  - 0.461188942193985
  - 0.5350179672241211
  - 0.5725008845329285
  - 0.5617843270301819
  - 0.7137691378593445
  - 0.5811755657196045
  - 0.6068538427352905
  - 0.5998808741569519
  - 0.541917085647583
  - 0.4925025403499603
  - 0.5321301221847534
  - 0.5366090536117554
  - 0.5474634170532227
  - 0.5546277165412903
  - 0.6391299962997437
  - 0.569013774394989
  - 0.5934150218963623
  - 0.6618970036506653
  - 0.5887773633003235
  - 0.47232338786125183
  - 0.5717279314994812
  - 0.5442953109741211
  - 0.7921061515808105
  - 0.5129005908966064
  - 0.5591353178024292
  - 0.5864309668540955
  - 0.513421356678009
  - 0.5463988184928894
  - 0.5384011268615723
  - 0.5356410145759583
  - 0.46137067675590515
  - 0.5298594832420349
  - 0.5278964042663574
  - 0.5087605714797974
  - 0.5984905362129211
  - 0.4935721755027771
  - 0.5239747166633606
  - 0.5499035120010376
  - 0.5943719744682312
  - 0.611236035823822
  - 0.48823583126068115
  - 0.6520591378211975
  - 0.5344356894493103
  - 0.6701029539108276
  - 0.4712947607040405
  - 0.35447755455970764
  - 0.47061672806739807
  - 0.5600467920303345
  - 0.6121640205383301
  - 0.5852006077766418
  - 0.7291449308395386
  - 0.6574521660804749
  - 0.5169124603271484
  - 0.48261934518814087
  - 0.4853702187538147
  - 0.5075231194496155
  - 0.5241479277610779
  - 0.4818943440914154
  - 0.5362975001335144
  - 0.518984854221344
  - 0.41492223739624023
  - 0.46828749775886536
  - 0.5205850601196289
  - 0.6107128262519836
  - 0.5833582282066345
  - 0.5242183804512024
  - 0.5120184421539307
  - 0.6908869743347168
  - 0.5056660771369934
  - 0.4866703152656555
  - 0.5844755172729492
  - 0.547781765460968
  - 0.5490025281906128
  - 0.5486201047897339
  - 0.6001869440078735
  - 0.4821394383907318
  - 0.6106695532798767
  - 0.488626629114151
  - 0.5572680830955505
  - 0.575639009475708
  - 0.5263888835906982
  - 0.6049767136573792
  - 0.5837039947509766
  - 0.5478789806365967
  - 0.5583460927009583
  - 0.585117757320404
loss_records_fold4:
  train_losses:
  - 1.2814377397298813
  - 1.296603536605835
  - 1.2747129261493684
  - 1.2889013051986695
  - 1.2892491281032563
  - 1.2936071455478668
  - 1.32358295917511
  - 1.2858278691768648
  - 1.2618067532777788
  - 1.2886131942272188
  - 1.3246605932712556
  - 1.3126571983098985
  - 1.2924063920974733
  - 1.2761038422584534
  - 1.2878311216831209
  - 1.270482236146927
  - 1.2660703867673875
  - 1.2486486673355104
  - 1.2720358669757843
  - 1.2719700157642366
  - 1.2961415708065034
  - 1.3287177026271821
  - 1.2816894471645357
  - 1.28472501039505
  - 1.2953140318393708
  - 1.2838461458683015
  - 1.252543380856514
  - 1.275416374206543
  - 1.2580969870090486
  - 1.298081135749817
  - 1.3709481716156007
  - 1.279365247488022
  - 1.2809093475341797
  - 1.3058780193328858
  - 1.3009742081165314
  - 1.266692501306534
  - 1.2201279789209367
  - 1.269716250896454
  - 1.2619988083839417
  - 1.2913162529468538
  - 1.2443215906620027
  - 1.3325888872146607
  - 1.270973938703537
  - 1.308686363697052
  - 1.3177153557538988
  - 1.287818044424057
  - 1.2839234352111817
  - 1.2226466655731203
  - 1.252245181798935
  - 1.3042948961257936
  - 1.3223714739084245
  - 1.3539582014083864
  - 1.3052875816822054
  - 1.2560681730508805
  - 1.2715868413448335
  - 1.3437378227710726
  - 1.336546629667282
  - 1.2765811502933504
  - 1.2882819026708603
  - 1.3051627784967423
  - 1.2622327864170075
  - 1.3009103357791902
  - 1.2764021664857865
  - 1.2643123626708985
  - 1.2531014740467072
  - 1.237233030796051
  - 1.2560725688934327
  - 1.2009530425071717
  - 1.2286701947450638
  - 1.2458666801452638
  - 1.2041968286037446
  - 1.2726698338985445
  - 1.2643219947814943
  - 1.232994568347931
  - 1.2167076438665392
  - 1.2494872391223908
  - 1.2654792666435242
  - 1.2594366520643234
  - 1.2560343384742738
  - 1.2420700877904893
  - 1.3208436340093614
  - 1.2654097497463228
  - 1.2093478173017502
  - 1.2780840039253236
  - 1.263416749238968
  - 1.2789088189601898
  - 1.2986771523952485
  - 1.2087729752063752
  - 1.2266182452440262
  - 1.2692455649375916
  - 1.2569532722234726
  - 1.2305187404155733
  - 1.2131968945264817
  - 1.2609153300523759
  - 1.31253799200058
  - 1.297537523508072
  - 1.3364954888820648
  - 1.240368476510048
  - 1.2433777332305909
  - 1.2241875290870667
  validation_losses:
  - 0.6189207434654236
  - 0.7018274664878845
  - 0.7065976858139038
  - 0.6407803297042847
  - 0.6227371096611023
  - 0.6384937167167664
  - 0.5810997486114502
  - 0.5435538291931152
  - 0.5792168378829956
  - 0.6257907152175903
  - 0.5314038991928101
  - 0.4985277056694031
  - 0.6037442088127136
  - 0.6956113576889038
  - 0.566192090511322
  - 0.6241765022277832
  - 0.5996222496032715
  - 0.5789719820022583
  - 0.681858241558075
  - 0.6486548185348511
  - 0.65239417552948
  - 0.6216895580291748
  - 0.7587444186210632
  - 0.6090735197067261
  - 0.6821798086166382
  - 0.5878063440322876
  - 0.5868374109268188
  - 0.5074606537818909
  - 0.6376585364341736
  - 0.6681750416755676
  - 0.5923285484313965
  - 0.5020229816436768
  - 0.7111939191818237
  - 0.6196867823600769
  - 0.6904745697975159
  - 0.7956752777099609
  - 0.7970626354217529
  - 0.6983630061149597
  - 0.7481751441955566
  - 0.6510350108146667
  - 0.6157041788101196
  - 0.8202216625213623
  - 0.6487735509872437
  - 0.6600601673126221
  - 0.6020906567573547
  - 0.6036763787269592
  - 0.7437689900398254
  - 0.706658661365509
  - 0.6954644322395325
  - 0.5640964508056641
  - 0.544618546962738
  - 0.736143171787262
  - 0.6790964007377625
  - 0.6985957622528076
  - 0.8427846431732178
  - 0.753212034702301
  - 0.7518855333328247
  - 0.6141336560249329
  - 0.5658167600631714
  - 0.6086871027946472
  - 0.6128835082054138
  - 0.6552192568778992
  - 0.6455007195472717
  - 0.6795186400413513
  - 0.6226802468299866
  - 0.6960790753364563
  - 0.6128665804862976
  - 0.7216281890869141
  - 0.7386061549186707
  - 0.6188702583312988
  - 0.7131007313728333
  - 0.8466510772705078
  - 0.7838770151138306
  - 0.6368645429611206
  - 0.6298487186431885
  - 0.573522686958313
  - 0.787236213684082
  - 0.6453734040260315
  - 0.7478153705596924
  - 0.6155389547348022
  - 0.6174582242965698
  - 0.5441604256629944
  - 0.5620843768119812
  - 0.4710729122161865
  - 0.6109382510185242
  - 0.5841707587242126
  - 0.7800596356391907
  - 0.6775718331336975
  - 0.6662487387657166
  - 0.7251548171043396
  - 0.6091541647911072
  - 0.6431413888931274
  - 0.6207772493362427
  - 0.7105139493942261
  - 0.6092331409454346
  - 0.8053376078605652
  - 0.7616226673126221
  - 0.6802346110343933
  - 0.6832329034805298
  - 0.598842442035675
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8181818181818182, 0.8096054888507719, 0.8301886792452831,
    0.8350515463917526]'
  fold_eval_f1: '[0.0, 0.22058823529411764, 0.2745098039215686, 0.31724137931034485,
    0.21311475409836067]'
  mean_eval_accuracy: 0.8301320931548514
  mean_f1_accuracy: 0.20509083452487836
  total_train_time: '0:39:24.325006'
