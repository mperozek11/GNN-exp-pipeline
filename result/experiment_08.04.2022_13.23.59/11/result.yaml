config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 14:31:58.852338'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/11/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/11/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/11/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/11/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/11/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/11/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/11/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/11/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/11/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/11/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 20.520945519208908
  - 19.075641214847565
  - 17.229586601257324
  - 16.288909524679184
  - 17.330889835953712
  - 16.09796702861786
  - 16.31709012389183
  - 17.956051737070084
  - 16.78494730591774
  - 16.483124017715454
  - 16.939003080129623
  - 16.404220819473267
  - 16.1977057158947
  - 16.44868403673172
  - 16.64054709672928
  - 17.092371106147766
  - 16.698205560445786
  - 17.079782962799072
  - 17.972034633159637
  - 16.66299244761467
  - 16.612568199634552
  - 16.44159245491028
  - 16.359638273715973
  - 16.50167551636696
  - 16.027205914258957
  - 16.243187427520752
  - 16.865348428487778
  - 16.4110726416111
  - 16.686548978090286
  - 15.939951330423355
  - 16.39220303297043
  - 16.337358474731445
  - 16.819888204336166
  - 16.634936943650246
  - 16.78026095032692
  - 16.418415248394012
  - 16.77521274983883
  - 16.12525051832199
  - 16.045290648937225
  - 17.19701999425888
  - 16.47094216942787
  - 16.21726170182228
  - 16.0178644657135
  - 16.77995401620865
  - 16.321350902318954
  - 16.03271386027336
  - 16.34826783835888
  - 16.007166996598244
  - 16.391292542219162
  - 16.153738737106323
  - 16.280086278915405
  - 16.410929709672928
  - 16.274449229240417
  - 16.040384232997894
  - 16.047447592020035
  - 16.37627398967743
  - 16.547858595848083
  - 16.33958812057972
  - 16.08510336279869
  - 16.47963047027588
  - 17.24989938735962
  - 16.119450569152832
  - 16.51341137290001
  - 15.908565014600754
  - 16.795962184667587
  - 16.576574832201004
  - 16.333118051290512
  - 16.84951478242874
  - 16.827839016914368
  - 16.244541078805923
  - 16.542484939098358
  - 16.771418869495392
  - 16.2592596411705
  - 17.128762125968933
  - 15.997435837984085
  - 15.814652055501938
  - 15.923137798905373
  - 16.39553612470627
  - 16.27086991071701
  - 16.431052267551422
  - 16.327289432287216
  - 16.381774246692657
  - 16.202081829309464
  - 16.194816201925278
  - 16.382956236600876
  - 16.853619039058685
  - 16.065673112869263
  - 16.299680322408676
  - 16.585196316242218
  - 16.675020426511765
  - 16.08454740047455
  - 16.420644998550415
  - 16.651590883731842
  - 16.126328706741333
  - 16.18365553021431
  - 16.109438329935074
  - 16.491920679807663
  - 16.35826303064823
  - 16.477654963731766
  - 16.306729808449745
  validation_losses:
  - 0.4199848175048828
  - 0.41001468896865845
  - 0.40234479308128357
  - 0.4159010946750641
  - 0.41638511419296265
  - 0.417713463306427
  - 0.4082208275794983
  - 0.4180755317211151
  - 0.420719176530838
  - 0.7146994471549988
  - 0.4606166183948517
  - 0.41725242137908936
  - 0.42864689230918884
  - 0.4151574671268463
  - 0.4652020037174225
  - 0.4211673438549042
  - 0.4204568564891815
  - 0.41862809658050537
  - 0.4383166432380676
  - 0.4070563316345215
  - 0.41515210270881653
  - 0.4247967302799225
  - 0.41197469830513
  - 1463523.75
  - 2248201.75
  - 2262033.75
  - 2334014.0
  - 0.41236671805381775
  - 0.4215230643749237
  - 0.4241703152656555
  - 0.4140029549598694
  - 0.4283968508243561
  - 0.4098886251449585
  - 6601038.0
  - 15063069.0
  - 15562262.0
  - 15555500.0
  - 15624937.0
  - 138662775881728.0
  - 162243295576064.0
  - 158987811028992.0
  - 155831261724672.0
  - 155831647600640.0
  - 155351030693888.0
  - 155836395552768.0
  - 154369798438912.0
  - 185678432305152.0
  - 154370184314880.0
  - 155093550759936.0
  - 154629056757760.0
  - 153893879152640.0
  - 155096704876544.0
  - 154157465993216.0
  - 153441397637120.0
  - 153627725398016.0
  - 153421281755136.0
  - 154083361030144.0
  - 154363506982912.0
  - 152931571597312.0
  - 154624124256256.0
  - 153880876810240.0
  - 152934524387328.0
  - 153624571281408.0
  - 154378421927936.0
  - 154363104329728.0
  - 154621775446016.0
  - 154629056757760.0
  - 154624124256256.0
  - 154621188243456.0
  - 154363506982912.0
  - 153880893587456.0
  - 154079200280576.0
  - 153157510365184.0
  - 152176714317824.0
  - 153864703574016.0
  - 153615511584768.0
  - 152905785016320.0
  - 153866297409536.0
  - 152390456049664.0
  - 153866297409536.0
  - 153146286407680.0
  - 153336372264960.0
  - 153339744485376.0
  - 152876240338944.0
  - 153116372631552.0
  - 152115242598400.0
  - 152856325783552.0
  - 152400102948864.0
  - 152385540325376.0
  - 153593852198912.0
  - 184382576918528.0
  - 152409548521472.0
  - 153590698082304.0
  - 153593868976128.0
  - 152622904377344.0
  - 152622887600128.0
  - 153336372264960.0
  - 152616982020096.0
  - 152383946489856.0
  - 152852383137792.0
loss_records_fold1:
  train_losses:
  - 16.18427601456642
  - 16.824720829725266
  - 17.12619924545288
  - 16.297679036855698
  - 16.175498813390732
  - 16.781005650758743
  - 16.418330490589142
  - 16.091107547283173
  - 16.51832640171051
  - 16.22283324599266
  - 16.704841941595078
  - 16.804832994937897
  - 16.354811936616898
  - 16.242956697940826
  - 16.233159482479095
  - 16.379312932491302
  - 16.377368837594986
  - 16.324626803398132
  - 16.304541915655136
  - 16.29988642036915
  - 15.936912536621094
  - 16.214052081108093
  - 16.47293046116829
  - 16.309962332248688
  - 16.176343083381653
  - 16.286532908678055
  - 16.31660857796669
  - 16.468382596969604
  - 17.801052898168564
  - 17.465820163488388
  - 16.041886001825333
  - 16.515958309173584
  - 16.691533893346786
  - 16.14067456126213
  - 16.449514776468277
  - 16.604643523693085
  - 16.424591064453125
  - 16.722331017255783
  - 16.3663629591465
  - 17.291712373495102
  - 16.21965256333351
  - 15.866334170103073
  - 16.309726536273956
  - 16.469964265823364
  - 16.261218190193176
  - 16.7669917345047
  - 15.85720282793045
  - 16.293681204319
  - 16.775443136692047
  - 16.112235099077225
  - 16.35881382226944
  - 16.374535337090492
  - 16.300945520401
  - 16.952913910150528
  - 17.022112429142
  - 16.644868806004524
  - 16.43112549185753
  - 16.4000526368618
  - 16.152602285146713
  - 16.266671061515808
  - 16.481511935591698
  - 16.3179070353508
  - 16.025940150022507
  - 16.361651688814163
  - 16.291411995887756
  - 16.536586746573448
  - 16.245456844568253
  - 16.556216925382614
  - 16.393984749913216
  - 16.677727043628693
  - 16.372194916009903
  - 16.22127255797386
  - 16.397896885871887
  - 16.341149374842644
  - 16.23544606566429
  - 16.524971216917038
  - 16.511507868766785
  - 16.297388911247253
  - 16.2398678958416
  - 16.085546910762787
  - 16.493676900863647
  - 16.2948290258646
  - 16.50384631752968
  - 16.49819879233837
  - 16.150413870811462
  - 16.452806040644646
  - 16.442063450813293
  - 16.274957418441772
  - 15.902570098638535
  - 16.53869904577732
  - 16.26747825741768
  - 16.457051515579224
  - 16.33423575758934
  - 16.488585740327835
  - 16.587335228919983
  - 16.556501865386963
  - 16.33565267920494
  - 16.936543971300125
  - 16.32031375169754
  - 15.928365170955658
  validation_losses:
  - 136591267856384.0
  - 164656010231808.0
  - 137668398678016.0
  - 137009272193024.0
  - 136593222402048.0
  - 136398413758464.0
  - 137444406067200.0
  - 137003760877568.0
  - 137667601760256.0
  - 137445395922944.0
  - 137232065232896.0
  - 137223399800832.0
  - 138292444004352.0
  - 137888230539264.0
  - 137663080300544.0
  - 137221629804544.0
  - 137009255415808.0
  - 137221629804544.0
  - 136780951060480.0
  - 164446043373568.0
  - 137892382900224.0
  - 137223399800832.0
  - 138726252478464.0
  - 136805781340160.0
  - 137885084811264.0
  - 137883717468160.0
  - 137221629804544.0
  - 136786856640512.0
  - 137890789064704.0
  - 137003760877568.0
  - 137223399800832.0
  - 136390335528960.0
  - 137223399800832.0
  - 138296009162752.0
  - 137230295236608.0
  - 137446972981248.0
  - 136788450476032.0
  - 137238163750912.0
  - 137445395922944.0
  - 138079708905472.0
  - 137009272193024.0
  - 137221629804544.0
  - 164653661421568.0
  - 137009272193024.0
  - 138079708905472.0
  - 137230295236608.0
  - 137238163750912.0
  - 137446972981248.0
  - 137238163750912.0
  - 137888238927872.0
  - 136805781340160.0
  - 136367879225344.0
  - 137223399800832.0
  - 138513861312512.0
  - 137007485419520.0
  - 138303877677056.0
  - 137243096252416.0
  - 137663474565120.0
  - 137232065232896.0
  - 136143509127168.0
  - 136594405195776.0
  - 137440488587264.0
  - 138074969341952.0
  - 137659733245952.0
  - 136593239179264.0
  - 137232048455680.0
  - 137663088689152.0
  - 164463357460480.0
  - 137245453451264.0
  - 137892366123008.0
  - 138945690075136.0
  - 137223399800832.0
  - 137486768537600.0
  - 137221613027328.0
  - 137659716468736.0
  - 136780934283264.0
  - 137890789064704.0
  - 137007485419520.0
  - 136780951060480.0
  - 137440488587264.0
  - 136586536681472.0
  - 136605645930496.0
  - 136788450476032.0
  - 138076747726848.0
  - 137883717468160.0
  - 136540214788096.0
  - 136367879225344.0
  - 138079708905472.0
  - 137892382900224.0
  - 137009255415808.0
  - 136586536681472.0
  - 137232048455680.0
  - 137007485419520.0
  - 137229498318848.0
  - 137883717468160.0
  - 137006914994176.0
  - 137883717468160.0
  - 137667601760256.0
  - 137006914994176.0
  - 136592047996928.0
loss_records_fold2:
  train_losses:
  - 16.169957891106606
  - 16.364199340343475
  - 16.425573021173477
  - 17.223223596811295
  - 17.171106457710266
  - 16.236336261034012
  - 16.53616863489151
  - 16.48028963804245
  - 16.036819577217102
  - 16.293167263269424
  - 15.91266793012619
  - 15.776638180017471
  - 16.692282617092133
  - 16.329146951436996
  - 16.345794171094894
  - 16.34654513001442
  - 16.279923975467682
  - 16.472368210554123
  - 16.14306530356407
  - 16.709050208330154
  - 16.08054345846176
  - 16.574149280786514
  - 16.476692646741867
  - 16.093831479549408
  - 16.180133402347565
  - 16.224534273147583
  - 16.214724868535995
  - 16.388197541236877
  - 16.33364623785019
  - 16.1276815533638
  - 16.806670278310776
  - 16.63797441124916
  - 16.422551214694977
  - 16.568672448396683
  - 16.253360837697983
  - 16.3798768222332
  - 16.36877891421318
  - 16.345313608646393
  - 16.593959748744965
  - 16.23999834060669
  - 16.82907608151436
  - 16.428437769412994
  - 16.256174474954605
  - 16.507218420505524
  - 16.37906152009964
  - 16.5602086186409
  - 16.28204533457756
  - 16.94700649380684
  - 16.47495037317276
  - 16.226131111383438
  - 16.568108916282654
  - 16.407545164227486
  - 16.4787800014019
  - 16.094080686569214
  - 16.490496397018433
  - 16.284961819648743
  - 16.357858508825302
  - 16.13198009133339
  - 16.246125280857086
  - 16.48033857345581
  - 16.28607326745987
  - 16.687076687812805
  - 16.413607239723206
  - 16.29907563328743
  - 16.564256608486176
  - 16.394066959619522
  - 16.61529752612114
  - 16.324920773506165
  - 16.561973869800568
  - 16.702049285173416
  - 16.514037996530533
  - 16.44450044631958
  - 16.511372089385986
  - 17.099540442228317
  - 15.99903853237629
  - 16.921804010868073
  - 16.68163102865219
  - 16.588685378432274
  - 16.387799948453903
  - 16.19854635000229
  - 16.555801063776016
  - 16.712473690509796
  - 16.131101429462433
  - 16.668535232543945
  - 16.497597336769104
  - 16.119009882211685
  - 16.40697705745697
  - 16.535251170396805
  - 16.909057945013046
  - 16.932347744703293
  - 16.484061300754547
  - 16.260952442884445
  - 16.27637469768524
  - 16.05937172472477
  - 16.412658274173737
  - 15.894584774971008
  - 16.88050803542137
  - 16.103357881307602
  - 16.07184463739395
  - 16.463940739631653
  validation_losses:
  - 171611541995520.0
  - 169490381799424.0
  - 169211410251776.0
  - 169211410251776.0
  - 170011767341056.0
  - 170523757641728.0
  - 171107235659776.0
  - 168692742619136.0
  - 171095021846528.0
  - 169722997899264.0
  - 168928697384960.0
  - 168925560045568.0
  - 169201360699392.0
  - 170011566014464.0
  - 169478738411520.0
  - 169214748917760.0
  - 168683867471872.0
  - 170014116151296.0
  - 170014116151296.0
  - 168922204602368.0
  - 168672861618176.0
  - 170287366668288.0
  - 170294463430656.0
  - 169482295181312.0
  - 169214765694976.0
  - 169719223025664.0
  - 168695124983808.0
  - 169776215228416.0
  - 170005693988864.0
  - 170527700287488.0
  - 169490381799424.0
  - 170011750563840.0
  - 168695124983808.0
  - 169211427028992.0
  - 170527717064704.0
  - 170287366668288.0
  - 168120085905408.0
  - 169204514816000.0
  - 170018645999616.0
  - 169486405599232.0
  - 168981361065984.0
  - 169214765694976.0
  - 170298993278976.0
  - 171611541995520.0
  - 170011784118272.0
  - 169214765694976.0
  - 170583853629440.0
  - 203080884813824.0
  - 169776198451200.0
  - 169478738411520.0
  - 169214765694976.0
  - 169201360699392.0
  - 168984682954752.0
  - 170299010056192.0
  - 169481892528128.0
  - 169217332609024.0
  - 168688984522752.0
  - 169486422376448.0
  - 170528086163456.0
  - 169719239802880.0
  - 170011767341056.0
  - 169490381799424.0
  - 169215352897536.0
  - 170011750563840.0
  - 169221073928192.0
  - 168930878423040.0
  - 168925560045568.0
  - 169499626045440.0
  - 170017270267904.0
  - 170298993278976.0
  - 169777775509504.0
  - 170287366668288.0
  - 169481892528128.0
  - 170592711999488.0
  - 170287366668288.0
  - 168984682954752.0
  - 170014116151296.0
  - 170011750563840.0
  - 168911953723392.0
  - 168922221379584.0
  - 168928714162176.0
  - 169204514816000.0
  - 170583853629440.0
  - 168928697384960.0
  - 168699839381504.0
  - 169214765694976.0
  - 170285990936576.0
  - 169217332609024.0
  - 168930878423040.0
  - 170527700287488.0
  - 168994547957760.0
  - 170017270267904.0
  - 169499626045440.0
  - 170294463430656.0
  - 168922221379584.0
  - 170299010056192.0
  - 169719239802880.0
  - 169478738411520.0
  - 203349387378688.0
  - 169214765694976.0
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 99 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:10:31.438403'
