config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:43:32.655818'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_56fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_56fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_56fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_56fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_56fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_56fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_56fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_56fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_56fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_56fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 57.27537546753884
  - 44.38444293290377
  - 30.165184789896013
  - 23.143581518530848
  - 16.22855948507786
  - 16.090777149796487
  - 13.909782692790031
  - 9.332053962349892
  - 13.072996705770493
  - 11.412073221802713
  - 11.785802364349365
  - 11.788792100548745
  - 10.551960995793344
  - 10.89741891771555
  - 10.352453178167345
  - 12.75195088684559
  - 11.120944496989251
  - 9.37227676808834
  - 9.756595110893251
  - 9.871352773904801
  - 8.01737201809883
  - 8.446973705291748
  - 9.008187180757522
  - 13.408458928763867
  - 7.566973665356636
  - 8.099943149089814
  - 7.412884208559991
  - 6.6351415634155275
  - 7.604179641604424
  - 6.341990984976292
  - 6.073332118988038
  - 6.130509543418885
  - 6.4090601950883865
  - 6.3626070588827135
  - 6.614317384362221
  - 6.341110321879388
  - 6.301122352480888
  - 6.786549746990204
  - 6.5974319845438005
  - 6.569886741042137
  - 6.351371151208878
  - 6.174545839428902
  - 5.972265464067459
  - 6.120831486582756
  - 6.063460865616799
  - 6.164816349744797
  - 6.317589515447617
  validation_losses:
  - 3.53147029876709
  - 8.882575035095215
  - 3.7926104068756104
  - 0.8522319793701172
  - 0.4973490238189697
  - 0.5579515695571899
  - 0.501737654209137
  - 0.45090577006340027
  - 0.5733585357666016
  - 0.7976046204566956
  - 0.43042516708374023
  - 0.5699451565742493
  - 0.4528546929359436
  - 0.42336198687553406
  - 0.45317190885543823
  - 0.714759349822998
  - 0.4019922614097595
  - 0.41300034523010254
  - 0.4015904664993286
  - 0.3925434648990631
  - 0.43911781907081604
  - 0.41174763441085815
  - 1.36211359500885
  - 0.38767170906066895
  - 0.4454994797706604
  - 0.3831578493118286
  - 0.3955962359905243
  - 0.39341986179351807
  - 0.39234083890914917
  - 0.40852442383766174
  - 0.39485466480255127
  - 0.42967280745506287
  - 0.3799644410610199
  - 0.40868768095970154
  - 0.38465264439582825
  - 0.4422554075717926
  - 0.4069669544696808
  - 0.4219895601272583
  - 0.4031935930252075
  - 0.41658294200897217
  - 1.5538898706436157
  - 0.38947755098342896
  - 0.3905394971370697
  - 0.3898428678512573
  - 0.3925394117832184
  - 0.38994690775871277
  - 0.3991945683956146
loss_records_fold1:
  train_losses:
  - 6.023589384555817
  - 5.87230159342289
  - 5.85044738650322
  - 5.980548599362374
  - 7.213079804182053
  - 6.676467272639275
  - 6.3727797180414205
  - 6.195671764016152
  - 5.903816875815392
  - 6.406507179141045
  - 5.979338118433953
  - 5.923079378902912
  - 5.8040919512510305
  - 5.877556455135346
  - 6.217507827281953
  - 6.026152658462525
  - 5.99585936665535
  - 5.947280639410019
  - 5.948913860321046
  - 6.1609169989824295
  - 5.964435967803002
  - 6.025199815630913
  - 6.09612205028534
  - 5.945297580957413
  - 5.982614536583424
  - 6.429564821720124
  - 8.462949866056443
  - 6.912072755396366
  - 6.302055010199547
  - 5.993716022372246
  - 6.087692768871785
  - 5.958428758382798
  - 5.883550235629082
  - 6.02085708975792
  - 6.040032923221588
  - 5.9948913007974625
  - 5.951010474562645
  - 5.841418242454529
  - 5.906433445215225
  - 5.8466120809316635
  - 5.886059671640396
  - 6.123707321286202
  - 5.953172540664673
  - 5.978996399044991
  - 6.104782310128212
  - 5.89584128856659
  - 6.063064849376679
  - 5.921236130595208
  - 5.803440582752228
  - 5.834051713347435
  - 5.966447627544404
  - 6.034961175918579
  - 5.788731792569161
  - 5.882761937379837
  validation_losses:
  - 0.40688183903694153
  - 0.41789939999580383
  - 0.41546639800071716
  - 0.4059411287307739
  - 0.44021639227867126
  - 0.4452705383300781
  - 0.40163111686706543
  - 0.406383752822876
  - 0.4191961884498596
  - 0.4053530991077423
  - 0.42836520075798035
  - 0.4125783443450928
  - 0.40386638045310974
  - 0.44817131757736206
  - 0.4257570803165436
  - 0.4025854468345642
  - 0.4021313190460205
  - 0.43709078431129456
  - 0.44445958733558655
  - 0.4064614176750183
  - 0.40407657623291016
  - 0.40507906675338745
  - 0.8695580363273621
  - 0.41146010160446167
  - 0.886041522026062
  - 1.2359373569488525
  - 0.4110126495361328
  - 0.8267355561256409
  - 0.47377949953079224
  - 0.4051084518432617
  - 0.46251344680786133
  - 0.47513633966445923
  - 0.45958206057548523
  - 20.236167907714844
  - 26.724937438964844
  - 1133.295654296875
  - 1036.5506591796875
  - 8352.4423828125
  - 20.969438552856445
  - 3422.403076171875
  - 31371.548828125
  - 100204.3984375
  - 0.41101929545402527
  - 7200.41552734375
  - 1.5106406211853027
  - 3.648540496826172
  - 0.4192289113998413
  - 6.145293712615967
  - 0.4050653874874115
  - 0.4061261713504791
  - 0.41607341170310974
  - 0.4250854551792145
  - 0.4129227101802826
  - 0.40466901659965515
loss_records_fold2:
  train_losses:
  - 6.1444930911064155
  - 6.095855233073235
  - 5.8554064542055135
  - 5.9121247917413715
  - 6.014070335030556
  - 6.002908033132553
  - 5.912217581272126
  - 5.86533913910389
  - 5.971663257479668
  - 5.952152022719384
  - 5.934202033281327
  - 5.96419752240181
  - 5.854434955120087
  - 5.918504723906517
  - 6.006227916479111
  - 6.492561422288418
  - 6.135281607508659
  - 5.835338738560677
  - 6.016300469636917
  - 6.135044199228287
  - 6.044768619537354
  - 6.0592262744903564
  - 6.098189014196397
  - 5.88086089193821
  - 6.030918903648853
  - 6.131283596158028
  - 5.870605108141899
  - 5.899164685606957
  - 5.9335502862930305
  - 5.957704252004624
  validation_losses:
  - 0.4186190068721771
  - 0.3861407935619354
  - 0.3822663724422455
  - 0.3820779621601105
  - 0.3884366750717163
  - 0.3851727843284607
  - 1.104271650314331
  - 649.62548828125
  - 0.3834908604621887
  - 0.4066324234008789
  - 0.383665531873703
  - 0.3913772404193878
  - 0.39727216958999634
  - 0.3836076557636261
  - 0.3836410939693451
  - 0.4589434564113617
  - 0.4231719374656677
  - 0.38345766067504883
  - 0.4183100759983063
  - 0.4269014000892639
  - 0.3902260363101959
  - 0.38473063707351685
  - 0.38787001371383667
  - 0.40390512347221375
  - 0.3848806917667389
  - 0.38378939032554626
  - 0.3888987600803375
  - 0.39267900586128235
  - 0.38361844420433044
  - 0.3904164731502533
loss_records_fold3:
  train_losses:
  - 5.979669731855393
  - 6.070728844404221
  - 5.946065893769265
  - 5.839220860600472
  - 5.892033825814725
  - 6.069823920726776
  - 6.179455879330636
  - 6.211085921525956
  - 6.1440759181976325
  - 5.864105859398842
  - 5.949321135878563
  - 5.969425228238106
  - 5.82653056383133
  - 5.909679326415063
  - 5.970153260231019
  - 5.977699387073518
  - 5.94196697473526
  - 5.850647443532944
  - 5.890193632245064
  - 5.904179283976555
  - 5.999750626087189
  - 6.044015324115754
  - 5.982502537965775
  - 5.924793672561646
  - 6.001462435722352
  - 5.9853862762451175
  - 6.40559330880642
  - 6.116243481636047
  - 5.845608760416508
  - 5.88051875680685
  - 5.87837041914463
  - 5.951960733532906
  - 5.9254814207553865
  - 6.145554077625275
  - 5.840452572703362
  - 6.033250188827515
  - 5.974651598930359
  - 5.865059474110604
  - 5.982569152116776
  - 5.937352177500725
  - 5.99086736291647
  - 5.870538851618767
  - 5.988097015023232
  - 6.087326835095883
  - 6.174927616119385
  - 5.9049313306808475
  - 5.996887457370758
  - 5.938062053918839
  - 5.854465830326081
  - 6.038212290406228
  - 6.080977162718773
  - 5.985055184364319
  - 6.048529255390168
  - 5.950073677301408
  - 6.373990920186043
  - 6.076547457277775
  - 5.840227901935577
  - 5.960888403654099
  - 5.86057962179184
  - 5.863550831377506
  validation_losses:
  - 0.42410409450531006
  - 0.40138882398605347
  - 2588183296.0
  - 24016339337216.0
  - 76553273344.0
  - 207327953158144.0
  - 0.4137018918991089
  - 13.271553993225098
  - 339.1510314941406
  - 0.4097573161125183
  - 0.4014168083667755
  - 0.42013245820999146
  - 0.3991607129573822
  - 19717024645120.0
  - 0.41798076033592224
  - 0.4287252724170685
  - 0.4097714126110077
  - 0.4190688729286194
  - 0.4042878746986389
  - 0.43856289982795715
  - 7676854534144.0
  - 2897580358893568.0
  - 0.42804887890815735
  - 0.43192538619041443
  - 0.4005441665649414
  - 0.40045979619026184
  - 0.4697212278842926
  - 0.39771392941474915
  - 0.3976905941963196
  - 0.4203249216079712
  - 0.43113425374031067
  - 0.42373231053352356
  - 0.40086954832077026
  - 0.39951542019844055
  - 0.41552671790122986
  - 0.39738762378692627
  - 0.40226373076438904
  - 0.3990875482559204
  - 0.5061511397361755
  - 0.399126261472702
  - 0.40582922101020813
  - 0.4135942757129669
  - 0.39855173230171204
  - 0.4465841054916382
  - 0.40887296199798584
  - 0.3988853693008423
  - 0.4123912453651428
  - 0.39973077178001404
  - 0.44754505157470703
  - 0.3983422815799713
  - 0.39887747168540955
  - 0.45317667722702026
  - 0.40422704815864563
  - 0.5099928379058838
  - 0.4066711962223053
  - 0.40960970520973206
  - 0.3996031582355499
  - 0.39762169122695923
  - 0.3984948396682739
  - 0.40454915165901184
loss_records_fold4:
  train_losses:
  - 6.116254761815071
  - 5.961842724680901
  - 5.9901941388845446
  - 6.088746643066407
  - 5.9789722800254825
  - 5.92478076517582
  - 5.869165787100792
  - 5.858212479948998
  - 5.953050282597542
  - 6.199789452552796
  - 5.926878851652146
  - 6.115879219770432
  - 6.131312701106072
  - 6.049565725028515
  - 5.983107742667198
  - 5.83125124424696
  - 5.895982551574708
  - 5.91205470263958
  - 5.873131999373436
  validation_losses:
  - 0.39163485169410706
  - 0.3921966552734375
  - 0.4124164581298828
  - 0.39226341247558594
  - 0.39487433433532715
  - 0.3924311101436615
  - 0.39102065563201904
  - 0.39232921600341797
  - 0.3911760747432709
  - 0.3962027430534363
  - 0.4157814383506775
  - 0.39222970604896545
  - 0.41611626744270325
  - 0.41756513714790344
  - 0.3917911648750305
  - 0.4008495509624481
  - 0.40342772006988525
  - 0.3938808739185333
  - 0.3918936252593994
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 47 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 54 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 60 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:22:34.823738'
