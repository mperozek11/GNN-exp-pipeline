config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 17:58:50.617528'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/46/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/46/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/46/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/46/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/46/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/46/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/46/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/46/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/46/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/46/fold_4_state_dict.pt
loss_records_fold3:
  train_losses:
  - 4.434767812490463
  - 4.285904347896576
  - 4.1772215366363525
  - 4.161763966083527
  - 3.854426860809326
  - 4.0262545347213745
  - 4.061044663190842
  - 4.025583475828171
  - 4.297554582357407
  - 4.317893147468567
  - 4.057523280382156
  - 4.073223322629929
  - 4.354774445295334
  - 4.447102725505829
  - 4.1564881503582
  - 4.243183493614197
  - 4.03687846660614
  - 4.231426894664764
  - 4.531361848115921
  - 4.127836525440216
  - 4.2031165063381195
  - 4.1962074637413025
  - 4.1038199961185455
  - 4.085525214672089
  - 4.181348949670792
  - 3.879929691553116
  - 3.9522963762283325
  - 4.185705721378326
  - 4.240382820367813
  - 4.191408544778824
  - 3.927321642637253
  - 4.0515609085559845
  - 4.109221875667572
  - 4.012895882129669
  - 4.026522725820541
  - 4.0594925582408905
  - 3.960127502679825
  - 4.046491324901581
  - 3.958202302455902
  - 3.883339673280716
  - 3.928440809249878
  - 4.048647224903107
  - 3.9586760699748993
  - 3.9384404718875885
  - 3.9122833907604218
  - 4.018573641777039
  - 3.944859564304352
  - 3.792848914861679
  - 3.9170803129673004
  - 4.0930303037166595
  - 4.033890336751938
  - 4.318651437759399
  - 4.179481416940689
  - 4.122851252555847
  - 4.440503537654877
  - 4.357726126909256
  - 4.084483981132507
  - 3.988081157207489
  - 4.394781589508057
  - 4.153614640235901
  - 4.108372896909714
  - 4.213456720113754
  - 4.011084258556366
  - 4.239959627389908
  - 4.555423527956009
  - 4.064041286706924
  - 4.063301891088486
  - 4.247743666172028
  - 4.2653878927230835
  - 3.954153388738632
  - 4.182131767272949
  - 4.6443299651145935
  - 4.338295549154282
  - 4.1790973246097565
  - 4.181763738393784
  - 3.9729221165180206
  - 3.9395751953125
  - 3.809447228908539
  - 4.225380539894104
  - 4.033434629440308
  - 3.8318271934986115
  - 4.1047473549842834
  - 4.345588266849518
  - 3.905212864279747
  - 4.099911272525787
  - 4.034571975469589
  - 3.9136145561933517
  - 3.9496916830539703
  - 3.9971503019332886
  - 3.816370904445648
  - 3.7949429899454117
  - 4.181220144033432
  - 3.828105702996254
  - 3.942215621471405
  - 4.106525808572769
  - 4.021436959505081
  - 4.422562152147293
  - 4.060145407915115
  - 3.969958007335663
  - 3.8699111938476562
  validation_losses:
  - 0.3983990252017975
  - 0.46605294942855835
  - 0.38485217094421387
  - 0.37445902824401855
  - 0.3854738175868988
  - 0.38506457209587097
  - 0.3944360017776489
  - 0.43143129348754883
  - 0.4734751582145691
  - 0.6604592204093933
  - 0.6108226776123047
  - 0.5573762059211731
  - 0.42651495337486267
  - 0.48326119780540466
  - 0.38368502259254456
  - 0.38801732659339905
  - 0.37569454312324524
  - 0.6930084824562073
  - 0.8626169562339783
  - 0.6886117458343506
  - 0.7403855919837952
  - 0.37658384442329407
  - 0.5604615807533264
  - 0.5143598914146423
  - 0.9718018770217896
  - 0.5737950801849365
  - 0.4535616934299469
  - 0.43095648288726807
  - 0.4281008541584015
  - 0.4820702373981476
  - 0.45956674218177795
  - 0.372199147939682
  - 0.5821554064750671
  - 1.1990203857421875
  - 1.0413730144500732
  - 0.39958545565605164
  - 0.4142681658267975
  - 0.8189774751663208
  - 0.7293756604194641
  - 0.3827894330024719
  - 0.6125943064689636
  - 1.1967641115188599
  - 0.964739203453064
  - 0.894342303276062
  - 0.41603022813796997
  - 0.4353345036506653
  - 0.40477076172828674
  - 0.6210930943489075
  - 0.38197317719459534
  - 0.40928417444229126
  - 0.45706403255462646
  - 0.43370774388313293
  - 0.430962473154068
  - 0.4120297431945801
  - 0.4613426625728607
  - 0.5285393595695496
  - 0.5188614726066589
  - 0.5721337199211121
  - 0.718985915184021
  - 0.5441336631774902
  - 0.4449496567249298
  - 0.43073219060897827
  - 0.4822063148021698
  - 0.37888044118881226
  - 0.46455979347229004
  - 0.5197135806083679
  - 0.8479596376419067
  - 0.4266032874584198
  - 0.40317806601524353
  - 0.7626869678497314
  - 0.6293473243713379
  - 0.5839898586273193
  - 0.41632723808288574
  - 0.4503921866416931
  - 0.5481149554252625
  - 0.4541800022125244
  - 0.6331833004951477
  - 0.6024767756462097
  - 0.5950610637664795
  - 0.6464926600456238
  - 0.3805585205554962
  - 0.4844517111778259
  - 0.9318090081214905
  - 0.8192716836929321
  - 0.49450618028640747
  - 0.3700297772884369
  - 0.37803977727890015
  - 0.5344146490097046
  - 0.5773472189903259
  - 0.8042894601821899
  - 1.0245009660720825
  - 0.6474428176879883
  - 0.5393760204315186
  - 0.9294531345367432
  - 0.3749147355556488
  - 0.37547367811203003
  - 0.393606960773468
  - 0.39246976375579834
  - 0.3841567039489746
  - 0.3871343731880188
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 9 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:01:15.913036'
