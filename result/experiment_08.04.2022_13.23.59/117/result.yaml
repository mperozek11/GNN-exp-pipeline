config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 03:20:54.763068'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/117/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/117/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/117/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/117/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/117/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/117/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/117/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/117/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/117/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/117/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 7.567673414945602
  - 8.283524364233017
  - 8.119286894798279
  - 8.03938141465187
  - 8.636337578296661
  - 8.015444964170456
  - 7.7566739320755005
  - 9.179146409034729
  - 11.430152997374535
  - 9.062772691249847
  - 7.873687773942947
  - 7.863410696387291
  - 7.943160742521286
  - 9.399205356836319
  - 9.340639650821686
  - 10.33430826663971
  - 8.048161029815674
  - 12.918048053979874
  - 9.17864316701889
  - 7.805405020713806
  - 7.600666970014572
  - 7.55116131901741
  - 8.419303625822067
  - 7.591338276863098
  - 8.000826954841614
  - 8.458355337381363
  - 7.312676668167114
  - 8.438281744718552
  - 7.8054424077272415
  - 7.731512948870659
  - 7.9089952409267426
  - 7.97904446721077
  - 10.50233256816864
  - 8.344894081354141
  - 7.656555682420731
  - 7.788597494363785
  - 7.597615420818329
  - 7.752370536327362
  - 10.25089144706726
  - 11.622661247849464
  - 7.830923825502396
  - 10.840591549873352
  - 7.966597005724907
  - 7.8272621631622314
  - 8.198335275053978
  - 8.036857813596725
  - 8.337658882141113
  - 12.306423634290695
  - 17.337884783744812
  - 9.407596230506897
  - 7.4448718428611755
  - 7.752880871295929
  - 7.643337190151215
  - 11.03553682565689
  - 13.304282486438751
  - 8.091009885072708
  - 7.747725069522858
  - 9.060397982597351
  - 9.12435694038868
  - 10.394251108169556
  - 8.782382935285568
  - 8.30655112862587
  - 8.425355583429337
  - 9.139029949903488
  - 8.181007400155067
  - 7.9569211304187775
  - 9.663550525903702
  - 7.944197237491608
  - 7.773380994796753
  - 8.555073261260986
  - 8.72917240858078
  - 16.73213592171669
  - 20.91047900915146
  - 10.839900195598602
  - 9.326927334070206
  - 9.148753598332405
  - 8.685136169195175
  - 8.490811467170715
  - 8.40654918551445
  - 7.575184881687164
  - 7.9906491339206696
  - 7.946998655796051
  - 8.773788213729858
  - 10.679128259420395
  - 8.172961011528969
  - 8.216275691986084
  - 8.88253328204155
  - 11.305388510227203
  - 14.434106677770615
  - 8.633581578731537
  - 7.826721489429474
  - 8.631028860807419
  - 8.041439861059189
  - 8.051138252019882
  - 8.872212499380112
  - 8.373472601175308
  - 12.43374389410019
  - 8.329335033893585
  - 8.222188025712967
  - 8.650558799505234
  validation_losses:
  - 0.4337444305419922
  - 0.40641161799430847
  - 0.4627133905887604
  - 0.41790571808815
  - 0.42296794056892395
  - 0.410510778427124
  - 0.4226429760456085
  - 0.45269688963890076
  - 0.4245695173740387
  - 0.5077755451202393
  - 0.421010285615921
  - 0.42683571577072144
  - 0.6393415927886963
  - 0.6565989851951599
  - 0.6596207618713379
  - 0.4167797863483429
  - 0.5019537806510925
  - 0.6017891764640808
  - 0.4312143623828888
  - 0.4077683389186859
  - 0.405362606048584
  - 0.5235982537269592
  - 0.41158315539360046
  - 0.46577519178390503
  - 0.49503904581069946
  - 0.4073556959629059
  - 0.5105450749397278
  - 0.4171545207500458
  - 0.4225247800350189
  - 0.4146159887313843
  - 0.46319958567619324
  - 0.5910317301750183
  - 0.5162666440010071
  - 0.41943326592445374
  - 0.4351465702056885
  - 0.43641436100006104
  - 0.4160333275794983
  - 0.4455450177192688
  - 0.621456503868103
  - 0.4135185480117798
  - 0.6018895506858826
  - 0.5364324450492859
  - 0.5474414229393005
  - 0.450271338224411
  - 0.41666117310523987
  - 0.4133569300174713
  - 0.6533376574516296
  - 0.6227651834487915
  - 0.9053157567977905
  - 0.4048962891101837
  - 0.4629978537559509
  - 0.40676355361938477
  - 0.4352085292339325
  - 1.7397390604019165
  - 0.4692147374153137
  - 0.42916205525398254
  - 0.5138033032417297
  - 0.7425033450126648
  - 0.5847235321998596
  - 0.46290645003318787
  - 0.41411080956459045
  - 0.6526314616203308
  - 0.5980843305587769
  - 0.5849086046218872
  - 0.4810047745704651
  - 0.411731094121933
  - 0.4343968629837036
  - 0.4098756015300751
  - 0.40573650598526
  - 0.42105793952941895
  - 0.5802383422851562
  - 1.3298481702804565
  - 0.5520555377006531
  - 0.6250605583190918
  - 0.6140425801277161
  - 0.4079413414001465
  - 0.5259411334991455
  - 0.49226582050323486
  - 0.4411628544330597
  - 0.4146208167076111
  - 0.4282546639442444
  - 0.40502601861953735
  - 0.500656247138977
  - 0.4494358003139496
  - 0.43934792280197144
  - 0.5297000408172607
  - 0.4292033314704895
  - 1.0177911520004272
  - 0.8734884858131409
  - 0.4195428490638733
  - 0.42393046617507935
  - 0.4160706102848053
  - 0.4237254559993744
  - 0.5573496222496033
  - 0.40903130173683167
  - 0.7334173917770386
  - 0.4950045049190521
  - 0.40932631492614746
  - 0.4120648205280304
  - 0.4517911374568939
loss_records_fold2:
  train_losses:
  - 7.864662200212479
  - 8.268987625837326
  - 8.36526334285736
  - 8.116821676492691
  - 7.814627170562744
  - 7.6994616240262985
  - 8.217829629778862
  - 7.911016091704369
  - 7.834820479154587
  - 7.978707641363144
  - 8.322673946619034
  - 10.16407835483551
  - 32.70964777469635
  - 22.114841371774673
  - 12.525824934244156
  - 8.330547869205475
  - 8.528029799461365
  - 8.040345877408981
  - 10.29036232829094
  - 9.596530169248581
  - 13.738211452960968
  - 11.77887836098671
  - 8.793096601963043
  - 10.143488377332687
  - 9.385518968105316
  - 9.083125561475754
  - 7.8487962782382965
  - 7.896198391914368
  - 11.932080775499344
  - 7.9978585839271545
  - 7.824590176343918
  - 8.661939516663551
  - 14.592897355556488
  - 10.768848776817322
  - 8.195116639137268
  - 7.7666318118572235
  - 8.061191529035568
  - 9.142453208565712
  - 8.261070221662521
  - 7.788637280464172
  - 7.934105694293976
  - 12.816181093454361
  - 16.145530551671982
  - 9.545363783836365
  - 7.585725873708725
  - 7.483023837208748
  - 8.374148696660995
  - 8.133740037679672
  - 9.006424963474274
  - 11.044352114200592
  - 11.625867664813995
  - 8.144249975681305
  - 8.015621364116669
  - 8.069801852107048
  - 8.225860700011253
  - 7.990675926208496
  - 8.981728285551071
  - 8.096948504447937
  - 7.732547909021378
  - 7.932697236537933
  - 7.839245557785034
  - 8.877642214298248
  - 8.150805681943893
  - 8.22039544582367
  - 8.506935760378838
  - 11.169243812561035
  - 9.753479957580566
  - 10.154281586408615
  - 9.415482759475708
  - 7.996827080845833
  - 11.831066131591797
  - 8.747417122125626
  - 7.975773602724075
  - 8.456432193517685
  - 8.311541199684143
  - 8.461414128541946
  - 14.87515190243721
  - 19.562493860721588
  - 15.694184005260468
  - 9.11388984322548
  - 8.315629571676254
  - 7.838879644870758
  - 7.654785752296448
  - 7.826800063252449
  - 8.711036160588264
  - 7.573684424161911
  - 8.154007911682129
  - 9.29768168926239
  - 14.243304669857025
  - 14.250432133674622
  - 16.669067353010178
  - 10.178200870752335
  - 8.474898219108582
  - 7.971341699361801
  - 9.38197073340416
  - 8.69174125790596
  - 8.106218829751015
  - 8.247532546520233
  - 7.7652199268341064
  - 8.069557398557663
  validation_losses:
  - 0.3898811340332031
  - 0.4709360897541046
  - 0.39079251885414124
  - 0.38498586416244507
  - 0.38473308086395264
  - 0.4081833064556122
  - 0.4413314163684845
  - 0.38590383529663086
  - 0.4340985119342804
  - 0.4039999842643738
  - 0.4669063985347748
  - 4965201.0
  - 7.58596134185791
  - 9.8052339553833
  - 13.442022323608398
  - 4.281247615814209
  - 0.40442606806755066
  - 0.39992737770080566
  - 0.8270049691200256
  - 0.3844248950481415
  - 1.1543306112289429
  - 0.3984898328781128
  - 0.5371926426887512
  - 0.4142133593559265
  - 0.5098506212234497
  - 0.41395318508148193
  - 0.398158460855484
  - 0.48138150572776794
  - 0.3920342028141022
  - 0.39338645339012146
  - 0.44799941778182983
  - 0.5759011507034302
  - 0.48658064007759094
  - 0.45294055342674255
  - 0.4082658588886261
  - 0.47498852014541626
  - 0.4105391502380371
  - 0.3836319148540497
  - 0.38559490442276
  - 0.384078711271286
  - 0.4235221743583679
  - 1.062951683998108
  - 0.5820040702819824
  - 0.3889220058917999
  - 0.388037770986557
  - 0.4270818829536438
  - 0.42404836416244507
  - 0.4250722825527191
  - 0.40849876403808594
  - 0.8823644518852234
  - 0.5979637503623962
  - 0.38405296206474304
  - 0.3868296146392822
  - 0.3855321407318115
  - 0.5120842456817627
  - 0.413164883852005
  - 0.39024823904037476
  - 0.3940172493457794
  - 0.3858833611011505
  - 0.3839806616306305
  - 0.42061087489128113
  - 0.38768190145492554
  - 0.4095182418823242
  - 0.5648679733276367
  - 0.47036874294281006
  - 0.3885655701160431
  - 0.39306357502937317
  - 0.5131158828735352
  - 0.5012357831001282
  - 0.4525780975818634
  - 0.4112185537815094
  - 0.3919963240623474
  - 0.5199532508850098
  - 0.5067633390426636
  - 0.39289048314094543
  - 0.43320220708847046
  - 0.7630473971366882
  - 0.5816183686256409
  - 0.5593252182006836
  - 0.4357331395149231
  - 0.38669466972351074
  - 0.38789626955986023
  - 0.4226699471473694
  - 0.38576653599739075
  - 0.3866521418094635
  - 0.39706698060035706
  - 0.3835611641407013
  - 0.4766327142715454
  - 0.43807923793792725
  - 1.030116319656372
  - 0.6942898631095886
  - 0.4032086133956909
  - 0.3835057318210602
  - 0.7644984126091003
  - 0.4130775034427643
  - 0.45322850346565247
  - 0.39668187499046326
  - 0.3911193609237671
  - 0.39468273520469666
  - 0.3936378061771393
loss_records_fold4:
  train_losses:
  - 8.844605296850204
  - 8.106523841619492
  - 8.197054356336594
  - 7.726375162601471
  - 7.668089747428894
  - 9.223116755485535
  - 10.332319408655167
  - 12.338729500770569
  - 8.886344015598297
  - 8.16968148946762
  - 7.6707877814769745
  - 7.760916292667389
  - 7.701334923505783
  - 8.888733237981796
  - 11.229102715849876
  - 14.198289185762405
  - 10.883324652910233
  - 11.36820900440216
  - 15.580944865942001
  - 11.32284015417099
  - 9.868479907512665
  - 8.285197138786316
  - 9.238623917102814
  - 7.678180038928986
  - 7.962528109550476
  - 7.766511976718903
  - 9.239232659339905
  - 9.780035197734833
  - 8.715595036745071
  - 8.456503599882126
  - 7.661600053310394
  - 9.234279215335846
  - 8.915482848882675
  - 8.388126790523529
  - 9.19774043560028
  - 8.507930725812912
  - 8.434312343597412
  - 9.562297135591507
  - 8.011497855186462
  - 10.340445503592491
  - 9.75069835782051
  - 8.391188606619835
  - 10.190955579280853
  - 11.376441895961761
  - 9.994617521762848
  - 8.09891825914383
  - 8.362121909856796
  - 8.031136602163315
  - 9.264811098575592
  - 7.717659816145897
  - 7.782116994261742
  - 10.703179001808167
  - 8.196683883666992
  - 8.361824095249176
  - 8.39582273364067
  - 9.543626457452774
  - 10.449144691228867
  - 9.168029993772507
  - 9.320881307125092
  - 8.389291256666183
  - 7.875198721885681
  - 7.712064117193222
  - 7.69306480884552
  - 9.032992869615555
  - 8.26521122455597
  - 9.019666343927383
  - 9.741086632013321
  - 10.630121052265167
  - 13.64816865324974
  - 11.623952805995941
  - 9.465363025665283
  - 7.673038572072983
  - 8.131907254457474
  - 7.951064884662628
  - 7.961533933877945
  - 7.469017595052719
  - 8.999297499656677
  - 13.828543543815613
  - 14.987813025712967
  - 10.944485604763031
  - 9.846870005130768
  - 7.965897023677826
  - 8.580446392297745
  - 8.84576490521431
  - 8.415146380662918
  - 8.156776189804077
  - 7.719556897878647
  - 7.950298339128494
  - 8.74445067346096
  - 9.373135507106781
  - 9.361006408929825
  - 9.111607372760773
  - 13.191066026687622
  - 11.760248094797134
  - 12.488651096820831
  - 9.452318549156189
  - 7.592001408338547
  - 8.069027423858643
  - 7.949838072061539
  - 7.771743565797806
  validation_losses:
  - 0.5144205093383789
  - 0.3925590515136719
  - 0.42626750469207764
  - 0.4158661961555481
  - 0.39092588424682617
  - 0.435851126909256
  - 0.42612048983573914
  - 0.5021018385887146
  - 0.3990667760372162
  - 0.3943112790584564
  - 0.4076729416847229
  - 0.39521780610084534
  - 0.3946324586868286
  - 0.6452597379684448
  - 0.7261233925819397
  - 0.7600441575050354
  - 0.42614102363586426
  - 0.7528674006462097
  - 0.758308470249176
  - 0.5516508221626282
  - 0.41934388875961304
  - 0.5488512516021729
  - 0.3963405191898346
  - 0.3933846354484558
  - 0.39312252402305603
  - 0.3932422697544098
  - 0.5405042767524719
  - 0.3923584818840027
  - 0.5321345925331116
  - 0.3919204771518707
  - 0.4230143427848816
  - 0.5879610776901245
  - 0.6018451452255249
  - 0.396384596824646
  - 0.4468529522418976
  - 0.39184144139289856
  - 0.5566878914833069
  - 0.41248762607574463
  - 0.5283637046813965
  - 0.5436391234397888
  - 0.40274474024772644
  - 0.5365448594093323
  - 0.5552470684051514
  - 0.4342121183872223
  - 0.39656296372413635
  - 0.46617069840431213
  - 0.4456450641155243
  - 0.4064218997955322
  - 0.47955313324928284
  - 0.42575258016586304
  - 0.49026793241500854
  - 0.3939720094203949
  - 0.5201975107192993
  - 0.41415274143218994
  - 0.3954722583293915
  - 0.709843099117279
  - 1.0060456991195679
  - 0.5031588673591614
  - 0.44938141107559204
  - 0.39150509238243103
  - 0.4052317440509796
  - 0.3939846456050873
  - 0.5017620921134949
  - 0.4704746603965759
  - 0.869094729423523
  - 0.5977988839149475
  - 0.7796215415000916
  - 0.4409450888633728
  - 0.43973666429519653
  - 0.3967115879058838
  - 0.39285793900489807
  - 0.41278359293937683
  - 0.42121225595474243
  - 0.40187591314315796
  - 0.4054073989391327
  - 0.41294732689857483
  - 0.685712456703186
  - 0.6818039417266846
  - 0.5588540434837341
  - 0.4370439052581787
  - 0.6519660949707031
  - 1.1865242719650269
  - 0.4107411503791809
  - 0.39225858449935913
  - 0.4099039137363434
  - 0.4856288433074951
  - 0.3943823277950287
  - 0.502865195274353
  - 0.413642555475235
  - 0.4287991523742676
  - 0.4752413332462311
  - 0.5277326703071594
  - 0.5260353684425354
  - 0.6431635618209839
  - 0.5063620209693909
  - 0.393984317779541
  - 0.39329397678375244
  - 0.3932168781757355
  - 0.39404579997062683
  - 0.43196162581443787
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 82 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 9 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:06:22.567872'
