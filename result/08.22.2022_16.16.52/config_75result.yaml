config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:54:13.969599'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_75fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_75fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_75fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_75fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_75fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_75fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_75fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_75fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_75fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_75fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 14.805243182182313
  - 5.438976049423218
  - 3.638869154453278
  - 2.281278443336487
  - 3.8643134474754337
  - 1.6078930139541627
  - 1.2975869953632355
  - 1.0390414476394654
  - 1.1813074290752412
  - 1.0196882784366608
  - 0.8990834474563599
  - 1.184368544816971
  - 1.7383027225732803
  - 1.32920298576355
  - 1.577876937389374
  - 4.321782171726227
  - 5.769603526592255
  - 2.5173310518264773
  - 2.476498943567276
  - 2.678966760635376
  - 2.0391607463359835
  - 1.3963140010833741
  - 1.3192422032356264
  - 0.8741898894309998
  - 0.8300200998783112
  - 0.8270458340644837
  - 1.047747391462326
  - 1.0284797728061676
  - 0.8337961196899415
  - 0.8184084653854371
  - 1.0097297608852387
  - 1.1225858151912689
  - 0.8919404268264771
  - 3.4319871187210085
  - 0.9372740507125855
  - 1.1261627733707429
  - 1.05094153881073
  - 0.8812908232212067
  - 0.9692892611026764
  - 0.8801761627197267
  - 1.5917133748531342
  - 0.7684097588062286
  - 0.8166181176900864
  validation_losses:
  - 3.3959429264068604
  - 1.8339452743530273
  - 1.1654248237609863
  - 2.9417526721954346
  - 0.5586351156234741
  - 0.7003719210624695
  - 0.40953150391578674
  - 0.44463595747947693
  - 0.6927174925804138
  - 0.4173266589641571
  - 0.7466675639152527
  - 0.5111856460571289
  - 0.5295333862304688
  - 1.1468384265899658
  - 0.5388031601905823
  - 0.8308217525482178
  - 0.6644189953804016
  - 1.2458933591842651
  - 0.4794357120990753
  - 1.3004158735275269
  - 0.47439420223236084
  - 0.4459880292415619
  - 0.4008772671222687
  - 0.3978695869445801
  - 0.5295589566230774
  - 0.38839513063430786
  - 0.40036633610725403
  - 0.38478174805641174
  - 0.4157622754573822
  - 0.41299867630004883
  - 0.38447773456573486
  - 0.40730687975883484
  - 0.39693596959114075
  - 0.3935208320617676
  - 0.39999645948410034
  - 0.4450395405292511
  - 0.49371254444122314
  - 0.48052698373794556
  - 0.4702509641647339
  - 0.4182995855808258
  - 0.4170505106449127
  - 0.38676273822784424
  - 0.38176465034484863
loss_records_fold1:
  train_losses:
  - 0.8373375236988068
  - 0.9378859341144562
  - 0.9364609211683274
  - 0.9846644282341004
  - 0.9734504103660584
  - 0.8144172668457031
  - 5.0545876026153564
  - 0.8073381900787354
  - 0.9721784114837647
  - 0.9467484891414643
  - 5.886116003990174
  - 1.04087033867836
  - 1.048570428788662
  - 0.9362730562686921
  - 1.0591369032859803
  - 1.0298370122909546
  - 1.391971105337143
  - 1.0627577483654023
  - 0.8582588076591492
  - 0.948254492878914
  - 0.8906386494636536
  - 0.8683020234107972
  - 0.9888890981674194
  - 0.8986254632472992
  - 1.1200980782508851
  - 0.8260730683803559
  - 0.8735279262065888
  - 0.8353407621383667
  - 0.7908189833164215
  - 0.7880122184753419
  - 0.8610494315624238
  - 0.8191371113061905
  - 0.7911946892738343
  - 0.8037483334541321
  - 0.8459217727184296
  - 0.9266166925430298
  - 1.1016148924827576
  - 0.9133526861667634
  - 0.806060802936554
  - 1.572100669145584
  - 2.5072775185108185
  - 1.1715604066848755
  - 0.940778374671936
  - 1.060040718317032
  - 0.8492007493972779
  - 0.9802649676799775
  - 1.0091373145580291
  - 0.878319612145424
  - 0.8572130143642426
  - 0.9828254461288453
  - 1.0573372781276704
  - 0.8138222813606263
  - 0.8410539388656617
  - 0.8721267521381378
  - 0.8088329493999482
  - 1.1172721087932587
  - 0.9379105627536775
  - 0.9408334553241731
  - 0.8519246280193329
  - 0.85646550655365
  validation_losses:
  - 0.4197630286216736
  - 0.4044564962387085
  - 0.47826632857322693
  - 0.4496456980705261
  - 0.40444523096084595
  - 0.4371281862258911
  - 0.39910686016082764
  - 0.4171045422554016
  - 0.5358378291130066
  - 0.4074341058731079
  - 0.4525327682495117
  - 0.7759997248649597
  - 0.5188106298446655
  - 0.42638638615608215
  - 0.43273791670799255
  - 0.49380040168762207
  - 0.4478280544281006
  - 0.4225574731826782
  - 0.4027431905269623
  - 0.42610570788383484
  - 0.5075700283050537
  - 0.6273362636566162
  - 0.4002525210380554
  - 0.48696330189704895
  - 0.43190106749534607
  - 0.40993091464042664
  - 0.4015215039253235
  - 0.41381487250328064
  - 0.39689332246780396
  - 0.4175879955291748
  - 0.7360503077507019
  - 0.41114890575408936
  - 0.41603884100914
  - 0.39658522605895996
  - 0.4109589457511902
  - 0.4533158540725708
  - 0.5717071890830994
  - 0.3945581018924713
  - 0.4719112813472748
  - 0.9117708206176758
  - 0.44846591353416443
  - 0.4119652211666107
  - 0.5490569472312927
  - 0.4262487292289734
  - 0.415128231048584
  - 0.405141144990921
  - 0.39348122477531433
  - 0.4274372160434723
  - 0.4080277681350708
  - 0.4182637333869934
  - 0.4024083614349365
  - 0.41049692034721375
  - 0.40063712000846863
  - 0.42915117740631104
  - 0.4076981544494629
  - 0.3972238004207611
  - 0.39477041363716125
  - 0.40049073100090027
  - 0.392589807510376
  - 0.4006851613521576
loss_records_fold2:
  train_losses:
  - 0.8640069246292115
  - 1.803345799446106
  - 1.0347096383571626
  - 0.9925449967384339
  - 1.0050099551677705
  - 1.059166169166565
  - 0.8832183718681336
  - 0.941104030609131
  - 1.574920725822449
  - 1.0344610154628755
  - 0.7709941536188126
  - 0.7997220665216447
  - 0.8740766525268555
  - 0.8424786329269409
  - 0.8241566300392151
  - 0.8254097431898118
  - 0.8008577823638916
  - 0.9795391619205476
  - 1.1958306074142457
  - 1.1664678454399109
  - 0.9814196705818177
  - 0.9066067039966583
  - 0.904812628030777
  - 0.8043506205081941
  - 0.7809345424175262
  - 0.8330370366573334
  - 0.9635129630565644
  - 0.8889031052589417
  - 2.8257913887500763
  - 1.7281553864479067
  - 1.0301320552825928
  - 4.193427664041519
  - 1.0937314331531525
  - 0.8645338892936707
  - 1.018169629573822
  - 0.8919393360614777
  - 0.882705307006836
  - 2.723805248737335
  - 1.2551353454589844
  - 0.8448687613010407
  - 0.8045002371072769
  - 0.8342869818210602
  - 0.8147547066211701
  - 0.7838464200496674
  - 0.7754575192928315
  - 0.7898205935955048
  - 0.8534993052482606
  - 0.8009231775999069
  - 0.7869882881641388
  - 0.8658315241336823
  - 0.9543868511915208
  - 1.058007925748825
  - 0.8061712622642517
  - 0.7797732532024384
  - 0.8117085874080658
  - 0.7771851122379303
  - 0.787191653251648
  - 0.7764085710048676
  - 0.8065832972526551
  - 0.7775526702404023
  - 0.8300359427928925
  - 0.7507799923419953
  - 0.7541641473770142
  - 0.8122705936431885
  - 0.7696137487888337
  - 0.8048661947250366
  validation_losses:
  - 0.43670332431793213
  - 0.6081990003585815
  - 0.9042335748672485
  - 0.3851722478866577
  - 0.4856107532978058
  - 0.4903087615966797
  - 0.43158382177352905
  - 0.3825837969779968
  - 0.3793025612831116
  - 0.3808530271053314
  - 0.4304421544075012
  - 0.39852625131607056
  - 0.3941482603549957
  - 0.38632261753082275
  - 0.43487268686294556
  - 0.3809605836868286
  - 0.4426864683628082
  - 0.54227614402771
  - 0.47253838181495667
  - 0.4439758062362671
  - 0.38669273257255554
  - 0.4522014856338501
  - 0.37752196192741394
  - 0.3785119950771332
  - 0.3764590322971344
  - 0.37718749046325684
  - 0.41223448514938354
  - 0.4138551652431488
  - 0.3778251111507416
  - 0.38405290246009827
  - 0.39199841022491455
  - 0.3756546974182129
  - 0.46549656987190247
  - 0.389856219291687
  - 0.3897361159324646
  - 0.40008410811424255
  - 0.41765934228897095
  - 0.38940417766571045
  - 0.37616974115371704
  - 0.37917155027389526
  - 0.3924054205417633
  - 0.3876762092113495
  - 0.38380202651023865
  - 0.3824520707130432
  - 0.3767677843570709
  - 0.3946220576763153
  - 0.43516403436660767
  - 1.4573434591293335
  - 0.4801630675792694
  - 0.3843792676925659
  - 0.48126420378685
  - 0.3836381137371063
  - 0.3813056945800781
  - 0.38706687092781067
  - 0.41607868671417236
  - 0.37944361567497253
  - 0.38255736231803894
  - 0.3821134865283966
  - 0.4054989516735077
  - 0.44779956340789795
  - 0.3799833357334137
  - 0.37858322262763977
  - 0.379367470741272
  - 0.3810993432998657
  - 0.38302749395370483
  - 0.3787110149860382
loss_records_fold3:
  train_losses:
  - 1.0581208646297455
  - 0.9781547009944916
  - 0.8107950448989869
  - 0.897882503271103
  - 0.7686338603496552
  - 3.709672385454178
  - 0.8091469764709474
  - 0.7834926664829255
  - 0.7985327184200287
  - 0.7641231775283814
  - 0.8021389961242676
  - 0.8144799530506135
  - 0.770056626200676
  - 0.8087660610675812
  - 0.7788171768188477
  - 0.9097174942493439
  - 0.9128804326057435
  - 0.8798925876617432
  - 0.9072255969047547
  - 0.8576227009296418
  - 0.8136281490325928
  - 0.7834665477275848
  - 0.7928268492221833
  - 0.7873672127723694
  - 0.7791957557201385
  - 0.7748770475387574
  validation_losses:
  - 0.38661104440689087
  - 0.3824358880519867
  - 0.39858222007751465
  - 0.3806944489479065
  - 0.3789624273777008
  - 0.3779417872428894
  - 0.39090704917907715
  - 0.38062167167663574
  - 0.3811489939689636
  - 0.3820326626300812
  - 0.39320164918899536
  - 0.39017295837402344
  - 0.4331570267677307
  - 0.3857952356338501
  - 0.38847485184669495
  - 0.3886888027191162
  - 0.4508058726787567
  - 0.3916371762752533
  - 0.3885117769241333
  - 0.40023818612098694
  - 0.3918225169181824
  - 0.3934599459171295
  - 0.38412466645240784
  - 0.38936540484428406
  - 0.38915741443634033
  - 0.38474181294441223
loss_records_fold4:
  train_losses:
  - 0.7795810461044312
  - 0.8319448590278626
  - 0.9048899769783021
  - 0.7934905648231507
  - 0.7591356039047241
  - 0.8512102127075196
  - 0.8322535932064057
  - 0.8213764905929566
  - 0.8115021646022798
  - 0.7926436245441437
  - 0.8283183217048645
  - 0.9151342928409577
  - 1.2090322375297546
  - 0.9426108479499817
  - 0.8342805445194245
  - 1.1230202317237854
  - 0.7537103652954102
  - 0.7692233383655549
  - 0.7545266807079316
  - 0.7948882043361665
  - 0.8168712973594666
  - 0.7763691484928131
  - 0.7740566432476044
  - 0.7463976114988328
  - 0.9129754424095154
  - 0.9804773032665253
  - 0.9101838648319245
  - 0.827733188867569
  - 0.7797090291976929
  - 0.7401396095752717
  - 0.859196311235428
  - 0.7812407195568085
  - 0.813421791791916
  validation_losses:
  - 0.3899625539779663
  - 0.43293070793151855
  - 0.4089950919151306
  - 0.3996398448944092
  - 0.3950570821762085
  - 0.4135439097881317
  - 0.39351630210876465
  - 0.38841643929481506
  - 0.4096139967441559
  - 0.39423394203186035
  - 0.38634005188941956
  - 0.3832857310771942
  - 0.382219523191452
  - 0.4554954171180725
  - 0.4178871810436249
  - 0.4163975119590759
  - 0.40684911608695984
  - 0.39705607295036316
  - 0.39785999059677124
  - 0.4736325144767761
  - 0.40236619114875793
  - 0.3834287226200104
  - 0.41483280062675476
  - 0.4088621437549591
  - 0.43392154574394226
  - 0.40712180733680725
  - 0.42614123225212097
  - 0.39867594838142395
  - 0.39807432889938354
  - 0.39730972051620483
  - 0.3984262943267822
  - 0.39652377367019653
  - 0.3952735960483551
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 43 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 60 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 66 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 33 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:18:45.497051'
