config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:27:20.204791'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_46fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_46fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_46fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_46fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_46fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_46fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_46fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_46fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_46fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_46fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 34.87987062931061
  - 11.856704133749009
  - 11.752066910266876
  - 4.902702105045319
  - 3.3457746505737305
  - 3.2651527881622315
  - 6.132133919000626
  - 2.7693791508674623
  - 2.2099194407463076
  - 3.1190344333648685
  - 2.3595035672187805
  - 1.9769885003566743
  - 2.4246290624141693
  - 2.30177099108696
  - 2.107950708270073
  - 1.6974525392055513
  - 5.03532230257988
  - 2.557442969083786
  - 4.7689855098724365
  - 3.2665981352329254
  - 2.4026843428611757
  - 3.1910086154937747
  - 8.681846636533738
  - 5.974490332603455
  - 9.12051848769188
  - 6.353020098805428
  - 6.268398553133011
  - 3.4565553903579715
  - 4.475945591926575
  - 4.725910747051239
  - 4.066426378488541
  - 3.6248632192611696
  - 2.8256785809993747
  - 1.8462091445922852
  - 1.9858080327510834
  - 1.7595265030860903
  - 1.9630624175071718
  - 2.3862983644008637
  - 2.1163597255945206
  - 1.9780034065246583
  - 2.796094834804535
  - 2.0281191647052768
  - 4.904128068685532
  - 2.8285073041915894
  - 1.8673844695091248
  - 2.2803700625896455
  - 1.7867529928684236
  - 1.6113122642040254
  - 2.065362775325775
  - 1.8307737588882447
  - 1.7525678038597108
  - 1.7812676429748535
  - 1.8069923639297487
  - 1.6895180940628052
  - 1.900017297267914
  - 1.9379200875759126
  - 1.7617151975631715
  - 1.7556487619876862
  - 1.795887964963913
  - 1.5687840402126314
  - 1.5745247304439545
  - 2.005893462896347
  - 2.0616247177124025
  - 3.379077571630478
  - 3.864769244194031
  - 3.853051257133484
  - 2.0907898902893067
  - 1.7586477994918823
  - 2.387110197544098
  - 1.891396600008011
  - 2.4598122179508213
  - 3.0267117023468018
  - 4.3662727832794195
  - 4.137434959411621
  - 1.691800493001938
  - 3.8701676368713382
  - 1.8850467264652253
  - 1.782971090078354
  - 1.693341362476349
  - 1.6400089472532273
  - 2.1250450730323793
  - 2.3636569499969484
  - 1.7318736255168916
  - 1.7882998228073121
  - 2.7002939105033876
  - 1.8605656325817108
  - 1.7145991683006288
  - 1.6441478848457338
  validation_losses:
  - 4.7643537521362305
  - 1.8642337322235107
  - 0.8896402716636658
  - 0.6534920334815979
  - 0.48044976592063904
  - 0.7719459533691406
  - 0.5593342781066895
  - 0.6499849557876587
  - 0.47200387716293335
  - 0.6200604438781738
  - 0.7700809240341187
  - 0.5298280119895935
  - 0.6519612073898315
  - 0.4482390582561493
  - 0.5946551561355591
  - 0.5299972891807556
  - 0.5146905183792114
  - 0.6669105291366577
  - 0.4150114953517914
  - 0.4743030071258545
  - 0.41453656554222107
  - 0.4057157635688782
  - 0.3959353268146515
  - 0.42690733075141907
  - 0.5331510901451111
  - 0.4799569249153137
  - 0.4429759681224823
  - 0.5892462134361267
  - 0.5286148190498352
  - 3.2637133598327637
  - 0.4520666301250458
  - 0.40423569083213806
  - 0.40645337104797363
  - 0.40000370144844055
  - 0.39570826292037964
  - 0.4106655716896057
  - 0.41228580474853516
  - 0.4979795217514038
  - 0.49596500396728516
  - 0.4252527356147766
  - 0.43872663378715515
  - 0.39998745918273926
  - 0.5232515931129456
  - 0.46590501070022583
  - 0.3989996612071991
  - 0.3862500488758087
  - 0.4086306393146515
  - 0.43324247002601624
  - 0.4035601317882538
  - 0.4594399631023407
  - 0.41812649369239807
  - 0.3875459134578705
  - 0.4163818359375
  - 0.3998333811759949
  - 0.40718352794647217
  - 0.40203261375427246
  - 0.4419378340244293
  - 0.40624797344207764
  - 0.3979147672653198
  - 0.39273950457572937
  - 0.39938053488731384
  - 0.418396532535553
  - 0.43269121646881104
  - 0.4387188255786896
  - 0.4140152931213379
  - 0.510566234588623
  - 0.4447927176952362
  - 0.5172247290611267
  - 0.4137149453163147
  - 0.42888370156288147
  - 0.39668524265289307
  - 0.41389694809913635
  - 0.4367568790912628
  - 0.4044448137283325
  - 0.39622795581817627
  - 0.406693696975708
  - 0.43127888441085815
  - 0.39891189336776733
  - 0.415372371673584
  - 0.3934502601623535
  - 0.39812201261520386
  - 0.4120556712150574
  - 0.40170398354530334
  - 0.3965964615345001
  - 0.40182188153266907
  - 0.402774453163147
  - 0.3925510346889496
  - 0.39392179250717163
loss_records_fold1:
  train_losses:
  - 1.5773167192935944
  - 1.572051101922989
  - 1.8324995398521424
  - 1.5751563251018526
  - 1.5971150517463686
  - 1.738041412830353
  - 2.2283461928367614
  - 1.6088497757911684
  - 1.8045952737331392
  - 1.632128578424454
  - 3.490665626525879
  validation_losses:
  - 0.4390663504600525
  - 0.43897736072540283
  - 0.4204843044281006
  - 0.4361214339733124
  - 0.4162203073501587
  - 0.41184765100479126
  - 0.41016966104507446
  - 0.41496726870536804
  - 0.41909146308898926
  - 0.4110592007637024
  - 0.40901461243629456
loss_records_fold2:
  train_losses:
  - 1.8309554219245912
  - 1.8572530329227448
  - 2.0468110382556914
  - 3.2589621365070345
  - 1.889096596837044
  - 2.0490140855312347
  - 1.9585962653160096
  - 1.759544199705124
  - 2.820180439949036
  - 1.7528894543647766
  - 3.4448012948036197
  - 2.5167832255363467
  - 2.0123864710330963
  - 1.9152476847171784
  - 1.6998776495456696
  - 1.6646628320217134
  - 1.6046048283576966
  - 2.65281642973423
  - 1.7827810972929001
  - 1.742598295211792
  - 1.599763298034668
  - 1.7340442001819611
  - 1.6259535014629365
  - 1.6213009893894197
  - 1.6465545117855074
  - 1.6544572293758393
  - 1.7885077893733978
  - 1.661691564321518
  - 1.7065422534942627
  - 2.086161601543427
  - 1.8068936288356783
  - 1.696823459863663
  - 1.5886641323566437
  - 3.581632500886917
  - 1.6880388081073763
  - 1.8994853615760805
  - 1.6302512705326082
  - 2.1693696647882463
  - 2.330917203426361
  - 2.395850765705109
  - 2.332962465286255
  - 3.681962651014328
  - 3.4282649934291842
  - 2.888233572244644
  - 1.9647888839244843
  - 1.8601172089576723
  - 2.187280124425888
  - 2.010994189977646
  - 2.0677749633789064
  - 1.8671694040298463
  - 1.8163026928901673
  - 1.652932345867157
  - 1.9386222898960115
  - 1.9742089331150057
  - 1.683779925107956
  - 2.4037054598331453
  - 5.009356802701951
  - 2.3904620468616486
  - 1.8578545451164246
  - 2.4009702801704407
  - 1.8012647807598114
  - 1.632160270214081
  - 2.0572736859321594
  - 1.8620510876178742
  - 2.3663107812404633
  - 2.170859569311142
  - 1.8716603040695192
  - 1.6060277640819551
  - 1.7157441496849062
  - 1.6145214915275574
  - 1.6711222469806672
  - 1.7505698651075363
  - 1.703103280067444
  - 1.638061374425888
  - 1.6617552638053894
  - 1.7043092489242555
  - 1.6050347685813904
  - 1.6125844180583955
  - 1.7213584184646606
  - 1.7205814063549043
  - 1.6375788807868958
  - 1.7007013857364655
  - 1.5965286850929261
  - 1.7550593614578247
  - 1.6830080926418305
  validation_losses:
  - 0.3984675407409668
  - 0.40107160806655884
  - 0.4035342037677765
  - 0.3921009600162506
  - 0.41719239950180054
  - 0.39895692467689514
  - 0.41269972920417786
  - 0.4501732885837555
  - 0.4046095609664917
  - 0.3878847658634186
  - 0.38421374559402466
  - 0.44944337010383606
  - 0.43534472584724426
  - 0.3994810879230499
  - 0.40794286131858826
  - 0.46065741777420044
  - 0.384117990732193
  - 0.3780131936073303
  - 0.3923766613006592
  - 0.38359153270721436
  - 0.4248602092266083
  - 0.3966085612773895
  - 0.3976001739501953
  - 0.3931003510951996
  - 0.3902412950992584
  - 0.3980123996734619
  - 0.46667996048927307
  - 0.3999408781528473
  - 0.40637636184692383
  - 0.38770297169685364
  - 0.40260621905326843
  - 0.38238051533699036
  - 0.3831000030040741
  - 0.40524905920028687
  - 0.41877955198287964
  - 0.4533785283565521
  - 0.4035063087940216
  - 8.870981216430664
  - 0.4106540083885193
  - 0.41446420550346375
  - 0.4111119210720062
  - 0.44992971420288086
  - 0.41389739513397217
  - 0.39292261004447937
  - 0.42401590943336487
  - 0.41997337341308594
  - 0.43719398975372314
  - 0.3945259153842926
  - 0.546252965927124
  - 0.4248106777667999
  - 0.39242812991142273
  - 0.38879144191741943
  - 0.4004017412662506
  - 0.39751702547073364
  - 0.3968035876750946
  - 0.4242037832736969
  - 0.41308191418647766
  - 0.40546613931655884
  - 0.41163328289985657
  - 0.4242030382156372
  - 0.4022660553455353
  - 0.4043274521827698
  - 0.41206058859825134
  - 0.4250507950782776
  - 0.4288276731967926
  - 0.4133297801017761
  - 0.402543842792511
  - 0.3865988850593567
  - 0.40713685750961304
  - 0.40060609579086304
  - 0.39571651816368103
  - 0.4109771251678467
  - 0.40231063961982727
  - 0.40069419145584106
  - 0.42063501477241516
  - 0.4018913507461548
  - 0.4002164900302887
  - 0.404116153717041
  - 0.4188598692417145
  - 0.42412614822387695
  - 0.4120144248008728
  - 0.40242090821266174
  - 0.39973485469818115
  - 0.3980223536491394
  - 0.39662736654281616
loss_records_fold3:
  train_losses:
  - 1.584839814901352
  - 1.7089610636234285
  - 2.168725037574768
  - 3.1508198559284213
  - 1.7741130471229554
  - 1.7347998321056366
  - 1.8692525744438173
  - 1.6982269883155823
  - 1.6448882281780244
  - 1.7405425012111664
  - 1.7213231921195984
  - 1.6502229273319244
  - 1.7544746696949005
  - 1.751064169406891
  - 1.6155063569545747
  - 1.6625757455825807
  - 1.6530048191547395
  - 1.6752056121826173
  - 1.738041514158249
  - 1.727455246448517
  - 1.6426186859607697
  - 2.2235657155513766
  - 1.5958565413951875
  - 1.5930865943431856
  - 2.035580098628998
  - 1.6272418081760407
  - 1.6067533671855927
  - 1.8920020520687104
  - 3.1286906123161318
  - 1.677478724718094
  - 1.723251360654831
  - 1.9040374398231508
  validation_losses:
  - 0.4111905097961426
  - 0.455610066652298
  - 0.43286871910095215
  - 0.40066561102867126
  - 0.40328603982925415
  - 0.4124436676502228
  - 0.4538264572620392
  - 0.4114026725292206
  - 0.44470927119255066
  - 0.42147505283355713
  - 0.4184260666370392
  - 0.3944033682346344
  - 0.44354113936424255
  - 0.4097610116004944
  - 0.4033598303794861
  - 0.4057891070842743
  - 0.41658076643943787
  - 0.40922942757606506
  - 0.41803085803985596
  - 0.4466674327850342
  - 0.40313228964805603
  - 0.40095844864845276
  - 0.4265140891075134
  - 0.4140689969062805
  - 0.40930047631263733
  - 0.6724389791488647
  - 0.4297376573085785
  - 0.42465466260910034
  - 0.4013601243495941
  - 0.408005952835083
  - 0.3981611132621765
  - 0.4052400290966034
loss_records_fold4:
  train_losses:
  - 1.642943698167801
  - 1.6697367787361146
  - 1.5908543825149537
  - 1.619745284318924
  - 1.6491425454616548
  - 1.6630538880825043
  - 1.5681192934513093
  - 1.7548505306243898
  - 1.6726328253746034
  - 1.6552609622478487
  - 1.5878007829189302
  validation_losses:
  - 0.448853999376297
  - 0.43904250860214233
  - 0.4337712526321411
  - 0.40022242069244385
  - 0.42702215909957886
  - 0.41112279891967773
  - 0.4194001853466034
  - 0.4139431118965149
  - 0.41755494475364685
  - 0.4057828187942505
  - 0.415266215801239
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 88 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 85 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 32 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:20:19.873998'
