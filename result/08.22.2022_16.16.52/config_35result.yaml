config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:55:04.373700'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_35fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_35fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_35fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_35fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_35fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_35fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_35fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_35fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_35fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_35fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 0.9612267017364502
  - 0.7947008132934571
  - 0.9122287213802338
  - 0.8037685751914978
  - 0.7719504117965699
  - 0.8075537264347077
  - 0.8298079669475555
  - 0.7722129166126251
  - 0.7505064219236375
  - 0.7942206799983978
  - 0.8303264319896698
  - 0.7754907965660096
  - 0.8153672814369202
  - 0.8064657270908356
  validation_losses:
  - 0.4135529696941376
  - 0.42753976583480835
  - 0.46864810585975647
  - 0.40335068106651306
  - 0.4029931128025055
  - 0.39887094497680664
  - 0.40127015113830566
  - 0.41207969188690186
  - 0.39274832606315613
  - 0.39579716324806213
  - 0.3875175714492798
  - 0.3843524158000946
  - 0.38863614201545715
  - 0.39178404211997986
loss_records_fold1:
  train_losses:
  - 0.7349350839853287
  - 0.8651437103748322
  - 0.8053860604763031
  - 0.7773991703987122
  - 0.7836884140968323
  - 0.7239347845315933
  - 0.7527733087539673
  - 0.8104739248752595
  - 0.8170323312282562
  - 0.769664603471756
  - 0.8124810874462128
  - 0.7761769771575928
  - 0.7485784709453583
  - 0.7523168742656708
  - 0.727284449338913
  validation_losses:
  - 0.38903963565826416
  - 0.3975314497947693
  - 0.41473838686943054
  - 0.3912527859210968
  - 0.392548531293869
  - 0.39071282744407654
  - 0.3957423269748688
  - 0.38940754532814026
  - 0.40476077795028687
  - 0.39191707968711853
  - 0.3912582993507385
  - 0.39458250999450684
  - 0.392790824174881
  - 0.39073678851127625
  - 0.38971611857414246
loss_records_fold2:
  train_losses:
  - 0.7597684979438782
  - 0.7412072896957398
  - 0.7165285259485246
  - 0.7445712327957154
  - 0.7547439336776733
  - 0.7348904222249986
  - 0.7209387063980103
  - 0.8047149240970612
  - 0.7254269570112228
  - 0.7760603368282318
  - 0.7602225005626679
  validation_losses:
  - 0.38865840435028076
  - 0.3849261999130249
  - 0.387777715921402
  - 0.3858819305896759
  - 0.3871833384037018
  - 0.384311705827713
  - 0.38415560126304626
  - 0.38980531692504883
  - 0.3887021839618683
  - 0.3906923830509186
  - 0.3912017345428467
loss_records_fold3:
  train_losses:
  - 0.7589930891990662
  - 0.741008621454239
  - 0.7860568404197693
  - 0.7836728930473328
  - 0.754396939277649
  - 0.7849091172218323
  - 0.7429742157459259
  - 0.7737617254257203
  - 0.7702583312988281
  - 0.7738018393516541
  - 0.7784830749034882
  - 0.7921658337116242
  validation_losses:
  - 0.3663924038410187
  - 0.36995404958724976
  - 0.4114793539047241
  - 0.371165931224823
  - 0.3688875138759613
  - 0.38540223240852356
  - 0.3733743727207184
  - 0.3706074059009552
  - 0.37003013491630554
  - 0.36937516927719116
  - 0.379130482673645
  - 0.3740127682685852
loss_records_fold4:
  train_losses:
  - 0.7431797564029694
  - 0.750061422586441
  - 0.7697133123874664
  - 0.7207931965589524
  - 0.7604388892650604
  - 0.7823632240295411
  - 0.7202724874019624
  - 0.7516232430934906
  - 0.7734598159790039
  - 0.7687724471092224
  - 0.7348531544208527
  - 0.7416199862957001
  - 0.7468941748142243
  - 0.7575964987277986
  - 0.7357708632946015
  - 0.7674068212509155
  validation_losses:
  - 0.37744173407554626
  - 0.3803224265575409
  - 0.38217294216156006
  - 0.3815617859363556
  - 0.3984820544719696
  - 0.38622918725013733
  - 0.38927263021469116
  - 0.40348801016807556
  - 0.4096505045890808
  - 0.42006683349609375
  - 0.4059683680534363
  - 0.38829919695854187
  - 0.3895549476146698
  - 0.38717275857925415
  - 0.3848702013492584
  - 0.3907797932624817
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:05:39.428904'
