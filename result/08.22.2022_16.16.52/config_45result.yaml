config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:06:43.485903'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_45fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_45fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_45fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_45fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_45fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_45fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_45fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_45fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_45fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_45fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 53.736395525932316
  - 24.03502935767174
  - 13.365603250265123
  - 12.945061072707176
  - 8.400780880451203
  - 10.072798424959183
  - 9.178129708766937
  - 7.018218010663986
  - 4.738168850541115
  - 5.553476160764695
  - 9.298913854360581
  - 10.645516645908357
  - 8.258666473627091
  - 4.309777301549912
  - 4.192652240395546
  - 6.269435733556747
  - 12.732025739550592
  - 12.537209546566011
  - 10.70735777616501
  - 9.654999989271165
  - 5.963587635755539
  - 10.701509207487106
  - 8.807492089271546
  - 7.607409262657166
  - 7.597515651583672
  - 6.566455149650574
  - 5.592522388696671
  - 7.652669727802277
  - 7.2190270066261295
  - 4.8572301030159
  - 5.506346607208252
  - 5.121222913265228
  - 5.571060210466385
  - 9.543806272745133
  - 4.698418307304382
  - 4.3823260247707365
  - 5.178375750780106
  - 4.808863484859467
  - 4.794119122624397
  - 3.442284604907036
  - 3.6724044322967533
  - 3.9552559792995456
  - 3.5470365285873413
  - 3.610030007362366
  - 3.496559494733811
  - 4.988581788539887
  - 4.425074523687363
  - 4.789808982610703
  - 5.099284803867341
  - 6.667141425609589
  - 3.7919824510812763
  - 3.764793038368225
  - 3.5128940373659137
  - 4.577753180265427
  - 3.432674378156662
  - 3.554710477590561
  - 3.680554485321045
  - 3.4165939152240754
  - 3.766279807686806
  - 3.1627857923507694
  - 3.8351877450942995
  - 3.4789313435554505
  - 3.810423243045807
  - 3.751806539297104
  - 3.46629433631897
  - 3.9042563438415527
  - 3.212567538022995
  - 3.2479368269443514
  - 3.1850987792015077
  - 3.0247853815555574
  - 3.2817837595939636
  - 3.577867162227631
  - 3.2854979217052462
  - 3.6465381383895874
  - 3.136744749546051
  - 3.3772216826677326
  - 3.232462576031685
  - 3.0826091647148135
  - 3.3064711272716525
  - 3.3111136615276338
  - 3.23240085542202
  - 3.058004739880562
  - 3.1003385424613956
  - 3.254742914438248
  - 3.4175118029117586
  - 3.0854200690984728
  - 3.058770513534546
  - 3.203462100028992
  - 3.1935925751924517
  - 3.2597548186779024
  - 3.2036670386791233
  - 3.200925731658936
  - 3.112467670440674
  - 3.1177665233612064
  - 3.252915060520172
  - 3.2122118771076202
  - 3.2005526900291446
  - 3.082389008998871
  - 3.0529412388801576
  - 3.0557834655046463
  validation_losses:
  - 3.797172784805298
  - 0.4769173264503479
  - 0.5245131254196167
  - 0.4964577853679657
  - 0.4562408924102783
  - 0.543303906917572
  - 0.8940498232841492
  - 0.4779718816280365
  - 0.6051077842712402
  - 0.4475080072879791
  - 0.9321084022521973
  - 0.4439491629600525
  - 0.5392223596572876
  - 0.415540486574173
  - 0.42011335492134094
  - 0.4809463620185852
  - 0.5816370248794556
  - 0.43396326899528503
  - 0.5871021151542664
  - 0.42736300826072693
  - 0.5248030424118042
  - 0.4128046929836273
  - 0.4787605106830597
  - 0.8447952270507812
  - 0.5357838869094849
  - 0.5699909925460815
  - 0.42441996932029724
  - 0.8292577862739563
  - 0.4904474914073944
  - 0.43626147508621216
  - 0.47069597244262695
  - 0.3966365158557892
  - 0.44588810205459595
  - 0.9305660724639893
  - 0.420502632856369
  - 0.5481874346733093
  - 0.7328791618347168
  - 0.6061200499534607
  - 0.4361533522605896
  - 0.37960943579673767
  - 0.4560249149799347
  - 0.3874507546424866
  - 0.3961403965950012
  - 0.42813432216644287
  - 0.4163532257080078
  - 0.38124921917915344
  - 0.3864278197288513
  - 0.38223424553871155
  - 0.6513421535491943
  - 0.4400911331176758
  - 0.609541118144989
  - 0.38830137252807617
  - 0.47066643834114075
  - 0.4106052815914154
  - 0.40458279848098755
  - 0.3892413079738617
  - 0.39041101932525635
  - 0.4054661691188812
  - 0.4454382061958313
  - 0.39028167724609375
  - 0.4369827210903168
  - 0.6040408611297607
  - 0.4116270840167999
  - 0.41413888335227966
  - 0.3993428945541382
  - 0.47323739528656006
  - 0.4727298617362976
  - 0.4060296416282654
  - 0.38523873686790466
  - 0.39058998227119446
  - 0.41798484325408936
  - 0.48235300183296204
  - 0.43158018589019775
  - 0.38765960931777954
  - 0.3783043622970581
  - 0.4022561311721802
  - 0.39071932435035706
  - 0.4136938452720642
  - 0.4360450506210327
  - 0.402512788772583
  - 0.4147430658340454
  - 0.38335946202278137
  - 0.3885835111141205
  - 0.38528260588645935
  - 0.3775669038295746
  - 0.381780743598938
  - 0.4136354923248291
  - 0.3920283913612366
  - 0.45102784037590027
  - 0.40549373626708984
  - 0.41319742798805237
  - 0.42772868275642395
  - 0.44086751341819763
  - 0.4057912230491638
  - 0.4058196246623993
  - 0.42712119221687317
  - 0.3979054391384125
  - 0.38430121541023254
  - 0.40672945976257324
  - 0.39073145389556885
loss_records_fold1:
  train_losses:
  - 3.03930926322937
  - 3.0958888292312623
  - 3.0573065996170046
  - 3.0098494946956635
  - 3.3275276482105256
  - 3.087320899963379
  - 3.1809373378753665
  - 3.3188124358654023
  - 3.2659894645214083
  - 3.2819811314344407
  - 3.291030448675156
  - 3.242232191562653
  - 3.2853413403034213
  - 3.2144236952066425
  - 3.195766580104828
  - 2.9946290433406832
  - 3.0233308732509614
  - 3.0545370757579806
  - 2.9982050955295563
  - 3.0506629139184955
  - 2.9711487323045733
  - 3.1528395265340805
  - 3.0612095654010774
  - 3.236272230744362
  - 3.065835464000702
  - 3.0529888778924943
  - 3.088787978887558
  - 3.2915317118167877
  - 3.136998707056046
  - 3.0779517352581025
  - 3.178702184557915
  - 3.0637203752994537
  - 3.10828697681427
  - 3.091767793893814
  - 2.9572622299194338
  - 3.215642207860947
  - 3.017656582593918
  - 3.0990936666727067
  - 2.9791984736919406
  - 3.117702090740204
  - 3.0149574398994448
  - 3.0791980147361757
  - 3.171990814805031
  - 3.2615186870098114
  - 3.024299138784409
  - 3.191018426418305
  - 3.188899666070938
  - 2.994876736402512
  - 3.2157902956008915
  - 3.165597081184387
  - 3.067365822196007
  - 2.9605613440275196
  - 2.959044981002808
  - 2.9586822748184205
  - 3.0474431931972505
  - 3.2204455733299255
  - 3.068408811092377
  - 2.994834765791893
  - 3.1125811994075776
  - 3.0816715508699417
  - 2.9750116705894474
  - 3.1011314004659654
  - 3.08281763792038
  - 3.2533480525016785
  - 3.079767346382141
  - 3.0712218105793
  - 2.9669761657714844
  - 3.259551799297333
  - 3.026107442378998
  - 3.0734980523586275
  - 3.043229749798775
  - 2.9805165231227875
  - 3.453866654634476
  - 3.5403377771377564
  - 5.915648055076599
  - 3.7534387886524203
  - 4.636811929941177
  - 3.600240629911423
  - 3.5967548072338107
  - 3.651863580942154
  - 3.5213083624839783
  - 3.3059925079345707
  - 3.1343959957361225
  - 3.0907821655273438
  - 3.163549941778183
  - 3.1128830134868624
  - 3.2049247801303866
  - 3.246812400221825
  - 3.17489852309227
  - 3.1852538824081424
  - 3.3672512233257295
  - 3.758536583185196
  - 3.735901486873627
  - 3.182478600740433
  - 3.6760668218135835
  - 3.710831817984581
  - 3.1705607354640963
  - 3.0926039010286335
  - 3.2794287085533145
  - 3.1170370638370515
  validation_losses:
  - 0.3955436944961548
  - 0.4231926202774048
  - 0.39719945192337036
  - 0.40532150864601135
  - 0.417325884103775
  - 0.4442775845527649
  - 0.4342772662639618
  - 0.4381788671016693
  - 0.4751468300819397
  - 0.4263586699962616
  - 0.4010346531867981
  - 0.3928107023239136
  - 0.4064628481864929
  - 0.5070865750312805
  - 0.417598158121109
  - 0.40298351645469666
  - 0.40449461340904236
  - 0.3914618492126465
  - 0.38914939761161804
  - 0.4073370695114136
  - 0.3944151699542999
  - 0.4029470980167389
  - 0.39468106627464294
  - 0.42097365856170654
  - 0.4020942151546478
  - 0.4594654440879822
  - 0.42611122131347656
  - 0.44810613989830017
  - 0.3986852169036865
  - 0.39383023977279663
  - 0.3972053825855255
  - 0.3948043882846832
  - 0.4201754033565521
  - 0.41688719391822815
  - 0.3942570090293884
  - 0.40202659368515015
  - 0.44943296909332275
  - 0.3987572491168976
  - 0.43327459692955017
  - 0.4697422385215759
  - 0.41810140013694763
  - 0.3954344391822815
  - 0.4190759062767029
  - 0.45665982365608215
  - 0.3993004560470581
  - 0.5017951726913452
  - 0.43193894624710083
  - 0.40200719237327576
  - 0.39683273434638977
  - 0.43932902812957764
  - 0.40817588567733765
  - 0.4030236303806305
  - 0.4206582307815552
  - 0.4028398394584656
  - 0.3980049788951874
  - 0.4296016991138458
  - 0.40316951274871826
  - 0.40090513229370117
  - 0.5572227239608765
  - 0.41793322563171387
  - 0.433163046836853
  - 0.4034894108772278
  - 0.4316691756248474
  - 0.41574737429618835
  - 0.5371284484863281
  - 0.40377697348594666
  - 0.4295713007450104
  - 0.41363731026649475
  - 0.48702272772789
  - 0.42775842547416687
  - 0.3965052664279938
  - 0.39120760560035706
  - 0.4389221668243408
  - 206.59848022460938
  - 0.4250287413597107
  - 0.4191693067550659
  - 0.4220481812953949
  - 0.47954267263412476
  - 0.43489721417427063
  - 0.42585647106170654
  - 136.0897674560547
  - 293630.4375
  - 800192192.0
  - 1625683394560.0
  - 0.4546220004558563
  - 0.4220276176929474
  - 0.4298163950443268
  - 0.41787564754486084
  - 0.4179494082927704
  - 399237.4375
  - 0.40217551589012146
  - 0.4211258292198181
  - 0.4102870523929596
  - 0.44041022658348083
  - 3.5326144695281982
  - 0.44234639406204224
  - 0.4077887237071991
  - 9696447.0
  - 0.4135529696941376
  - 0.41317683458328247
loss_records_fold2:
  train_losses:
  - 3.0718938767910005
  - 3.1902488231658936
  - 3.234154760837555
  - 3.3101655423641207
  - 3.3193220496177673
  - 3.238691306114197
  - 3.190340051054955
  - 3.121427419781685
  - 3.20255800485611
  - 3.1164179980754856
  - 3.3591074585914615
  - 3.2443179666996005
  - 3.132553565502167
  - 3.223745155334473
  - 3.4674719333648683
  - 3.216378512978554
  - 3.1242752969264984
  - 3.1950375974178318
  - 3.2065425515174866
  - 3.518105435371399
  - 3.449301779270172
  - 3.357786846160889
  - 3.2535424768924717
  - 3.3066881477832797
  - 3.265551143884659
  - 3.1757834494113926
  - 3.1720451533794405
  - 3.174690166115761
  - 3.2138031125068665
  validation_losses:
  - 0.41141021251678467
  - 0.42143091559410095
  - 0.42744672298431396
  - 0.4374256730079651
  - 0.42123377323150635
  - 0.41389480233192444
  - 0.40715664625167847
  - 0.4118882417678833
  - 0.4256349503993988
  - 0.4135861098766327
  - 0.3977448046207428
  - 0.4039308726787567
  - 0.41464194655418396
  - 0.4180336594581604
  - 0.4211372435092926
  - 0.4000168442726135
  - 0.41701439023017883
  - 0.4204168915748596
  - 0.4142787456512451
  - 127125.6875
  - 22775754752.0
  - 128437733818368.0
  - 914322234540032.0
  - 0.40495190024375916
  - 0.4137530028820038
  - 0.4130285084247589
  - 0.41960304975509644
  - 0.4139101207256317
  - 0.3958645462989807
loss_records_fold3:
  train_losses:
  - 3.15345071554184
  - 3.1846871316432956
  - 3.048118430376053
  - 3.246467059850693
  - 3.16906401515007
  - 3.1960829854011537
  - 3.170051485300064
  - 3.180080285668373
  - 3.257005035877228
  - 3.239471870660782
  - 3.2456671059131623
  - 3.2297684073448183
  - 3.3163494169712067
  - 3.2741196691989902
  - 6.057014927268028
  - 4.326310288906098
  - 3.314357447624207
  - 3.1830655813217166
  - 3.193306076526642
  - 3.174006086587906
  - 3.092502108216286
  - 3.2314562499523163
  - 3.244683820009232
  - 3.4052785992622376
  - 3.2009331166744235
  - 3.277824342250824
  - 3.240596139431
  - 3.1491309314966203
  - 3.1939979910850527
  - 3.1197329819202424
  - 3.1349573969841007
  - 3.163489866256714
  - 3.283877003192902
  - 3.2124415874481205
  - 3.23441618680954
  - 3.173129951953888
  - 3.3008882105350494
  - 3.1799540460109714
  - 3.2466908901929856
  - 3.2149531185626987
  - 3.123311394453049
  - 3.2660201102495194
  - 3.1499816656112674
  - 3.237797033786774
  - 3.3046342670917515
  - 3.256571823358536
  - 3.111793550848961
  - 3.189048939943314
  - 3.1460342288017276
  - 3.221995544433594
  - 3.2132172822952274
  - 3.1253268331289292
  - 3.1441276609897617
  - 3.2039785563945773
  - 3.3065246045589447
  - 3.113136458396912
  - 3.1168826282024384
  - 3.1187534958124163
  - 3.214141955971718
  - 3.185665482282639
  - 3.1605047374963764
  - 3.2036064803600315
  - 3.162161868810654
  - 3.1929615259170534
  - 3.300727528333664
  - 3.162548393011093
  - 3.107923373579979
  - 3.153731679916382
  - 3.2348741710186006
  - 3.23726978302002
  - 3.1561959445476533
  - 3.25480495095253
  - 3.240476590394974
  - 3.2223563075065615
  - 3.24755494594574
  - 3.1558295369148257
  - 3.0710256427526477
  - 3.1877685487270355
  - 3.1802840113639834
  - 3.147252279520035
  - 3.2286176264286044
  - 3.182031786441803
  - 3.129179632663727
  - 3.2103394448757174
  - 3.1922261714935303
  - 3.2508876860141758
  validation_losses:
  - 0.4255595803260803
  - 0.41599610447883606
  - 0.4252185523509979
  - 0.4111926257610321
  - 0.4156857132911682
  - 0.4180694818496704
  - 0.42740947008132935
  - 0.4036957323551178
  - 0.4091750979423523
  - 0.48410287499427795
  - 0.40617236495018005
  - 0.42529889941215515
  - 0.4189644753932953
  - 0.41728055477142334
  - 0.4884193539619446
  - 369.5025329589844
  - 4.742499828338623
  - 1793596288.0
  - 0.4025477468967438
  - 0.4222644865512848
  - 0.4469461143016815
  - 0.40898382663726807
  - 0.4229905605316162
  - 0.41469505429267883
  - 0.4086749851703644
  - 0.401121586561203
  - 0.4159046709537506
  - 0.41642850637435913
  - 0.4160429537296295
  - 0.40675222873687744
  - 0.41029250621795654
  - 0.4580021798610687
  - 0.4362715780735016
  - 0.48024651408195496
  - 0.41530680656433105
  - 0.42250725626945496
  - 0.4602760672569275
  - 0.4296668469905853
  - 0.4497358202934265
  - 0.41558754444122314
  - 0.41706863045692444
  - 0.43584302067756653
  - 0.40606990456581116
  - 0.4169267416000366
  - 0.4077601730823517
  - 0.4122227728366852
  - 0.41141101717948914
  - 0.4158388674259186
  - 0.42728307843208313
  - 0.42117464542388916
  - 0.40511542558670044
  - 0.42490482330322266
  - 0.4138232469558716
  - 0.4236415922641754
  - 0.40952348709106445
  - 0.4104284346103668
  - 0.42028892040252686
  - 0.4723398983478546
  - 0.4245099723339081
  - 0.41169074177742004
  - 0.4131626486778259
  - 0.402262419462204
  - 0.42199721932411194
  - 0.407168984413147
  - 0.43524712324142456
  - 0.4083876609802246
  - 0.4160425066947937
  - 0.40422481298446655
  - 0.414249449968338
  - 0.4305657148361206
  - 0.4076690673828125
  - 0.44838035106658936
  - 0.4512272775173187
  - 0.4094448685646057
  - 0.4254424273967743
  - 0.4087468981742859
  - 0.42275330424308777
  - 0.4085797369480133
  - 0.4403955638408661
  - 0.48525118827819824
  - 0.4204578399658203
  - 0.41711917519569397
  - 0.416610449552536
  - 0.42444565892219543
  - 0.4136125445365906
  - 0.41734033823013306
loss_records_fold4:
  train_losses:
  - 3.134237435460091
  - 3.228471666574478
  - 3.156909161806107
  - 3.1690956711769105
  - 3.2141032040119173
  - 3.319281625747681
  - 3.426019012928009
  - 3.226890727877617
  - 3.2366121828556063
  - 3.2227366209030155
  - 3.265953302383423
  - 3.2581076085567475
  - 3.3514882147312166
  - 3.2062489092350006
  - 3.2814177840948107
  - 3.171604359149933
  - 3.1480456173419955
  - 3.1100951075553898
  - 3.2688638865947723
  - 3.4665862679481507
  - 3.1658767223358155
  - 3.1730782806873323
  - 3.3707465440034867
  - 3.215345597267151
  - 3.203828561306
  - 3.1882996141910556
  validation_losses:
  - 0.4039313495159149
  - 0.4111507833003998
  - 0.4118027091026306
  - 0.4221458435058594
  - 0.39857420325279236
  - 0.5107184052467346
  - 0.43393173813819885
  - 0.41498705744743347
  - 0.43413209915161133
  - 0.41269662976264954
  - 0.4283345937728882
  - 0.40608862042427063
  - 0.4051802456378937
  - 0.39739078283309937
  - 0.42393726110458374
  - 0.4052397608757019
  - 0.4119741916656494
  - 0.41980618238449097
  - 0.4142612814903259
  - 0.4382646679878235
  - 0.40927115082740784
  - 0.41073980927467346
  - 0.41264116764068604
  - 0.4121728539466858
  - 0.406966894865036
  - 0.4141565263271332
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 86 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:32:14.500530'
