config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:43:02.971060'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_66fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_66fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_66fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_66fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_66fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_66fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_66fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_66fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_66fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_66fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.6905763924121857
  - 1.5357226312160492
  - 1.5025383830070496
  - 1.4637132853269579
  - 1.57725191116333
  - 1.4879441261291504
  - 1.4959732472896576
  - 1.4468207389116288
  - 1.5177177906036379
  - 1.4968921899795533
  - 1.5485928535461426
  - 1.4826559066772462
  - 1.4870389699935913
  - 1.5030757308006288
  - 1.5261419653892518
  - 1.5793012261390686
  - 1.5468215167522432
  - 1.470565837621689
  - 1.436911678314209
  - 1.4484159111976624
  - 1.453712958097458
  - 1.5683071792125702
  - 1.5145279169082642
  - 1.4825985789299012
  - 1.4345656096935273
  - 1.4614717721939088
  - 1.509978348016739
  validation_losses:
  - 0.4029077887535095
  - 0.4145645499229431
  - 0.39166024327278137
  - 0.3888187110424042
  - 0.39266079664230347
  - 0.38835105299949646
  - 0.4141697287559509
  - 0.38308337330818176
  - 0.3877282738685608
  - 0.43830418586730957
  - 0.39809414744377136
  - 0.39997562766075134
  - 0.3954704999923706
  - 0.38431471586227417
  - 0.3952997922897339
  - 0.46000736951828003
  - 0.4121890068054199
  - 0.3832637667655945
  - 0.38093170523643494
  - 0.38176172971725464
  - 1.183132290840149
  - 0.3965107202529907
  - 0.39215004444122314
  - 0.38438478112220764
  - 0.3840184509754181
  - 0.3850659728050232
  - 0.386182963848114
loss_records_fold1:
  train_losses:
  - 1.4854068756103516
  - 1.4326459348201752
  - 1.4682476937770845
  - 1.4425338745117189
  - 1.4140434920787812
  - 1.460536414384842
  - 1.4960917651653292
  - 1.4624228537082673
  - 1.4419352352619172
  - 1.4348217070102693
  - 1.461315703392029
  - 1.4069303542375566
  - 1.4421761453151705
  - 1.442797213792801
  - 1.4945151686668396
  - 1.4509419560432435
  - 1.449759590625763
  - 1.425471043586731
  - 1.4302193105220795
  - 1.4459355771541595
  - 1.4350807785987856
  - 1.430618792772293
  - 1.4402253448963167
  - 1.4406461328268052
  - 1.4719752788543703
  - 1.4645994424819948
  - 1.4572498977184296
  - 1.4831498980522158
  - 1.4343619763851168
  - 1.458035945892334
  - 1.5243381261825562
  - 1.4193585097789765
  - 1.4186904668807985
  - 1.4570916056632996
  - 1.4252046346664429
  - 1.4028648078441621
  - 1.4328615605831148
  - 1.4881239414215088
  - 1.4478923678398132
  - 1.438823112845421
  - 1.4335953652858735
  - 1.4358543097972871
  - 1.4191670715808868
  - 1.421847152709961
  - 1.405332410335541
  - 1.4151249885559083
  - 1.422833973169327
  - 1.3753728777170182
  - 1.4028866827487947
  - 1.4481422781944275
  - 1.4165020763874054
  - 1.4331887066364288
  - 1.4407844215631487
  - 1.4400607109069825
  - 1.4275926232337952
  - 1.4011806905269624
  - 1.4283571898937226
  - 1.4259238839149475
  - 1.4337002456188204
  - 1.4292784333229065
  - 1.3784468412399293
  - 1.4095975935459137
  - 1.4021805346012117
  - 1.3702328383922577
  - 1.3950323641300202
  - 1.4330841302871704
  - 1.405069661140442
  - 1.4080792546272278
  - 1.3918905556201935
  - 1.390676385164261
  - 1.3808014899492265
  - 1.3742753565311432
  - 1.394823133945465
  - 1.3933439761400224
  - 1.3908360421657564
  - 1.35966312289238
  - 1.4363853812217713
  - 1.423573923110962
  - 1.4330128848552706
  - 1.3789499521255495
  - 1.4158442437648775
  - 1.3925487816333773
  - 1.4002316415309908
  - 1.3717049300670625
  - 1.3995638489723206
  - 1.3792644500732423
  - 1.3944566488265993
  - 1.4102171540260315
  - 1.4098168522119523
  - 1.3612390577793123
  - 1.36550087928772
  - 1.3721390902996065
  - 1.3697561442852022
  - 1.3803926527500154
  - 1.3918241560459137
  - 1.3995113909244539
  - 1.406872844696045
  - 1.402869302034378
  - 1.38521369099617
  - 1.4028881251811982
  validation_losses:
  - 0.3890301287174225
  - 0.3853878080844879
  - 0.3860810399055481
  - 0.3885646164417267
  - 0.38402530550956726
  - 0.3923191726207733
  - 0.40915223956108093
  - 0.38498997688293457
  - 0.3890494704246521
  - 0.38611751794815063
  - 0.3884531557559967
  - 0.3846878111362457
  - 0.43149134516716003
  - 0.3850233256816864
  - 0.38738590478897095
  - 0.38644132018089294
  - 0.40302029252052307
  - 0.3940008878707886
  - 0.40368735790252686
  - 0.4271644949913025
  - 0.3865557014942169
  - 0.3841366469860077
  - 0.3832005560398102
  - 0.38471660017967224
  - 0.4188169836997986
  - 0.38403719663619995
  - 0.3933720290660858
  - 0.3874446451663971
  - 0.391743004322052
  - 0.38866499066352844
  - 0.44949159026145935
  - 0.4675114154815674
  - 0.38430672883987427
  - 0.3948540687561035
  - 0.4023502767086029
  - 0.4093875288963318
  - 0.4772285223007202
  - 0.7216960787773132
  - 1.5554759502410889
  - 0.3926612138748169
  - 0.3847041428089142
  - 0.3862893283367157
  - 0.39763689041137695
  - 0.40501606464385986
  - 0.4389844238758087
  - 0.40822580456733704
  - 0.49154606461524963
  - 0.47780275344848633
  - 0.39276474714279175
  - 0.4257112145423889
  - 0.40039002895355225
  - 0.456614226102829
  - 0.43380650877952576
  - 0.413729727268219
  - 0.4338085353374481
  - 0.46615204215049744
  - 0.5289984941482544
  - 0.43918660283088684
  - 0.48210608959198
  - 0.49587610363960266
  - 0.55055832862854
  - 0.6384838819503784
  - 0.5898298025131226
  - 0.45552584528923035
  - 0.4530026316642761
  - 0.5750976800918579
  - 0.9934263229370117
  - 0.515345573425293
  - 0.5478914380073547
  - 0.5290833711624146
  - 0.4543556571006775
  - 1.0137730836868286
  - 0.9235296845436096
  - 1.181487798690796
  - 0.6569703817367554
  - 0.5145609378814697
  - 0.6872810125350952
  - 0.6187514066696167
  - 0.4634457230567932
  - 0.40136587619781494
  - 0.509946882724762
  - 0.4864499568939209
  - 0.7049863934516907
  - 0.5824600458145142
  - 0.6308490037918091
  - 0.622955858707428
  - 1.0517301559448242
  - 0.5286352634429932
  - 0.7188917994499207
  - 0.7880891561508179
  - 0.6639625430107117
  - 0.6983478665351868
  - 0.38180798292160034
  - 0.3745194375514984
  - 0.626994252204895
  - 0.7729267477989197
  - 0.7442781925201416
  - 0.6084387898445129
  - 1.109904170036316
  - 0.4138665795326233
loss_records_fold2:
  train_losses:
  - 1.478911352157593
  - 1.3865128576755525
  - 1.4189307868480683
  - 1.4152729392051697
  - 1.3823793411254883
  - 1.4205634951591493
  - 1.5005440652370454
  - 1.5218984603881838
  - 1.4022634267807008
  - 1.382207775115967
  - 1.3790322601795197
  - 1.3942648470401764
  - 1.4008489072322847
  - 1.4513736367225647
  - 1.3980225443840029
  - 1.370164144039154
  - 1.3528500854969026
  - 1.3686564266681671
  - 1.3788593828678133
  - 1.37864608168602
  - 1.4054882764816286
  - 1.393327969312668
  - 1.3938278138637543
  - 1.4083523094654085
  - 1.3766936898231508
  - 1.3953443825244904
  - 1.3915446966886522
  - 1.4254107058048249
  - 1.3832809805870057
  - 1.368101978302002
  - 1.3679636120796204
  - 1.376423391699791
  - 1.3844306528568269
  - 1.344662019610405
  - 1.4269239366054536
  - 1.392245876789093
  - 1.377101492881775
  - 1.3827365517616272
  - 1.3510492950677873
  - 1.3779828906059266
  - 1.3513107895851135
  - 1.399757218360901
  - 1.3930395543575287
  - 1.4026773154735566
  - 1.3916721165180208
  - 1.3787998914718629
  - 1.3796272337436677
  - 1.3338583379983904
  - 1.3666083574295045
  - 1.344233125448227
  - 1.3776655495166779
  - 1.3472748070955278
  - 1.3636854767799378
  - 1.3586292743682862
  - 1.4097368180751801
  - 1.356725311279297
  - 1.3176019087433817
  - 1.3697464644908905
  - 1.3433077216148377
  - 1.337886640429497
  - 1.348107522726059
  - 1.3563483178615572
  - 1.3496902287006378
  - 1.3528291195631028
  - 1.3978999555110931
  - 1.3788311243057252
  - 1.3530361950397491
  - 1.4157930374145509
  - 1.4131772577762605
  - 1.4773148059844972
  - 1.4270092010498048
  - 1.3781444311141968
  - 1.3354324221611025
  - 1.3843647480010988
  - 1.389033091068268
  - 1.41396324634552
  - 1.3545565009117126
  - 1.3763654828071594
  - 1.3521168470382692
  - 1.393970561027527
  - 1.3620611011981965
  - 1.3709203898906708
  - 1.3572138965129854
  - 1.374253284931183
  - 1.3396107077598574
  - 1.3146006911993027
  - 1.3281649112701417
  - 1.3725584506988526
  - 1.327597039937973
  - 1.3464979112148285
  - 1.3842345476150513
  - 1.3467590868473054
  - 1.3470845043659212
  - 1.31823288500309
  - 1.354843944311142
  - 1.334546834230423
  - 1.3688563346862794
  - 1.3469244480133058
  - 1.3412131428718568
  - 1.3679928064346314
  validation_losses:
  - 0.44643649458885193
  - 0.46151843667030334
  - 0.764909029006958
  - 0.6015496253967285
  - 0.8123172521591187
  - 0.8751317858695984
  - 1.206992506980896
  - 0.6771687269210815
  - 0.937795877456665
  - 0.6318551301956177
  - 0.443443238735199
  - 0.5620533227920532
  - 0.5159106850624084
  - 0.5912090539932251
  - 0.41419076919555664
  - 0.43480992317199707
  - 0.6806238293647766
  - 0.5386713743209839
  - 0.4990086853504181
  - 0.4180198013782501
  - 0.4387359619140625
  - 0.39819300174713135
  - 0.47767919301986694
  - 0.43064600229263306
  - 0.44792503118515015
  - 0.4470888674259186
  - 0.4674324095249176
  - 0.4771493077278137
  - 0.4088764190673828
  - 0.4084734320640564
  - 0.4083813428878784
  - 0.4423702657222748
  - 0.44800224900245667
  - 0.43311190605163574
  - 0.5281123518943787
  - 0.4335795044898987
  - 0.45399028062820435
  - 0.4685012102127075
  - 0.573689877986908
  - 0.47721877694129944
  - 0.4455229341983795
  - 0.45101678371429443
  - 0.45420804619789124
  - 0.44032710790634155
  - 0.5816322565078735
  - 0.5140025019645691
  - 0.5238778591156006
  - 0.4888593256473541
  - 0.4790195822715759
  - 0.4948717951774597
  - 0.47397369146347046
  - 0.4950447082519531
  - 0.45611292123794556
  - 0.5557677745819092
  - 1.0055798292160034
  - 0.6291638016700745
  - 0.5893998146057129
  - 0.5892536044120789
  - 0.586834192276001
  - 0.5810949802398682
  - 0.6264883875846863
  - 0.5636978149414062
  - 0.5241401791572571
  - 0.5553857088088989
  - 0.7145789861679077
  - 0.5488008260726929
  - 0.4632463753223419
  - 0.6062200665473938
  - 0.5301128625869751
  - 0.5937023758888245
  - 0.5923027992248535
  - 0.570810854434967
  - 0.5020343661308289
  - 0.37979164719581604
  - 0.4958503544330597
  - 0.5462520718574524
  - 0.5507614016532898
  - 0.4720485210418701
  - 0.39504900574684143
  - 0.4638364315032959
  - 0.44012922048568726
  - 0.518772542476654
  - 0.512566089630127
  - 0.590424120426178
  - 0.5388234257698059
  - 0.6142727136611938
  - 0.5533785820007324
  - 0.6119418144226074
  - 0.5862237811088562
  - 0.6107684373855591
  - 0.5835228562355042
  - 0.6151436567306519
  - 0.6417379975318909
  - 0.5656591057777405
  - 0.5152875781059265
  - 0.533950924873352
  - 0.5459063053131104
  - 0.4697359502315521
  - 0.5914414525032043
  - 0.605118453502655
loss_records_fold3:
  train_losses:
  - 1.3704805314540864
  - 1.3606888562440873
  - 1.3668326020240784
  - 1.348071163892746
  - 1.3327730506658555
  - 1.3899599254131318
  - 1.3883816540241243
  - 1.3766317486763002
  - 1.3855761349201203
  - 1.3534351706504824
  - 1.3309889614582062
  - 1.4287984430789948
  - 1.3873030662536623
  - 1.3597896933555604
  - 1.358122593164444
  - 1.3258739650249483
  - 1.4184594333171845
  - 1.3568621814250947
  - 1.365569841861725
  - 1.3435679018497468
  - 1.3503264963626862
  - 1.3205644607543947
  - 1.3790412664413454
  - 1.3501622557640076
  - 1.3682444810867311
  - 1.3167513757944107
  - 1.3401225328445436
  - 1.3499892055988312
  - 1.356425404548645
  - 1.3387039601802826
  - 1.3207218855619431
  - 1.3648021221160889
  - 1.4194907665252687
  - 1.3913226008415223
  - 1.3693095356225968
  - 1.3875800609588624
  - 1.3539300560951233
  - 1.4023344933986666
  - 1.3934713423252107
  - 1.3431021094322206
  - 1.3084475070238115
  - 1.3480342209339142
  - 1.3241558730602265
  - 1.3529318869113922
  - 1.3715453982353212
  - 1.3869614779949189
  - 1.3774377465248109
  - 1.3371126234531403
  - 1.4953739643096924
  - 1.388618677854538
  - 1.3893788158893585
  - 1.3389890789985657
  - 1.344638991355896
  - 1.3627470493316651
  - 1.3383550077676774
  - 1.2916420578956604
  - 1.326266872882843
  - 1.3579271137714386
  - 1.3628294348716736
  - 1.3618161261081696
  - 1.3280240178108216
  - 1.332057535648346
  - 1.3270165115594865
  - 1.3552131742239
  - 1.3755323886871338
  - 1.3249156057834626
  - 1.3557448089122772
  - 1.3106377303600312
  - 1.3187095820903778
  - 1.3121472358703614
  - 1.3347550570964815
  - 1.322578650712967
  - 1.3087736666202545
  - 1.3239367842674257
  - 1.347435885667801
  - 1.3314021557569504
  - 1.3386280298233033
  - 1.2971455931663514
  - 1.3789531350135804
  - 1.2902979522943498
  - 1.3511333614587784
  - 1.338816076517105
  - 1.337216028571129
  - 1.375753888487816
  - 1.3114448547363282
  - 1.3332267701625824
  - 1.3279109418392183
  - 1.3423334300518037
  - 1.3381415843963624
  - 1.3733680605888368
  - 1.308755421638489
  - 1.4086603581905366
  - 1.306948721408844
  - 1.3135557532310487
  - 1.3056111156940462
  - 1.2988733470439913
  - 1.3051988542079926
  - 1.324314296245575
  - 1.3238108932971955
  - 1.2905170351266861
  validation_losses:
  - 0.8007317185401917
  - 0.9585641622543335
  - 1.0027754306793213
  - 0.8810065984725952
  - 0.9807916283607483
  - 1.313075065612793
  - 1.4672021865844727
  - 1.4171794652938843
  - 0.6417480111122131
  - 0.8729053139686584
  - 0.775610625743866
  - 0.7800636887550354
  - 0.6681728959083557
  - 0.4311538338661194
  - 0.7690591216087341
  - 0.41058728098869324
  - 0.6162534356117249
  - 0.813458263874054
  - 0.8013391494750977
  - 0.6989220976829529
  - 0.5899590849876404
  - 0.6578385829925537
  - 1.0495266914367676
  - 0.5911046862602234
  - 0.6645970344543457
  - 1.3354723453521729
  - 0.5091533660888672
  - 0.8907874822616577
  - 1.9342231750488281
  - 0.7211939692497253
  - 0.6659772992134094
  - 0.7663537859916687
  - 0.7987861037254333
  - 2.124823570251465
  - 3.1965982913970947
  - 1.6242403984069824
  - 0.6395778656005859
  - 0.5865119695663452
  - 0.5231873989105225
  - 0.5495429635047913
  - 0.6117794513702393
  - 0.6305015683174133
  - 0.6373375058174133
  - 0.6768067479133606
  - 0.7934556007385254
  - 0.554802656173706
  - 0.8263542652130127
  - 0.7277536988258362
  - 0.5824874043464661
  - 0.4438592791557312
  - 0.6652064323425293
  - 0.6250703930854797
  - 0.739741325378418
  - 0.6236321330070496
  - 0.7354198694229126
  - 0.8105905055999756
  - 1.069719910621643
  - 0.7992785573005676
  - 0.6868547201156616
  - 0.8127638697624207
  - 0.8847962021827698
  - 0.46333011984825134
  - 1.007948398590088
  - 0.7073903679847717
  - 0.5932087898254395
  - 0.8797848224639893
  - 1.0141798257827759
  - 0.6734035611152649
  - 0.8064080476760864
  - 1.0452665090560913
  - 0.707882821559906
  - 0.8445472717285156
  - 0.8908360600471497
  - 0.6538962125778198
  - 0.8627331852912903
  - 0.9579182267189026
  - 0.6011598110198975
  - 0.6698830723762512
  - 0.6656915545463562
  - 0.7674555778503418
  - 0.9977357387542725
  - 0.644117534160614
  - 0.6645545959472656
  - 0.5621822476387024
  - 0.6705939769744873
  - 0.7302358150482178
  - 0.5418393015861511
  - 0.9003549218177795
  - 0.9873743057250977
  - 0.9430968165397644
  - 1.0791712999343872
  - 1.0443905591964722
  - 0.7489719390869141
  - 1.0814648866653442
  - 0.7629027962684631
  - 0.8079511523246765
  - 0.8190973997116089
  - 1.0927698612213135
  - 0.8574919104576111
  - 0.6385335922241211
loss_records_fold4:
  train_losses:
  - 1.382679945230484
  - 1.3595671653747559
  - 1.3216252565383912
  - 1.3843142151832581
  - 1.3518146336078645
  - 1.4685742855072021
  - 1.5544952929019928
  - 1.402956408262253
  - 1.4125056624412538
  - 1.446772199869156
  - 1.4772332429885866
  - 1.4200089275836945
  - 1.3942114412784576
  - 1.3950019180774689
  - 1.4190657198429109
  - 1.3633615851402283
  - 1.3978449523448946
  validation_losses:
  - 0.48907455801963806
  - 0.5190125703811646
  - 0.41746559739112854
  - 0.5651949048042297
  - 0.6077097654342651
  - 0.44175201654434204
  - 0.3718961775302887
  - 0.3622739613056183
  - 0.36533254384994507
  - 0.3628661036491394
  - 0.5572476387023926
  - 0.35768163204193115
  - 0.35746386647224426
  - 0.3589331805706024
  - 0.35559511184692383
  - 0.35998299717903137
  - 0.35657617449760437
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8456260720411664, 0.8078902229845626, 0.8473413379073756,
    0.8642611683848798]'
  fold_eval_f1: '[0.0, 0.021739130434782608, 0.3170731707317073, 0.1981981981981982,
    0.07058823529411765]'
  mean_eval_accuracy: 0.8445503468845231
  mean_f1_accuracy: 0.12151974693176115
  total_train_time: '0:29:32.021239'
