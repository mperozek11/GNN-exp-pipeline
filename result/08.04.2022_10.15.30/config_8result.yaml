config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 10:15:54.127648'
fold_0_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_8fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_8fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_8fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_8fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_8fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_8fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_8fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_8fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_8fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_8fold_4_state_dict.pt
loss_records:
  train_losses:
  - 7.109993994235992
  - 6.8609938621521
  - 6.96885646879673
  - 7.383656173944473
  - 7.343017101287842
  - 7.111119866371155
  - 7.163392186164856
  - 6.919582054018974
  - 6.985088348388672
  - 6.9634906351566315
  - 7.103539824485779
  - 7.2052357494831085
  - 6.981534510850906
  - 7.110852003097534
  - 7.072527229785919
  - 7.131264105439186
  - 6.874290466308594
  - 6.948320955038071
  - 6.884922698140144
  - 7.086156725883484
  - 6.916274130344391
  - 7.054231375455856
  - 7.013504534959793
  - 6.885938823223114
  - 6.86649414896965
  - 6.987094521522522
  - 6.961376309394836
  - 6.9766106605529785
  - 6.859773650765419
  - 6.794029891490936
  - 6.738823965191841
  - 7.012889176607132
  - 7.012152433395386
  - 7.121161580085754
  - 6.988342076539993
  - 6.97939869761467
  - 6.638783976435661
  - 6.932535827159882
  - 6.96642991900444
  - 7.226923376321793
  - 7.003034293651581
  - 7.109829694032669
  - 6.597711756825447
  - 6.721409052610397
  - 6.778005510568619
  - 6.805265337228775
  - 6.79642491042614
  - 6.831599295139313
  - 6.94056211411953
  - 6.768840491771698
  - 6.751760363578796
  - 6.723088055849075
  - 6.789872735738754
  - 6.78287610411644
  - 6.722290426492691
  - 6.645891979336739
  - 6.799983948469162
  - 7.070996046066284
  - 6.904579520225525
  - 7.026221379637718
  - 6.83699306845665
  - 6.840164393186569
  - 6.770635724067688
  - 6.646571949124336
  - 6.687697738409042
  - 6.885392993688583
  - 6.87138506770134
  - 7.0716425478458405
  - 6.766909718513489
  - 6.992755502462387
  - 6.65729184448719
  - 6.828903019428253
  - 6.602873384952545
  - 6.6837975680828094
  - 6.775818020105362
  - 6.64458292722702
  - 6.637810558080673
  - 6.559004291892052
  - 6.85703432559967
  - 6.68906107544899
  - 6.505554556846619
  - 6.5342982560396194
  - 6.608638674020767
  - 6.745521456003189
  - 6.748549729585648
  - 6.923335462808609
  - 7.077903926372528
  - 7.093916684389114
  - 7.052086621522903
  - 7.052028208971024
  - 7.051635235548019
  - 6.883288666605949
  - 6.9471409022808075
  - 6.940265566110611
  - 7.004399538040161
  - 6.862910032272339
  - 7.1544153690338135
  - 7.055874973535538
  - 6.82168173789978
  - 6.826360777020454
  validation_losses:
  - 0.4405654966831207
  - 0.4887813925743103
  - 0.4807615578174591
  - 0.4542791545391083
  - 0.4482620656490326
  - 0.382573664188385
  - 0.39542505145072937
  - 0.3656594753265381
  - 0.3808623254299164
  - 0.4257757067680359
  - 0.36920255422592163
  - 0.4053666889667511
  - 0.4109000563621521
  - 0.3809923827648163
  - 0.3926961123943329
  - 0.5437027812004089
  - 0.3618910312652588
  - 0.3586866855621338
  - 0.4403809905052185
  - 0.36691752076148987
  - 0.4390583634376526
  - 0.43816888332366943
  - 0.3770003616809845
  - 0.41904380917549133
  - 0.5663511157035828
  - 0.5614206194877625
  - 0.5808019638061523
  - 0.4269729554653168
  - 0.4493783116340637
  - 0.3923378586769104
  - 0.39347711205482483
  - 0.4867939054965973
  - 0.5143924355506897
  - 0.3899261951446533
  - 0.36883649230003357
  - 0.3952205181121826
  - 0.3936752378940582
  - 0.3896637558937073
  - 0.45580846071243286
  - 0.3620798885822296
  - 0.3633720874786377
  - 0.38284415006637573
  - 0.37443238496780396
  - 0.3718280792236328
  - 0.4581305682659149
  - 0.4502388834953308
  - 0.39329564571380615
  - 0.40405017137527466
  - 0.39939379692077637
  - 0.3807697296142578
  - 0.38440802693367004
  - 0.3683423399925232
  - 0.38332512974739075
  - 0.4138053357601166
  - 0.3926820456981659
  - 0.3635166883468628
  - 0.3869687616825104
  - 0.4657880663871765
  - 0.44378432631492615
  - 0.4047078788280487
  - 0.482757568359375
  - 0.41098931431770325
  - 0.3721887767314911
  - 0.37771493196487427
  - 0.41057029366493225
  - 0.3813101053237915
  - 0.36653271317481995
  - 0.505436897277832
  - 0.4923780560493469
  - 0.3676566183567047
  - 0.37574928998947144
  - 0.3686780035495758
  - 0.3688995838165283
  - 0.38060811161994934
  - 0.43757352232933044
  - 0.4167513847351074
  - 0.49134936928749084
  - 0.4691065549850464
  - 0.44989967346191406
  - 0.4081730246543884
  - 0.3863997757434845
  - 0.49723613262176514
  - 0.5817446708679199
  - 0.40133944153785706
  - 0.4933331608772278
  - 0.632564902305603
  - 0.3986643850803375
  - 0.42774060368537903
  - 0.3971918821334839
  - 0.36742737889289856
  - 0.4164021611213684
  - 0.4687013328075409
  - 0.8355493545532227
  - 0.4153755009174347
  - 0.4543648958206177
  - 0.4022488594055176
  - 0.4855324625968933
  - 0.4731566905975342
  - 0.3940621018409729
  - 0.5245474576950073
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8439108061749572, 0.7873070325900514,
    0.7766323024054983]'
  fold_eval_f1: '[0.0, 0.0, 0.08080808080808081, 0.3608247422680413, 0.2696629213483146]'
  mean_eval_accuracy: 0.8246232014759538
  mean_f1_accuracy: 0.14225914888488736
  total_train_time: '0:13:24.756449'
