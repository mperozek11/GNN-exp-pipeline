config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 22:44:17.986751'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/85/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/85/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/85/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/85/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/85/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/85/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/85/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/85/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/85/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/85/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 346.5749543905258
  - 37.99455505609512
  - 56.78497165441513
  - 24.78654459118843
  - 22.012779235839844
  - 12.554003953933716
  - 14.569584131240845
  - 9.306371420621872
  - 8.503923058509827
  - 10.320284485816956
  - 9.985125571489334
  - 9.097484022378922
  - 8.570520281791687
  - 11.330813258886337
  - 9.658648759126663
  - 8.847327023744583
  - 85.6891705840826
  - 12.393347799777985
  - 13.348566323518753
  - 12.26714101433754
  - 8.907959938049316
  - 11.631483852863312
  - 40.38055804371834
  - 28.749205082654953
  - 19.108514338731766
  - 17.723217710852623
  - 13.354265630245209
  - 12.059649586677551
  - 11.785404086112976
  - 8.802705109119415
  - 9.630714893341064
  - 10.559953719377518
  - 10.415787398815155
  - 16.9516279399395
  - 14.185992300510406
  - 15.938557386398315
  - 14.538212150335312
  - 9.112238079309464
  - 8.006703197956085
  - 8.13100814819336
  - 13.694577634334564
  - 10.290497392416
  - 15.624872535467148
  - 12.176261216402054
  - 9.282742321491241
  - 17.34997582435608
  - 11.838172793388367
  - 8.242269486188889
  - 10.11121079325676
  - 8.835420846939087
  - 8.56343388557434
  - 9.1747065782547
  - 8.584412306547165
  - 11.797458320856094
  - 10.71299034357071
  - 12.122266471385956
  - 9.70650339126587
  - 7.839264452457428
  - 9.916122317314148
  - 9.88707971572876
  - 7.631337583065033
  - 7.92885547876358
  - 10.901318222284317
  - 16.31127217411995
  - 21.314579516649246
  - 24.905439138412476
  - 19.06117382645607
  - 14.31286996603012
  - 10.506477624177933
  - 15.931839436292648
  - 22.059448063373566
  - 61.95601089298725
  - 13.702217698097229
  - 18.205422699451447
  - 9.0476453602314
  - 9.705907553434372
  - 19.017687529325485
  - 16.445695847272873
  - 13.13658994436264
  - 11.051829367876053
  - 12.276803106069565
  - 14.04068049788475
  - 9.344681918621063
  - 8.384737700223923
  - 10.032557755708694
  - 8.698299795389175
  - 9.3052017390728
  - 11.727905482053757
  - 10.675816714763641
  - 8.92059776186943
  - 10.979009002447128
  - 7.7151049971580505
  - 8.525268703699112
  - 12.519456803798676
  - 18.931541711091995
  - 176.0863557457924
  - 58.99526110291481
  - 66.13189333677292
  - 48.45256847143173
  - 14.145416229963303
  validation_losses:
  - 98.58856201171875
  - 9.626635551452637
  - 0.7899230718612671
  - 0.9037991166114807
  - 1.1453782320022583
  - 0.6778215765953064
  - 0.4022834300994873
  - 0.4522857666015625
  - 0.5125929117202759
  - 0.4435696005821228
  - 0.47606152296066284
  - 0.49643412232398987
  - 0.45276498794555664
  - 0.48588818311691284
  - 0.41805166006088257
  - 0.40950992703437805
  - 0.4234239161014557
  - 0.40762239694595337
  - 0.5220685005187988
  - 0.4419848918914795
  - 0.48820018768310547
  - 0.3898528814315796
  - 0.6340245604515076
  - 3.7985012531280518
  - 0.4970986545085907
  - 0.41396716237068176
  - 0.44701144099235535
  - 0.4771229326725006
  - 0.4500277042388916
  - 0.38990938663482666
  - 0.6285166144371033
  - 0.7207065224647522
  - 1.4056532382965088
  - 1.262181043624878
  - 0.7858940362930298
  - 0.46489161252975464
  - 0.565994381904602
  - 0.4130859076976776
  - 0.3912445902824402
  - 0.39071375131607056
  - 0.4502550959587097
  - 0.48484161496162415
  - 0.4904504716396332
  - 0.5211484432220459
  - 0.42033857107162476
  - 0.3963439464569092
  - 0.42480531334877014
  - 0.5070687532424927
  - 0.4321238398551941
  - 0.39701712131500244
  - 0.4353615343570709
  - 0.3911149799823761
  - 0.5958861708641052
  - 0.6044686436653137
  - 0.7363049983978271
  - 0.41770321130752563
  - 0.3978268504142761
  - 0.56121426820755
  - 0.4628937840461731
  - 0.4072878360748291
  - 0.3968023657798767
  - 0.48607394099235535
  - 0.6329052448272705
  - 0.6763389706611633
  - 1.4939684867858887
  - 9.809412956237793
  - 0.9129441976547241
  - 0.4278299808502197
  - 0.4132234752178192
  - 5.3818278312683105
  - 14.234283447265625
  - 425.8547058105469
  - 601.1572265625
  - 0.47398945689201355
  - 0.4146468937397003
  - 672.3638916015625
  - 0.9336224794387817
  - 0.48288729786872864
  - 0.433388888835907
  - 445.44110107421875
  - 0.4561913311481476
  - 0.4228825867176056
  - 0.38959038257598877
  - 0.3873957395553589
  - 0.48902592062950134
  - 0.4288100004196167
  - 0.41333967447280884
  - 0.5618242025375366
  - 0.5292014479637146
  - 0.3922309875488281
  - 0.39908403158187866
  - 0.3877703845500946
  - 0.405834436416626
  - 7.714879035949707
  - 0.6068469285964966
  - 25.520004272460938
  - 105.87612915039062
  - 3.039189100265503
  - 1.6672918796539307
  - 0.49226289987564087
loss_records_fold1:
  train_losses:
  - 9.952739506959915
  - 9.901954561471939
  - 8.021262735128403
  - 7.977482050657272
  - 9.00201028585434
  - 8.936352044343948
  - 8.431704670190811
  - 30.625702023506165
  - 24.296095937490463
  - 45.61763450503349
  - 36.16255941987038
  - 505.08748203516006
  - 272.9745423793793
  - 23.937182426452637
  - 15.148529291152954
  - 7.851619780063629
  - 83.0073853135109
  - 172.46949446201324
  - 203.9831372499466
  - 25.881944090127945
  - 22.654234498739243
  - 39.98097723722458
  - 50.08850198239088
  - 403.39199662208557
  - 39.53938031196594
  - 26.142408683896065
  - 19.23712107539177
  - 140.36788019537926
  - 7.777025520801544
  - 8.78269998729229
  - 90.50130400061607
  - 20.24962604045868
  - 14.182435274124146
  - 18.798523545265198
  - 99.71081152558327
  - 13.669723868370056
  - 9.675722628831863
  - 11.962588250637054
  - 7.830949664115906
  - 9.971146523952484
  - 50.64407515525818
  - 8.760926768183708
  - 11.222460269927979
  - 30.829717963933945
  - 11.001365780830383
  - 7.808166861534119
  - 9.066184982657433
  - 7.860183417797089
  - 8.06530487537384
  - 9.145713657140732
  - 9.943734973669052
  - 9.904391884803772
  - 9.169408351182938
  - 7.533459842205048
  - 8.520501405000687
  - 12.063352882862091
  - 11.883061200380325
  - 37.10790991783142
  - 11.215914070606232
  - 18.742450714111328
  - 8.2066111266613
  - 7.495294943451881
  - 8.477262109518051
  - 7.86216327548027
  - 9.96718892455101
  - 10.732494443655014
  - 12.854299858212471
  - 13.449194252490997
  - 16.34957331418991
  - 14.44154104590416
  - 12.157492458820343
  - 8.735586404800415
  - 8.63863754272461
  - 13.314502865076065
  - 7.805687695741653
  - 9.0298792719841
  - 8.803929775953293
  - 8.034386679530144
  - 10.023795038461685
  - 8.29677477478981
  - 26.696217238903046
  - 10.56911113858223
  - 9.198704540729523
  - 9.065671175718307
  - 7.856362134218216
  - 7.495365679264069
  - 7.8523761332035065
  - 10.494898408651352
  - 7.974176287651062
  - 7.586800530552864
  - 7.705257028341293
  - 9.80574345588684
  - 15.236257255077362
  - 12.584962785243988
  - 12.412783861160278
  - 9.762301802635193
  - 8.265659987926483
  - 8.260091006755829
  - 9.043874382972717
  - 8.54490864276886
  validation_losses:
  - 0.4388637840747833
  - 0.4135282039642334
  - 0.40502622723579407
  - 0.48490720987319946
  - 0.4058007001876831
  - 0.561581015586853
  - 0.40813231468200684
  - 2134.46533203125
  - 2.59651255607605
  - 0.5988287329673767
  - 0.4517797827720642
  - 28.181276321411133
  - 469.70556640625
  - 15.375164985656738
  - 0.4054941236972809
  - 0.4839175045490265
  - 59.0832405090332
  - 714.3468627929688
  - 3477.3466796875
  - 5179.63916015625
  - 3391.906982421875
  - 0.736977219581604
  - 0.9452595710754395
  - 0.7956501245498657
  - 9.70169448852539
  - 4.247200012207031
  - 0.4161047637462616
  - 0.41533419489860535
  - 0.42057380080223083
  - 0.4068203270435333
  - 0.49229273200035095
  - 0.6880743503570557
  - 0.4939776360988617
  - 9.262452125549316
  - 0.5958516597747803
  - 0.5506806373596191
  - 0.44456151127815247
  - 0.47538650035858154
  - 0.565973699092865
  - 0.424941748380661
  - 0.45256826281547546
  - 0.5998094081878662
  - 0.7180113196372986
  - 0.8655958771705627
  - 0.4163115322589874
  - 0.43960806727409363
  - 0.500194251537323
  - 0.4026983678340912
  - 0.40426579117774963
  - 0.4070386588573456
  - 0.5023300051689148
  - 0.4538347125053406
  - 0.4135919213294983
  - 0.40440303087234497
  - 0.4403139650821686
  - 0.4176947772502899
  - 0.407875657081604
  - 0.4150490462779999
  - 0.5113423466682434
  - 0.5426961183547974
  - 0.4305165112018585
  - 0.41223573684692383
  - 0.4428858757019043
  - 0.40415316820144653
  - 0.489921897649765
  - 0.49694427847862244
  - 0.7303745746612549
  - 0.6213067173957825
  - 1.175171971321106
  - 0.8457189798355103
  - 0.42241472005844116
  - 0.46685776114463806
  - 0.4160788059234619
  - 0.4448518753051758
  - 0.4071555733680725
  - 0.47621870040893555
  - 0.40477797389030457
  - 0.6087389588356018
  - 0.41143175959587097
  - 0.41348814964294434
  - 0.4600677788257599
  - 0.5298265814781189
  - 0.556425929069519
  - 0.4460048973560333
  - 0.4069519639015198
  - 0.4056045413017273
  - 0.6388192176818848
  - 0.5394685864448547
  - 0.41246843338012695
  - 0.4909457564353943
  - 0.4613173305988312
  - 0.5331953763961792
  - 0.5531246662139893
  - 0.519224226474762
  - 0.5211527943611145
  - 0.45080798864364624
  - 0.4039081037044525
  - 0.7325161099433899
  - 0.4220429062843323
  - 0.40631136298179626
loss_records_fold4:
  train_losses:
  - 10.855679303407669
  - 8.409828454256058
  - 8.334943801164627
  - 8.736989796161652
  - 8.003418415784836
  - 7.825393199920654
  - 8.023842990398407
  - 7.686589032411575
  - 8.13736093044281
  - 9.752729922533035
  - 15.230699241161346
  - 13.492689818143845
  - 12.77400055527687
  - 8.651587694883347
  - 10.040124326944351
  - 8.06979525089264
  - 7.641645431518555
  - 7.969543427228928
  - 8.433732718229294
  - 8.693118751049042
  - 8.279114335775375
  - 11.869937241077423
  - 8.493476212024689
  - 8.27298629283905
  - 7.754464656114578
  - 7.933404862880707
  - 8.05805191397667
  - 7.8253403455019
  - 11.462538331747055
  - 10.492644518613815
  - 14.025148123502731
  - 13.977345734834671
  - 12.718513816595078
  - 13.902309536933899
  - 9.25784718990326
  - 10.600972086191177
  - 8.708680748939514
  - 8.167896151542664
  - 7.793125003576279
  - 8.217164158821106
  - 11.415053561329842
  - 10.772186577320099
  - 11.214463084936142
  - 11.958936013281345
  - 13.000572323799133
  - 8.119938760995865
  - 7.912365198135376
  - 8.303997546434402
  - 8.448563873767853
  - 8.014353573322296
  - 7.700479418039322
  - 7.70693376660347
  - 10.25311353802681
  - 12.043049544095993
  - 8.468198955059052
  - 8.502826184034348
  - 7.791698724031448
  - 7.650765478610992
  - 7.636690586805344
  - 7.662130802869797
  - 8.221299350261688
  - 9.052029728889465
  - 7.770597755908966
  - 7.79748609662056
  - 9.569481700658798
  - 9.284821897745132
  - 8.8487988114357
  - 8.626766473054886
  - 7.545733094215393
  - 7.847629278898239
  - 8.244696140289307
  - 7.664953798055649
  - 7.739086717367172
  - 9.35374990105629
  - 10.423803627490997
  - 9.29712262749672
  - 13.497806131839752
  - 9.663365960121155
  - 11.12634202092886
  - 19.681130707263947
  - 12.647249102592468
  - 12.462907135486603
  - 9.420529782772064
  - 8.731153070926666
  - 9.039955645799637
  - 8.043469458818436
  - 9.238369464874268
  - 9.004435151815414
  - 7.518487602472305
  - 7.619444668292999
  - 8.01495823264122
  - 8.61502492427826
  - 7.878740429878235
  - 8.33971557021141
  - 8.011490762233734
  - 9.467958480119705
  - 11.616400495171547
  - 8.165214955806732
  - 9.666725814342499
  - 8.369200021028519
  validation_losses:
  - 1.6009439826345732e+28
  - 1.5834597749092346e+28
  - 1.6080306018970916e+28
  - 1.5802671009893285e+28
  - 1.5869661320227653e+28
  - 1.58681596076861e+28
  - 1.6041633379251076e+28
  - 1.5931403900807932e+28
  - 1.569567989426577e+28
  - 1.6018048700444004e+28
  - 1.5684485524518127e+28
  - 1.5558760781053068e+28
  - 1.5392534661447677e+28
  - 1.528027574600852e+28
  - 1.5213488497432916e+28
  - 1.5349826759568225e+28
  - 1.5275855610980555e+28
  - 1.5246171995860857e+28
  - 0.612399697303772
  - 0.3912767171859741
  - 0.4577111303806305
  - 0.8929265141487122
  - 0.4012008607387543
  - 0.3942870497703552
  - 0.40858694911003113
  - 3.92016685248394e+22
  - 1.7964316752243367e+25
  - 1.2064346237209628e+27
  - 9.710428061460796e+27
  - 1.7072219028640133e+28
  - 1.8647201447357644e+28
  - 1.8883130876841811e+28
  - 1.865741474546847e+28
  - 1.9039539196529316e+28
  - 1.8747567082809693e+28
  - 1.892571717778433e+28
  - 1.9023366271917108e+28
  - 1.9152609178411905e+28
  - 1.9036710499006077e+28
  - 1.8945338610520653e+28
  - 1.8897849312577295e+28
  - 1.905672742993534e+28
  - 1.9147258737186814e+28
  - 1.9081925977487933e+28
  - 1.8737486010960387e+28
  - 1.844086708862162e+28
  - 1.8727735504764882e+28
  - 1.8385879853295086e+28
  - 1.8197432638204552e+28
  - 1.8603374344621751e+28
  - 1.8683097335585557e+28
  - 1.8595207011789628e+28
  - 1.882535154233228e+28
  - 1.8578323371021749e+28
  - 1.8718126669563863e+28
  - 1.8719713384702107e+28
  - 1.8771089190260867e+28
  - 1.8672424787334271e+28
  - 1.8568725161145316e+28
  - 1.886035254211169e+28
  - 1.878582415427904e+28
  - 1.8829339580827064e+28
  - 1.8799386790817842e+28
  - 1.8916893436011088e+28
  - 1.9100015002300565e+28
  - 1.902906380707869e+28
  - 1.9135638173864093e+28
  - 1.9577109785106782e+28
  - 1.9145785358844159e+28
  - 2.1498892173001654e+28
  - 1.897138482285692e+28
  - 1.8389952894386561e+28
  - 1.8346529553984954e+28
  - 1.8278173299145416e+28
  - 1.8306446107278357e+28
  - 1.8009176678956575e+28
  - 1.8095206390358252e+28
  - 1.810025341953682e+28
  - 1.8207046195772054e+28
  - 1.8094337474925404e+28
  - 1.7650152864138306e+28
  - 1.7466018350238253e+28
  - 1.752032084242477e+28
  - 1.7284144669291874e+28
  - 1.7099640630214536e+28
  - 1.7253649987728743e+28
  - 1.71514839500551e+28
  - 1.6897430078009678e+28
  - 1.716805709522673e+28
  - 1.704707006593561e+28
  - 1.6766008979974658e+28
  - 1.6311640585867251e+28
  - 1.646583529626591e+28
  - 1.6409641496303003e+28
  - 1.631841954295341e+28
  - 1.6436414953077633e+28
  - 1.6361020010995378e+28
  - 1.6425398852664719e+28
  - 1.6652947262233413e+28
  - 1.8743174101389004e+28
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 33 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.14065180102915953, 0.14065180102915953,
    0.140893470790378]'
  fold_eval_f1: '[0.0, 0.0, 0.24661654135338348, 0.24661654135338348, 0.24698795180722893]'
  mean_eval_accuracy: 0.42749258781159194
  mean_f1_accuracy: 0.14804420690279918
  total_train_time: '0:05:41.778674'
