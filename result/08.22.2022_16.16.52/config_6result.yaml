config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.185513'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_6fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_6fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_6fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_6fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_6fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_6fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_6fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_6fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_6fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_6fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.9667317390441896
  - 1.6122868180274965
  - 1.6118902802467348
  - 1.5856894850730896
  - 1.6321072936058045
  - 1.5852249711751938
  - 1.5798669517040254
  - 1.6316534519195558
  - 1.5944399356842043
  - 1.586406433582306
  - 1.5071910232305528
  - 1.5513743400573732
  - 1.5785016238689424
  - 1.6164836049079896
  - 1.5766786575317384
  - 1.5824264585971832
  - 1.550108051300049
  - 1.4924741029739381
  - 1.5329146802425386
  - 1.5496120870113375
  - 1.5715791881084442
  - 1.5519440412521364
  - 1.5840277433395387
  - 1.5947359085083008
  - 1.7106485426425935
  - 1.6027647316455842
  - 1.685919386148453
  - 1.6111481487751007
  - 1.6127471864223482
  - 1.5407380163669586
  - 1.5214570701122285
  - 1.5556990265846253
  validation_losses:
  - 0.4120346009731293
  - 0.42054155468940735
  - 0.413598895072937
  - 0.3871650695800781
  - 0.3917941749095917
  - 0.39263805747032166
  - 0.6064864993095398
  - 0.38758280873298645
  - 0.38357558846473694
  - 0.3940572142601013
  - 0.39152783155441284
  - 0.3953173756599426
  - 0.39371898770332336
  - 0.38746798038482666
  - 0.6943047642707825
  - 0.38144800066947937
  - 0.4047500491142273
  - 0.38213860988616943
  - 0.39092177152633667
  - 0.39151284098625183
  - 0.41529369354248047
  - 0.3913925588130951
  - 0.3869045078754425
  - 0.3912227749824524
  - 0.3799310326576233
  - 0.3914809226989746
  - 0.39064469933509827
  - 0.3933286666870117
  - 0.39061740040779114
  - 0.38995546102523804
  - 0.39711669087409973
  - 0.395946204662323
loss_records_fold1:
  train_losses:
  - 1.513279914855957
  - 1.5463455379009248
  - 1.5047029316425324
  - 1.527993619441986
  - 1.520165276527405
  - 1.5916004657745362
  - 1.4977828919887544
  - 1.5371237754821778
  - 1.5432656049728395
  - 1.52350210249424
  - 1.586909854412079
  - 1.5505668044090273
  - 1.5565615713596346
  - 1.4745768874883654
  - 1.585154587030411
  - 1.5365991652011872
  - 1.5102260023355485
  - 1.5484998106956482
  - 1.5315279066562653
  - 1.5976397156715394
  - 1.516761738061905
  validation_losses:
  - 0.39708074927330017
  - 0.39752957224845886
  - 0.38673821091651917
  - 0.3877372443675995
  - 0.3897278308868408
  - 0.39824727177619934
  - 0.38842684030532837
  - 0.40352463722229004
  - 0.3943573236465454
  - 0.40065738558769226
  - 0.43395641446113586
  - 0.398847758769989
  - 0.390143483877182
  - 0.39699703454971313
  - 0.4179132878780365
  - 0.4095258116722107
  - 0.3959236443042755
  - 0.40137943625450134
  - 0.3925419747829437
  - 0.39746415615081787
  - 0.4026881754398346
loss_records_fold2:
  train_losses:
  - 1.481369897723198
  - 1.5819101214408875
  - 1.5700534760951996
  - 1.5272991418838502
  - 1.51563276052475
  - 1.5920326232910158
  - 1.6256244003772737
  - 1.5317710280418397
  - 1.5356743454933168
  - 1.550804603099823
  - 1.5209443747997284
  - 1.5069089353084566
  - 1.490031534433365
  - 1.5398377180099487
  - 1.5309850573539734
  - 1.4750276625156404
  - 1.5102066069841387
  - 1.5415858864784242
  - 1.51445529460907
  - 1.5321836888790132
  - 1.5749678194522858
  - 1.5410623252391815
  - 1.5008023262023926
  - 1.5269756734371187
  - 1.5115397334098817
  - 1.5070475220680237
  - 1.6311006724834443
  - 1.5292614340782167
  - 1.544925367832184
  - 1.5215328991413117
  - 1.5652237951755525
  - 1.538359206914902
  - 1.454681932926178
  - 1.5587123095989228
  - 1.5632355093955994
  - 1.5312142193317415
  - 1.5323154151439669
  - 1.5277002394199373
  - 1.5065395891666413
  - 1.5534465789794922
  - 1.530498665571213
  - 1.5117927372455597
  - 1.5341647565364838
  - 1.525562822818756
  - 1.5283616900444033
  validation_losses:
  - 0.4976097345352173
  - 0.4222376346588135
  - 0.379557341337204
  - 0.40675991773605347
  - 0.40129175782203674
  - 0.390023797750473
  - 0.38582929968833923
  - 0.39831992983818054
  - 0.39580750465393066
  - 0.5424199104309082
  - 0.38984984159469604
  - 0.6033569574356079
  - 0.4769880771636963
  - 0.42238861322402954
  - 0.40208837389945984
  - 0.38564634323120117
  - 0.4107656180858612
  - 0.43777996301651
  - 0.4023679196834564
  - 0.43605029582977295
  - 0.4626084864139557
  - 0.4008238613605499
  - 0.4310153126716614
  - 0.3856111466884613
  - 0.40275371074676514
  - 0.4459761083126068
  - 0.6174634099006653
  - 0.4868026077747345
  - 0.3858875632286072
  - 0.3913061320781708
  - 0.49081680178642273
  - 0.3897314965724945
  - 0.39693766832351685
  - 0.39406293630599976
  - 0.43086522817611694
  - 0.6251370310783386
  - 0.6951942443847656
  - 0.41165274381637573
  - 0.5425547957420349
  - 0.40105554461479187
  - 0.4042273461818695
  - 0.39938443899154663
  - 0.40287166833877563
  - 0.39224234223365784
  - 0.3835974931716919
loss_records_fold3:
  train_losses:
  - 1.5756627798080445
  - 1.532255893945694
  - 1.5523254692554476
  - 1.5689690470695496
  - 1.5957848608493805
  - 1.5445305526256563
  - 1.5413863003253938
  - 1.5431485533714295
  - 1.631816339492798
  - 1.5435649454593658
  - 1.5764613568782808
  - 1.5478640794754028
  - 1.5532801628112793
  - 1.5310064792633058
  validation_losses:
  - 0.47900041937828064
  - 0.37405312061309814
  - 0.38194629549980164
  - 0.3989367187023163
  - 0.3841363787651062
  - 0.37440225481987
  - 0.4480980336666107
  - 0.7174587845802307
  - 0.3760277330875397
  - 0.3817189931869507
  - 0.37513378262519836
  - 0.3770170509815216
  - 0.37174850702285767
  - 0.37465840578079224
loss_records_fold4:
  train_losses:
  - 1.5574457347393036
  - 1.513831150531769
  - 1.5367177784442902
  - 1.5360407650470735
  - 1.5163356840610505
  - 1.5520391702651979
  - 1.5284866452217103
  - 1.6121049284934998
  - 1.5047296345233918
  - 1.5512286901474
  - 1.5711485564708711
  - 1.5415880322456361
  - 1.5257187187671661
  - 1.5398238241672517
  - 1.5015256226062776
  - 1.507299166917801
  - 1.5499799311161042
  - 1.5629773139953613
  - 1.5313903868198395
  - 1.5374016225337983
  - 1.6058133959770204
  - 1.5714074790477754
  - 1.5376958310604096
  - 1.5118091464042664
  - 1.5879816472530366
  - 1.539711618423462
  - 1.5205819845199586
  - 1.5190043449401855
  validation_losses:
  - 0.38115745782852173
  - 0.3767085075378418
  - 0.37085044384002686
  - 0.3806005120277405
  - 0.37808483839035034
  - 0.4020906686782837
  - 0.40057608485221863
  - 0.39273738861083984
  - 0.38291293382644653
  - 0.41665413975715637
  - 0.48826321959495544
  - 0.3694266676902771
  - 0.3932994306087494
  - 0.40294596552848816
  - 0.3938203454017639
  - 0.40073779225349426
  - 0.3723948001861572
  - 0.3913937509059906
  - 0.3797381520271301
  - 0.3719061017036438
  - 0.3699357509613037
  - 0.38519197702407837
  - 0.37729310989379883
  - 0.386474072933197
  - 0.37947654724121094
  - 0.3744339048862457
  - 0.372327983379364
  - 0.3722670078277588
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 32 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8593481989708405, 0.8593481989708405, 0.8576329331046312,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.02380952380952381, 0.0, 0.023529411764705882, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.009467787114845938
  total_train_time: '0:11:37.326256'
