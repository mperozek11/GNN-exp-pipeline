config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:28:06.421775'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_85fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_85fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_85fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_85fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_85fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_85fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_85fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_85fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_85fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_85fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.514426308870316
  - 3.243940252065659
  - 3.272314429283142
  - 3.350859582424164
  - 3.4949462652206424
  - 3.1462097346782687
  - 3.2858657836914062
  - 3.1188028752803802
  - 3.407138901948929
  - 3.3173902332782745
  - 3.3413458585739138
  - 3.1928465217351913
  - 3.2010420441627505
  - 3.0386634081602097
  - 3.159952384233475
  - 3.2614030063152315
  - 3.0883747160434725
  - 3.102474862337113
  - 3.1205124616622926
  - 3.0597331047058107
  - 3.1952102541923524
  validation_losses:
  - 0.3919580280780792
  - 0.4175546169281006
  - 0.3924369215965271
  - 0.40979740023612976
  - 0.40584954619407654
  - 0.40923959016799927
  - 0.4081483483314514
  - 0.39794617891311646
  - 0.4007095694541931
  - 0.48327910900115967
  - 0.4364919364452362
  - 0.40136173367500305
  - 0.3938804864883423
  - 0.40148067474365234
  - 0.42427414655685425
  - 0.4073217511177063
  - 0.4002494513988495
  - 0.39426693320274353
  - 0.3952578902244568
  - 0.39533528685569763
  - 0.3966892659664154
loss_records_fold1:
  train_losses:
  - 3.191459447145462
  - 3.1072268307209017
  - 2.9920247882604603
  - 3.028740680217743
  - 2.9929127395153046
  - 3.0904367089271547
  - 3.0574946939945224
  - 2.996829551458359
  - 3.0425146877765656
  - 3.0384508967399597
  - 3.019703757762909
  validation_losses:
  - 0.39323005080223083
  - 0.394986093044281
  - 0.3998042643070221
  - 0.41271722316741943
  - 0.3909635543823242
  - 0.39537161588668823
  - 0.3934183120727539
  - 0.3970516324043274
  - 0.4006212055683136
  - 0.3931594491004944
  - 0.39717838168144226
loss_records_fold2:
  train_losses:
  - 3.082399779558182
  - 3.0776737809181216
  - 3.062769538164139
  - 2.953064978122711
  - 3.1405935645103455
  - 2.981821608543396
  - 3.04192271232605
  - 3.166766506433487
  - 3.005245137214661
  - 3.018995648622513
  - 3.014050525426865
  - 2.983464998006821
  - 3.016537696123123
  - 2.980028831958771
  - 2.995300203561783
  - 2.9888091027736667
  validation_losses:
  - 0.4088992178440094
  - 0.3908270001411438
  - 0.4014124870300293
  - 0.3925671875476837
  - 0.39726051688194275
  - 0.3967229127883911
  - 0.39600443840026855
  - 0.3824993371963501
  - 0.3846862316131592
  - 0.4287060797214508
  - 0.4262879490852356
  - 0.40597450733184814
  - 0.39333575963974
  - 0.3943869173526764
  - 0.3993997275829315
  - 0.3923431634902954
loss_records_fold3:
  train_losses:
  - 2.992992925643921
  - 2.9978244453668594
  - 2.9784829378128053
  - 3.1036621749401094
  - 3.085666984319687
  - 3.0531560450792314
  - 2.9790323019027714
  - 2.9706444233655933
  - 3.0328120410442354
  - 2.972039216756821
  - 3.0449874222278597
  - 3.019520175457001
  - 3.058796000480652
  - 2.944141903519631
  - 3.001950001716614
  - 2.9457509517669678
  - 3.068741583824158
  - 2.988957840204239
  - 3.0552478045225144
  - 3.041399019956589
  - 3.029241627454758
  - 3.0402022242546085
  - 3.0565630912780763
  - 3.0146763533353806
  - 3.1132450580596926
  - 2.9896860122680664
  - 2.99909765124321
  - 3.000792640447617
  - 2.9149102836847307
  - 3.0686618924140934
  - 3.1129038989543916
  - 3.0607514083385468
  - 3.0876491129398347
  - 2.99201130270958
  - 3.128313559293747
  - 3.0049881637096405
  - 3.0523915112018587
  - 3.0273976624011993
  - 3.1798360109329225
  - 3.0412699341773988
  - 3.1377454817295076
  - 2.9909617424011232
  - 3.051186087727547
  - 3.1140285789966584
  - 3.0499441981315614
  - 2.9934271186590196
  - 3.040465396642685
  - 3.0303430348634723
  - 3.0271268397569657
  - 3.030918163061142
  - 2.9488662481307983
  - 2.9671377420425418
  - 3.0025658786296847
  - 3.007918989658356
  - 3.0377939999103547
  - 2.980603203177452
  - 2.926265448331833
  - 2.932974112033844
  - 2.9137571245431904
  - 2.9198292374610904
  - 2.927034565806389
  - 2.945197039842606
  - 3.010875588655472
  - 3.0164510905742645
  - 2.8869576126337053
  - 2.9629832565784455
  - 2.9965612232685093
  - 3.000267368555069
  - 3.0118788182735443
  - 3.0111352771520616
  - 2.9626731753349307
  - 3.03027526140213
  - 2.914559942483902
  - 2.9487386345863342
  - 2.940153047442436
  - 2.96095427274704
  - 2.938222134113312
  - 2.9720108032226564
  - 2.990456849336624
  - 2.9792632043361666
  - 2.9895401477813723
  - 2.9331351637840273
  - 2.953733405470848
  - 3.056014555692673
  - 2.90945685505867
  - 2.973417317867279
  - 2.9652759701013567
  - 2.9185209393501284
  - 2.9045524537563328
  - 2.9437023758888246
  - 2.876151326298714
  - 2.994020748138428
  - 2.9913825899362565
  - 2.999801218509674
  - 2.920876157283783
  - 2.933152544498444
  - 2.927541434764862
  - 2.9030594468116764
  - 2.9587954699993135
  - 2.984684205055237
  validation_losses:
  - 0.40331563353538513
  - 0.3837670087814331
  - 0.7142603397369385
  - 0.38241222500801086
  - 0.44563183188438416
  - 0.41880354285240173
  - 0.42585790157318115
  - 0.39482372999191284
  - 0.406538188457489
  - 0.45810025930404663
  - 0.48306575417518616
  - 0.5897868275642395
  - 0.3858410716056824
  - 0.4321533143520355
  - 0.6228246092796326
  - 0.5770707726478577
  - 0.37983036041259766
  - 0.42890873551368713
  - 0.48610609769821167
  - 1.2884488105773926
  - 0.48127833008766174
  - 1.1888692378997803
  - 0.38686898350715637
  - 0.39645475149154663
  - 0.4118722975254059
  - 0.4271974265575409
  - 0.5051936507225037
  - 0.5459248423576355
  - 0.6750162839889526
  - 0.5368525385856628
  - 0.4189511835575104
  - 0.3773405849933624
  - 0.4488530158996582
  - 0.3964574337005615
  - 0.5743964314460754
  - 0.38837888836860657
  - 0.4244263470172882
  - 1.2894021272659302
  - 0.38835906982421875
  - 0.3900695741176605
  - 0.3823114037513733
  - 0.3779774010181427
  - 0.41964811086654663
  - 0.37726834416389465
  - 0.38813331723213196
  - 0.39021557569503784
  - 0.39718905091285706
  - 0.38727325201034546
  - 0.4884313941001892
  - 0.4821750223636627
  - 0.47585615515708923
  - 0.5074478983879089
  - 0.5284148454666138
  - 0.5389334559440613
  - 0.459256649017334
  - 0.4315981864929199
  - 0.4612334370613098
  - 0.49090471863746643
  - 0.6837664842605591
  - 0.5303910374641418
  - 0.555558443069458
  - 0.62156081199646
  - 0.45304638147354126
  - 0.40016162395477295
  - 0.40242934226989746
  - 0.4477734863758087
  - 0.46203485131263733
  - 0.4750785529613495
  - 0.49113160371780396
  - 0.38392654061317444
  - 0.45709413290023804
  - 0.4157731533050537
  - 0.4131864011287689
  - 0.727972686290741
  - 0.5019059181213379
  - 0.46823054552078247
  - 2.401010513305664
  - 0.4227433204650879
  - 0.4180377423763275
  - 0.44137778878211975
  - 0.47937703132629395
  - 0.45409467816352844
  - 0.4969745874404907
  - 0.48033779859542847
  - 0.927623450756073
  - 0.6212241053581238
  - 0.6682138442993164
  - 0.5256457924842834
  - 0.706932544708252
  - 2.0711987018585205
  - 0.631987988948822
  - 1.220044493675232
  - 0.6086831092834473
  - 0.5894978046417236
  - 0.6381849646568298
  - 0.6188035011291504
  - 0.8408104777336121
  - 1.0875173807144165
  - 0.6131247878074646
  - 0.6287224292755127
loss_records_fold4:
  train_losses:
  - 2.9984242022037506
  - 2.935372853279114
  - 2.97098508477211
  - 2.9189407140016557
  - 2.9034666597843173
  - 2.9163492023944855
  - 2.9669794499874116
  - 2.9094645559787753
  - 2.898908400535584
  - 2.9104035407304765
  - 2.977945011854172
  - 2.84464892745018
  - 3.02343122959137
  - 2.949207305908203
  - 2.8848885476589206
  - 2.959005126357079
  - 2.915143865346909
  - 2.889874613285065
  - 3.004913938045502
  - 2.8431496381759644
  - 2.9612949013710024
  - 2.9728308618068695
  - 2.95847744345665
  - 2.9949633419513706
  - 2.902078920602799
  - 2.9188129365444184
  - 2.8663070738315586
  - 2.9371635913848877
  - 2.875751715898514
  - 3.008052515983582
  - 2.9072579562664034
  - 2.9516992509365085
  - 2.9583473026752474
  - 2.9529054522514344
  - 2.935492253303528
  - 2.912460201978684
  - 2.917440983653069
  - 3.0073850691318516
  - 2.923017382621765
  - 2.881582456827164
  - 2.8644290179014207
  - 2.9181958913803103
  - 2.95984023809433
  - 2.9731653809547427
  - 2.9803611397743226
  - 2.848170030117035
  - 2.9077430069446564
  - 2.890285247564316
  - 2.936927539110184
  - 2.9353571981191635
  - 2.8917597830295563
  - 2.9406138539314273
  - 2.8942764550447464
  - 3.002184414863587
  - 2.918213325738907
  - 3.0930370271205905
  - 2.9967541098594666
  - 2.922394770383835
  - 2.9466498434543613
  - 2.925753128528595
  - 2.8726595997810365
  - 2.9557927131652835
  - 2.888130354881287
  - 2.9234875679016117
  - 2.9018216133117676
  - 2.889964306354523
  - 2.908699107170105
  - 2.8998889535665513
  - 2.948608487844467
  - 2.946888655424118
  - 3.0275805056095124
  - 2.979813611507416
  - 2.960564535856247
  - 2.9973376929759983
  - 2.817816096544266
  - 2.880661290884018
  - 2.9544499546289447
  - 2.838387554883957
  - 2.905308669805527
  - 2.8348966181278232
  - 2.9028722375631335
  - 2.983144462108612
  - 2.853502753376961
  - 2.931151878833771
  - 2.89396870136261
  - 2.8627553939819337
  - 2.923234769701958
  - 2.890886676311493
  - 2.901384636759758
  - 2.890688544511795
  - 2.908233100175858
  - 2.959640955924988
  - 2.933426880836487
  - 2.9052808701992037
  - 2.8902460247278214
  - 2.9551636666059498
  - 2.803864449262619
  - 2.8532698929309848
  - 2.9178861737251283
  - 2.834189474582672
  validation_losses:
  - 0.39769935607910156
  - 0.5352264046669006
  - 0.370246559381485
  - 0.4458727240562439
  - 0.570871889591217
  - 0.4947011172771454
  - 0.5034100413322449
  - 0.44382864236831665
  - 0.4972180724143982
  - 0.4696473479270935
  - 0.4452819228172302
  - 0.42705604434013367
  - 0.43441784381866455
  - 0.48261913657188416
  - 0.3627198040485382
  - 0.3722883462905884
  - 0.5646300911903381
  - 0.592001736164093
  - 0.4400821030139923
  - 0.6985791325569153
  - 0.39344891905784607
  - 0.4276147782802582
  - 0.5476598739624023
  - 0.39797937870025635
  - 0.5101335048675537
  - 0.45949509739875793
  - 0.43352559208869934
  - 0.6556236147880554
  - 0.9032909870147705
  - 0.4344467520713806
  - 0.4247145652770996
  - 0.4621988534927368
  - 0.4802408814430237
  - 0.4955493211746216
  - 0.47451359033584595
  - 0.44123977422714233
  - 0.42289045453071594
  - 0.4376359283924103
  - 0.4537454843521118
  - 0.45944076776504517
  - 0.43595072627067566
  - 0.4582333564758301
  - 0.48397964239120483
  - 0.41607239842414856
  - 0.4719931483268738
  - 0.45735451579093933
  - 0.4227958023548126
  - 0.42005711793899536
  - 0.4508742690086365
  - 0.4523310363292694
  - 0.39415866136550903
  - 0.44805362820625305
  - 0.5336776375770569
  - 0.39807119965553284
  - 0.3843677043914795
  - 0.4060206413269043
  - 0.37733250856399536
  - 0.4019109606742859
  - 0.4215013086795807
  - 0.42807212471961975
  - 0.4369066655635834
  - 0.4822639524936676
  - 0.46809640526771545
  - 0.4193223714828491
  - 0.4831264019012451
  - 0.4253503084182739
  - 0.4897000193595886
  - 0.41480499505996704
  - 0.5537443161010742
  - 0.6689481735229492
  - 0.48171353340148926
  - 0.40405353903770447
  - 0.4599003195762634
  - 0.5542027354240417
  - 0.6315728425979614
  - 0.4880750775337219
  - 0.5641467571258545
  - 0.5817033648490906
  - 0.6845055222511292
  - 0.722975492477417
  - 0.5109412670135498
  - 0.5442536473274231
  - 0.4817056655883789
  - 0.42625147104263306
  - 0.7086115479469299
  - 0.6927610039710999
  - 0.4345328211784363
  - 0.44295772910118103
  - 0.5106683373451233
  - 0.401965856552124
  - 0.5910066962242126
  - 0.5451956987380981
  - 0.4510447680950165
  - 0.6550083160400391
  - 0.5201843976974487
  - 0.5075609087944031
  - 0.6666446328163147
  - 0.8639548420906067
  - 0.5855764150619507
  - 0.5463607311248779
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8576329331046312, 0.8490566037735849,
    0.7457044673539519]'
  fold_eval_f1: '[0.0, 0.0, 0.06741573033707865, 0.12, 0.2371134020618557]'
  mean_eval_accuracy: 0.833531974088286
  mean_f1_accuracy: 0.08490582647978687
  total_train_time: '0:22:51.529323'
