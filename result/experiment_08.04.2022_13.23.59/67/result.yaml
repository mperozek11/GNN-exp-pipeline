config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 20:43:04.451802'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/67/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/67/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/67/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/67/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/67/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/67/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/67/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/67/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/67/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/67/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 16.28811338543892
  - 16.91437566280365
  - 16.212350592017174
  - 15.649430438876152
  - 14.659421652555466
  - 14.48764255642891
  - 14.428160279989243
  - 14.689389377832413
  - 14.596181944012642
  - 14.495320409536362
  - 15.02664440870285
  - 14.58537644147873
  - 15.965106219053268
  - 14.611438393592834
  - 14.446647822856903
  - 15.47137962281704
  - 14.981423646211624
  - 15.496685415506363
  - 14.560186117887497
  - 15.719788238406181
  - 15.108210355043411
  - 14.975253880023956
  - 14.484237909317017
  - 14.494615882635117
  - 14.53460618853569
  - 14.763160288333893
  - 14.89442366361618
  - 14.598101019859314
  - 14.547817215323448
  - 14.558950066566467
  - 15.650035083293915
  - 14.396528482437134
  - 14.537871748209
  - 14.65204468369484
  - 14.670292437076569
  - 14.61956399679184
  - 17.403173565864563
  - 16.256141275167465
  - 15.407905280590057
  - 14.680337488651276
  - 15.109976023435593
  - 14.763216197490692
  - 14.8649183511734
  - 14.704843640327454
  - 15.447422087192535
  - 15.152405366301537
  - 14.71209904551506
  - 14.852357938885689
  - 15.415347218513489
  - 14.928312122821808
  - 14.543018877506256
  - 14.992883771657944
  - 15.456190288066864
  - 14.81136754155159
  - 14.819248020648956
  - 14.62024387717247
  - 14.635637506842613
  - 15.220780044794083
  - 15.59145513176918
  - 14.636512339115143
  - 14.681071177124977
  - 14.571387633681297
  - 14.940972208976746
  - 14.791848868131638
  - 14.935072287917137
  - 14.850601375102997
  - 15.782708197832108
  - 15.006189465522766
  - 15.277908772230148
  - 14.889431104063988
  - 14.766771256923676
  - 14.839528232812881
  - 14.88926175236702
  - 15.006040066480637
  - 14.400782719254494
  - 14.480959981679916
  - 15.019418731331825
  - 14.739959359169006
  - 14.547264233231544
  - 14.949934542179108
  - 16.463400408625603
  - 15.052745208144188
  - 15.27711033821106
  - 14.538914144039154
  - 14.529007226228714
  - 14.515766218304634
  - 15.46884423494339
  - 14.741644382476807
  - 14.685808956623077
  - 14.645030573010445
  - 14.835679024457932
  - 14.489384263753891
  - 14.600531995296478
  - 14.586896449327469
  - 15.39579039812088
  - 15.391350537538528
  - 15.618754968047142
  - 14.569269627332687
  - 14.726854681968689
  - 14.897809386253357
  validation_losses:
  - 0.44101178646087646
  - 0.3995700776576996
  - 0.4448765814304352
  - 0.45330020785331726
  - 0.4114813506603241
  - 0.4479696750640869
  - 0.40353772044181824
  - 0.4554632902145386
  - 0.7889376282691956
  - 0.4416452646255493
  - 0.639470100402832
  - 0.5417406558990479
  - 0.5361974239349365
  - 0.590301513671875
  - 0.48702412843704224
  - 0.42788001894950867
  - 0.4092182517051697
  - 0.40713921189308167
  - 0.397429496049881
  - 0.40888160467147827
  - 0.4063737094402313
  - 1.5962713956832886
  - 1.2481094598770142
  - 1.7576098442077637
  - 2.3393545150756836
  - 1.2587333917617798
  - 1.9968194961547852
  - 1.3250757455825806
  - 1.7900298833847046
  - 1.265939474105835
  - 1.916664719581604
  - 1.3483200073242188
  - 2.0567421913146973
  - 1.4780092239379883
  - 1.557134985923767
  - 2.1768407821655273
  - 1.1300472021102905
  - 1.1383438110351562
  - 0.8736065030097961
  - 1.4134873151779175
  - 1.636590600013733
  - 1.7234110832214355
  - 1.4646921157836914
  - 1.259209394454956
  - 1.7269829511642456
  - 1.6381568908691406
  - 1.4071390628814697
  - 2.003227710723877
  - 0.8185557126998901
  - 1.7644675970077515
  - 1.0583866834640503
  - 1.416750192642212
  - 1.6050500869750977
  - 0.9233136773109436
  - 1.7260922193527222
  - 1.4323959350585938
  - 1.7763961553573608
  - 1.766360878944397
  - 2.396714925765991
  - 1.4256889820098877
  - 1.7113285064697266
  - 1.4084289073944092
  - 1.5527764558792114
  - 1.4676871299743652
  - 1.057316541671753
  - 1.2610547542572021
  - 1.0516691207885742
  - 0.854782223701477
  - 1.6027956008911133
  - 1.2452392578125
  - 1.2017722129821777
  - 1.2214055061340332
  - 2.1803138256073
  - 1.530615210533142
  - 1.1603516340255737
  - 1.4550362825393677
  - 1.2548108100891113
  - 1.377511739730835
  - 1.6985368728637695
  - 1.8942570686340332
  - 1.2606208324432373
  - 1.6182631254196167
  - 1.873750925064087
  - 1.4257062673568726
  - 1.7123149633407593
  - 1.7011812925338745
  - 1.8800981044769287
  - 1.701108694076538
  - 1.6108065843582153
  - 1.4284634590148926
  - 1.8359400033950806
  - 1.4489216804504395
  - 1.6077682971954346
  - 0.9503812789916992
  - 1.1634517908096313
  - 0.8742473721504211
  - 1.4053411483764648
  - 1.4707872867584229
  - 1.1302999258041382
  - 1.141236662864685
loss_records_fold2:
  train_losses:
  - 15.218871861696243
  - 14.491973802447319
  - 16.020519971847534
  - 14.883070290088654
  - 15.0294109582901
  - 14.390666425228119
  - 14.668689638376236
  - 14.485002398490906
  - 14.507642209529877
  - 14.943543016910553
  - 15.04292842745781
  - 15.401592820882797
  - 15.184304773807526
  - 14.695865422487259
  - 14.524578541517258
  - 14.576208531856537
  - 15.006894960999489
  - 15.295714780688286
  - 14.759373664855957
  - 14.84646512567997
  - 14.61460654437542
  - 15.468457639217377
  - 15.02113801240921
  - 14.740991860628128
  - 14.420457661151886
  - 14.456649988889694
  - 14.685626655817032
  - 14.538147211074829
  - 14.557586312294006
  - 14.59591868519783
  - 14.588733181357384
  - 14.825026497244835
  - 15.470817297697067
  - 14.733051210641861
  - 14.350848659873009
  - 14.938767492771149
  - 14.91465339064598
  - 14.517783284187317
  - 14.816193401813507
  - 14.579488009214401
  - 14.673944115638733
  - 14.461576461791992
  - 14.599515557289124
  - 15.031807780265808
  - 15.133810460567474
  - 15.30018949508667
  - 14.453476801514626
  - 14.805048823356628
  - 14.807299226522446
  - 14.507866978645325
  - 14.725539229810238
  - 14.91732819378376
  - 15.067810893058777
  - 14.74735301733017
  - 14.48563802242279
  - 14.778126209974289
  - 14.690871864557266
  - 14.546638205647469
  - 14.535812757909298
  - 14.77566084265709
  - 14.704686507582664
  - 14.699692860245705
  - 15.838988900184631
  - 14.757660120725632
  - 14.791488260030746
  - 15.194463193416595
  - 14.469910502433777
  - 14.527697265148163
  - 15.213629841804504
  - 15.245453476905823
  - 15.132782995700836
  - 14.579030275344849
  - 14.416906297206879
  - 14.726954132318497
  - 14.84289638698101
  - 15.001249626278877
  - 14.512059062719345
  - 15.036814212799072
  - 15.364915430545807
  - 14.753044962882996
  - 14.451043471693993
  - 14.767635390162468
  - 15.045583873987198
  - 15.054258927702904
  - 14.662339895963669
  - 14.426416590809822
  - 14.346568256616592
  - 14.463858515024185
  - 14.970267564058304
  - 14.683949708938599
  - 14.420384541153908
  - 15.25682257115841
  - 14.493702232837677
  - 15.18373703956604
  - 14.552117481827736
  - 14.837283164262772
  - 14.970187097787857
  - 15.034016251564026
  - 14.954341799020767
  - 15.125799790024757
  validation_losses:
  - 1.3573756217956543
  - 1.004540205001831
  - 1.322383165359497
  - 1.4569146633148193
  - 1.6855227947235107
  - 1.8678916692733765
  - 1.5688692331314087
  - 1.1080690622329712
  - 1.5125747919082642
  - 1.7637077569961548
  - 1.2675055265426636
  - 1.9063771963119507
  - 1.4476027488708496
  - 1.5238131284713745
  - 1.7151496410369873
  - 1.206650972366333
  - 1.402026891708374
  - 1.792730689048767
  - 1.378382682800293
  - 1.8188201189041138
  - 0.7536810636520386
  - 1.6979180574417114
  - 1.3471064567565918
  - 1.7372968196868896
  - 1.6625854969024658
  - 1.9607218503952026
  - 1.3958560228347778
  - 1.5007632970809937
  - 2.071702003479004
  - 1.4679588079452515
  - 0.9610840678215027
  - 1.3737189769744873
  - 1.096199870109558
  - 1.4740419387817383
  - 1.9046300649642944
  - 1.422841191291809
  - 1.6710408926010132
  - 1.5639296770095825
  - 1.4492295980453491
  - 1.3853682279586792
  - 1.2229291200637817
  - 1.1395341157913208
  - 1.7634005546569824
  - 1.9777778387069702
  - 2.129844903945923
  - 1.4325227737426758
  - 1.0650603771209717
  - 1.3212902545928955
  - 0.9885327219963074
  - 1.5639592409133911
  - 1.079254150390625
  - 1.2471126317977905
  - 0.980075478553772
  - 1.4850058555603027
  - 0.8875887393951416
  - 1.7650882005691528
  - 1.1357989311218262
  - 1.1800202131271362
  - 1.061360239982605
  - 1.5712714195251465
  - 1.4542312622070312
  - 0.9231818914413452
  - 1.045269250869751
  - 0.9647107124328613
  - 0.9586477279663086
  - 1.5225439071655273
  - 1.1035977602005005
  - 1.5302855968475342
  - 1.3653532266616821
  - 1.4896494150161743
  - 1.232064962387085
  - 1.46828031539917
  - 0.8779041171073914
  - 1.120957851409912
  - 1.247796893119812
  - 1.2324193716049194
  - 1.1872912645339966
  - 1.709429144859314
  - 1.256986141204834
  - 1.5344979763031006
  - 0.7692431807518005
  - 1.4267394542694092
  - 1.1013654470443726
  - 1.2334967851638794
  - 1.529447078704834
  - 1.350540280342102
  - 1.3444828987121582
  - 1.0646748542785645
  - 0.8284419775009155
  - 0.909050703048706
  - 0.8345130681991577
  - 1.604572057723999
  - 0.40252038836479187
  - 0.4227626323699951
  - 0.42868417501449585
  - 0.41398829221725464
  - 0.448199063539505
  - 0.40757933259010315
  - 0.5408196449279785
  - 0.41056978702545166
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 41 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 9 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.17152658662092624, 0.8593481989708405, 0.14065180102915953,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.25347758887171556, 0.0, 0.24661654135338348, 0.0]'
  mean_eval_accuracy: 0.5776532097870359
  mean_f1_accuracy: 0.10001882604501981
  total_train_time: '0:07:50.365252'
