config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 21:27:06.276086'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/74/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/74/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/74/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/74/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/74/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/74/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/74/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/74/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/74/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/74/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 15.508319526910782
  - 16.166925460100174
  - 16.58783659338951
  - 15.70046815276146
  - 15.587456822395325
  - 15.393764853477478
  - 16.303381472826004
  - 16.255038052797318
  - 15.940238267183304
  - 15.03408545255661
  - 15.409477889537811
  - 15.20228810608387
  - 15.003680646419525
  - 15.140055477619171
  - 14.965967297554016
  - 15.243318974971771
  - 15.235022366046906
  - 15.76343446969986
  - 15.221664994955063
  - 14.815717279911041
  - 14.840869575738907
  - 15.055698037147522
  - 15.168149769306183
  - 15.173292964696884
  - 15.145605117082596
  - 14.976288944482803
  - 14.628672629594803
  - 14.832569301128387
  - 15.068390637636185
  - 15.139063596725464
  - 14.907151013612747
  - 14.879688009619713
  - 15.113507241010666
  - 14.871842965483665
  - 15.180286645889282
  - 14.85412421822548
  - 14.807340294122696
  - 14.718153432011604
  - 14.913022458553314
  - 14.978307619690895
  - 14.974555402994156
  - 14.736639648675919
  - 14.785820007324219
  - 14.942856505513191
  - 14.81199187040329
  - 14.782418817281723
  - 14.98961716890335
  - 15.029850155115128
  - 14.847080707550049
  - 14.853796184062958
  - 14.858663439750671
  - 14.824851006269455
  - 14.87135162949562
  - 14.799586176872253
  - 15.119064927101135
  - 14.696577280759811
  - 14.483757048845291
  - 14.702486991882324
  - 14.945580065250397
  - 14.51580148935318
  - 15.036638170480728
  - 15.161884292960167
  - 14.792437314987183
  - 15.383653789758682
  - 14.896120935678482
  - 14.757551729679108
  - 14.869949907064438
  - 14.920432358980179
  - 14.376335263252258
  - 14.69259737432003
  - 14.712540492415428
  - 15.068336337804794
  - 14.447780668735504
  - 14.585979402065277
  - 14.929214298725128
  - 14.759478330612183
  - 15.090157449245453
  - 14.223118335008621
  - 14.500598639249802
  - 15.013451099395752
  - 14.896274864673615
  - 14.711212232708931
  - 14.501976996660233
  - 14.88259670138359
  - 14.878242522478104
  - 14.489199668169022
  - 14.732958242297173
  - 14.714216247200966
  - 14.698601305484772
  - 14.498175725340843
  - 14.779777199029922
  - 15.087924867868423
  - 16.174249589443207
  - 14.8860984146595
  - 14.94522488117218
  - 14.7862249314785
  - 14.92218692600727
  - 14.589547082781792
  - 14.530865460634232
  - 14.732768535614014
  validation_losses:
  - 0.39489975571632385
  - 0.3944960832595825
  - 0.45345720648765564
  - 0.3999291956424713
  - 0.40337133407592773
  - 0.39417195320129395
  - 0.41678673028945923
  - 0.39950913190841675
  - 0.3950408101081848
  - 0.391000896692276
  - 0.4017292261123657
  - 0.4104459881782532
  - 0.4124641716480255
  - 0.39409568905830383
  - 0.39355865120887756
  - 0.38780418038368225
  - 0.4430660903453827
  - 0.3896702826023102
  - 0.4070826768875122
  - 0.4022625684738159
  - 0.3878389596939087
  - 0.39670097827911377
  - 0.4075503945350647
  - 0.3961624503135681
  - 0.4004327356815338
  - 0.399868369102478
  - 0.4043412208557129
  - 0.7913217544555664
  - 1.053735375404358
  - 0.39563167095184326
  - 0.4248563051223755
  - 0.3943804204463959
  - 0.5212041139602661
  - 0.4025782644748688
  - 0.3920418620109558
  - 0.6587011218070984
  - 0.8512134552001953
  - 0.4879379868507385
  - 1.0472064018249512
  - 1.5208085775375366
  - 1.2847331762313843
  - 0.8181270956993103
  - 0.4113974869251251
  - 0.5244911313056946
  - 1.861611247062683
  - 1.8659694194793701
  - 1.279009461402893
  - 0.4827079176902771
  - 0.8217588067054749
  - 0.4386124014854431
  - 0.46311667561531067
  - 0.78997403383255
  - 0.6948103904724121
  - 0.9457210302352905
  - 1.5596352815628052
  - 1.152092695236206
  - 1.6946079730987549
  - 1.7349990606307983
  - 0.418892502784729
  - 1.8679488897323608
  - 3.3787190914154053
  - 1.5123608112335205
  - 1.0353773832321167
  - 1.1250814199447632
  - 2.030277967453003
  - 0.6226246953010559
  - 0.7302054762840271
  - 1.0229171514511108
  - 1.123341679573059
  - 1.0930637121200562
  - 1.5418152809143066
  - 1.282993197441101
  - 1.574103593826294
  - 1.1332859992980957
  - 1.3286564350128174
  - 0.9839377403259277
  - 0.9082490801811218
  - 0.605380654335022
  - 2.0638222694396973
  - 1.0127137899398804
  - 0.7435396313667297
  - 1.1314433813095093
  - 1.1313323974609375
  - 2.426753282546997
  - 0.8962548971176147
  - 1.368736743927002
  - 1.1334445476531982
  - 0.6554587483406067
  - 1.3696625232696533
  - 1.6472748517990112
  - 1.2117849588394165
  - 1.1698930263519287
  - 0.3985803723335266
  - 0.6958727240562439
  - 0.9097285270690918
  - 1.0907729864120483
  - 1.0103278160095215
  - 1.324310541152954
  - 0.974524199962616
  - 0.8848331570625305
loss_records_fold2:
  train_losses:
  - 14.665615528821945
  - 15.296402215957642
  - 14.947020441293716
  - 15.490224450826645
  - 15.466765344142914
  - 15.492220938205719
  - 15.15231516957283
  - 14.767504826188087
  - 14.637717366218567
  - 15.254212960600853
  - 16.008067160844803
  - 15.245276778936386
  - 14.638962388038635
  - 14.721318706870079
  - 14.957725942134857
  - 14.770830899477005
  - 14.631742119789124
  - 14.508920714259148
  - 14.77647814154625
  - 14.59922944009304
  - 14.734033018350601
  - 14.507740885019302
  - 15.007817029953003
  - 14.452319249510765
  - 14.552063077688217
  - 14.93000403046608
  - 14.688535302877426
  - 14.395434364676476
  - 14.70803639292717
  - 14.943114250898361
  - 14.979536980390549
  - 14.74201250076294
  - 14.636272132396698
  - 14.583392709493637
  - 14.428359150886536
  - 14.78491148352623
  - 14.712787568569183
  - 14.645262688398361
  - 14.554685950279236
  - 14.63268557190895
  - 14.382861852645874
  - 14.165274396538734
  - 14.62034410238266
  - 14.645415395498276
  - 14.56313157081604
  - 14.805165737867355
  - 14.803220480680466
  - 14.152060806751251
  - 14.318979978561401
  - 14.525254040956497
  - 14.426419019699097
  - 14.764986217021942
  - 15.048605173826218
  - 15.045251429080963
  - 15.104388356208801
  - 14.847054213285446
  - 14.905872166156769
  - 14.757468193769455
  - 15.238893151283264
  - 14.508383348584175
  - 14.82745173573494
  - 14.698229879140854
  - 14.371294647455215
  - 14.590505748987198
  - 14.559644907712936
  - 14.344510287046432
  - 17.206653520464897
  - 15.623653620481491
  - 14.65648901462555
  - 14.704007118940353
  - 14.69157412648201
  - 14.812846094369888
  - 14.595334440469742
  - 14.793607413768768
  - 14.685670047998428
  - 14.772440403699875
  - 14.892577946186066
  - 14.528794556856155
  - 14.772236078977585
  - 14.47521698474884
  - 14.536107122898102
  - 14.51773439347744
  - 14.732896834611893
  - 14.524090826511383
  - 15.211641132831573
  - 14.650159567594528
  - 14.878119513392448
  - 17.68310011923313
  - 15.309400469064713
  - 14.575525939464569
  - 14.600011587142944
  - 14.784955769777298
  - 14.702621072530746
  - 14.86284425854683
  - 14.640312790870667
  - 14.53916147351265
  - 14.42704039812088
  - 14.632280081510544
  - 15.468930542469025
  - 14.34288015961647
  validation_losses:
  - 1.142168641090393
  - 1.832226037979126
  - 2.5414915084838867
  - 0.9077919125556946
  - 0.6732717752456665
  - 0.4085489809513092
  - 0.4202277958393097
  - 0.5527465343475342
  - 1.1940091848373413
  - 1.510969877243042
  - 0.4080071747303009
  - 0.42842844128608704
  - 0.4833680987358093
  - 0.6265799403190613
  - 1.0879745483398438
  - 1.3566213846206665
  - 1.8451474905014038
  - 1.2049753665924072
  - 2.607712507247925
  - 1.0622639656066895
  - 0.8011839389801025
  - 1.0747087001800537
  - 0.9317101836204529
  - 1.396750569343567
  - 1.49459969997406
  - 0.4250001907348633
  - 1.202882170677185
  - 1.0883350372314453
  - 0.5084086060523987
  - 1.6632658243179321
  - 1.4277877807617188
  - 1.3799145221710205
  - 1.398855209350586
  - 0.8204914927482605
  - 1.090261697769165
  - 1.7629116773605347
  - 1.4497137069702148
  - 1.367879867553711
  - 3.100769281387329
  - 1.2850468158721924
  - 1.9826250076293945
  - 1.176411747932434
  - 0.4674640893936157
  - 0.5720608830451965
  - 0.6267397999763489
  - 0.6962605118751526
  - 1.8302114009857178
  - 0.9001194834709167
  - 0.9571793079376221
  - 0.6173338890075684
  - 0.58296138048172
  - 1.4793791770935059
  - 3.017322301864624
  - 0.6778072714805603
  - 1.5857455730438232
  - 0.6772514581680298
  - 0.5882158279418945
  - 1.204416275024414
  - 0.3878743052482605
  - 4.14508581161499
  - 4.084330081939697
  - 3.2253711223602295
  - 3.566255569458008
  - 4.092119216918945
  - 3.740605115890503
  - 3.4367759227752686
  - 0.4131663143634796
  - 0.4348064064979553
  - 0.5034071803092957
  - 1.346543312072754
  - 1.2664629220962524
  - 3.078000783920288
  - 8.964158058166504
  - 4.759280204772949
  - 8.44045352935791
  - 1.105921745300293
  - 0.3955497443675995
  - 1.183342695236206
  - 1.5173708200454712
  - 2.840513229370117
  - 4.690555572509766
  - 1.7994691133499146
  - 0.853121280670166
  - 1.2800447940826416
  - 2.1091320514678955
  - 3.590212345123291
  - 2.7246198654174805
  - 0.4233299493789673
  - 0.5580737590789795
  - 1.3495041131973267
  - 1.2856508493423462
  - 1.2107326984405518
  - 1.0549746751785278
  - 1.7688632011413574
  - 1.9457430839538574
  - 1.2263545989990234
  - 2.248591184616089
  - 2.35683012008667
  - 0.4403180480003357
  - 0.47710439562797546
loss_records_fold3:
  train_losses:
  - 14.771755516529083
  - 15.062379330396652
  - 14.708152323961258
  - 14.892811000347137
  - 14.907343208789825
  - 14.485315352678299
  - 14.88287079334259
  - 14.982095316052437
  - 14.880138278007507
  - 14.655997171998024
  - 14.905582964420319
  - 14.680118381977081
  - 14.750649392604828
  - 14.968501344323158
  - 14.817408382892609
  - 14.999035209417343
  - 14.729306980967522
  - 14.940684229135513
  - 15.107911497354507
  - 14.683421343564987
  - 14.786282271146774
  - 14.529004961252213
  - 14.700785547494888
  - 15.004713296890259
  - 14.622952818870544
  - 14.85162615776062
  - 14.736935451626778
  - 14.640359222888947
  - 14.83360493183136
  - 14.749078378081322
  - 14.89968928694725
  - 14.782175213098526
  - 14.623494416475296
  - 14.799104660749435
  - 14.557289838790894
  - 14.73637393116951
  - 14.961948588490486
  - 14.99783855676651
  - 14.818094998598099
  - 14.654582858085632
  - 14.887637257575989
  - 15.028402924537659
  - 14.84676405787468
  - 14.810762494802475
  - 14.935239553451538
  - 14.667254760861397
  - 14.729423522949219
  - 15.113992035388947
  - 15.115360513329506
  - 15.430648028850555
  - 14.782046437263489
  - 15.099360018968582
  - 14.89809563755989
  - 14.87968447804451
  - 14.791625156998634
  - 14.529884815216064
  - 14.548316866159439
  - 14.501253351569176
  - 14.847546771168709
  - 14.649124890565872
  - 14.659742534160614
  - 14.840741083025932
  - 14.64438647031784
  - 14.725436821579933
  - 14.6665049046278
  - 14.454050451517105
  - 14.896823167800903
  - 14.65350079536438
  - 14.430209562182426
  - 14.59264862537384
  - 14.354422181844711
  - 14.588184654712677
  - 14.438913464546204
  - 14.772955566644669
  - 14.52904899418354
  - 14.624679923057556
  - 14.59155648946762
  - 14.67469534277916
  - 14.791482418775558
  - 14.787076845765114
  - 14.95724593102932
  - 14.504094868898392
  - 14.761631175875664
  - 14.840783432126045
  - 14.647837281227112
  - 14.414443343877792
  - 14.558883875608444
  - 14.897429138422012
  - 14.88005244731903
  - 14.39825987815857
  - 14.833568915724754
  - 14.24534758925438
  - 14.635036706924438
  - 15.045617058873177
  - 14.397549003362656
  - 14.583612486720085
  - 14.862004339694977
  - 15.063007831573486
  - 14.503605604171753
  - 14.771071836352348
  validation_losses:
  - 3.53584361076355
  - 1.7232943773269653
  - 4.783890247344971
  - 1.0940505266189575
  - 1.200599193572998
  - 3.3032495975494385
  - 0.907727837562561
  - 4.586055755615234
  - 5.783026218414307
  - 3.095477819442749
  - 2.301786184310913
  - 2.5759918689727783
  - 3.6736176013946533
  - 4.895258903503418
  - 3.382873296737671
  - 2.5427639484405518
  - 4.299067974090576
  - 0.7823020219802856
  - 1.9967981576919556
  - 3.9393372535705566
  - 0.7046120762825012
  - 1.664707064628601
  - 1.8014917373657227
  - 3.5537607669830322
  - 6.597904205322266
  - 4.7221269607543945
  - 3.94142484664917
  - 3.1516478061676025
  - 2.843947410583496
  - 3.5215518474578857
  - 3.937270164489746
  - 2.9703686237335205
  - 8.749427795410156
  - 2.006898880004883
  - 5.177213668823242
  - 4.441821098327637
  - 1.7835884094238281
  - 0.4317909777164459
  - 1.5442535877227783
  - 0.8771861791610718
  - 0.8173424005508423
  - 2.5545217990875244
  - 5.020764350891113
  - 1.2203099727630615
  - 2.4038941860198975
  - 2.32016658782959
  - 1.210516095161438
  - 0.9563433527946472
  - 2.2982702255249023
  - 1.6233632564544678
  - 2.338149070739746
  - 3.3419992923736572
  - 1.8156393766403198
  - 3.1278882026672363
  - 1.91148042678833
  - 4.066305160522461
  - 3.850550413131714
  - 2.909569263458252
  - 3.657283306121826
  - 5.177133083343506
  - 1.9109976291656494
  - 4.218801021575928
  - 4.8360748291015625
  - 3.7992703914642334
  - 7.028267860412598
  - 6.3000168800354
  - 3.05185604095459
  - 2.0667505264282227
  - 4.331914901733398
  - 1.5432041883468628
  - 2.255751371383667
  - 2.5911967754364014
  - 3.901198148727417
  - 1.5275025367736816
  - 1.0116021633148193
  - 1.1160808801651
  - 0.8497190475463867
  - 0.9331555366516113
  - 1.5582884550094604
  - 2.0104782581329346
  - 1.4979465007781982
  - 1.7633227109909058
  - 2.9459152221679688
  - 0.6773887872695923
  - 0.61585932970047
  - 0.6767798662185669
  - 0.7282095551490784
  - 0.6555639505386353
  - 0.6392427682876587
  - 0.767932653427124
  - 0.7798258066177368
  - 0.9420798420906067
  - 0.8994105458259583
  - 1.385815143585205
  - 1.194981336593628
  - 1.0517489910125732
  - 1.487237811088562
  - 1.202032208442688
  - 1.0730680227279663
  - 0.7669970989227295
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 10 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 57 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:10:32.632163'
