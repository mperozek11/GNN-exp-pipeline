config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:20:58.537285'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_40fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_40fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_40fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_40fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_40fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_40fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_40fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_40fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_40fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_40fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 60.06638978123665
  - 23.543356955051422
  - 17.54223721921444
  - 17.607672098279
  - 14.536318754404784
  - 14.427250611782075
  - 14.381811365485191
  - 11.805046707391739
  - 21.650808960199356
  - 17.448620107769965
  - 12.089477401971818
  - 12.126234555244446
  - 14.45989525616169
  - 11.89787372648716
  - 9.175801081955433
  - 8.532102805376054
  - 12.0857045263052
  - 7.411820560693741
  - 10.304754519462586
  - 9.04527514576912
  - 7.15343086719513
  - 8.293913519382476
  - 6.71290557384491
  - 7.43303747177124
  - 6.540073341131211
  - 6.539404964447022
  - 6.296550378203392
  - 6.221626272797585
  - 6.891380864381791
  - 6.313933549821377
  - 6.2773855894804
  - 5.946976718306542
  - 6.066722968220711
  - 6.015152549743653
  - 6.070211660861969
  - 5.893633648753166
  - 5.874454393982887
  - 6.080773881077767
  - 5.730857408046723
  - 6.1002519726753235
  - 5.889147874712944
  - 6.078545534610749
  - 5.751524463295937
  - 5.720174798369408
  - 5.957545599341393
  - 5.9442425012588505
  - 6.160603412985802
  - 6.1879686444997795
  - 5.880037391185761
  - 6.131477022171021
  - 6.5925203114748
  - 6.167855933308601
  - 6.032867899537087
  - 6.478386697173119
  - 7.516128331422806
  - 8.457727840542793
  - 6.443685352802277
  - 6.615475910902024
  - 6.156996965408325
  - 6.027420979738236
  - 6.26292936205864
  - 5.846128636598587
  - 5.951361387968063
  - 5.89597040116787
  - 6.025105288624764
  - 5.99454557299614
  - 5.89958847463131
  - 6.164404559135438
  - 5.932284101843834
  - 6.094620653986931
  - 5.904444414377213
  - 6.016208153963089
  - 6.178446346521378
  - 5.936402049660683
  - 6.177342592179776
  - 9.47420419305563
  - 10.527312099188567
  - 8.544285079836845
  - 7.730528663098813
  - 6.584518700838089
  - 6.587708887457848
  - 6.483074954152108
  - 6.046269044280052
  - 6.0523718982934955
  - 6.7152110755443575
  - 5.8742727637290955
  - 6.064264345169068
  - 5.995027190446854
  - 6.219434869289398
  - 6.068407282233238
  - 6.036679649353028
  - 6.194114872813225
  - 6.145569434762002
  - 5.940191984176636
  - 6.195088037848473
  - 6.111516952514648
  - 6.069089645147324
  - 5.887057609856129
  - 6.058086851239205
  - 6.189559832215309
  validation_losses:
  - 4.393001079559326
  - 0.6879542469978333
  - 1.404738187789917
  - 0.8047553896903992
  - 0.4443148672580719
  - 0.6842074990272522
  - 0.391862690448761
  - 0.5793293118476868
  - 0.8129628300666809
  - 0.5620805621147156
  - 0.5911434888839722
  - 0.7635325789451599
  - 0.9985823035240173
  - 0.3942318558692932
  - 0.4558977782726288
  - 0.38400933146476746
  - 0.4048060476779938
  - 0.5826548933982849
  - 0.3855210840702057
  - 0.38000959157943726
  - 0.568899393081665
  - 0.3948802053928375
  - 0.4137096107006073
  - 0.39759212732315063
  - 0.48232463002204895
  - 0.38884249329566956
  - 0.4123421609401703
  - 0.3918023407459259
  - 0.41567403078079224
  - 0.38890644907951355
  - 0.3898618519306183
  - 0.4267560541629791
  - 0.38497281074523926
  - 0.392447292804718
  - 0.3934394419193268
  - 0.4029855728149414
  - 0.37628114223480225
  - 0.6818233728408813
  - 0.3890939950942993
  - 0.4167022109031677
  - 0.39115700125694275
  - 0.3910805881023407
  - 0.3775673508644104
  - 0.4762248694896698
  - 0.4882603883743286
  - 0.6644778251647949
  - 0.6389008164405823
  - 0.4011983871459961
  - 0.4062301218509674
  - 0.706415593624115
  - 0.5850257873535156
  - 2.9230494499206543
  - 0.4434816539287567
  - 1.7519104480743408
  - 0.41148191690444946
  - 0.4002743661403656
  - 0.44361060857772827
  - 0.4054705798625946
  - 0.44125476479530334
  - 0.3920304477214813
  - 0.4003124237060547
  - 0.47104912996292114
  - 0.38984495401382446
  - 0.3938814103603363
  - 0.44322967529296875
  - 0.3930157423019409
  - 0.3913511633872986
  - 0.42428722977638245
  - 0.4203532040119171
  - 0.3926298916339874
  - 0.39924949407577515
  - 0.4377326965332031
  - 0.3924039900302887
  - 0.3988480567932129
  - 0.41313695907592773
  - 5.604752063751221
  - 0.3900010585784912
  - 0.3992503583431244
  - 0.4107819199562073
  - 0.44704777002334595
  - 0.43357783555984497
  - 0.39082634449005127
  - 0.39205819368362427
  - 0.4170014560222626
  - 0.38885045051574707
  - 0.42639991641044617
  - 0.39942967891693115
  - 0.4096502363681793
  - 0.5155777931213379
  - 0.409552663564682
  - 0.39908459782600403
  - 0.39210107922554016
  - 0.39072370529174805
  - 0.390800416469574
  - 0.41380545496940613
  - 0.46025580167770386
  - 0.39172571897506714
  - 0.41940778493881226
  - 0.39147713780403137
  - 0.3923843801021576
loss_records_fold1:
  train_losses:
  - 5.851304158568382
  - 5.771116474270821
  - 5.9567102789878845
  - 6.076165050268173
  - 5.913802875578404
  - 6.066647139191628
  - 5.988327538967133
  - 6.085205027461052
  - 5.831830161809922
  - 5.877858665585518
  - 5.992010456323624
  - 5.972806838154793
  - 6.007380792498589
  - 5.949070936441422
  - 5.758959782123566
  - 5.865623739361763
  - 5.805567589402199
  - 5.77953332066536
  - 5.882998842000962
  - 6.111300665140153
  - 6.036283877491951
  - 5.749120116233826
  - 5.915333262085915
  - 6.0412301734089855
  - 6.058266285061837
  - 6.0903101652860645
  - 5.920748934149742
  - 5.872925680875778
  - 5.827853259444237
  - 6.0154340177774435
  - 5.842137905955315
  - 5.781789050996304
  - 5.862862411141396
  - 5.845826211571694
  - 5.810209396481515
  - 5.95718754529953
  - 5.83765698671341
  - 5.8716883033514025
  - 5.921675217151642
  - 6.002175495028496
  - 5.948030135035515
  - 5.909273120760918
  - 6.179428833723069
  - 5.975872904062271
  - 5.8341162502765656
  - 5.952373351156712
  - 5.871136119961739
  - 5.811828088760376
  - 5.867615300416947
  - 5.897411343455315
  - 5.879351842403413
  - 5.881195387244225
  - 6.017881581187249
  - 5.945143264532089
  - 5.884697991609574
  - 5.816312092542649
  - 5.98434161245823
  - 5.943806543946266
  - 5.840169131755829
  - 5.839284774661064
  - 5.797380313277245
  - 5.948250582814217
  - 5.883688995242119
  - 6.116335862874985
  - 5.780417621135712
  - 5.84003955423832
  - 5.89134407043457
  - 5.907643750309944
  - 5.916404303908348
  - 6.074655249714851
  - 5.972975587844849
  - 6.212326860427857
  - 5.8115145653486255
  - 5.970626461505891
  - 5.964883634448052
  - 6.024865576624871
  - 5.959719461202622
  - 6.039850887656212
  - 5.990377077460289
  - 5.91991401463747
  - 5.833266007900239
  - 5.945080077648163
  - 5.8160751342773445
  - 6.026427191495896
  - 5.842407250404358
  - 6.055975455045701
  - 5.8247243702411655
  - 5.822678577899933
  validation_losses:
  - 0.40456199645996094
  - 0.40747371315956116
  - 0.4087265431880951
  - 24.448942184448242
  - 0.42501187324523926
  - 0.4220259189605713
  - 0.44353818893432617
  - 0.43614423274993896
  - 0.4759410619735718
  - 0.46424680948257446
  - 128648.6796875
  - 0.4557467997074127
  - 0.41479137539863586
  - 0.43728142976760864
  - 0.426654189825058
  - 0.42352989315986633
  - 0.4071623682975769
  - 0.41521257162094116
  - 0.4840226173400879
  - 0.40711915493011475
  - 0.40650033950805664
  - 0.40932852029800415
  - 0.4411698281764984
  - 0.46071478724479675
  - 0.41707882285118103
  - 0.4385533034801483
  - 0.42413562536239624
  - 0.40823128819465637
  - 0.40999671816825867
  - 0.4331955015659332
  - 0.41680800914764404
  - 0.41233065724372864
  - 0.4366106688976288
  - 0.404114305973053
  - 0.437188982963562
  - 0.4271366000175476
  - 0.4172450304031372
  - 0.42075541615486145
  - 0.4128393530845642
  - 0.46396273374557495
  - 0.41139936447143555
  - 0.5013726353645325
  - 0.4557437598705292
  - 0.47782188653945923
  - 0.44130733609199524
  - 0.4112820029258728
  - 0.42015692591667175
  - 0.40481457114219666
  - 0.42287230491638184
  - 0.40412795543670654
  - 0.40775740146636963
  - 0.42866572737693787
  - 0.41616272926330566
  - 0.45708832144737244
  - 0.40606293082237244
  - 0.4047136604785919
  - 0.43449023365974426
  - 0.40381237864494324
  - 0.4397081732749939
  - 0.4288387894630432
  - 0.43639546632766724
  - 0.40603774785995483
  - 0.4586072266101837
  - 0.4240454137325287
  - 0.4116552472114563
  - 0.41038182377815247
  - 0.44666773080825806
  - 0.41839879751205444
  - 0.4066900610923767
  - 0.4122768044471741
  - 0.4029708504676819
  - 0.4159674346446991
  - 0.40491440892219543
  - 0.4057603180408478
  - 0.41067245602607727
  - 0.43598708510398865
  - 0.4281176030635834
  - 0.5516186952590942
  - 0.40625670552253723
  - 0.406461238861084
  - 0.4235861003398895
  - 0.4806819260120392
  - 0.4334452152252197
  - 0.40768876671791077
  - 0.4148527681827545
  - 0.4056757688522339
  - 0.4056229889392853
  - 0.40584319829940796
loss_records_fold2:
  train_losses:
  - 6.018218591809273
  - 6.058911409974098
  - 6.118970715999604
  - 6.223199969530106
  - 6.615831685066223
  - 6.421322250366211
  - 9.155419555306436
  - 7.351948714256287
  - 6.262867167592049
  - 6.041473695635796
  - 6.027203926444054
  - 6.042749413847924
  - 6.082330709695817
  - 5.979693111777306
  - 5.918447953462601
  - 6.024682700634003
  - 5.9296006798744205
  - 5.993825164437294
  - 5.9055570602417
  - 5.921234089136124
  - 5.968898490071297
  - 5.867328682541848
  - 5.908728939294815
  - 6.063049417734146
  - 6.025708693265916
  - 6.013041624426842
  - 6.03745645582676
  - 5.99272885620594
  - 5.9289774179458625
  - 5.993611758947373
  - 5.928962415456772
  - 6.018643766641617
  - 5.9066945105791095
  - 6.001439547538758
  - 6.041473689675332
  - 6.02534228861332
  - 6.080056765675545
  - 6.0026164025068285
  - 6.03627468943596
  - 6.020298734307289
  - 5.990871569514275
  - 5.913632249832154
  - 6.0780781105160715
  - 5.9238588333129885
  - 6.059697076678276
  - 6.093241944909096
  validation_losses:
  - 2507125.75
  - 608020594688.0
  - 0.39483755826950073
  - 0.40012311935424805
  - 0.3867514729499817
  - 0.38292157649993896
  - 0.3962094783782959
  - 17.429990768432617
  - 0.3900488018989563
  - 0.38662078976631165
  - 0.38495007157325745
  - 0.42490696907043457
  - 0.40828290581703186
  - 0.3867419958114624
  - 0.4086713194847107
  - 8551202304.0
  - 0.40686097741127014
  - 0.384958952665329
  - 0.38686108589172363
  - 0.40890952944755554
  - 0.39921897649765015
  - 0.4295748770236969
  - 0.3851228356361389
  - 0.3840160369873047
  - 0.3975709080696106
  - 0.3865605294704437
  - 0.38823917508125305
  - 0.3838280141353607
  - 0.40458908677101135
  - 0.3849429786205292
  - 0.39377304911613464
  - 0.4130248725414276
  - 0.39551955461502075
  - 0.3864227831363678
  - 0.38941362500190735
  - 0.38618385791778564
  - 0.4139586091041565
  - 0.39813169836997986
  - 0.3874780833721161
  - 0.3995005488395691
  - 0.38449326157569885
  - 0.39170363545417786
  - 0.38869425654411316
  - 0.39259421825408936
  - 0.3838491141796112
  - 0.3840312361717224
loss_records_fold3:
  train_losses:
  - 5.909948247671128
  - 5.943076869845391
  - 5.93286199271679
  - 5.958364380896092
  - 5.805425277352334
  - 5.838416314125062
  - 5.95489885956049
  - 5.85132449567318
  - 5.91836471259594
  - 6.011366683244706
  - 5.854971523582936
  validation_losses:
  - 0.41533175110816956
  - 0.4006017744541168
  - 0.4224051833152771
  - 0.46222805976867676
  - 0.40235018730163574
  - 0.40903910994529724
  - 0.41623324155807495
  - 0.39918625354766846
  - 0.4016547203063965
  - 0.4013794958591461
  - 0.4090414047241211
loss_records_fold4:
  train_losses:
  - 6.228534549474716
  - 6.015352270007134
  - 5.898390257358551
  - 5.8803513109684
  - 5.903404593467712
  - 6.245320548117161
  - 6.001805144548417
  - 5.836618682742119
  - 6.157165488600731
  - 6.081918567419052
  - 6.1077058732509615
  - 5.908873084187508
  - 6.318814173340797
  - 6.158194914460182
  - 6.033837920427323
  - 5.964538416266442
  - 5.856030294299126
  - 6.032083135843277
  validation_losses:
  - 0.46191051602363586
  - 0.4260745346546173
  - 0.393545925617218
  - 0.3913411796092987
  - 0.39093947410583496
  - 0.3968120813369751
  - 0.39461979269981384
  - 0.3911520540714264
  - 0.4422663152217865
  - 0.43151336908340454
  - 0.39341598749160767
  - 0.42875078320503235
  - 0.3918910622596741
  - 0.40182244777679443
  - 0.39834100008010864
  - 0.3940769135951996
  - 0.3921676576137543
  - 0.3945169448852539
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 88 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 46 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:28:28.367949'
