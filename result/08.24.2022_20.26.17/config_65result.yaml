config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:55:37.667862'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_65fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_65fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_65fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_65fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_65fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_65fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_65fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_65fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_65fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_65fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.198619878292084
  - 2.968376272916794
  - 2.9172178298234943
  - 2.9679982483386995
  - 3.0281757533550264
  - 3.0186483502388004
  - 2.948602938652039
  - 2.848181387782097
  - 2.9607960790395738
  - 2.965275526046753
  - 2.9561612367630006
  - 2.883268561959267
  - 2.9290278673172
  - 2.9120137959718706
  - 2.867992639541626
  - 3.025916558504105
  - 2.912310385704041
  - 2.970078954100609
  - 2.869217583537102
  - 2.818804493546486
  - 2.961214151978493
  validation_losses:
  - 0.40350982546806335
  - 0.4436012804508209
  - 0.3916119933128357
  - 0.39175885915756226
  - 0.4027273654937744
  - 0.3937996029853821
  - 0.42158615589141846
  - 0.3884388208389282
  - 0.3885693848133087
  - 0.4750348627567291
  - 0.38861677050590515
  - 0.3984968066215515
  - 0.4018493592739105
  - 0.3860587179660797
  - 0.39997395873069763
  - 0.39393195509910583
  - 0.39011865854263306
  - 0.39732375741004944
  - 0.38654929399490356
  - 0.38492482900619507
  - 0.3900570869445801
loss_records_fold1:
  train_losses:
  - 2.9492012560367584
  - 2.87260023355484
  - 2.8182668685913086
  - 2.826980745792389
  - 2.807841345667839
  - 2.9256813853979113
  - 2.9192760556936266
  - 2.8146015226840975
  - 2.879556342959404
  - 2.872230565547943
  - 2.7958878457546237
  - 2.8308539330959324
  - 2.8935092985630035
  validation_losses:
  - 0.4189299941062927
  - 0.42037898302078247
  - 0.3960409164428711
  - 0.3979335427284241
  - 0.3904830813407898
  - 0.38654661178588867
  - 0.40850913524627686
  - 0.3937966525554657
  - 0.3915233612060547
  - 0.39462319016456604
  - 0.3966866731643677
  - 0.4040343761444092
  - 0.4020117521286011
loss_records_fold2:
  train_losses:
  - 2.8724334418773654
  - 2.8640672326087953
  - 2.862598213553429
  - 2.8392899453639986
  - 2.8624448239803315
  - 2.8312613487243654
  - 2.8541038751602175
  - 2.836816954612732
  - 2.8121626615524296
  - 2.799348682165146
  - 2.823781329393387
  - 2.810084667801857
  - 2.8166403979063035
  - 2.8226416587829593
  - 2.7929907143115997
  - 2.8349832564592363
  - 2.8569973677396776
  - 2.811948719620705
  - 2.8253784000873567
  - 2.806290477514267
  - 2.8237760931253435
  - 2.8217896401882174
  - 2.7987140983343126
  - 2.826389873027802
  - 2.825790196657181
  - 2.824674314260483
  - 2.8174855887889865
  - 2.8039801836013796
  - 2.796657291054726
  - 2.8144512295722963
  - 2.784927159547806
  - 2.7923375338315966
  - 2.8774518162012104
  - 2.823846533894539
  - 2.811814448237419
  - 2.821676898002625
  - 2.803764668107033
  - 2.793277880549431
  - 2.81178013086319
  - 2.7862467586994173
  - 2.812104889750481
  - 2.780448725819588
  - 2.8247572392225266
  - 2.790950185060501
  - 2.7693059742450714
  - 2.7864832431077957
  - 2.756544917821884
  - 2.735779243707657
  - 2.7805694282054905
  - 2.775912007689476
  - 2.886417305469513
  - 2.777164098620415
  - 2.746998199820519
  - 2.7321712523698807
  - 2.7752012044191363
  - 2.765261393785477
  - 2.7401294827461244
  - 2.839429694414139
  - 2.800107190012932
  - 2.778400975465775
  - 2.7756929337978367
  - 2.7844763100147247
  - 2.7431904494762422
  - 2.7386691570281982
  - 2.773853299021721
  - 2.788260278105736
  - 2.7677137076854708
  - 2.754392215609551
  - 2.760891565680504
  - 2.7534520566463474
  - 2.7814795017242435
  - 2.756551033258438
  - 2.832066622376442
  - 2.7416179120540622
  - 2.816091272234917
  - 2.752764004468918
  - 2.743358713388443
  - 2.768571212887764
  - 2.757806006073952
  - 2.757220357656479
  - 2.717967334389687
  - 2.7588211327791217
  - 2.7495736062526706
  - 2.744993650913239
  - 2.7228862285614017
  - 2.7474081397056582
  - 2.77961095571518
  - 2.8387571781873704
  - 2.7539951503276825
  - 2.7330641925334933
  - 2.7440449029207232
  - 2.7649433791637423
  - 2.764398342370987
  - 2.749965685606003
  - 2.7428207218647005
  - 2.753872418403626
  - 2.7686077058315277
  - 2.7636522591114048
  - 2.755201146006584
  - 2.79019615650177
  validation_losses:
  - 0.38376888632774353
  - 0.38337185978889465
  - 0.3830968141555786
  - 0.38861000537872314
  - 0.41726207733154297
  - 0.3987073302268982
  - 0.38136377930641174
  - 0.3903670907020569
  - 0.4140802323818207
  - 0.3855307102203369
  - 0.4078279733657837
  - 0.4046785533428192
  - 0.38622915744781494
  - 0.43879473209381104
  - 0.3980588912963867
  - 0.39584654569625854
  - 0.40757548809051514
  - 0.3861328959465027
  - 0.3838876187801361
  - 0.3858972489833832
  - 0.3978930115699768
  - 0.392476886510849
  - 0.38623926043510437
  - 0.41329729557037354
  - 0.3814568817615509
  - 0.44954413175582886
  - 0.4039839506149292
  - 0.38764581084251404
  - 0.38918712735176086
  - 0.39352545142173767
  - 0.3827962875366211
  - 0.40113210678100586
  - 0.3826352655887604
  - 0.3973452150821686
  - 0.39703071117401123
  - 0.38706937432289124
  - 0.3793661296367645
  - 0.39647436141967773
  - 0.4166889190673828
  - 0.3852156400680542
  - 0.4279489517211914
  - 0.4611462652683258
  - 0.4039941430091858
  - 0.3853592872619629
  - 0.43014419078826904
  - 0.42100822925567627
  - 0.41147494316101074
  - 0.723064661026001
  - 0.40857547521591187
  - 0.3860812485218048
  - 0.4115309715270996
  - 0.4253452718257904
  - 0.43130871653556824
  - 0.4350273013114929
  - 0.43025335669517517
  - 0.4194723963737488
  - 0.537670910358429
  - 0.4068446159362793
  - 0.3893424868583679
  - 0.4087653160095215
  - 0.40043988823890686
  - 0.3987112045288086
  - 0.4153801500797272
  - 0.5405983924865723
  - 0.37918078899383545
  - 0.3976016044616699
  - 0.5225009918212891
  - 0.4978710114955902
  - 0.4125998020172119
  - 0.5815060138702393
  - 0.5655680298805237
  - 4.428786277770996
  - 0.4076618552207947
  - 0.48308080434799194
  - 0.4002925455570221
  - 0.4769410789012909
  - 0.4302692115306854
  - 0.44188323616981506
  - 0.37472882866859436
  - 0.4218637943267822
  - 0.38323256373405457
  - 0.3848867118358612
  - 0.38435083627700806
  - 0.42799606919288635
  - 0.4168759882450104
  - 0.39091575145721436
  - 0.4054732322692871
  - 0.37697628140449524
  - 0.8040401339530945
  - 0.5938941836357117
  - 0.6441593766212463
  - 0.38677874207496643
  - 0.4003392457962036
  - 0.3843532204627991
  - 0.37861713767051697
  - 0.4460669159889221
  - 0.437943696975708
  - 0.6478891372680664
  - 0.5748839378356934
  - 0.4283269941806793
loss_records_fold3:
  train_losses:
  - 2.777834212779999
  - 2.775304594635964
  - 2.769972211122513
  - 2.7742110162973406
  - 2.732322955131531
  - 2.7907232224941256
  - 2.7674023866653443
  - 2.7702161729335786
  - 2.7569118022918704
  - 2.759030681848526
  - 2.7843846797943117
  - 2.7990390717983247
  - 2.7583889901638035
  - 2.761598080396652
  - 2.7698011279106143
  - 2.8046889245510105
  - 2.760399577021599
  - 2.7801914095878604
  - 2.758045694231987
  - 2.8416755557060243
  - 2.7713887929916385
  - 2.752749797701836
  - 2.7850253194570542
  - 2.7579973101615907
  - 2.76735417842865
  - 2.778054052591324
  - 2.772935682535172
  - 2.7591144144535065
  - 2.7417670488357544
  - 2.765359655022621
  - 2.7216061770915987
  - 2.7479269772768022
  - 2.7140553623437884
  - 2.803493916988373
  - 2.713328367471695
  - 2.7693752616643907
  - 2.8105286538600924
  - 2.7748194605112078
  - 2.764988508820534
  - 2.7312356472015384
  - 2.7813024759292606
  - 2.747025638818741
  - 2.7537904620170597
  - 2.747413009405136
  - 2.67817862033844
  - 2.775281608104706
  - 2.7897789120674137
  - 2.7272177636623383
  - 2.7347527146339417
  - 2.7766768485307693
  - 2.823990470170975
  - 2.7833377659320835
  - 2.7062629580497743
  - 2.735666072368622
  - 2.7485721647739414
  - 2.7890540152788166
  - 2.7675099998712542
  - 2.7489126652479174
  - 2.836111986637116
  - 2.821335527300835
  - 2.7343088269233706
  - 2.724246323108673
  - 2.762902238965035
  - 2.773978328704834
  - 2.709956252574921
  - 2.6930459275841714
  - 2.7361217081546787
  - 2.7095866769552233
  - 2.7760013818740847
  - 2.7300579965114595
  - 2.758630129694939
  - 2.731864193081856
  - 2.680052933096886
  - 2.7454300224781036
  - 2.7309959828853607
  - 2.726854541897774
  - 2.7278369247913363
  - 2.757318004965782
  - 2.7279427319765093
  - 2.751522728800774
  - 2.7952054977416996
  - 2.864052498340607
  - 2.7957128256559374
  - 2.7796757936477663
  - 2.7538196593523026
  - 2.7433295190334324
  - 2.766745001077652
  - 2.7656946629285812
  - 2.767721372842789
  - 2.7731394648551944
  - 2.754282328486443
  - 2.78431892991066
  - 2.735588324069977
  - 2.752966517210007
  - 2.7227612763643267
  - 2.7136184453964236
  - 2.745479100942612
  - 2.6988543421030045
  - 2.7614920973777775
  - 2.723277279734612
  validation_losses:
  - 0.554888904094696
  - 0.4537006616592407
  - 0.4154323637485504
  - 0.47044774889945984
  - 0.4817203879356384
  - 0.718498945236206
  - 0.6597168445587158
  - 0.687639057636261
  - 0.7750024795532227
  - 0.5213183760643005
  - 0.6309371590614319
  - 0.44269034266471863
  - 0.4182210862636566
  - 0.4653071165084839
  - 0.5060611963272095
  - 0.3881906569004059
  - 0.6413170695304871
  - 1.1295428276062012
  - 0.6399226188659668
  - 0.40752097964286804
  - 0.4279126226902008
  - 1.1269932985305786
  - 0.3849368989467621
  - 0.49944615364074707
  - 0.633821964263916
  - 0.6571879386901855
  - 0.5399211049079895
  - 0.7720662951469421
  - 0.3684726357460022
  - 1.166832685470581
  - 0.6681180596351624
  - 0.8658502101898193
  - 0.6153731942176819
  - 0.7726665735244751
  - 1.5938267707824707
  - 0.771612286567688
  - 0.7561072111129761
  - 0.643230676651001
  - 1.4904201030731201
  - 1.292410969734192
  - 2.501084566116333
  - 1.3151370286941528
  - 1.9689772129058838
  - 2.050931453704834
  - 0.5454559326171875
  - 2.3624653816223145
  - 1.1383163928985596
  - 2.356492757797241
  - 0.9924423694610596
  - 0.8424182534217834
  - 1.9275710582733154
  - 4.271170139312744
  - 2.0145485401153564
  - 5.256460666656494
  - 2.857971668243408
  - 1.706754207611084
  - 2.005286693572998
  - 0.8448478579521179
  - 0.44880932569503784
  - 0.5741260051727295
  - 3.8532724380493164
  - 6.623737812042236
  - 0.614399254322052
  - 0.5133492350578308
  - 1.670669436454773
  - 0.39482900500297546
  - 0.8854271173477173
  - 0.587242603302002
  - 0.39610403776168823
  - 0.5672846436500549
  - 0.48491889238357544
  - 0.5178567171096802
  - 0.8363034129142761
  - 0.6392795443534851
  - 0.5377049446105957
  - 0.7260915040969849
  - 0.41327419877052307
  - 1.3399279117584229
  - 3.485992670059204
  - 1.2910457849502563
  - 0.5636341571807861
  - 0.37754637002944946
  - 2.9547841548919678
  - 0.41464659571647644
  - 2.317913770675659
  - 0.7471964955329895
  - 0.48792406916618347
  - 2.2347970008850098
  - 1.618916392326355
  - 1.0752897262573242
  - 2.7901012897491455
  - 1.1356544494628906
  - 3.061298370361328
  - 1.73878014087677
  - 2.4844353199005127
  - 0.39103734493255615
  - 0.9797036647796631
  - 2.048370838165283
  - 6.316102504730225
  - 0.8316248655319214
loss_records_fold4:
  train_losses:
  - 2.765933352708817
  - 2.7686165392398836
  - 2.7581940561532976
  - 2.769292324781418
  - 2.723884180188179
  - 2.7959562301635743
  - 2.7329288840293886
  - 2.7383513361215592
  - 2.764414867758751
  - 2.746473750472069
  - 2.7227011531591416
  - 2.738887330889702
  - 2.753247484564781
  - 2.7441376179456713
  - 2.704461392760277
  - 2.7155919283628465
  - 2.76588032245636
  - 2.747075915336609
  - 2.704708078503609
  - 2.7155673593282703
  - 2.715172877907753
  - 2.7821510583162308
  - 2.705189192295075
  - 2.737059229612351
  - 2.7171651363372806
  - 2.691553530097008
  - 2.676463979482651
  - 2.7347524493932727
  - 2.717525637149811
  - 2.7257508456707003
  - 2.7128946632146835
  - 2.708984485268593
  - 2.7544038474559787
  - 2.743311542272568
  - 2.6936680734157563
  - 2.7201005071401596
  - 2.754623067378998
  - 2.7039410293102266
  - 2.7448431670665743
  - 2.737487503886223
  - 2.7733815371990205
  - 2.8189856946468357
  - 2.7333188325166704
  - 2.737151324748993
  - 2.7617823958396914
  - 2.740197664499283
  validation_losses:
  - 0.3723948895931244
  - 0.3638041019439697
  - 0.3628568947315216
  - 0.3646797239780426
  - 0.6710121035575867
  - 0.38847169280052185
  - 0.3618980050086975
  - 0.5647664070129395
  - 0.39238956570625305
  - 0.40107569098472595
  - 0.3690529465675354
  - 0.47575438022613525
  - 0.3731253147125244
  - 0.8262788653373718
  - 0.6397481560707092
  - 0.6687201261520386
  - 0.36126330494880676
  - 0.44448885321617126
  - 0.5378465056419373
  - 0.47577381134033203
  - 0.6585092544555664
  - 1.4089372158050537
  - 0.759010910987854
  - 1.5073223114013672
  - 0.371486634016037
  - 0.7074098587036133
  - 0.8866977095603943
  - 0.7077642679214478
  - 0.5256658792495728
  - 0.8162800073623657
  - 0.7695446610450745
  - 1.0000978708267212
  - 0.6159411072731018
  - 0.6927751302719116
  - 0.7025516629219055
  - 0.36380866169929504
  - 0.7014997005462646
  - 0.7472205758094788
  - 0.7601372003555298
  - 1.0993674993515015
  - 1.097329020500183
  - 0.956034779548645
  - 0.7212346196174622
  - 0.6326412558555603
  - 0.3994048833847046
  - 0.3850369453430176
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 46 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8456260720411664, 0.8439108061749572,
    0.8608247422680413]'
  fold_eval_f1: '[0.0, 0.0, 0.0425531914893617, 0.09900990099009901, 0.06896551724137931]'
  mean_eval_accuracy: 0.8531254973386855
  mean_f1_accuracy: 0.042105721944168006
  total_train_time: '0:25:54.196668'
