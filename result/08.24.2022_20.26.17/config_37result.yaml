config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:14:35.576908'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_37fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_37fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_37fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_37fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_37fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_37fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_37fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_37fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_37fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_37fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.585006403923035
  - 3.239832764863968
  - 3.216606801748276
  - 3.211678546667099
  - 3.12394238114357
  - 3.218887382745743
  - 3.15899720788002
  - 3.068983942270279
  - 2.996871069073677
  - 2.9468488693237305
  - 3.089957642555237
  - 3.1108853816986084
  - 3.1297059237957003
  - 3.0260128617286686
  - 2.9771201938390734
  - 3.09971981048584
  - 3.1795890212059024
  - 3.040531569719315
  - 3.0166470378637316
  - 3.036762768030167
  - 2.9919970601797106
  - 3.0077223777770996
  - 3.078101110458374
  - 3.042203366756439
  - 3.0201267451047897
  - 2.9147868752479553
  - 2.9470324665308
  - 3.050003641843796
  - 3.046765446662903
  - 2.9899074494838715
  - 2.9227982938289645
  - 2.9142071425914766
  - 2.9285609841346742
  - 3.037415063381195
  - 2.9952730000019074
  - 3.013905644416809
  - 2.969360274076462
  - 3.02808935046196
  - 2.940751919150353
  - 2.9420120418071747
  - 2.979479902982712
  - 3.0147100865840915
  - 2.9759441733360292
  - 2.9418274551630024
  - 2.9691407263278964
  - 3.018879407644272
  - 3.02517751455307
  - 2.963339424133301
  - 2.9766428589820864
  - 3.0045832812786104
  - 2.9864508628845217
  - 2.9811513364315037
  validation_losses:
  - 0.4680618643760681
  - 0.41566041111946106
  - 0.39220330119132996
  - 0.41671690344810486
  - 0.41016247868537903
  - 0.3918638229370117
  - 0.39159104228019714
  - 0.3926156759262085
  - 0.40230992436408997
  - 0.3876875340938568
  - 0.4351103901863098
  - 0.4170885384082794
  - 0.3970925509929657
  - 0.4026558995246887
  - 0.3844704031944275
  - 0.4178827702999115
  - 0.388858437538147
  - 0.38935020565986633
  - 0.39624035358428955
  - 0.39530593156814575
  - 0.40324488282203674
  - 0.6107061505317688
  - 0.3889876902103424
  - 0.3865499794483185
  - 0.38601404428482056
  - 0.3901793658733368
  - 0.4204306900501251
  - 0.41728734970092773
  - 0.45758235454559326
  - 0.5352578163146973
  - 0.4038885533809662
  - 0.3857673108577728
  - 0.38607120513916016
  - 0.4262843132019043
  - 0.4268561899662018
  - 0.4049789011478424
  - 0.3902919292449951
  - 0.409920334815979
  - 0.3989902138710022
  - 0.3927478492259979
  - 0.3875797390937805
  - 0.3906317353248596
  - 1.906206727027893
  - 1.8041484355926514
  - 0.4030355215072632
  - 0.4221334457397461
  - 0.3955865502357483
  - 0.4042617082595825
  - 0.3982385993003845
  - 0.38864800333976746
  - 0.398515909910202
  - 0.38620638847351074
loss_records_fold1:
  train_losses:
  - 2.9760379135608677
  - 2.9730643153190615
  - 2.9869039297103885
  - 2.952928024530411
  - 2.9872764497995377
  - 3.006176596879959
  - 2.9722539693117143
  - 2.9715750575065614
  - 3.0096796035766604
  - 2.9959323018789292
  - 3.008716005086899
  - 2.956733274459839
  - 3.006009191274643
  - 2.9354631781578067
  - 2.8977533996105196
  - 2.9647142559289934
  - 3.0016125321388247
  - 2.8984583914279938
  - 3.003411537408829
  - 3.035770827531815
  - 2.929893428087235
  - 3.0397554993629456
  - 2.9767863243818287
  - 2.9317165434360506
  - 3.167519834637642
  - 3.022851026058197
  - 2.9690747410058975
  - 2.94183686375618
  - 2.98147566318512
  - 3.033032548427582
  validation_losses:
  - 0.39947372674942017
  - 0.39875563979148865
  - 0.3988366723060608
  - 0.4094436764717102
  - 0.409341037273407
  - 0.41166192293167114
  - 0.40733659267425537
  - 0.45396605134010315
  - 0.4703182876110077
  - 0.4008931815624237
  - 0.38896480202674866
  - 0.40841570496559143
  - 0.4296603202819824
  - 0.42737868428230286
  - 0.3968532085418701
  - 0.39452192187309265
  - 0.4034713804721832
  - 0.40459179878234863
  - 0.5377382636070251
  - 0.5910303592681885
  - 0.40742239356040955
  - 0.498514324426651
  - 0.6754539012908936
  - 0.7212463021278381
  - 0.41954636573791504
  - 0.4128585159778595
  - 0.42150887846946716
  - 0.40732136368751526
  - 0.4131464660167694
  - 0.4220789074897766
loss_records_fold2:
  train_losses:
  - 2.9812758147716525
  - 2.956296730041504
  - 2.9916009247303013
  - 3.0154486894607544
  - 3.0502672612667086
  - 3.0028157591819764
  - 2.9819927632808687
  - 2.995246994495392
  - 3.006249511241913
  - 3.010091665387154
  - 2.9728165149688723
  - 2.9611267268657686
  - 3.014359468221665
  - 2.9950888454914093
  - 2.959961807727814
  validation_losses:
  - 0.38701730966567993
  - 0.38303878903388977
  - 0.3870122730731964
  - 0.405189573764801
  - 0.3866426944732666
  - 0.3883154094219208
  - 0.3928873836994171
  - 0.3817852735519409
  - 0.432330459356308
  - 0.39427971839904785
  - 0.394878089427948
  - 0.3949752748012543
  - 0.4013581871986389
  - 0.39462000131607056
  - 0.3870212137699127
loss_records_fold3:
  train_losses:
  - 2.9814275801181793
  - 2.9930009067058565
  - 2.979609173536301
  - 2.95421399474144
  - 2.946309816837311
  - 3.0569369047880173
  - 3.0173382222652436
  - 2.955913466215134
  - 3.0483042269945146
  - 3.033628910779953
  - 2.981225031614304
  validation_losses:
  - 0.38876962661743164
  - 0.38178983330726624
  - 0.3949490785598755
  - 0.3896023631095886
  - 5.494496822357178
  - 0.4158118665218353
  - 0.38803237676620483
  - 0.3955431580543518
  - 0.4046052098274231
  - 0.3972005546092987
  - 0.3885349631309509
loss_records_fold4:
  train_losses:
  - 2.9291091561317444
  - 2.982847720384598
  - 2.979420667886734
  - 2.98731027841568
  - 3.0098584175109866
  - 2.919481208920479
  - 2.9957884669303896
  - 2.9868828415870667
  - 3.009451058506966
  - 2.9745585799217227
  - 2.9708103358745577
  - 2.991777461767197
  - 2.92399560213089
  - 2.9903756201267244
  - 3.015365701913834
  - 2.917328524589539
  - 3.0014434933662417
  - 2.9378985643386843
  - 2.907718294858933
  - 3.0219782650470735
  - 2.933808255195618
  - 2.97718340754509
  - 2.9314090073108674
  - 2.979739081859589
  - 3.0013248324394226
  - 2.9021036446094515
  - 2.9500701367855076
  - 2.971482163667679
  - 2.9661790370941166
  - 3.0154795885086063
  - 2.93438640832901
  - 2.905933526158333
  - 2.944492828845978
  - 2.910324290394783
  - 2.9507447659969332
  - 2.9183420300483705
  - 2.957937347888947
  - 2.918357840180397
  - 2.9292153030633927
  - 2.931726831197739
  - 2.967423325777054
  - 2.891427749395371
  - 2.943003249168396
  - 2.8761925876140597
  - 2.9501740992069245
  - 2.9671470642089846
  - 2.891229599714279
  - 2.972322791814804
  - 2.9810480117797855
  - 2.921092647314072
  - 2.9538406491279603
  - 3.0007844269275665
  - 2.934031528234482
  - 2.943311384320259
  - 2.9398198217153553
  - 2.8908607006073
  - 2.9088197112083436
  - 3.0257019966840746
  - 2.880013325810433
  - 2.9446699619293213
  - 2.949161857366562
  - 2.9620175540447238
  - 2.9945625185966493
  - 3.019091653823853
  - 3.0784159481525424
  - 3.013395690917969
  - 2.9883278489112857
  - 2.9686903148889545
  - 2.9294636815786363
  - 2.890637177228928
  - 2.9253519713878635
  - 2.9186258018016815
  - 2.986770111322403
  - 2.9870664834976197
  - 2.8562619626522068
  - 2.931936013698578
  - 2.90000878572464
  - 3.0525561153888705
  - 2.993105018138886
  - 2.9235359311103823
  - 2.992265996336937
  - 2.962777751684189
  - 2.985777235031128
  - 2.9716916680336
  - 2.9722371101379395
  - 2.9126038491725925
  - 2.9855059117078784
  - 3.00162313580513
  - 2.9227787256240845
  - 2.879974672198296
  - 2.981932467222214
  - 2.963789382576943
  - 2.9063331186771393
  - 2.922121071815491
  - 3.035464632511139
  - 2.8591127693653107
  - 2.9487922847270966
  - 2.945336037874222
  - 2.9230054765939713
  - 3.006576490402222
  validation_losses:
  - 0.3826712965965271
  - 0.38152286410331726
  - 0.3807779550552368
  - 0.3715033233165741
  - 0.39733898639678955
  - 0.38498276472091675
  - 0.38149577379226685
  - 0.37397411465644836
  - 0.375034362077713
  - 0.394440233707428
  - 0.4270522892475128
  - 0.40714231133461
  - 0.3964133858680725
  - 0.37453338503837585
  - 0.3806389272212982
  - 0.37191006541252136
  - 0.386227011680603
  - 0.3755297064781189
  - 0.37672802805900574
  - 0.39146682620048523
  - 0.3812776505947113
  - 0.4020668566226959
  - 0.45225468277931213
  - 0.44109949469566345
  - 0.39789554476737976
  - 0.37258225679397583
  - 0.3716212511062622
  - 0.40537580847740173
  - 0.4046739637851715
  - 0.38338783383369446
  - 0.394248366355896
  - 0.3892582356929779
  - 0.40270113945007324
  - 0.37463241815567017
  - 0.3791298270225525
  - 0.4741215109825134
  - 0.39626219868659973
  - 0.37691959738731384
  - 0.3778156042098999
  - 0.39961275458335876
  - 0.4074435532093048
  - 0.37919676303863525
  - 0.4246498942375183
  - 0.4295453131198883
  - 0.4140721559524536
  - 0.38355308771133423
  - 0.4160533845424652
  - 0.39166516065597534
  - 0.39617064595222473
  - 0.5094476938247681
  - 0.42462167143821716
  - 0.41681134700775146
  - 0.49191346764564514
  - 0.4280206561088562
  - 0.4729655683040619
  - 0.37630054354667664
  - 0.382521390914917
  - 0.43955790996551514
  - 0.5718527436256409
  - 0.36905530095100403
  - 0.444973349571228
  - 0.5450917482376099
  - 0.40630578994750977
  - 0.3790462613105774
  - 0.3786354660987854
  - 0.38903626799583435
  - 0.40087980031967163
  - 0.37065398693084717
  - 0.39414098858833313
  - 0.3743533790111542
  - 0.42903101444244385
  - 0.37351858615875244
  - 0.36908236145973206
  - 0.4251501262187958
  - 0.39535143971443176
  - 0.42137423157691956
  - 0.371860533952713
  - 0.40147414803504944
  - 0.38544195890426636
  - 0.519277811050415
  - 0.4037962555885315
  - 0.38362404704093933
  - 0.4017086625099182
  - 0.4108937084674835
  - 0.41292276978492737
  - 0.419104665517807
  - 0.3797256648540497
  - 0.41735002398490906
  - 0.398815393447876
  - 0.4581190347671509
  - 0.389608234167099
  - 0.3780343532562256
  - 0.39757803082466125
  - 0.43091973662376404
  - 0.400779128074646
  - 0.410611629486084
  - 0.372384250164032
  - 0.4248012900352478
  - 0.42494767904281616
  - 0.4681452214717865
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 52 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8593481989708405, 0.8593481989708405,
    0.8470790378006873]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.1359223300970874]'
  mean_eval_accuracy: 0.8558652072170843
  mean_f1_accuracy: 0.02718446601941748
  total_train_time: '0:19:45.856471'
