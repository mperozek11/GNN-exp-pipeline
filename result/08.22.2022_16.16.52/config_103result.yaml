config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:38:20.689925'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_103fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_103fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_103fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_103fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_103fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_103fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_103fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_103fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_103fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_103fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.1138185858726501
  - 0.9240472376346589
  - 0.961803549528122
  - 0.8615867674350739
  - 1.0050125002861023
  - 0.9025198519229889
  - 0.846067625284195
  - 0.7917039036750794
  - 0.818321567773819
  - 0.8132205665111543
  - 0.8199924767017365
  - 0.820515012741089
  - 0.8139250993728638
  - 0.9334049820899963
  - 0.853368753194809
  - 0.8159221529960633
  - 0.8515370011329652
  - 0.7865799069404602
  - 0.8339353859424592
  - 0.8141132056713105
  - 0.8306166768074036
  validation_losses:
  - 0.4211980700492859
  - 0.45724165439605713
  - 0.4381944239139557
  - 0.4118662476539612
  - 0.4049651026725769
  - 0.44973891973495483
  - 0.4011155068874359
  - 0.4065883457660675
  - 0.3942096531391144
  - 0.4048283100128174
  - 0.3896982669830322
  - 0.3935025632381439
  - 0.3866714537143707
  - 0.39255422353744507
  - 0.42657676339149475
  - 0.40992045402526855
  - 0.41036510467529297
  - 0.39175325632095337
  - 0.39490368962287903
  - 0.39307087659835815
  - 0.39155468344688416
loss_records_fold1:
  train_losses:
  - 0.8508502542972565
  - 0.7990980029106141
  - 0.8365747392177583
  - 0.7742647767066956
  - 0.8055196702480316
  - 0.7988030672073365
  - 0.8198416948318482
  - 0.8559777975082398
  - 0.8352740347385407
  - 0.8195681869983673
  - 0.7821870923042298
  validation_losses:
  - 0.3900894224643707
  - 0.3977992832660675
  - 0.38576024770736694
  - 0.39947080612182617
  - 0.39507168531417847
  - 0.3947419822216034
  - 0.38674449920654297
  - 0.39188823103904724
  - 0.3954157531261444
  - 0.3990751802921295
  - 0.3987364172935486
loss_records_fold2:
  train_losses:
  - 0.8055436491966248
  - 0.8273692309856415
  - 0.8405111193656922
  - 0.8225528597831726
  - 0.7971305012702943
  - 0.7815864950418473
  - 0.8583651065826416
  - 0.7792908161878587
  - 0.8036620914936066
  - 0.7966866791248322
  - 0.7968827545642854
  validation_losses:
  - 0.4164760112762451
  - 0.4132671058177948
  - 0.39652541279792786
  - 0.40493690967559814
  - 0.39243650436401367
  - 0.3943800926208496
  - 0.4043639898300171
  - 0.39386048913002014
  - 0.4002804160118103
  - 0.3903064429759979
  - 0.3885568380355835
loss_records_fold3:
  train_losses:
  - 0.8522661805152894
  - 0.8313392221927643
  - 0.8244714558124543
  - 0.8158139646053315
  - 0.7759691894054414
  - 0.8063649833202362
  - 0.7677937805652619
  - 0.8342346906661988
  - 0.8360103726387025
  - 0.7976515889167786
  - 0.8249753415584564
  - 0.8209419488906861
  validation_losses:
  - 0.41244760155677795
  - 0.3832336962223053
  - 0.38672974705696106
  - 0.3813585937023163
  - 0.37853869795799255
  - 0.3948562741279602
  - 0.40067335963249207
  - 0.401080846786499
  - 0.3981006443500519
  - 0.3782678246498108
  - 0.37738075852394104
  - 0.3801387548446655
loss_records_fold4:
  train_losses:
  - 0.8090512335300446
  - 0.8269011020660401
  - 0.7474526375532151
  - 1.0211740136146545
  - 0.9371138095855713
  - 0.8013438522815705
  - 0.7986632466316224
  - 0.7986067593097688
  - 0.78577099442482
  - 0.7924477398395539
  - 0.784793245792389
  - 0.8003446340560914
  validation_losses:
  - 0.3876388370990753
  - 0.3719296455383301
  - 0.3823481798171997
  - 0.3886830508708954
  - 0.38457825779914856
  - 0.40579161047935486
  - 0.3862195611000061
  - 0.39418578147888184
  - 0.40145885944366455
  - 0.38076457381248474
  - 0.38772153854370117
  - 0.3960925340652466
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:05:51.764550'
