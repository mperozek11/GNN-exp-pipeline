config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:58:11.293200'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_111fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_111fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_111fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_111fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_111fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_111fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_111fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_111fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_111fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_111fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 11.706505060195923
  - 11.14911277294159
  - 13.999295258522034
  - 5.024303805828095
  - 4.044033133983612
  - 3.518395292758942
  - 2.1210260510444643
  - 2.4107154607772827
  - 2.3229477167129517
  - 1.8881372928619387
  - 1.6369441747665405
  - 1.3008256018161775
  - 1.2238508701324464
  - 3.910147333145142
  - 7.540681326389313
  - 2.0654847502708438
  - 4.142900204658509
  - 2.9097403645515443
  - 2.1081182420253755
  - 2.263383835554123
  - 1.06902636885643
  - 9.311573487520219
  - 1.4375037610530854
  - 2.0917014658451083
  - 1.1825845539569855
  - 1.1526960492134095
  - 5.093909722566605
  - 8.766741293668748
  - 4.3383952379226685
  - 2.1217543482780457
  - 1.6731925010681152
  - 1.7625596940517427
  - 1.2255880922079088
  - 1.4966708600521088
  - 2.854008632898331
  - 10.308212411403657
  - 4.379144686460495
  - 1.817390286922455
  - 1.3835614979267121
  - 1.1828126966953278
  - 1.7591691613197327
  - 24.161558729410174
  - 5.170148181915284
  - 3.623997908830643
  - 2.2339494466781615
  - 2.085090845823288
  - 2.3216515243053437
  - 2.973380255699158
  - 1.5144904613494874
  - 1.7650040686130524
  - 8.94635488986969
  - 1.6277454674243927
  - 15.848953241109848
  - 5.663904142379761
  - 1.897439730167389
  - 1.492374736070633
  - 7.583815038204193
  - 4.148527824878693
  - 2.1600707530975343
  - 2.2920537471771243
  - 1.6844366729259492
  - 1.2588097631931305
  - 1.897145712375641
  - 1.1258813917636872
  - 1.0145470559597016
  - 0.8283062398433686
  validation_losses:
  - 6.03053092956543
  - 4.141031265258789
  - 2.5741515159606934
  - 1.257132887840271
  - 1.6656739711761475
  - 0.7474659085273743
  - 1.1499096155166626
  - 1.3516508340835571
  - 0.7636949419975281
  - 0.6828421950340271
  - 0.49036750197410583
  - 0.556713342666626
  - 0.5418838858604431
  - 0.6253471374511719
  - 0.5775423049926758
  - 1.1439241170883179
  - 1.5265542268753052
  - 1.3210490942001343
  - 0.5752536654472351
  - 0.662678599357605
  - 0.41592565178871155
  - 0.4926328659057617
  - 0.559960663318634
  - 0.43195977807044983
  - 0.431197851896286
  - 0.41487908363342285
  - 0.6315271258354187
  - 0.7309756875038147
  - 1.4368611574172974
  - 1.4052749872207642
  - 0.5432722568511963
  - 0.5568451285362244
  - 0.48435303568840027
  - 0.7142860889434814
  - 0.48623529076576233
  - 0.5698817372322083
  - 0.5025195479393005
  - 0.5441068410873413
  - 0.48397964239120483
  - 0.39160844683647156
  - 0.40808701515197754
  - 0.4972962439060211
  - 0.6206469535827637
  - 0.726858913898468
  - 0.6808323264122009
  - 0.6263155937194824
  - 0.5325493216514587
  - 0.4883662760257721
  - 0.4698502719402313
  - 0.4872289001941681
  - 0.43603965640068054
  - 0.5672817826271057
  - 0.5795864462852478
  - 0.7430000305175781
  - 0.4836653172969818
  - 0.4116407334804535
  - 0.45645251870155334
  - 0.4060328006744385
  - 0.5039198994636536
  - 0.8478628993034363
  - 0.6349284648895264
  - 0.5025231838226318
  - 0.4451783001422882
  - 0.43693751096725464
  - 0.4137689173221588
  - 0.41211017966270447
loss_records_fold1:
  train_losses:
  - 0.8599672973155976
  - 0.8955677807331086
  - 0.838398027420044
  - 1.1743026375770569
  - 0.8676545381546021
  - 1.2008891046047212
  - 0.8387587666511536
  - 0.7790732622146607
  - 0.81535884141922
  - 1.0138987481594086
  - 0.8294782221317292
  - 0.8899651587009431
  - 0.8290390491485596
  - 0.8769999504089356
  - 0.904766458272934
  - 0.8547732114791871
  - 0.8094414234161378
  - 0.8685200691223145
  - 0.8260238885879517
  - 0.8974739968776704
  - 0.8541936576366425
  - 0.9641453564167023
  - 3.0394079446792603
  - 1.4724899291992188
  - 0.8506785213947297
  - 0.9060588896274567
  - 0.8268931269645692
  - 1.0136756718158721
  - 0.9056417047977448
  - 0.8859687089920044
  - 0.9014425575733185
  - 0.9766541242599488
  - 0.8701640129089356
  - 0.8422045469284059
  - 0.8359175503253937
  - 1.0112494647502899
  - 0.9971986770629884
  - 0.8346028983592988
  - 0.7672971218824387
  validation_losses:
  - 0.4078744649887085
  - 0.4180019497871399
  - 0.42300131916999817
  - 0.40266087651252747
  - 0.4195689857006073
  - 0.427066832780838
  - 0.41133713722229004
  - 0.3997574746608734
  - 0.4232109785079956
  - 0.40171724557876587
  - 0.4042033851146698
  - 0.3990385830402374
  - 0.414572536945343
  - 0.40793874859809875
  - 0.42051461338996887
  - 0.398275226354599
  - 0.405556857585907
  - 0.45472198724746704
  - 0.40414413809776306
  - 0.4014773368835449
  - 0.42227429151535034
  - 0.40829792618751526
  - 0.44191473722457886
  - 0.4316350817680359
  - 0.4046420753002167
  - 0.4085927903652191
  - 0.4051459729671478
  - 0.4103739261627197
  - 0.4340404272079468
  - 0.4079180955886841
  - 0.4055667519569397
  - 0.41284218430519104
  - 0.4370094835758209
  - 0.41066595911979675
  - 0.40517720580101013
  - 0.4086625874042511
  - 0.4156307876110077
  - 0.412350594997406
  - 0.4107949435710907
loss_records_fold2:
  train_losses:
  - 0.925529021024704
  - 0.8204903662204743
  - 0.9253585159778596
  - 2.9939285397529605
  - 1.1475608408451081
  - 0.8958500921726227
  - 1.4666245877742767
  - 0.9122828781604767
  - 1.0000134766101838
  - 0.8614319384098054
  - 0.9348635256290436
  - 0.8551274538040161
  - 0.8924729406833649
  - 1.0604244291782379
  - 0.8554583907127381
  - 0.8997786700725556
  - 0.8323363363742828
  - 0.9872470557689668
  - 5.114314651489258
  validation_losses:
  - 0.38245099782943726
  - 0.39823612570762634
  - 0.38678231835365295
  - 0.40155303478240967
  - 0.3935350179672241
  - 0.39537104964256287
  - 0.411232590675354
  - 0.4032469689846039
  - 0.3963409662246704
  - 0.3901098072528839
  - 0.40051719546318054
  - 0.38533955812454224
  - 0.41995298862457275
  - 0.41567984223365784
  - 0.4026750326156616
  - 0.3990098536014557
  - 0.38170450925827026
  - 0.3892687261104584
  - 0.39549753069877625
loss_records_fold3:
  train_losses:
  - 4.97984288930893
  - 1.1165918648242952
  - 1.0012045860290528
  - 1.3500627517700197
  - 1.0646527647972108
  - 0.8712234437465668
  - 1.085030671954155
  - 0.8949501097202301
  - 1.2152112662792207
  - 0.9104063868522645
  - 1.399394863843918
  - 0.8420996248722077
  - 0.8848782479763031
  - 0.8540480852127076
  - 0.897150307893753
  - 0.95078284740448
  - 0.8468442916870118
  - 1.0168165683746337
  - 0.8836731791496277
  - 0.9799288213253021
  - 0.8372212111949922
  - 3.8223115980625155
  - 1.0728338360786438
  - 1.5061734437942507
  - 1.230328232049942
  - 0.9127259373664857
  - 0.9203434705734254
  - 0.8781363904476166
  - 1.615200287103653
  - 0.9445657193660737
  - 0.8962999939918519
  - 0.8541303098201752
  - 0.8878552556037903
  - 0.8279665172100068
  - 0.8669298052787782
  - 0.8447236597537995
  - 0.8235776662826538
  - 0.8316358387470246
  - 0.8283415853977204
  - 0.8554699301719666
  - 0.8310916721820831
  - 0.8976039052009583
  - 1.9839598774909974
  - 8.920772683620454
  - 5.234947448968888
  - 3.417276415228844
  - 2.1765352189540863
  - 1.0737308382987976
  - 1.1019732236862183
  - 0.8317382514476777
  - 1.9558326423168184
  validation_losses:
  - 0.4261317253112793
  - 1.379130244255066
  - 0.49301815032958984
  - 0.4284749925136566
  - 0.5865989327430725
  - 0.46404701471328735
  - 0.4195375144481659
  - 0.398660808801651
  - 0.43202677369117737
  - 0.4179198145866394
  - 0.412274032831192
  - 0.44535961747169495
  - 0.40741580724716187
  - 0.42323949933052063
  - 0.4011615812778473
  - 0.39471691846847534
  - 0.4399658739566803
  - 0.4037388861179352
  - 0.5002001523971558
  - 0.41534551978111267
  - 0.4107381999492645
  - 0.40690141916275024
  - 0.48355183005332947
  - 0.43324047327041626
  - 0.41153740882873535
  - 0.4016449451446533
  - 0.4261663556098938
  - 0.41167640686035156
  - 0.4087587893009186
  - 0.45202574133872986
  - 0.4247455894947052
  - 0.4341130256652832
  - 0.42438194155693054
  - 58.236480712890625
  - 14.964519500732422
  - 0.6575781106948853
  - 0.4253082573413849
  - 0.3937743306159973
  - 0.41215407848358154
  - 0.433205246925354
  - 0.4184396266937256
  - 0.3978811800479889
  - 1021.1644897460938
  - 0.8292938470840454
  - 7.527769565582275
  - 0.8386282324790955
  - 0.49325788021087646
  - 0.4396490156650543
  - 0.407710462808609
  - 0.4072778820991516
  - 0.40971434116363525
loss_records_fold4:
  train_losses:
  - 1.0901835143566132
  - 1.046846044063568
  - 0.9177391588687898
  - 0.9178124427795411
  - 0.8749414563179017
  - 0.8520330727100373
  - 1.0739801585674287
  - 0.8608125746250153
  - 0.920147716999054
  - 0.8496420025825501
  - 0.8817715525627137
  - 0.8725471615791321
  - 0.8698957562446594
  - 0.8653559863567353
  - 0.7956144869327546
  - 1.7282721638679506
  - 0.8842220783233643
  - 0.8388467013835907
  - 0.8295010924339294
  - 0.8336297869682312
  - 0.8558636963367463
  - 0.7982517659664154
  - 0.9287306606769562
  - 0.9336254835128784
  - 1.7254221856594087
  - 0.8871625095605851
  - 1.0232314229011537
  - 0.8516527712345123
  - 0.8776538848876954
  - 0.8878341138362885
  - 0.9120565593242645
  - 0.8815660357475281
  - 0.8893047392368317
  - 0.8103095173835755
  - 0.8484274029731751
  - 0.8649262189865112
  - 2.8446843028068542
  - 1.0274344742298127
  - 1.0220231235027313
  - 0.8603819608688354
  - 1.0658745884895324
  - 1.2585114121437073
  - 0.871854442358017
  - 0.8798498570919038
  - 0.8775219976902009
  - 0.8793360650539399
  - 0.85097696185112
  - 0.8851999402046205
  - 0.8370888352394105
  - 0.8082541704177857
  - 0.827844250202179
  - 0.8531918346881867
  - 0.8642977774143219
  - 0.8410117208957673
  - 0.8304918825626374
  - 0.8232234776020051
  - 0.8292194485664368
  - 0.7983046710491181
  - 0.7977090299129487
  - 0.8168898046016694
  - 0.8671228289604187
  - 0.847131496667862
  - 0.8166422367095948
  - 0.7895755380392075
  - 0.9301721274852753
  - 1.069951981306076
  - 3.004035818576813
  - 2.1172810375690463
  - 0.9555155813694001
  - 0.8696815311908722
  - 0.8680413067340851
  - 0.9176555216312409
  - 0.8491744697093964
  - 0.8563124358654023
  - 0.9148689806461334
  - 0.926926988363266
  - 1.006027364730835
  - 0.8637413561344147
  - 0.8249818980693817
  - 6.768682348728181
  - 0.9349936783313751
  - 0.9271794378757477
  - 0.9653973937034608
  - 1.014855444431305
  - 0.8063025355339051
  - 0.873692160844803
  - 1.6283289551734925
  - 0.8549400091171265
  - 0.8237487375736237
  - 0.8377570271492005
  - 4.489568817615509
  - 0.8639713525772095
  - 1.013584417104721
  - 1.0272516787052155
  - 1.39295312166214
  - 0.9126323819160462
  - 0.9008744537830353
  - 8.842185240983964
  - 0.9808312892913819
  - 0.887977409362793
  validation_losses:
  - 0.4789382219314575
  - 0.4419146776199341
  - 0.44355979561805725
  - 0.46873119473457336
  - 0.4079514741897583
  - 0.41109699010849
  - 0.4745793342590332
  - 0.4431094229221344
  - 0.4272501766681671
  - 0.4336548447608948
  - 0.4550168216228485
  - 0.4584556818008423
  - 0.4063510596752167
  - 0.41564369201660156
  - 0.4451247751712799
  - 0.43572545051574707
  - 0.44159039855003357
  - 0.4116368889808655
  - 0.3933597505092621
  - 0.4221629202365875
  - 0.4183163046836853
  - 0.381989985704422
  - 0.4456750452518463
  - 0.4041263461112976
  - 0.41430148482322693
  - 0.47513750195503235
  - 0.412582129240036
  - 0.4401596486568451
  - 0.4754168391227722
  - 0.41544225811958313
  - 0.44815871119499207
  - 0.4229249358177185
  - 0.3962937295436859
  - 0.4109317660331726
  - 0.4435940980911255
  - 0.4187741279602051
  - 0.3929249048233032
  - 0.5468363165855408
  - 0.503020703792572
  - 0.6014561653137207
  - 0.5874354839324951
  - 0.5713059902191162
  - 0.42077603936195374
  - 0.4390740990638733
  - 0.44130662083625793
  - 0.45721352100372314
  - 0.438142329454422
  - 0.44083869457244873
  - 0.4327170252799988
  - 0.49084216356277466
  - 0.44304218888282776
  - 0.4888231158256531
  - 0.3966135084629059
  - 0.40474769473075867
  - 0.4178618788719177
  - 0.4326828420162201
  - 0.4335310161113739
  - 0.3986378014087677
  - 0.4301527142524719
  - 0.42106497287750244
  - 0.4182665944099426
  - 0.43313121795654297
  - 0.4068233370780945
  - 0.41999754309654236
  - 0.4345276355743408
  - 0.425116628408432
  - 0.4063733220100403
  - 0.4368710517883301
  - 0.5228703618049622
  - 0.4698306620121002
  - 0.6816182732582092
  - 0.4726588726043701
  - 0.6029791235923767
  - 0.5271867513656616
  - 0.4742605686187744
  - 0.5495070815086365
  - 0.7063475251197815
  - 0.40000978112220764
  - 0.4506933391094208
  - 0.6001959443092346
  - 0.4236684739589691
  - 0.45098352432250977
  - 0.45194441080093384
  - 0.4894580841064453
  - 0.4166449010372162
  - 0.4324815571308136
  - 0.4742840826511383
  - 0.41460761427879333
  - 0.39168694615364075
  - 0.4485134482383728
  - 0.4185938239097595
  - 0.41907602548599243
  - 0.4294571876525879
  - 0.39237603545188904
  - 0.6746418476104736
  - 0.5316986441612244
  - 0.7652197480201721
  - 0.5782804489135742
  - 0.5184640884399414
  - 0.4250200390815735
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 66 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.855917667238422, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8575845991523876
  mean_f1_accuracy: 0.0
  total_train_time: '0:23:37.487130'
