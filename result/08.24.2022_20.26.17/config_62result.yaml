config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:49:35.571857'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_62fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_62fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_62fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_62fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_62fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_62fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_62fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_62fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_62fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_62fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 39.29707202911377
  - 17.61885783672333
  - 11.232436382770539
  - 5.358058726787568
  - 3.505213457345963
  - 2.8096702575683596
  - 5.925080102682114
  - 2.4421305000782016
  - 1.7848799645900728
  - 2.318674498796463
  - 2.2352198123931886
  - 2.1710724592208863
  - 2.181034076213837
  - 2.0511345505714416
  - 1.9374691009521485
  - 2.2067729115486148
  - 5.1158310085535055
  - 3.1048272758722306
  - 3.536133086681366
  - 2.7723967075347904
  - 2.1587459206581117
  - 4.854881286621094
  - 7.817561292648316
  - 7.21645896434784
  - 10.831944340467453
  - 7.446109816431999
  - 6.1962402939796455
  - 5.209194254875183
  - 2.762830924987793
  - 1.9912840485572816
  - 2.1620332568883898
  - 8.996703308820726
  - 8.194487804174424
  - 2.9118318974971773
  - 2.6604195535182953
  - 2.0613844454288484
  - 2.75211518406868
  - 2.6279929220676426
  - 1.7749461233615875
  - 1.8028025686740876
  - 2.9991165697574615
  - 1.930919808149338
  - 3.809473931789398
  - 1.9432272911071777
  - 1.8295208871364594
  - 1.903776490688324
  - 1.8322863161563874
  - 1.6299010276794434
  - 2.1609639525413513
  - 1.922753119468689
  - 1.8215056836605072
  - 1.774645060300827
  - 1.7726010859012604
  - 1.6406333148479462
  - 1.6914420187473298
  - 1.8593015134334565
  - 1.931867003440857
  - 1.816327339410782
  - 1.9934789896011353
  - 1.669454735517502
  - 1.7753940463066102
  - 1.9205171406269075
  - 2.4474973738193513
  - 2.4114394545555116
  - 3.3204311370849613
  - 2.420404636859894
  - 2.0362433314323427
  - 1.8236574053764345
  - 2.680768519639969
  - 2.003027367591858
  - 3.120059442520142
  - 3.5522216081619264
  - 3.09713996052742
  - 3.2937858283519748
  - 1.628474408388138
  - 3.9399770110845567
  - 1.7758907973766327
  - 1.6946087062358857
  - 1.6986512243747711
  - 1.6821052670478822
  - 1.7181000649929048
  - 2.2258542597293856
  - 2.158436042070389
  - 1.7789108812808991
  - 2.7123351931571964
  - 1.6045602202415468
  - 1.6543792366981507
  - 1.7587407529354095
  - 2.1204249620437623
  - 1.6289054036140442
  - 1.9342332601547243
  - 1.678353410959244
  - 2.6725584387779238
  - 1.900990492105484
  - 2.1179516673088075
  - 2.324193298816681
  - 2.200113868713379
  - 1.9137560665607454
  - 2.082309812307358
  - 1.7222905397415162
  validation_losses:
  - 3.2707881927490234
  - 2.381215810775757
  - 3.1376311779022217
  - 0.7102689743041992
  - 0.6413393020629883
  - 0.8931002616882324
  - 0.5020725131034851
  - 0.4189377427101135
  - 0.41430607438087463
  - 0.48509058356285095
  - 0.5636080503463745
  - 0.48938706517219543
  - 0.4573724567890167
  - 0.412913978099823
  - 0.4012911319732666
  - 0.5596398711204529
  - 0.49519288539886475
  - 0.6635918021202087
  - 0.39709633588790894
  - 0.42733651399612427
  - 0.5008261203765869
  - 0.47541844844818115
  - 0.41707417368888855
  - 0.47936365008354187
  - 0.6355831623077393
  - 0.5432349443435669
  - 0.5164678692817688
  - 0.48550280928611755
  - 0.41130301356315613
  - 0.39524486660957336
  - 0.42516034841537476
  - 3.14833927154541
  - 0.5400470495223999
  - 0.5638001561164856
  - 0.4283411204814911
  - 0.4314108192920685
  - 0.4028826653957367
  - 0.3974803388118744
  - 0.4432200491428375
  - 0.4107275605201721
  - 0.4152253270149231
  - 0.3873574435710907
  - 0.4002685248851776
  - 0.4310910403728485
  - 0.4073956310749054
  - 0.3962046802043915
  - 0.3985472321510315
  - 0.4154353439807892
  - 0.39643189311027527
  - 0.5728874206542969
  - 0.481916218996048
  - 0.4270471930503845
  - 0.3954757750034332
  - 0.413606196641922
  - 0.40092572569847107
  - 0.39941516518592834
  - 0.462836354970932
  - 0.4122789800167084
  - 0.3991996645927429
  - 0.40983203053474426
  - 0.40965840220451355
  - 0.42914605140686035
  - 0.3992691934108734
  - 0.4058205783367157
  - 0.42271605134010315
  - 0.4074413776397705
  - 0.41118741035461426
  - 0.46588993072509766
  - 0.4235592186450958
  - 0.40825018286705017
  - 0.3938189446926117
  - 0.40263694524765015
  - 0.4588061571121216
  - 0.40090423822402954
  - 0.4109879434108734
  - 0.42833223938941956
  - 0.4029468894004822
  - 0.39118462800979614
  - 0.417655348777771
  - 0.405241459608078
  - 0.3981366753578186
  - 0.5000787377357483
  - 0.433171808719635
  - 0.4203052520751953
  - 0.42046183347702026
  - 0.38874351978302
  - 0.4245131015777588
  - 0.4302450716495514
  - 0.42620888352394104
  - 0.4504438638687134
  - 0.44555482268333435
  - 0.46366843581199646
  - 0.3996683657169342
  - 0.4085417687892914
  - 0.3878527879714966
  - 0.42258816957473755
  - 0.41288086771965027
  - 0.4153231084346771
  - 0.4295225739479065
  - 0.4254160523414612
loss_records_fold1:
  train_losses:
  - 2.2814234852790833
  - 1.8079317271709443
  - 1.7678051054477693
  - 1.7433316230773928
  - 1.7842785477638246
  - 1.674388861656189
  - 1.8505815088748934
  - 2.2031878858804705
  - 1.8251169204711915
  - 1.7213203370571137
  - 1.9313347458839418
  - 3.636987501382828
  - 2.280784434080124
  - 5.857080483436585
  - 2.729747760295868
  - 1.9826141059398652
  - 1.6984603881835938
  - 2.5366058468818666
  - 2.089521849155426
  - 2.8286281645298006
  - 1.6095476120710375
  - 1.6999820232391358
  - 1.6607014060020449
  - 2.8748897910118103
  - 1.9325913161039354
  - 1.6451056599617004
  - 1.6576450616121292
  validation_losses:
  - 0.4186284840106964
  - 0.5342037677764893
  - 0.43274280428886414
  - 0.42022520303726196
  - 0.4078814387321472
  - 0.4236751198768616
  - 0.4261496067047119
  - 0.43799906969070435
  - 0.44029107689857483
  - 0.412261962890625
  - 0.44788801670074463
  - 0.5889541506767273
  - 0.4281177222728729
  - 0.4074428379535675
  - 0.5958784818649292
  - 0.4247007369995117
  - 0.4263504147529602
  - 0.4201912581920624
  - 0.413584440946579
  - 0.41589415073394775
  - 0.4392838478088379
  - 0.4165993630886078
  - 0.4223926365375519
  - 0.4298287630081177
  - 0.41607508063316345
  - 0.4237002730369568
  - 0.41453567147254944
loss_records_fold2:
  train_losses:
  - 1.8583710551261903
  - 2.116125559806824
  - 1.714564210176468
  - 1.643277645111084
  - 1.5442435294389725
  - 2.1577871978282928
  - 1.9035949826240541
  - 1.8906534850597383
  - 1.5858600437641144
  - 2.359883725643158
  - 1.7695113837718965
  - 2.050356650352478
  - 1.7658636629581452
  - 1.9101341724395753
  - 1.6751414835453033
  - 1.789015179872513
  - 1.5888721644878387
  - 1.7354027450084688
  - 1.663882863521576
  - 1.7424732804298402
  - 2.267525553703308
  - 1.767631620168686
  - 1.658059585094452
  - 1.586429014801979
  - 1.573523682355881
  - 1.5607132107019426
  - 1.5804070055484774
  - 1.7486545622348786
  - 2.1152510166168215
  - 1.6782065689563752
  - 1.5802823990583421
  - 1.8000346004962922
  - 1.6699096858501434
  - 1.5867883384227754
  - 1.7500683903694154
  - 1.6954415619373322
  - 2.269913482666016
  - 1.6640738666057588
  - 1.7840656697750092
  - 1.6260142028331757
  - 1.6558142423629763
  - 1.5916974723339081
  - 1.6110431969165804
  validation_losses:
  - 0.4072544574737549
  - 0.39079537987709045
  - 0.4156549274921417
  - 0.38219016790390015
  - 0.3820035755634308
  - 0.432017982006073
  - 0.3877868354320526
  - 0.4376256465911865
  - 0.3949744999408722
  - 0.4309004545211792
  - 0.40276801586151123
  - 0.3891398012638092
  - 0.41716820001602173
  - 0.4481746256351471
  - 0.42983707785606384
  - 0.3943400979042053
  - 0.39988088607788086
  - 0.3789389729499817
  - 0.40235820412635803
  - 0.37589409947395325
  - 0.4857846796512604
  - 0.46550363302230835
  - 0.3877992331981659
  - 0.38660579919815063
  - 0.4045703113079071
  - 0.40454259514808655
  - 0.395854115486145
  - 0.42045754194259644
  - 0.4016893208026886
  - 0.4011053144931793
  - 0.4127292037010193
  - 0.39386996626853943
  - 0.4014900028705597
  - 0.4035116136074066
  - 0.404560923576355
  - 0.4008674621582031
  - 0.41177257895469666
  - 0.39929893612861633
  - 0.3986581265926361
  - 0.3878280222415924
  - 0.39750033617019653
  - 0.4031003415584564
  - 0.39702165126800537
loss_records_fold3:
  train_losses:
  - 1.6198637068271637
  - 1.5950741469860077
  - 1.7663895547389985
  - 1.6832177639007568
  - 1.7601514577865602
  - 1.6217011988162995
  - 1.5762434601783752
  - 2.0685344874858855
  - 1.7760537922382356
  - 1.8206076502799988
  - 1.9108122229576112
  - 1.6520381569862366
  - 1.7432517826557161
  - 1.6813628792762758
  - 1.6336338877677918
  - 1.8445374429225923
  - 1.653521102666855
  - 1.7301473379135133
  - 1.6864681661129
  - 1.6340756118297577
  - 1.6658624708652496
  - 1.6619368851184846
  - 1.577963936328888
  - 1.617056280374527
  - 1.6404486179351807
  - 1.610082495212555
  - 1.6469697952270508
  - 1.6849354028701784
  - 1.604993623495102
  - 1.627711582183838
  - 1.6150325119495392
  - 1.6198673963546755
  - 1.6179967880249024
  - 1.7203684151172638
  - 1.6311479687690735
  - 1.6915307819843293
  - 1.597922295331955
  - 1.5684589743614197
  - 1.575038242340088
  - 1.5639423727989197
  - 1.5915664970874788
  - 1.6903079092502595
  - 1.8040004611015321
  - 1.6914217412471773
  - 1.6022630274295808
  - 1.6263616740703584
  - 1.7297900319099426
  - 1.7528678357601166
  - 1.6573139846324922
  - 1.587255758047104
  - 1.593320193886757
  - 1.6341369211673737
  - 1.73570476770401
  - 1.605618739128113
  - 1.6073800921440125
  - 1.5719809234142303
  - 1.6412711024284363
  - 1.5617641627788545
  - 1.6700443744659426
  - 1.6631956636905671
  - 1.8432163953781129
  - 1.5757937252521517
  - 1.6182288050651552
  - 1.754902845621109
  - 1.6026201725006104
  - 1.5866079568862916
  - 1.9925362139940264
  - 1.6189097940921784
  - 1.69351264834404
  - 1.675844195485115
  - 1.6333392679691316
  - 1.680399864912033
  - 1.7137118339538575
  - 1.8494893312454224
  - 1.6178345620632173
  - 1.5666899383068085
  - 1.6511235535144806
  - 1.6393227338790894
  - 1.6379553556442261
  - 1.5654299706220627
  - 1.6107864797115328
  - 1.643866378068924
  - 1.596432626247406
  - 1.613742780685425
  - 1.5947367429733277
  - 1.6741560101509094
  - 1.598743134737015
  validation_losses:
  - 0.4150793254375458
  - 0.3968729078769684
  - 0.42420056462287903
  - 0.403308242559433
  - 0.4312574565410614
  - 0.43601173162460327
  - 0.3968709111213684
  - 0.40785056352615356
  - 0.4745854139328003
  - 0.4238930344581604
  - 0.39087873697280884
  - 0.40161871910095215
  - 0.41213932633399963
  - 0.4166122078895569
  - 0.43216273188591003
  - 0.40423303842544556
  - 0.4514170289039612
  - 0.4246867597103119
  - 0.4177309572696686
  - 0.4776633381843567
  - 0.4101749360561371
  - 0.41386091709136963
  - 0.4321247935295105
  - 0.418163537979126
  - 0.4135337769985199
  - 0.39642030000686646
  - 0.4353635013103485
  - 0.4176780879497528
  - 0.4065539836883545
  - 0.4059196710586548
  - 0.4182857275009155
  - 0.6038360595703125
  - 0.4240543842315674
  - 0.4492640793323517
  - 0.4088238775730133
  - 0.39957770705223083
  - 0.44446229934692383
  - 0.45351162552833557
  - 0.4278295636177063
  - 0.6154809594154358
  - 0.438678503036499
  - 0.46503666043281555
  - 0.5234017372131348
  - 0.41712793707847595
  - 0.40102699398994446
  - 0.4109859764575958
  - 0.4199705719947815
  - 0.4399382174015045
  - 0.4277322590351105
  - 0.4544701874256134
  - 0.47908976674079895
  - 0.5188722014427185
  - 0.4067917466163635
  - 0.3989695608615875
  - 0.4473458230495453
  - 0.48725685477256775
  - 0.4473676085472107
  - 0.49767830967903137
  - 0.5263937711715698
  - 0.5713241100311279
  - 0.6206085681915283
  - 11.176817893981934
  - 17.72308921813965
  - 0.4046076238155365
  - 0.395052969455719
  - 0.39506056904792786
  - 0.4474512040615082
  - 0.39513272047042847
  - 0.413762629032135
  - 0.39920929074287415
  - 0.41121721267700195
  - 0.4172874689102173
  - 0.4068460762500763
  - 0.40125513076782227
  - 0.43331265449523926
  - 0.39754945039749146
  - 0.4122311472892761
  - 0.4247640073299408
  - 0.41461488604545593
  - 0.4064372479915619
  - 0.41679757833480835
  - 0.4203714430332184
  - 0.40721452236175537
  - 0.40539881587028503
  - 0.4005472958087921
  - 0.4029691219329834
  - 0.4037729501724243
loss_records_fold4:
  train_losses:
  - 1.6666872262954713
  - 1.6236792206764221
  - 1.5890387058258058
  - 1.5869740426540375
  - 1.713213109970093
  - 1.681049805879593
  - 1.6585868775844574
  - 1.6201603293418885
  - 1.6227352619171143
  - 1.7194988310337067
  - 1.6239657700061798
  - 1.611086404323578
  - 1.6355960667133331
  - 1.6291334331035614
  - 1.5817297637462617
  - 1.5689829289913177
  - 1.6292958080768587
  validation_losses:
  - 0.4012411832809448
  - 0.4137760102748871
  - 0.3996182084083557
  - 0.4102974534034729
  - 0.404190331697464
  - 0.4486391842365265
  - 0.407250314950943
  - 0.4091191589832306
  - 0.3994184732437134
  - 0.40533170104026794
  - 0.4345366060733795
  - 0.4239920377731323
  - 0.4068886339664459
  - 0.41202443838119507
  - 0.39854079484939575
  - 0.40497422218322754
  - 0.4124734103679657
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 43 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 87 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:23:59.058972'
