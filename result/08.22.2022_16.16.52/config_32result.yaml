config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:51:19.119129'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_32fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_32fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_32fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_32fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_32fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_32fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_32fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_32fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_32fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_32fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.101805877685547
  - 5.916500401496887
  - 6.081119406223298
  - 6.061074921488762
  - 5.858781573176384
  - 6.032025155425072
  - 5.899476853013039
  - 5.948166593909264
  - 5.911543196439744
  - 5.81307233273983
  - 5.6693362474441535
  - 5.752678328752518
  - 5.680567346513271
  - 5.754165697097779
  - 5.677451860904694
  - 5.713905486464501
  - 6.026686894893647
  - 5.701769489049912
  - 5.643606671690941
  - 5.586166015267373
  - 5.59558326303959
  - 5.660695824027062
  validation_losses:
  - 0.45681998133659363
  - 0.4007793962955475
  - 0.40206998586654663
  - 0.4045088589191437
  - 0.4187125861644745
  - 0.40181028842926025
  - 0.49399909377098083
  - 0.45066380500793457
  - 0.40398484468460083
  - 0.38774216175079346
  - 0.40313202142715454
  - 0.3979244828224182
  - 0.3956567645072937
  - 0.38788843154907227
  - 0.38717031478881836
  - 0.3977152407169342
  - 0.38517817854881287
  - 0.3853726387023926
  - 0.39198756217956543
  - 0.3910723030567169
  - 0.39333558082580566
  - 0.3860020935535431
loss_records_fold1:
  train_losses:
  - 5.621361091732979
  - 5.586276519298554
  - 5.626329308748246
  - 5.669502925872803
  - 5.613035491108895
  - 5.610701274871826
  - 5.671622768044472
  - 5.609481126070023
  - 5.589780166745186
  - 5.5857485920190815
  - 5.583333423733712
  validation_losses:
  - 0.3888811767101288
  - 0.39199116826057434
  - 0.38448774814605713
  - 0.38596031069755554
  - 0.41254714131355286
  - 0.390571653842926
  - 0.39158570766448975
  - 0.3886808454990387
  - 0.3908601403236389
  - 0.3948669135570526
  - 0.3985309600830078
loss_records_fold2:
  train_losses:
  - 5.6341466248035434
  - 5.614808735251427
  - 5.630723109841347
  - 5.674732601642609
  - 5.560465589165688
  - 5.5876495212316515
  - 5.60175416469574
  - 5.5925876080989845
  - 5.580802738666534
  - 5.582286530733109
  - 5.6382495105266575
  - 5.668816158175469
  - 5.625695502758027
  - 5.6309762626886375
  - 5.6103032529354095
  - 5.574373206496239
  - 5.5446619048714645
  - 5.599156159162522
  - 5.524514263868332
  - 5.717558458447456
  validation_losses:
  - 0.38433948159217834
  - 0.39563947916030884
  - 0.40389037132263184
  - 0.38809606432914734
  - 0.42697805166244507
  - 0.3853740990161896
  - 0.3974265456199646
  - 0.4096249043941498
  - 0.3848556578159332
  - 0.38847553730010986
  - 0.3856559991836548
  - 0.38805991411209106
  - 0.3874424397945404
  - 0.4046518802642822
  - 0.3955290913581848
  - 0.391161173582077
  - 0.39233630895614624
  - 0.3926583230495453
  - 0.3997610807418823
  - 0.39307156205177307
loss_records_fold3:
  train_losses:
  - 5.661369639635087
  - 5.62197651565075
  - 5.6385473877191545
  - 5.619690093398095
  - 5.615389657020569
  - 5.5963973820209505
  - 5.64640857577324
  - 5.554860393702985
  - 5.661463177204133
  - 5.593103879690171
  - 5.638930770754815
  - 5.6187849879264835
  - 5.645269927382469
  - 5.609874531626701
  - 5.664253589510918
  - 5.609541723132134
  - 5.640717774629593
  - 5.602777475118637
  - 5.649801725149155
  - 5.57245661765337
  - 5.609618377685547
  - 5.613138952851296
  - 5.6249656856060035
  - 5.622407677769662
  - 5.6747643709182745
  - 5.631984150409699
  - 5.655077579617501
  - 5.659378045797348
  - 5.616575583815575
  - 5.618309965729714
  - 5.573053419589996
  - 5.632545590400696
  - 5.604228937625885
  validation_losses:
  - 0.37942758202552795
  - 0.3685373365879059
  - 0.37587621808052063
  - 0.3883912265300751
  - 0.48980948328971863
  - 9.050151824951172
  - 8.643913269042969
  - 0.40659183263778687
  - 0.3961469531059265
  - 0.4963386356830597
  - 1.7291826009750366
  - 0.3914031386375427
  - 0.5290766954421997
  - 2.333548069000244
  - 1.522913932800293
  - 0.40408995747566223
  - 3.2246651649475098
  - 0.7682965993881226
  - 0.7076423764228821
  - 0.7397271394729614
  - 0.7907082438468933
  - 0.5705333352088928
  - 0.4092157185077667
  - 0.620243489742279
  - 0.3915434181690216
  - 0.3789677023887634
  - 0.3957623243331909
  - 0.3807238042354584
  - 0.3804994225502014
  - 0.37966081500053406
  - 0.3819516599178314
  - 0.3788297772407532
  - 0.3762582242488861
loss_records_fold4:
  train_losses:
  - 5.637118381261826
  - 5.595695659518242
  - 5.5952234119176865
  - 5.590752878785134
  - 5.673619186878205
  - 5.650352299213409
  - 5.542675080895425
  - 5.55109096467495
  - 5.628859987854958
  - 5.570662832260132
  - 5.620247822999954
  validation_losses:
  - 0.37501487135887146
  - 0.37588101625442505
  - 0.37881869077682495
  - 0.39223119616508484
  - 0.38277319073677063
  - 0.3809559643268585
  - 0.37944909930229187
  - 0.3781406581401825
  - 0.38692644238471985
  - 0.3772682547569275
  - 0.378693163394928
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 33 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:10:13.471517'
