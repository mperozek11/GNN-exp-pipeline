config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:08:11.303814'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_116fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_116fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_116fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_116fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_116fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_116fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_116fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_116fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_116fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_116fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 7.079148241877556
  - 6.6876673907041555
  - 6.67214941084385
  - 7.236756804585458
  - 7.197368040680885
  - 6.381831973791122
  - 6.399770992994309
  - 6.422874683141709
  - 6.382851102948189
  - 6.518101507425309
  - 6.38115519285202
  - 6.304861557483673
  - 6.342610529065133
  - 6.082939499616623
  - 6.058289128541947
  - 5.98818893134594
  - 6.001197993755341
  - 6.048706686496735
  - 5.938026869297028
  - 5.9684722334146505
  - 5.89701434969902
  - 5.842319962382317
  - 5.974444863200188
  - 5.9905454337596895
  - 5.998530671000481
  - 6.041463997960091
  - 5.98434057533741
  - 5.863911446928978
  - 5.910789757966995
  - 5.917244729399681
  - 5.855273920297623
  - 6.188186064362526
  - 5.948806607723236
  - 5.931050035357476
  - 6.042193359136582
  - 5.955405801534653
  - 5.827810999751091
  - 5.857835909724236
  - 5.839363777637482
  - 5.978827351331711
  - 5.823650580644608
  - 5.900752738118172
  - 6.056641793251038
  - 6.012198352813721
  - 6.218959483504296
  - 5.961448693275452
  - 6.032530230283737
  - 5.977672231197357
  - 5.87267074584961
  - 5.992260968685151
  - 6.339305263757706
  - 6.14716121852398
  - 6.082612907886506
  - 5.956278210878373
  - 5.810559803247452
  - 6.0384824812412266
  - 5.945488569140434
  - 5.97238211631775
  - 5.881607019901276
  - 5.952161538600922
  - 6.000348153710366
  - 5.869634583592415
  - 5.982875245809556
  - 5.983256670832635
  - 5.94779306948185
  - 5.7769010990858085
  - 6.031474074721337
  - 5.812156328558922
  - 5.967768466472626
  - 5.835393950343132
  - 5.884804910421372
  - 5.851143610477448
  - 5.994412952661515
  - 5.967951479554177
  - 5.876912021636963
  - 6.007311719655991
  - 5.863349503278733
  - 5.962797132134438
  - 5.822289836406708
  - 5.77438080906868
  - 5.922206214070321
  - 5.8956699669361115
  - 5.831612649559975
  - 5.947308740019799
  - 5.966834828257561
  - 5.798811459541321
  - 5.891574320197106
  - 5.8110444277524955
  - 5.771099975705147
  - 5.747687003016472
  - 5.96923098564148
  - 5.859377700090409
  - 5.904734820127487
  - 5.870830881595612
  - 5.959313607215882
  - 5.7511580973863605
  validation_losses:
  - 0.5849898457527161
  - 0.4285566210746765
  - 0.4252888858318329
  - 0.40205147862434387
  - 0.3895973861217499
  - 0.40100815892219543
  - 0.40191856026649475
  - 0.3991723358631134
  - 0.3970353603363037
  - 0.40868765115737915
  - 0.42381343245506287
  - 0.3893604278564453
  - 0.3810620605945587
  - 0.3896602392196655
  - 0.38645583391189575
  - 0.3882949948310852
  - 0.41643062233924866
  - 0.41164687275886536
  - 0.38759997487068176
  - 0.39961618185043335
  - 0.38494566082954407
  - 0.3993801772594452
  - 0.39370328187942505
  - 0.594831109046936
  - 0.387174129486084
  - 0.3876580595970154
  - 0.49083998799324036
  - 0.38993698358535767
  - 0.3871971070766449
  - 0.4112289845943451
  - 0.4585372507572174
  - 0.3925456702709198
  - 0.5743668079376221
  - 0.391279399394989
  - 0.5680438876152039
  - 0.6175130605697632
  - 1.5498522520065308
  - 0.38794150948524475
  - 2.5229294300079346
  - 0.39741432666778564
  - 1.3941184282302856
  - 0.40920335054397583
  - 0.3928072154521942
  - 0.40302714705467224
  - 0.39886578917503357
  - 0.3850856423377991
  - 0.39368560910224915
  - 0.445981502532959
  - 0.41026604175567627
  - 1.0461487770080566
  - 0.380915105342865
  - 0.4535810947418213
  - 0.46113714575767517
  - 0.6345008611679077
  - 0.4270005524158478
  - 0.40432795882225037
  - 0.3817604184150696
  - 0.38723474740982056
  - 3.3340866565704346
  - 0.7017527222633362
  - 3.845599412918091
  - 0.38000792264938354
  - 0.39275985956192017
  - 0.3897775709629059
  - 0.42793047428131104
  - 0.4849879741668701
  - 0.41931432485580444
  - 3.2396812438964844
  - 0.8684084415435791
  - 1.850112795829773
  - 3.6668758392333984
  - 0.5287508964538574
  - 2.7237911224365234
  - 18.68491554260254
  - 1.3381215333938599
  - 0.7592432498931885
  - 0.3961506485939026
  - 0.39583855867385864
  - 0.38890618085861206
  - 0.40079182386398315
  - 0.40948978066444397
  - 0.4306367337703705
  - 0.4765606224536896
  - 0.42312994599342346
  - 0.43343254923820496
  - 0.38477495312690735
  - 0.4664585292339325
  - 0.4775371253490448
  - 0.4701010584831238
  - 0.5227767825126648
  - 0.5143696665763855
  - 0.44836658239364624
  - 0.40173256397247314
  - 0.3907639980316162
  - 0.3871764838695526
  - 0.3893316686153412
loss_records_fold1:
  train_losses:
  - 5.983648663759232
  - 5.850034704804421
  - 5.873141559958459
  - 5.6817168474197395
  - 5.829849186539651
  - 5.7742721974849704
  - 5.99945307970047
  - 5.908059278130532
  - 5.945073425769806
  - 5.891712331771851
  - 5.898359322547913
  - 5.859600535035134
  - 5.93142251521349
  - 5.984661290049553
  - 5.829490581154824
  - 5.907535490393639
  - 6.016937467455865
  - 5.781446993350983
  - 6.152581864595414
  - 5.887539184093476
  - 5.799957063794136
  - 6.097197315096856
  - 5.903719872236252
  - 6.012794578075409
  - 5.860985368490219
  - 5.823517152667046
  - 6.032195818424225
  - 6.146703240275383
  - 5.913299506902695
  - 5.767174315452576
  - 5.8441885858774185
  - 5.8919801175594335
  - 5.817694225907326
  - 5.841265651583672
  - 5.909882518649102
  - 5.953634294867516
  - 5.89970443546772
  - 5.669808313250542
  - 5.872657901048661
  - 5.8581737875938416
  - 5.734123316407204
  - 5.763134521245957
  - 5.822444680333138
  - 5.842811515927315
  - 5.795354720950127
  - 5.969949162006379
  - 5.883270597457886
  - 5.728366205096245
  - 5.860705414414406
  - 5.929324805736542
  - 5.829241010546685
  - 5.921940106153489
  - 5.703051412105561
  - 5.953444114327431
  - 5.855160227417946
  - 5.74581016600132
  - 5.746868717670441
  - 5.986443173885346
  - 5.872101762890816
  - 5.844355341792107
  - 5.769937252998353
  - 5.837662011384964
  - 5.796803766489029
  - 5.830824077129364
  - 5.844380128383637
  - 5.855566713213921
  - 5.859177431464196
  - 5.84595867395401
  - 5.9123607039451604
  - 5.837210658192635
  - 5.724719426035882
  - 5.831735244393349
  - 5.880419859290123
  - 5.831111872196198
  - 5.9789402991533285
  - 5.802677661180496
  - 5.889816188812256
  - 5.8881905823946
  - 5.753849518299103
  - 6.003986063599587
  - 5.801038718223572
  - 5.882778033614159
  - 5.845237430930138
  - 5.7977552145719535
  - 5.74874749481678
  - 5.673029226064682
  - 5.831499108672142
  - 5.651843863725663
  - 5.815380755066872
  - 5.668146660923958
  - 5.812104839086533
  - 5.948061096668244
  - 5.630189248919487
  - 5.836698335409165
  - 5.831548595428467
  - 5.715622648596764
  - 5.896167123317719
  - 5.813391149044037
  - 5.8588891446590425
  - 5.647026440501214
  validation_losses:
  - 0.39321231842041016
  - 0.3828791081905365
  - 0.388134241104126
  - 0.38636112213134766
  - 0.3897853195667267
  - 0.3949999511241913
  - 0.38079673051834106
  - 0.40144869685173035
  - 0.38872000575065613
  - 0.3907419443130493
  - 0.38199880719184875
  - 0.3981349468231201
  - 0.40289369225502014
  - 0.391799658536911
  - 0.39751192927360535
  - 0.3840973973274231
  - 0.40798836946487427
  - 0.3911800980567932
  - 0.40065187215805054
  - 0.3992709517478943
  - 1.2572381496429443
  - 0.3896223306655884
  - 0.3905077576637268
  - 0.4076522886753082
  - 0.38247576355934143
  - 0.5025065541267395
  - 0.9566734433174133
  - 0.4103159010410309
  - 0.44629189372062683
  - 1.088683843612671
  - 0.5229489207267761
  - 0.3923034965991974
  - 0.38126444816589355
  - 0.40735331177711487
  - 0.7436631917953491
  - 0.42086145281791687
  - 1.3932071924209595
  - 0.5849797129631042
  - 0.3894234001636505
  - 0.5971786379814148
  - 0.6220180988311768
  - 0.6600039601325989
  - 0.3966861367225647
  - 0.44876042008399963
  - 0.3783911168575287
  - 0.3968794643878937
  - 0.39396536350250244
  - 0.38964006304740906
  - 0.5529497861862183
  - 0.38325604796409607
  - 0.4589539170265198
  - 0.4193763732910156
  - 1.0759116411209106
  - 0.583297848701477
  - 0.5362390875816345
  - 0.8133546113967896
  - 0.6653395295143127
  - 0.4661157727241516
  - 0.6280745267868042
  - 0.5277964472770691
  - 0.4947913885116577
  - 0.7484515309333801
  - 0.5762112736701965
  - 0.5243300795555115
  - 0.3893355429172516
  - 0.9990261793136597
  - 0.4588930904865265
  - 0.39040425419807434
  - 0.5029975175857544
  - 0.45400023460388184
  - 0.7175272107124329
  - 0.5523366928100586
  - 0.4926908612251282
  - 0.409631609916687
  - 0.6187762022018433
  - 2.740800619125366
  - 0.5918883681297302
  - 0.522193968296051
  - 3.464468240737915
  - 1.0159869194030762
  - 1.4208641052246094
  - 1.1879976987838745
  - 0.5085239410400391
  - 0.5090664029121399
  - 0.5947810411453247
  - 0.5258519649505615
  - 0.6630771160125732
  - 0.8128143548965454
  - 0.6897724270820618
  - 0.7683917284011841
  - 1.0318996906280518
  - 0.8142881393432617
  - 0.63837730884552
  - 0.5697742700576782
  - 0.6778420209884644
  - 0.5991862416267395
  - 0.3936081826686859
  - 0.738900899887085
  - 0.4742957353591919
  - 0.5854422450065613
loss_records_fold2:
  train_losses:
  - 5.670295515656472
  - 5.798197934031487
  - 5.848597487807274
  - 5.925028610229493
  - 5.729660171270371
  - 5.815070274472237
  - 5.693946760892868
  - 5.891622883081436
  - 5.815291064977647
  - 5.845227286219597
  - 5.8599264323711395
  - 6.061875423789025
  - 5.977866223454476
  - 5.96797679066658
  - 6.064693245291711
  - 5.8975045651197435
  - 5.968014000356198
  - 5.936413869261742
  - 6.05497370660305
  - 6.012931221723557
  - 5.921559748053551
  - 5.981885316967965
  - 5.911524471640587
  - 5.981873267889023
  - 5.909628206491471
  - 5.992916777729988
  - 6.016141352057457
  - 6.113157135248184
  - 5.845920419692994
  - 6.057208579778671
  - 5.871808254718781
  - 6.058494532108307
  - 5.866962879896164
  - 5.881263229250909
  - 5.9062042593956
  - 5.99894763827324
  - 5.989249837398529
  - 5.91677725315094
  - 5.902432444691659
  - 5.902905142307282
  - 6.043583634495736
  - 5.887084758281708
  - 5.861171394586563
  - 5.856732168793679
  - 5.856876564025879
  - 5.987308612465859
  - 5.995128467679024
  - 5.9769379019737245
  - 5.938965934515
  - 5.98030249774456
  - 5.909976613521576
  - 6.000765481591225
  - 5.915302416682244
  - 5.899949711561203
  - 5.9848289668560035
  - 5.9056901991367345
  - 5.764598962664604
  - 5.985300919413567
  - 5.877755865454674
  - 5.7996916949749
  - 5.992501285672188
  - 6.323476487398148
  - 6.180236613750458
  - 6.324945878982544
  - 6.259911775588989
  - 6.139382413029671
  - 6.183161583542824
  - 6.217434522509575
  - 6.284126418828965
  - 6.219069567322731
  - 6.327114060521126
  - 6.175787481665612
  - 6.2127488464117055
  - 6.318643248081208
  - 6.261384546756744
  - 6.239435142278672
  - 6.218675145506859
  - 6.200618663430214
  - 6.26434591114521
  - 6.320441371202469
  - 6.216581085324288
  - 6.2246029525995255
  - 6.18797704577446
  - 6.3030476570129395
  - 6.194866138696671
  - 6.206620073318482
  - 6.245872926712036
  - 6.226927381753922
  - 6.157116639614106
  - 6.242847406864167
  - 6.284765738248826
  - 6.280075037479401
  - 6.260355150699616
  - 6.2475687891244895
  - 6.278165504336357
  - 6.2582897573709495
  - 6.259186539053918
  - 6.2184873729944234
  - 6.159149390459061
  - 6.277821552753449
  validation_losses:
  - 0.9356222748756409
  - 0.5299466848373413
  - 0.5986196994781494
  - 0.4915098547935486
  - 0.4637239873409271
  - 0.3899720311164856
  - 0.499956876039505
  - 0.39417243003845215
  - 0.39210179448127747
  - 0.47287699580192566
  - 8.080819129943848
  - 430.84515380859375
  - 25256.6328125
  - 39606.3828125
  - 688.3572387695312
  - 9927.349609375
  - 1062.0693359375
  - 3280.483642578125
  - 128284.7890625
  - 8208.5888671875
  - 5390.86767578125
  - 7794.58349609375
  - 8588.6953125
  - 13963.986328125
  - 14487.1787109375
  - 15319.1943359375
  - 10175.423828125
  - 52591.6015625
  - 33530.37109375
  - 70020.4921875
  - 224700.046875
  - 222530.03125
  - 251561.3125
  - 157898.625
  - 253020.765625
  - 143398.859375
  - 1470359.375
  - 99055.4921875
  - 241463.0
  - 44956.71875
  - 39127.1484375
  - 76714.1328125
  - 113586.421875
  - 12845.6220703125
  - 96370.7890625
  - 39087.078125
  - 1768.2276611328125
  - 3406.90625
  - 985.5242309570312
  - 24064.306640625
  - 86815.890625
  - 31795.16796875
  - 43238.4921875
  - 49851.64453125
  - 10591.42578125
  - 221397.03125
  - 23483.7578125
  - 32588.673828125
  - 34258.6484375
  - 83856.359375
  - 4436360.5
  - 5081358848.0
  - 348621757218816.0
  - 168079535374336.0
  - 51896530239488.0
  - 70310703398912.0
  - 531139278143488.0
  - 459125930590208.0
  - 682214685671424.0
  - 160362989092864.0
  - 71501952843776.0
  - 89118759452672.0
  - 115564869582848.0
  - 364719865069568.0
  - 100265281716224.0
  - 48460174721024.0
  - 212399722332160.0
  - 833424445145088.0
  - 84887855955968.0
  - 114165515550720.0
  - 171557066375168.0
  - 534148204724224.0
  - 429016834113536.0
  - 132534285369344.0
  - 829457573085184.0
  - 778848429932544.0
  - 920518999932928.0
  - 42700954927104.0
  - 552415069732864.0
  - 44141031456768.0
  - 69447796654080.0
  - 346676204142592.0
  - 187621770788864.0
  - 328752030547968.0
  - 131221359165440.0
  - 165141324759040.0
  - 93067637948416.0
  - 831786049339392.0
  - 852582079660032.0
  - 819149248921600.0
loss_records_fold3:
  train_losses:
  - 6.2234397590160375
  - 6.386394268274308
  - 6.213277071714401
  - 6.509611442685127
  - 6.228953391313553
  - 6.112598636746407
  - 6.173378202319146
  - 6.321239411830902
  - 6.238431298732758
  - 6.246457460522652
  - 6.20666732788086
  - 6.131602227687836
  - 6.156016460061074
  - 6.300056076049805
  - 6.21915827691555
  - 6.340988194942475
  - 6.196120244264603
  - 6.275570583343506
  - 6.28793643116951
  - 6.211491724848748
  - 6.1856715261936195
  - 6.207085472345352
  - 6.292429092526437
  - 6.239080095291138
  - 6.224250614643097
  - 6.237970837950707
  validation_losses:
  - 26905600327680.0
  - 7656140.0
  - 106.40498352050781
  - 691.5606689453125
  - 1227.6370849609375
  - 938.6897583007812
  - 9574.9423828125
  - 14725.271484375
  - 21134.181640625
  - 85548848.0
  - 1198513061888.0
  - 332166194003968.0
  - 106154847895552.0
  - 3286.08154296875
  - 160.0822296142578
  - 4.565691947937012
  - 725.998779296875
  - 2289.076904296875
  - 720.7667236328125
  - 1013.9400024414062
  - 50.45537567138672
  - 25.314943313598633
  - 20.36859130859375
  - 0.40351060032844543
  - 0.40841570496559143
  - 0.4081103801727295
loss_records_fold4:
  train_losses:
  - 6.183700549602509
  - 6.223886075615884
  - 6.331889972090721
  - 6.250423619151116
  - 6.153175035119057
  - 6.239096361398698
  - 6.276088327169418
  - 6.209884384274483
  - 6.278301569819451
  - 6.252451398968697
  - 6.180323851108551
  validation_losses:
  - 0.41611891984939575
  - 0.413093239068985
  - 0.411111980676651
  - 0.4115959107875824
  - 0.4155223071575165
  - 0.40910083055496216
  - 0.40707430243492126
  - 0.4087134897708893
  - 0.40822315216064453
  - 0.4072905480861664
  - 0.4160694181919098
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 96 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8421955403087479, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8551832269396945
  mean_f1_accuracy: 0.0
  total_train_time: '0:31:47.009472'
