config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:25:32.641894'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_58fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_58fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_58fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_58fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_58fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_58fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_58fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_58fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_58fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_58fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 31.465019392967225
  - 10.309349310398103
  - 6.572469234466553
  - 8.522682285308838
  - 3.266714978218079
  - 2.073959875106812
  - 2.349892371892929
  - 3.020880287885666
  - 2.4369814693927765
  - 2.9736498415470125
  - 2.206621664762497
  - 1.9070892632007599
  - 2.097170913219452
  - 1.7993642389774323
  - 1.5416120558977129
  - 1.6447792470455171
  - 4.891586804389954
  - 2.1615043371915816
  - 2.559941047430039
  - 2.1357396721839907
  - 1.640985977649689
  - 2.7994257271289826
  - 3.3155525088310243
  - 11.909398543834687
  - 2.056437957286835
  - 2.307213667035103
  - 2.0266439080238343
  - 2.1549472332000734
  - 1.8594967365264894
  - 1.7713873386383057
  - 1.6903442829847337
  - 2.4859148383140566
  - 1.7738624811172485
  - 1.6802154779434204
  - 1.8971943855285645
  - 1.9694464087486268
  - 3.329031151533127
  - 2.6072870790958405
  - 2.608303189277649
  - 1.9949484527111054
  - 2.22873837351799
  - 1.7310808181762696
  - 1.9599959433078766
  - 2.2775855839252475
  - 1.9217139899730684
  - 2.622138512134552
  - 1.7784873247146606
  - 1.520769715309143
  - 1.6883194208145142
  - 1.6686035692691803
  - 1.8156549453735353
  - 1.5204629957675935
  - 1.5361362457275392
  - 1.5072916567325594
  - 1.5976014375686647
  - 1.6056628584861756
  - 1.658106505870819
  - 1.6382084488868713
  - 1.600021767616272
  - 1.5112757802009584
  - 1.4404538720846176
  - 1.8732927918434144
  - 1.6798310816287996
  - 2.06178275346756
  - 1.7113525867462158
  - 1.5095226228237153
  - 1.5673292577266693
  - 1.47396297454834
  - 1.7073986530303955
  - 2.5446965754032136
  - 3.3265115439891817
  - 3.941306757926941
  - 2.26486508846283
  - 2.2592431902885437
  - 1.6097042620182038
  - 3.0159488379955293
  - 1.568542790412903
  - 1.4789136826992035
  validation_losses:
  - 7.6494059562683105
  - 1.3390740156173706
  - 5.448815822601318
  - 0.5453551411628723
  - 0.4878130257129669
  - 0.4587043523788452
  - 0.7277042865753174
  - 0.6046719551086426
  - 0.4752647578716278
  - 0.5567664504051208
  - 0.3978478014469147
  - 0.4534071981906891
  - 0.3931000530719757
  - 0.4339505732059479
  - 0.3974418044090271
  - 0.7651724815368652
  - 0.47006672620773315
  - 1.0223511457443237
  - 0.6660560369491577
  - 0.512850821018219
  - 0.39572766423225403
  - 0.48042237758636475
  - 0.6064541935920715
  - 0.4120192527770996
  - 0.3857367932796478
  - 0.3902302086353302
  - 0.3924727141857147
  - 0.3916124403476715
  - 0.4321896433830261
  - 0.4055113196372986
  - 0.3882802426815033
  - 0.6249025464057922
  - 0.4103539288043976
  - 0.4341381788253784
  - 0.38871172070503235
  - 0.61334228515625
  - 0.41506242752075195
  - 0.4137725532054901
  - 0.7712559700012207
  - 0.49384382367134094
  - 0.42581796646118164
  - 0.3788547217845917
  - 0.46254244446754456
  - 0.4682296812534332
  - 0.48630207777023315
  - 0.3804776668548584
  - 0.43682336807250977
  - 0.45164117217063904
  - 0.3847894072532654
  - 0.43900683522224426
  - 0.4396321475505829
  - 0.3843422532081604
  - 0.40249910950660706
  - 0.37917035818099976
  - 0.3909887969493866
  - 0.3789222836494446
  - 0.4263758957386017
  - 0.3803742825984955
  - 0.38511139154434204
  - 0.40997788310050964
  - 0.3773571848869324
  - 0.4535906910896301
  - 0.38460013270378113
  - 0.37311068177223206
  - 0.39856240153312683
  - 0.3870028555393219
  - 0.3844812214374542
  - 0.4765842854976654
  - 0.3901013731956482
  - 0.3793673813343048
  - 0.3912871778011322
  - 0.46454110741615295
  - 0.418582946062088
  - 0.40173280239105225
  - 0.4107440114021301
  - 0.3819698691368103
  - 0.38715752959251404
  - 0.38060131669044495
loss_records_fold1:
  train_losses:
  - 1.5232830107212068
  - 1.4823722422122956
  - 1.6115650773048402
  - 1.7061119318008424
  - 2.0793841183185577
  - 3.728529632091522
  - 1.702035701274872
  - 1.947186130285263
  - 1.764932030439377
  - 1.5376895070075989
  - 1.4562971234321596
  - 1.4811187088489532
  - 1.5373678237199784
  - 1.630039232969284
  - 1.5310438930988313
  - 1.499176636338234
  - 1.8988535761833192
  - 1.6771211564540864
  - 1.8252273976802826
  - 1.5804395973682404
  - 2.801271098852158
  - 4.615335828065873
  - 1.9494177579879761
  - 1.6104828000068665
  - 1.6635253369808198
  - 1.5072483360767366
  - 1.4983217835426332
  - 1.4530401229858398
  - 1.4937937200069429
  - 1.8182765424251557
  - 1.612614780664444
  - 1.4951162159442903
  - 1.5232660248875618
  - 2.38208549618721
  - 2.292248207330704
  - 2.3702934980392456
  - 1.7610745549201967
  - 1.5970740139484407
  - 1.5445450723171235
  - 1.5770970284938812
  - 1.5312560439109804
  - 1.6176579236984254
  - 1.4719247996807099
  - 1.5747842431068422
  - 1.4887829422950745
  - 3.2495047181844714
  - 1.546152251958847
  - 1.49457246363163
  - 1.5406169176101685
  - 2.424502903223038
  - 2.788657546043396
  - 1.6549635320901872
  - 1.7066119074821473
  - 1.5489823639392855
  - 1.4903437972068787
  - 1.4374688029289246
  - 2.166668474674225
  - 1.5265875279903414
  - 1.7637142539024353
  - 1.52985777258873
  - 1.5214918851852417
  - 1.4569839239120483
  - 1.537639406323433
  - 1.4311235010623933
  - 1.5117514193058015
  - 1.6039648413658143
  - 1.4784181714057922
  - 1.4346745610237122
  - 1.4185505986213685
  - 1.446847689151764
  - 1.5019756615161897
  - 1.5858128368854523
  - 1.529718452692032
  - 1.5657145440578462
  validation_losses:
  - 0.42200997471809387
  - 0.39972159266471863
  - 0.4175777733325958
  - 0.4876583516597748
  - 0.5290239453315735
  - 0.8088573813438416
  - 0.3964090347290039
  - 0.4025331437587738
  - 0.4020855128765106
  - 0.40058302879333496
  - 0.4267745614051819
  - 0.424747109413147
  - 0.4149596095085144
  - 0.39203551411628723
  - 0.41339924931526184
  - 0.39843469858169556
  - 0.40038806200027466
  - 0.3910951018333435
  - 0.39215001463890076
  - 0.4324710965156555
  - 0.4530509412288666
  - 0.41289740800857544
  - 0.41257065534591675
  - 0.463670551776886
  - 0.40233147144317627
  - 0.4257868230342865
  - 0.3966464102268219
  - 0.4133244752883911
  - 0.42913350462913513
  - 0.4223938584327698
  - 0.39970266819000244
  - 0.4066067039966583
  - 0.4212087392807007
  - 0.4539059102535248
  - 0.4010615050792694
  - 0.49103644490242004
  - 0.47625261545181274
  - 0.4237659275531769
  - 0.4036118686199188
  - 0.44490501284599304
  - 0.4208775758743286
  - 0.40420612692832947
  - 0.417490154504776
  - 0.39398708939552307
  - 0.39235639572143555
  - 0.4223730266094208
  - 0.39976373314857483
  - 0.3937186598777771
  - 0.3920590877532959
  - 0.40880507230758667
  - 0.3962768614292145
  - 0.4166686534881592
  - 0.3978988230228424
  - 0.39298638701438904
  - 0.3942430913448334
  - 0.394693523645401
  - 0.3908080756664276
  - 0.4102560579776764
  - 0.5419931411743164
  - 0.450325608253479
  - 0.3933868706226349
  - 0.39110279083251953
  - 0.39847713708877563
  - 0.41486799716949463
  - 0.41018590331077576
  - 0.4094865620136261
  - 0.3920271098613739
  - 0.4057804048061371
  - 0.40650326013565063
  - 0.40028101205825806
  - 0.39163634181022644
  - 0.3950410485267639
  - 0.3985685408115387
  - 0.3946494162082672
loss_records_fold2:
  train_losses:
  - 1.4631339311599731
  - 1.476787292957306
  - 1.9922649621963502
  - 2.8110910952091217
  - 2.107587504386902
  - 1.665792739391327
  - 2.0050065159797668
  - 1.5108747571706773
  - 1.4750612020492555
  - 1.8325216799974442
  - 1.4620266139507294
  validation_losses:
  - 0.40998923778533936
  - 0.374889075756073
  - 0.3916225731372833
  - 0.3991439640522003
  - 0.4047620892524719
  - 0.3727192282676697
  - 0.3816000819206238
  - 0.3743930757045746
  - 0.37243539094924927
  - 0.37863075733184814
  - 0.3746742606163025
loss_records_fold3:
  train_losses:
  - 1.4647540271282198
  - 1.4660880386829378
  - 1.687281596660614
  - 1.5423035442829134
  - 1.7481761515140535
  - 1.824145954847336
  - 1.4712174832820892
  - 1.4519571840763092
  - 1.563208442926407
  - 2.1103927433490752
  - 1.4823125898838043
  validation_losses:
  - 0.41247329115867615
  - 0.37916484475135803
  - 0.3939772844314575
  - 0.3767041862010956
  - 0.39920774102211
  - 0.3832184374332428
  - 0.3895764648914337
  - 0.3908366858959198
  - 0.3989799916744232
  - 0.38631120324134827
  - 0.3899144232273102
loss_records_fold4:
  train_losses:
  - 1.435000503063202
  - 1.4469000577926636
  - 1.5168044865131378
  - 1.5350355744361879
  - 1.4589740276336671
  - 1.437313336133957
  - 1.5422419309616089
  - 1.4775834143161775
  - 1.4256073027849199
  - 1.4537575900554658
  - 1.4902679085731507
  - 1.445213931798935
  - 1.4318144381046296
  - 1.6074074804782867
  - 1.5105337023735048
  - 1.4617596805095674
  - 1.5946891129016878
  - 1.4767191231250765
  - 1.56569681763649
  - 1.5232290029525757
  - 1.4978348910808563
  - 1.5827528774738313
  - 1.4909698307514192
  validation_losses:
  - 0.3778626322746277
  - 0.37720078229904175
  - 0.4002744257450104
  - 0.4033867418766022
  - 0.3837953209877014
  - 0.3809008300304413
  - 0.39026665687561035
  - 0.38008755445480347
  - 0.37154310941696167
  - 0.38372719287872314
  - 0.3919547200202942
  - 0.3854561448097229
  - 0.3770451247692108
  - 0.4905320107936859
  - 0.4019143283367157
  - 0.37336266040802
  - 0.3921475410461426
  - 0.39708659052848816
  - 0.3871428072452545
  - 0.3795281946659088
  - 0.3799010217189789
  - 0.3780152201652527
  - 0.3851300776004791
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 78 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 74 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:17:21.385332'
