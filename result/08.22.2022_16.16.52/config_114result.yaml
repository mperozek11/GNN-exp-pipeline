config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:46:46.355973'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_114fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_114fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_114fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_114fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_114fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_114fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_114fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_114fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_114fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_114fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.7398835480213166
  - 1.5792636930942536
  - 1.5431398630142212
  - 1.4885098218917847
  - 1.6230735123157503
  - 1.5554654061794282
  - 1.538974189758301
  - 1.4413299083709719
  - 1.4589703023433687
  - 1.4447105884552003
  - 1.463920521736145
  - 1.5734863460063935
  - 1.457844990491867
  - 1.7121340572834016
  - 1.5056795060634613
  - 1.475662809610367
  - 1.4414635062217713
  - 1.4202304363250733
  - 1.4670148491859436
  - 1.4420087516307831
  - 1.4956640958786012
  - 1.4751204252243042
  - 1.4614215254783631
  - 1.4492713928222658
  - 1.4998208045959474
  - 1.4334585130214692
  - 1.4520808517932893
  - 1.5322821497917176
  - 1.5193670213222505
  - 1.475337040424347
  - 1.4530943870544435
  - 1.5167048573493958
  - 1.4348862439394
  - 1.4445942759513857
  - 1.4720728158950807
  - 1.4673147320747377
  - 1.4424877703189851
  - 1.4799114763736725
  - 1.4149420976638796
  - 1.4211832642555238
  - 1.4412371456623079
  - 1.449055701494217
  - 1.4566619127988816
  - 1.467757296562195
  - 1.44523845911026
  - 1.5215895771980286
  - 1.531207823753357
  - 1.4740069806575775
  - 1.4467673480510712
  - 1.4444137990474701
  - 1.4912813186645508
  validation_losses:
  - 0.4207104444503784
  - 0.4109526574611664
  - 0.40388405323028564
  - 0.38881054520606995
  - 0.38747894763946533
  - 0.47517630457878113
  - 0.43327510356903076
  - 0.38161447644233704
  - 0.38846123218536377
  - 0.38213270902633667
  - 0.47233644127845764
  - 0.4233534038066864
  - 0.3821544647216797
  - 0.39635995030403137
  - 0.39205849170684814
  - 0.3792896270751953
  - 0.3925013840198517
  - 0.3782297968864441
  - 0.3961726427078247
  - 0.40890565514564514
  - 0.38273921608924866
  - 0.3743537366390228
  - 0.39695706963539124
  - 0.4160144329071045
  - 0.42619138956069946
  - 0.38592755794525146
  - 0.4427659511566162
  - 0.3913017809391022
  - 0.3822917342185974
  - 0.39617446064949036
  - 0.3819604218006134
  - 0.38486430048942566
  - 0.399581640958786
  - 0.39734122157096863
  - 0.385093629360199
  - 0.4229271113872528
  - 0.41366079449653625
  - 0.39908567070961
  - 0.47782015800476074
  - 0.575985312461853
  - 0.49773114919662476
  - 0.6414592266082764
  - 0.375627726316452
  - 0.41508206725120544
  - 1.3710588216781616
  - 0.38138601183891296
  - 0.3864719569683075
  - 0.3800746500492096
  - 0.3785286247730255
  - 0.38128501176834106
  - 0.3867962658405304
loss_records_fold1:
  train_losses:
  - 1.5488419711589814
  - 1.4677915275096893
  - 1.4265442371368409
  - 1.4717734217643739
  - 1.5598634779453278
  - 1.4521000564098359
  - 1.4169826209545135
  - 1.442415851354599
  - 1.4335611283779146
  - 1.4090836226940155
  - 1.4257364451885224
  - 1.4016275882720948
  - 1.4005805373191835
  - 1.4022293806076052
  - 1.435341364145279
  - 1.3980790078639984
  - 1.387989866733551
  - 1.4074728071689606
  - 1.4725542187690737
  - 1.412156081199646
  - 1.398904949426651
  - 1.4251138389110567
  - 1.3866651237010956
  - 1.3990137159824372
  - 1.4409836292266847
  - 1.4151479721069338
  - 1.4412381291389467
  - 1.409044998884201
  - 1.4582631289958954
  - 1.4076256036758423
  - 1.4234075546264648
  - 1.390231004357338
  - 1.424477207660675
  - 1.3687370002269745
  - 1.4558899641036989
  - 1.4126438558101655
  - 1.4088757395744325
  - 1.4290452122688295
  - 1.43404158949852
  - 1.4285035014152527
  - 1.4655435264110566
  - 1.393919149041176
  - 1.448760449886322
  - 1.445555418729782
  - 1.4362783789634705
  - 1.480495971441269
  - 1.4263407230377199
  - 1.4252460598945618
  - 1.4210562765598298
  - 1.4240842461585999
  - 1.4199508905410767
  - 1.4469139814376832
  - 1.4153006792068483
  - 1.387960484623909
  - 1.4408238410949707
  - 1.3886953949928285
  - 1.4184665858745575
  - 1.3895968139171602
  - 1.3938899874687196
  - 1.4118336200714112
  - 1.4354934036731721
  - 1.434436869621277
  - 1.4023552119731904
  - 1.3854286432266236
  - 1.4149048805236817
  - 1.4214519619941712
  - 1.3781221568584443
  - 1.3943222522735597
  - 1.3747352480888368
  - 1.3914626479148866
  - 1.3975070953369142
  - 1.3806607127189636
  - 1.3607873916625977
  - 1.3781105041503907
  - 1.3894517064094545
  - 1.415213054418564
  - 1.3910439550876619
  - 1.3696722447872163
  - 1.4576395928859711
  - 1.4238027930259705
  - 1.3993665635585786
  - 1.430550879240036
  - 1.4667537093162537
  - 1.4375625610351563
  - 1.4243857264518738
  - 1.378719684481621
  - 1.3901185512542726
  - 1.38755264878273
  - 1.3873299539089203
  - 1.3989147365093233
  - 1.3981375813484194
  - 1.3906237661838532
  - 1.381772154569626
  - 1.3619278788566591
  - 1.3921227902173996
  - 1.3758029639720917
  - 1.4194890201091768
  - 1.3402419418096543
  - 1.386688804626465
  - 1.37112517952919
  validation_losses:
  - 0.4023599922657013
  - 0.4007861316204071
  - 0.3999340236186981
  - 0.38524484634399414
  - 0.3858489990234375
  - 0.40614405274391174
  - 0.3934690058231354
  - 0.3943314254283905
  - 0.39333343505859375
  - 0.43083456158638
  - 0.40744373202323914
  - 0.4026558995246887
  - 0.4364479184150696
  - 0.41565003991127014
  - 0.5002608299255371
  - 0.4043915867805481
  - 0.44284215569496155
  - 0.393176406621933
  - 0.44703322649002075
  - 0.48047855496406555
  - 0.39710792899131775
  - 0.4039462208747864
  - 0.390694797039032
  - 0.3888610303401947
  - 0.3948293626308441
  - 0.4166167676448822
  - 0.4173888862133026
  - 0.4140176475048065
  - 0.40147891640663147
  - 0.4423447549343109
  - 0.4611472189426422
  - 0.45013490319252014
  - 0.4188462197780609
  - 0.420678973197937
  - 0.47342291474342346
  - 0.45285606384277344
  - 0.4374716281890869
  - 0.5779008865356445
  - 0.4924561083316803
  - 0.4790312349796295
  - 0.44710594415664673
  - 0.3935476541519165
  - 0.40142062306404114
  - 0.47567981481552124
  - 0.44552913308143616
  - 0.4092085361480713
  - 0.4165787994861603
  - 0.4082804024219513
  - 0.3935878872871399
  - 0.4047701060771942
  - 0.5259277820587158
  - 0.41535040736198425
  - 0.42230886220932007
  - 0.4225183427333832
  - 0.49146944284439087
  - 0.4562622904777527
  - 0.4534927010536194
  - 0.5220438838005066
  - 0.5206876993179321
  - 0.497700572013855
  - 0.4083389341831207
  - 0.4506503641605377
  - 0.5540437698364258
  - 0.42164909839630127
  - 0.47448596358299255
  - 0.5089127421379089
  - 0.4764055907726288
  - 0.4277530610561371
  - 0.39591315388679504
  - 0.4085550010204315
  - 0.415717750787735
  - 0.4611920714378357
  - 0.5716400146484375
  - 0.45534825325012207
  - 0.532232940196991
  - 0.5371165871620178
  - 0.5117774605751038
  - 0.5909170508384705
  - 0.38513168692588806
  - 0.3825805187225342
  - 0.39536556601524353
  - 0.4164257347583771
  - 0.5670433044433594
  - 0.5201476216316223
  - 0.4853336811065674
  - 0.6659352779388428
  - 0.6692864298820496
  - 0.5653218626976013
  - 0.6344339847564697
  - 0.5201750993728638
  - 0.8367326855659485
  - 1.1139259338378906
  - 0.9220129251480103
  - 0.5032098293304443
  - 0.8497241139411926
  - 0.9226274490356445
  - 0.643286406993866
  - 0.4303906261920929
  - 0.6557087898254395
  - 0.674054741859436
loss_records_fold2:
  train_losses:
  - 1.4402849555015564
  - 1.3810047119855882
  - 1.4038434326648712
  - 1.3874397158622742
  - 1.3944593727588654
  - 1.3735697805881502
  - 1.385354632139206
  - 1.3883841156959535
  - 1.4187583386898042
  - 1.389548981189728
  - 1.358481338620186
  - 1.3684129476547242
  - 1.3340739399194719
  - 1.3906439423561097
  - 1.3953071296215058
  - 1.373014110326767
  - 1.3753320634365083
  - 1.3800544440746307
  - 1.3602083325386047
  - 1.3604080855846405
  - 1.3547804683446885
  - 1.354625391960144
  - 1.4313029766082765
  - 1.4209255874156952
  - 1.3668098628520966
  - 1.3415482819080353
  - 1.360275995731354
  - 1.370185649394989
  - 1.3314588367938995
  - 1.3692475020885468
  - 1.3679613828659059
  - 1.3799819350242615
  - 1.4466138362884522
  - 1.3425987035036089
  - 1.3596575856208801
  - 1.3711649179458618
  - 1.3911212205886843
  - 1.3981588184833527
  - 1.3871838212013246
  - 1.3681713104248048
  - 1.434066039323807
  - 1.4025191366672516
  - 1.4656148850917816
  - 1.3854594886302949
  - 1.3749182969331741
  - 1.349061444401741
  - 1.330972257256508
  - 1.3739230871200563
  - 1.40638045668602
  - 1.3191617608070374
  - 1.3637167751789094
  - 1.3670360863208773
  - 1.3676724493503571
  - 1.3613157153129578
  - 1.3613644599914552
  - 1.3060976058244707
  - 1.3345463037490846
  - 1.4361142694950104
  - 1.4528135716915132
  - 1.3359808653593064
  - 1.3503454506397248
  - 1.359681236743927
  - 1.3721141278743745
  - 1.351094663143158
  - 1.394280904531479
  - 1.3653478145599367
  - 1.3668252766132356
  - 1.3939328789710999
  - 1.3331253826618195
  - 1.344643270969391
  - 1.410867863893509
  - 1.3537306725978853
  - 1.3660746276378632
  - 1.351954135298729
  - 1.391793751716614
  - 1.317991989850998
  - 1.4014306187629701
  - 1.3234156489372255
  - 1.332579791545868
  - 1.3485660374164583
  - 1.357020276784897
  - 1.3430486857891084
  - 1.3379869997501375
  - 1.316557538509369
  - 1.3205934703350068
  - 1.3908906370401384
  - 1.3445997267961503
  - 1.4208994567394257
  - 1.363926535844803
  - 1.3144115418195725
  - 1.392161723971367
  - 1.3759678125381472
  - 1.3919235408306123
  - 1.4382022023200989
  - 1.390280005335808
  - 1.412663894891739
  - 1.3603251844644548
  - 1.3814243227243423
  - 1.4133195221424104
  - 1.3963924884796144
  validation_losses:
  - 0.4784981608390808
  - 0.474261611700058
  - 0.3893337845802307
  - 0.3943384289741516
  - 0.4236362874507904
  - 0.635297417640686
  - 0.8438413143157959
  - 0.43759486079216003
  - 0.49661722779273987
  - 0.578947901725769
  - 0.46542787551879883
  - 0.4363307058811188
  - 0.4518525004386902
  - 0.5698376893997192
  - 0.9014620780944824
  - 0.7460249066352844
  - 0.5062291622161865
  - 0.5424558520317078
  - 0.6081485152244568
  - 0.4489500820636749
  - 0.38025960326194763
  - 0.6540386080741882
  - 0.5967947840690613
  - 1.0116283893585205
  - 0.8625766634941101
  - 1.0372779369354248
  - 1.1416254043579102
  - 0.6149914860725403
  - 0.4578670263290405
  - 0.44500863552093506
  - 0.5255507230758667
  - 0.4981866478919983
  - 0.41443532705307007
  - 0.5646560788154602
  - 0.5502305626869202
  - 0.49501559138298035
  - 0.41935616731643677
  - 0.4523555338382721
  - 0.4127737581729889
  - 0.4341046214103699
  - 0.4821649193763733
  - 0.5319041013717651
  - 0.5017765164375305
  - 0.4378696382045746
  - 0.4264916479587555
  - 0.39498642086982727
  - 0.6045958995819092
  - 0.4001981019973755
  - 0.42224159836769104
  - 0.5651466846466064
  - 0.6004845499992371
  - 0.5248295068740845
  - 0.7068250179290771
  - 0.47908347845077515
  - 0.600706934928894
  - 0.5509233474731445
  - 0.7436814904212952
  - 0.6698111295700073
  - 0.8705740571022034
  - 1.9194343090057373
  - 0.48399868607521057
  - 0.7336508631706238
  - 0.737829864025116
  - 0.48146647214889526
  - 0.5659162998199463
  - 0.48336097598075867
  - 0.49732714891433716
  - 0.4194778501987457
  - 0.49455615878105164
  - 0.586065948009491
  - 0.44406962394714355
  - 0.5623016357421875
  - 0.38701581954956055
  - 0.5445700287818909
  - 0.7584210634231567
  - 0.8399123549461365
  - 0.4253285527229309
  - 0.4416828453540802
  - 0.5595347881317139
  - 0.4218258261680603
  - 0.4665561616420746
  - 0.5014939904212952
  - 0.431365966796875
  - 0.7123252749443054
  - 0.8491945862770081
  - 0.5863487720489502
  - 0.4052302837371826
  - 0.4113970696926117
  - 0.451138973236084
  - 0.46924060583114624
  - 0.5731586813926697
  - 0.47491633892059326
  - 0.5137981176376343
  - 0.429534912109375
  - 0.3899643123149872
  - 0.3855783939361572
  - 0.3958282768726349
  - 0.4007062017917633
  - 0.37649694085121155
  - 0.40649157762527466
loss_records_fold3:
  train_losses:
  - 1.406500369310379
  - 1.3954416513442993
  - 1.411229819059372
  - 1.3897971153259279
  - 1.4567164540290833
  - 1.4118094444274902
  - 1.410894113779068
  - 1.3952922761440278
  - 1.429040324687958
  - 1.4376726269721987
  - 1.4386283278465273
  - 1.4264508038759232
  - 1.3881958425045013
  - 1.4360591530799867
  - 1.4350824236869812
  - 1.4148226141929627
  - 1.364233660697937
  - 1.3824987053871156
  - 1.4037557005882264
  - 1.3949845910072327
  - 1.4133969008922578
  - 1.351379320025444
  - 1.3455963790416718
  - 1.4691314995288849
  - 1.4502554059028627
  - 1.393992465734482
  - 1.3779725253582003
  - 1.4025759041309358
  - 1.3786801457405091
  - 1.3593484401702882
  - 1.3282756924629213
  - 1.4178877651691437
  - 1.3561311006546022
  - 1.4063236534595491
  - 1.3545683443546297
  - 1.3551314294338228
  - 1.4116061210632325
  - 1.3653157591819765
  - 1.3845196902751924
  - 1.3754265069961549
  - 1.3553937673568726
  - 1.3132959187030793
  - 1.3305148839950562
  - 1.3474180340766908
  - 1.3244891047477723
  - 1.3484136641025544
  - 1.335358050465584
  - 1.3404163956642152
  - 1.3159708946943285
  - 1.3504983842372895
  - 1.3494212210178376
  - 1.3685725390911103
  - 1.3469992756843567
  - 1.3658382952213288
  - 1.3187687397003174
  - 1.348391765356064
  - 1.3151289522647858
  - 1.316143697500229
  - 1.2829854488372803
  - 1.3359328448772432
  - 1.3054794013500215
  - 1.2956862777471543
  - 1.3166081428527834
  - 1.3514567762613297
  - 1.3260332465171816
  - 1.3291191875934603
  - 1.3633016347885132
  - 1.3566500425338746
  - 1.401257187128067
  - 1.3736786246299744
  - 1.3243298053741457
  - 1.3720494091510773
  - 1.3928121030330658
  - 1.3153373897075653
  - 1.329883271455765
  - 1.3400830268859865
  - 1.2961232453584672
  - 1.3665844917297365
  - 1.3005625277757646
  - 1.2915335536003114
  - 1.2860999256372452
  - 1.3187129378318787
  - 1.3426026701927185
  - 1.3257147669792175
  - 1.2804112017154694
  - 1.328526106476784
  - 1.3120809376239777
  - 1.3353503763675691
  - 1.2897036492824556
  - 1.305707722902298
  - 1.2978534698486328
  - 1.3093246281147004
  - 1.2826824963092804
  - 1.2965069979429247
  - 1.295573878288269
  - 1.286933845281601
  - 1.2703317165374757
  - 1.3237925469875336
  - 1.2953503012657166
  - 1.3127597868442535
  validation_losses:
  - 0.3825734853744507
  - 0.3834471106529236
  - 0.35985085368156433
  - 0.4070170223712921
  - 0.40282344818115234
  - 0.4004248082637787
  - 0.3871534466743469
  - 0.46463245153427124
  - 0.4175927937030792
  - 0.4033946990966797
  - 0.3884809911251068
  - 0.4141124188899994
  - 0.3710094690322876
  - 0.3685925304889679
  - 0.3947741389274597
  - 0.3836486041545868
  - 0.3801631033420563
  - 0.39482542872428894
  - 0.4038320779800415
  - 0.411857545375824
  - 0.403401643037796
  - 0.41356712579727173
  - 0.4008375406265259
  - 0.3879379332065582
  - 0.42058295011520386
  - 0.41442903876304626
  - 0.4663691818714142
  - 0.3941837251186371
  - 0.44344115257263184
  - 0.4572545289993286
  - 0.39959973096847534
  - 0.40122556686401367
  - 0.41126200556755066
  - 0.4052865505218506
  - 0.3924747407436371
  - 0.4073827266693115
  - 0.42318207025527954
  - 0.4801011383533478
  - 0.4338834881782532
  - 0.4427787661552429
  - 0.5389906167984009
  - 0.43069925904273987
  - 0.5141619443893433
  - 0.49336278438568115
  - 0.3908185064792633
  - 0.46485278010368347
  - 0.49202093482017517
  - 0.46055150032043457
  - 0.4681258797645569
  - 0.45138803124427795
  - 0.409000039100647
  - 0.4048660099506378
  - 0.4473220705986023
  - 0.42364761233329773
  - 0.49520209431648254
  - 0.45236819982528687
  - 0.4543440341949463
  - 0.4694505035877228
  - 0.47323185205459595
  - 0.4238866865634918
  - 0.45036664605140686
  - 0.4264187812805176
  - 0.5038953423500061
  - 0.46268993616104126
  - 0.5060893893241882
  - 0.471856951713562
  - 0.5346475839614868
  - 0.5471575856208801
  - 0.5167117118835449
  - 0.4748474061489105
  - 0.4500388205051422
  - 0.4763905107975006
  - 0.4264885485172272
  - 0.44556373357772827
  - 0.5061081647872925
  - 0.4715218245983124
  - 0.5045889616012573
  - 0.4434157609939575
  - 0.4192739427089691
  - 0.4944183826446533
  - 0.5138730406761169
  - 0.5436098575592041
  - 0.5238537192344666
  - 0.44158801436424255
  - 0.48976048827171326
  - 0.5907970666885376
  - 0.4508262872695923
  - 0.4639340043067932
  - 0.43939346075057983
  - 0.5591495633125305
  - 0.4976142644882202
  - 0.49420273303985596
  - 0.4342852830886841
  - 0.47493210434913635
  - 0.4365179240703583
  - 0.45425739884376526
  - 0.4474497437477112
  - 0.470755010843277
  - 0.4641909599304199
  - 0.45590269565582275
loss_records_fold4:
  train_losses:
  - 1.314120051264763
  - 1.374907374382019
  - 1.2951021432876588
  - 1.3717352271080019
  - 1.324516659975052
  - 1.2780255228281021
  - 1.2848097026348115
  - 1.273779356479645
  - 1.3111432135105134
  - 1.2916499078273773
  - 1.3870876789093018
  - 1.3187356770038605
  - 1.2925953686237337
  - 1.3055261254310608
  - 1.3471481144428255
  - 1.345336726307869
  - 1.3042504787445068
  - 1.2949426412582399
  - 1.270913216471672
  - 1.341158711910248
  - 1.3184287607669831
  - 1.2861404806375505
  - 1.3070793837308885
  - 1.3883548080921173
  - 1.307208561897278
  - 1.2842305481433869
  - 1.2740270793437958
  - 1.2781522154808045
  - 1.2900118470191957
  - 1.276053062081337
  - 1.267267793416977
  - 1.2961125135421754
  - 1.2820401191711426
  - 1.3402330607175827
  - 1.3329968571662905
  - 1.3528505265712738
  - 1.2987729907035828
  - 1.266574913263321
  - 1.2581315755844118
  - 1.3106884479522707
  - 1.2782270133495333
  - 1.2641230165958406
  - 1.2639681339263917
  - 1.3178213715553284
  - 1.328810340166092
  - 1.2973933041095735
  - 1.281634345650673
  - 1.2760644078254701
  - 1.2704011231660843
  - 1.3048810184001924
  - 1.2486251950263978
  - 1.236358880996704
  - 1.2540998369455338
  - 1.2394610315561296
  - 1.2694061517715456
  - 1.2625010073184968
  - 1.2800747096538545
  - 1.2730550438165666
  - 1.2997305631637575
  - 1.2511747062206269
  - 1.247791686654091
  - 1.2468294262886048
  - 1.3106950968503952
  - 1.2569046437740328
  - 1.2756851017475128
  - 1.2628231048583984
  - 1.3068702757358552
  - 1.269467604160309
  - 1.2828687846660616
  - 1.3091955959796906
  - 1.2519756495952608
  - 1.2420470952987672
  - 1.2306334167718889
  - 1.2458432704210283
  - 1.3709717214107515
  - 1.2886903285980225
  - 1.2799468725919725
  - 1.2781642735004426
  - 1.2808010280132294
  - 1.2383648455142975
  - 1.238695451617241
  - 1.2494032591581345
  - 1.2765155494213105
  - 1.3134657919406891
  - 1.2361182421445847
  - 1.2219619929790497
  - 1.3343732833862305
  - 1.285830295085907
  - 1.2672496885061264
  - 1.249470126628876
  - 1.2737522959709169
  - 1.3022245883941652
  - 1.2549415707588196
  - 1.2672376811504364
  - 1.2435234367847443
  - 1.2541133850812913
  - 1.278681865334511
  - 1.2241331458091738
  - 1.269725352525711
  - 1.3185187727212906
  validation_losses:
  - 0.38013210892677307
  - 0.3994113504886627
  - 0.4284416735172272
  - 0.4084838926792145
  - 0.4087916314601898
  - 0.37348538637161255
  - 0.4044356346130371
  - 0.46046411991119385
  - 0.3449707329273224
  - 0.4380320608615875
  - 0.39913326501846313
  - 0.3555000126361847
  - 0.44222649931907654
  - 0.4516127109527588
  - 0.47349071502685547
  - 0.4525558650493622
  - 0.3689073920249939
  - 0.4946398138999939
  - 0.35837700963020325
  - 0.522968053817749
  - 0.4724312126636505
  - 0.5159205794334412
  - 0.49937182664871216
  - 0.3991258144378662
  - 0.4889879822731018
  - 0.5074754357337952
  - 0.38937821984291077
  - 0.4095042049884796
  - 0.3716318607330322
  - 0.42320263385772705
  - 0.44568106532096863
  - 0.3948226571083069
  - 0.33778080344200134
  - 0.46910640597343445
  - 0.35282695293426514
  - 0.3846273720264435
  - 0.4522632360458374
  - 0.3556472957134247
  - 0.4869922697544098
  - 0.3529776930809021
  - 0.46496304869651794
  - 0.42518872022628784
  - 0.42498302459716797
  - 0.4789576232433319
  - 0.562241792678833
  - 0.42794784903526306
  - 0.45003676414489746
  - 0.49756720662117004
  - 0.4825499355792999
  - 0.5619141459465027
  - 0.5169668197631836
  - 0.5137009620666504
  - 0.5339252352714539
  - 0.5369750261306763
  - 0.5047202706336975
  - 0.533634603023529
  - 0.44137048721313477
  - 0.5123509764671326
  - 0.4610311985015869
  - 0.4145631492137909
  - 0.4248109459877014
  - 0.4265037178993225
  - 0.40222153067588806
  - 0.3472307324409485
  - 0.42647451162338257
  - 0.4223821759223938
  - 0.5051925182342529
  - 0.4322238266468048
  - 0.39754611253738403
  - 0.6053797006607056
  - 0.3627176880836487
  - 0.5729748606681824
  - 0.5096827745437622
  - 0.5250496864318848
  - 0.35309314727783203
  - 0.4196658432483673
  - 0.43508270382881165
  - 0.41442957520484924
  - 0.37496545910835266
  - 0.45593827962875366
  - 0.5102778077125549
  - 0.4616852402687073
  - 0.467751145362854
  - 0.46703243255615234
  - 0.4230234920978546
  - 0.35276657342910767
  - 0.4161059260368347
  - 0.47358080744743347
  - 0.45679447054862976
  - 0.46784088015556335
  - 0.4240413308143616
  - 0.35436707735061646
  - 0.4646439850330353
  - 0.4594355523586273
  - 0.4224224388599396
  - 0.4607192277908325
  - 0.5331767201423645
  - 0.535880446434021
  - 0.35976895689964294
  - 0.481625497341156
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8524871355060034, 0.8421955403087479, 0.8593481989708405, 0.8319039451114922,
    0.8333333333333334]'
  fold_eval_f1: '[0.0, 0.14814814814814814, 0.0, 0.27941176470588236, 0.2706766917293233]'
  mean_eval_accuracy: 0.8438536306460834
  mean_f1_accuracy: 0.13964732091667076
  total_train_time: '0:37:59.171725'
