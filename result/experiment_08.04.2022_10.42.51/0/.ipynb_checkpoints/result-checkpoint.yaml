config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 10:42:51.421470'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/0/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/0/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/0/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/0/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/0/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/0/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/0/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/0/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/0/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/0/fold_4_state_dict.pt
loss_records:
  train_losses:
  - 13.611195370554924
  - 13.924429804086685
  - 13.882113754749298
  - 14.10884042084217
  - 13.82047986984253
  - 13.758984804153442
  - 13.790410593152046
  - 13.735304832458496
  - 13.801674485206604
  - 13.767425924539566
  - 13.792943984270096
  - 13.872132986783981
  - 13.707087695598602
  - 14.10282352566719
  - 13.687832772731781
  - 13.69765618443489
  - 13.709145992994308
  - 13.742030948400497
  - 13.822670042514801
  - 13.694306164979935
  - 13.54765409231186
  - 13.73969042301178
  - 13.609423846006393
  - 13.670784205198288
  - 13.61637794971466
  - 14.050848424434662
  - 13.718420624732971
  - 13.75520108640194
  - 13.806256473064423
  - 13.871634051203728
  - 13.403943821787834
  - 13.567848712205887
  - 13.518288671970367
  - 13.748496115207672
  - 13.63187712430954
  - 13.607350498437881
  - 13.516928419470787
  - 13.538888558745384
  - 13.49512603878975
  - 13.288286671042442
  - 13.75816947221756
  - 13.543584942817688
  - 13.57154543697834
  - 13.849884301424026
  - 13.58755648136139
  - 13.599134147167206
  - 13.4414681494236
  - 13.506174579262733
  - 13.663043692708015
  - 13.50300008058548
  - 13.77227646112442
  - 13.449726060032845
  - 13.59029307961464
  - 13.543029770255089
  - 13.45989328622818
  - 13.680570274591446
  - 13.410329341888428
  - 13.320167139172554
  - 13.262782901525497
  - 13.361517161130905
  - 13.308962136507034
  - 13.474324181675911
  - 13.7723947763443
  - 13.820465758442879
  - 13.711753219366074
  - 13.404174953699112
  - 13.477857872843742
  - 13.51935438811779
  - 13.335004031658173
  - 13.551130563020706
  - 13.57895402610302
  - 13.306906253099442
  - 13.39480522274971
  - 13.68408678472042
  - 13.569284781813622
  - 13.479966551065445
  - 13.413672506809235
  - 13.367052972316742
  - 13.375894322991371
  - 13.259089559316635
  - 13.309730619192123
  - 13.263156712055206
  - 13.3340083360672
  - 13.394297927618027
  - 13.997119829058647
  - 14.01552438735962
  - 13.448815137147903
  - 13.44258177280426
  - 13.494423151016235
  - 13.314177721738815
  - 13.44528529047966
  - 13.609907373785973
  - 13.465984344482422
  - 13.303152963519096
  - 13.38274671137333
  - 13.331195160746574
  - 13.361808955669403
  - 13.410715252161026
  - 13.48685821890831
  - 13.527984201908112
  validation_losses:
  - 0.38685691356658936
  - 0.365721732378006
  - 0.3802410066127777
  - 0.3660898208618164
  - 0.36763691902160645
  - 0.36781951785087585
  - 0.3785760998725891
  - 0.39308640360832214
  - 0.38757073879241943
  - 0.5302898287773132
  - 0.3739212453365326
  - 0.37059760093688965
  - 0.3848564922809601
  - 0.36768975853919983
  - 0.36914125084877014
  - 0.3802119493484497
  - 0.3747289180755615
  - 0.38556045293807983
  - 0.37869229912757874
  - 0.3858821988105774
  - 0.4134952425956726
  - 0.41596078872680664
  - 0.36416810750961304
  - 0.37009990215301514
  - 0.36759626865386963
  - 0.4638977646827698
  - 0.37588363885879517
  - 0.4021143615245819
  - 0.5324394106864929
  - 0.39819666743278503
  - 0.4391182065010071
  - 0.4478958547115326
  - 0.4492891728878021
  - 0.42431899905204773
  - 0.3955335021018982
  - 0.46187201142311096
  - 0.3767252266407013
  - 1.1843457221984863
  - 2.403991222381592
  - 1.021729826927185
  - 1.7554503679275513
  - 0.7858444452285767
  - 3.186826467514038
  - 1.260723352432251
  - 1.359180212020874
  - 1.5964164733886719
  - 1.6084964275360107
  - 3.915771007537842
  - 1.979729175567627
  - 2.0934462547302246
  - 2.572417736053467
  - 0.7715444564819336
  - 0.84937584400177
  - 1.5643128156661987
  - 0.788071870803833
  - 2.6880929470062256
  - 3.2039756774902344
  - 1.7519948482513428
  - 1.837112545967102
  - 0.4446110129356384
  - 1.1220587491989136
  - 0.39460256695747375
  - 0.39905330538749695
  - 6.644268989562988
  - 0.6040393710136414
  - 0.615210235118866
  - 0.5093945860862732
  - 0.878853976726532
  - 0.8336490392684937
  - 0.6280684471130371
  - 1.2090305089950562
  - 1.0275765657424927
  - 1.2949163913726807
  - 0.5857503414154053
  - 0.5458194613456726
  - 1.6658680438995361
  - 1.136488437652588
  - 0.9841870069503784
  - 1.3261033296585083
  - 0.9973391890525818
  - 3.353477954864502
  - 1.3193844556808472
  - 5.405997276306152
  - 2.778679132461548
  - 0.8308997750282288
  - 0.4689960777759552
  - 0.5566840767860413
  - 2.4443230628967285
  - 2.822695732116699
  - 1.1223050355911255
  - 2.28066349029541
  - 1.4097644090652466
  - 3.0370564460754395
  - 2.7783048152923584
  - 2.4918034076690674
  - 2.9498064517974854
  - 0.6011613011360168
  - 0.7082599997520447
  - 1.2897684574127197
  - 2.6686325073242188
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 53 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8490566037735849, 0.8576329331046312,
    0.8075601374570447]'
  fold_eval_f1: '[0.0, 0.0, 0.04347826086956522, 0.0, 0.25333333333333335]'
  mean_eval_accuracy: 0.8455600549356628
  mean_f1_accuracy: 0.05936231884057972
  total_train_time: '0:06:47.256705'
