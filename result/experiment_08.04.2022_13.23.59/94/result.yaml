config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 00:10:03.591000'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/94/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/94/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/94/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/94/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/94/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/94/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/94/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/94/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/94/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/94/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 124.2119870185852
  - 62.756915807724
  - 68.82619214057922
  - 24.911894142627716
  - 20.879446506500244
  - 27.38707798719406
  - 22.112264931201935
  - 18.10563564300537
  - 14.590174034237862
  - 9.188012778759003
  - 11.101033955812454
  - 13.43732863664627
  - 10.184544444084167
  - 8.431466549634933
  - 6.237918496131897
  - 8.681533008813858
  - 13.934994488954544
  - 11.029698371887207
  - 6.159975826740265
  - 5.630298733711243
  - 5.6251314878463745
  - 5.3221405148506165
  - 17.251533210277557
  - 15.98243060708046
  - 26.482623398303986
  - 5.05406978726387
  - 6.656758785247803
  - 6.657874137163162
  - 6.879160821437836
  - 4.796593368053436
  - 5.556915313005447
  - 26.756347209215164
  - 4.781363695859909
  - 5.796886920928955
  - 5.0039368867874146
  - 5.110254853963852
  - 15.212081015110016
  - 8.913540184497833
  - 9.184282690286636
  - 5.124957740306854
  - 4.953603893518448
  - 5.3178931176662445
  - 12.128114759922028
  - 4.414555191993713
  - 11.108402401208878
  - 6.424501180648804
  - 4.5282785296440125
  - 4.2268295884132385
  - 4.496825665235519
  - 4.896300911903381
  - 5.195873379707336
  - 4.460191130638123
  - 5.128671854734421
  - 4.452506363391876
  - 4.483278125524521
  - 4.3492092192173
  - 4.359577059745789
  - 4.845404922962189
  - 4.462300807237625
  - 5.091898500919342
  - 4.215734302997589
  - 5.810537755489349
  - 4.283365607261658
  - 6.2402893006801605
  - 4.90897473692894
  - 5.327695935964584
  - 6.037169396877289
  - 5.347071617841721
  - 8.005398511886597
  - 4.491105407476425
  - 5.05745530128479
  - 4.395474880933762
  - 10.207216024398804
  - 5.994101196527481
  - 5.662786960601807
  - 6.216107800602913
  - 8.985085517168045
  - 4.441080033779144
  - 4.469311058521271
  - 4.862859979271889
  - 4.75119936466217
  - 4.156579673290253
  - 4.713672041893005
  - 4.350253254175186
  - 4.64245143532753
  - 4.847393244504929
  - 4.8518621027469635
  - 4.78701388835907
  - 4.7090375274419785
  - 4.373634964227676
  - 4.409011334180832
  - 4.058880150318146
  - 13.754620254039764
  - 9.048016905784607
  - 5.703058689832687
  - 5.386687994003296
  - 7.2214515209198
  - 7.054210782051086
  - 5.985303550958633
  - 5.141198366880417
  validation_losses:
  - 5.177006244659424
  - 2.3894498348236084
  - 2.3375511169433594
  - 1.5793055295944214
  - 3.711642026901245
  - 4.37847900390625
  - 1.6893720626831055
  - 1.8368583917617798
  - 1.1777806282043457
  - 0.8201158046722412
  - 2.2302236557006836
  - 0.8801363110542297
  - 0.6397143006324768
  - 0.6878694295883179
  - 0.6832634210586548
  - 0.5465245246887207
  - 1.3390755653381348
  - 0.5492541193962097
  - 0.4662012457847595
  - 0.4814712405204773
  - 0.73136967420578
  - 0.42186516523361206
  - 0.509772539138794
  - 1.5167427062988281
  - 0.5784631967544556
  - 0.528745174407959
  - 0.46072933077812195
  - 0.40521714091300964
  - 0.46465909481048584
  - 0.4536951184272766
  - 0.42794284224510193
  - 0.5377556681632996
  - 0.46144452691078186
  - 0.4036165177822113
  - 0.4828706383705139
  - 0.44706055521965027
  - 0.4760385751724243
  - 0.4319913387298584
  - 0.48131054639816284
  - 0.48356160521507263
  - 0.5443286299705505
  - 0.4986273944377899
  - 0.43712976574897766
  - 0.4310814142227173
  - 0.4763428270816803
  - 0.40729352831840515
  - 0.3795408606529236
  - 0.3954237997531891
  - 0.38454318046569824
  - 0.4130890369415283
  - 0.43829232454299927
  - 0.420345276594162
  - 0.4711565673351288
  - 0.4097047448158264
  - 0.42347973585128784
  - 0.3982941806316376
  - 0.42015373706817627
  - 0.39184898138046265
  - 0.4180515706539154
  - 0.43454521894454956
  - 0.4067130386829376
  - 0.4031924307346344
  - 0.49523136019706726
  - 0.4040348529815674
  - 0.3979373574256897
  - 0.4151941239833832
  - 0.3886970579624176
  - 0.5152121782302856
  - 0.4050939679145813
  - 0.449072927236557
  - 0.4608745276927948
  - 0.4511587619781494
  - 0.4011748731136322
  - 0.5726891160011292
  - 0.389614999294281
  - 0.45354190468788147
  - 0.39210745692253113
  - 0.3906462490558624
  - 0.4099160432815552
  - 0.42765697836875916
  - 0.43506917357444763
  - 0.3900262713432312
  - 0.4791252613067627
  - 0.5478999018669128
  - 0.49543216824531555
  - 0.5668944716453552
  - 0.49138420820236206
  - 0.4508415758609772
  - 0.4029850363731384
  - 0.3957081139087677
  - 0.4059310257434845
  - 0.39615023136138916
  - 0.6417585611343384
  - 0.4193808436393738
  - 0.4987216293811798
  - 0.6222692728042603
  - 0.44248640537261963
  - 0.49912229180336
  - 0.4401489794254303
  - 0.42703425884246826
loss_records_fold1:
  train_losses:
  - 10.327182561159134
  - 5.00753527879715
  - 4.537130385637283
  - 4.884533613920212
  - 4.580770522356033
  - 4.161809921264648
  - 4.476523339748383
  - 5.754737168550491
  - 4.29490701854229
  - 4.4003021121025085
  - 4.068827360868454
  - 14.501551181077957
  - 19.905608892440796
  - 54.474285542964935
  - 6.062493830919266
  - 9.6305271089077
  - 7.276275217533112
  - 4.934459239244461
  - 4.8957692086696625
  - 5.7814344465732574
  - 4.716423183679581
  - 4.622391849756241
  - 4.794604197144508
  - 46.593494921922684
  - 5.489980548620224
  - 7.236999675631523
  - 4.617498472332954
  - 21.02970051765442
  - 14.114972084760666
  - 6.389874458312988
  - 14.157285571098328
  - 5.223827242851257
  - 4.209846198558807
  - 4.443941444158554
  - 5.320137083530426
  - 14.98923197388649
  - 7.039648860692978
  - 5.470549315214157
  - 7.130874037742615
  - 5.997239917516708
  - 4.625790573656559
  - 5.307071879506111
  - 5.657310396432877
  - 4.935784637928009
  - 5.720671407878399
  - 4.608968034386635
  - 4.390187114477158
  - 4.405775040388107
  - 4.379113405942917
  - 9.024391651153564
  - 4.422759771347046
  - 4.769098609685898
  - 5.419438928365707
  - 4.046426087617874
  - 5.382260799407959
  - 4.418749541044235
  - 4.554383218288422
  - 5.367779910564423
  - 4.5365259647369385
  - 7.49999412894249
  - 5.158400684595108
  - 4.2988748252391815
  - 4.674608260393143
  - 4.771448582410812
  - 4.460901975631714
  - 4.6710688173770905
  - 4.239726200699806
  - 4.33205834031105
  - 4.5602138340473175
  - 4.374904125928879
  - 4.184681326150894
  - 4.1480326652526855
  - 4.523325324058533
  - 14.834993869066238
  - 5.851911008358002
  - 5.407083809375763
  - 5.325164288282394
  - 4.223560005426407
  - 5.8784103989601135
  - 4.537804871797562
  - 4.2358235120773315
  - 4.565508604049683
  - 4.299175173044205
  - 4.432549864053726
  - 4.34881529211998
  - 4.295499354600906
  - 7.772735387086868
  - 6.074509710073471
  - 4.702366352081299
  - 4.827932238578796
  - 4.287276059389114
  - 5.470201134681702
  - 4.979500412940979
  - 4.46751344203949
  - 4.661894589662552
  - 5.3426653444767
  - 4.369706898927689
  - 4.427830457687378
  - 4.362450331449509
  - 5.437149226665497
  validation_losses:
  - 0.4530540406703949
  - 0.557117223739624
  - 0.4807562530040741
  - 0.4486534297466278
  - 0.434139221906662
  - 0.46189868450164795
  - 0.40076690912246704
  - 0.4159378111362457
  - 0.4423621594905853
  - 0.4057270288467407
  - 0.4167540669441223
  - 0.7239813208580017
  - 0.5016180276870728
  - 0.5551000833511353
  - 1.692116141319275
  - 1.0648901462554932
  - 0.4470908045768738
  - 0.41185253858566284
  - 0.440475195646286
  - 0.4564220607280731
  - 0.4622347354888916
  - 0.40097349882125854
  - 0.4127669036388397
  - 0.3993295431137085
  - 0.40622714161872864
  - 0.42621049284935
  - 0.39822518825531006
  - 0.43877625465393066
  - 0.46271997690200806
  - 0.5367358922958374
  - 0.40925049781799316
  - 0.42371809482574463
  - 0.430442750453949
  - 0.40859970450401306
  - 0.40349745750427246
  - 0.5491990447044373
  - 0.757030189037323
  - 0.5015726089477539
  - 0.649790346622467
  - 0.6388652324676514
  - 0.48854246735572815
  - 0.45014119148254395
  - 0.4174806773662567
  - 0.5331776142120361
  - 0.5213297605514526
  - 0.6016088724136353
  - 0.4435798227787018
  - 0.4458164870738983
  - 0.41929399967193604
  - 0.4297945201396942
  - 0.47633984684944153
  - 0.42126527428627014
  - 0.4194149076938629
  - 0.4463232457637787
  - 0.42141932249069214
  - 0.4099658727645874
  - 0.41898950934410095
  - 0.40249499678611755
  - 0.43715956807136536
  - 0.40918609499931335
  - 0.40270501375198364
  - 0.42652183771133423
  - 0.4144647419452667
  - 0.4107670187950134
  - 0.44454047083854675
  - 0.4176500141620636
  - 0.44657230377197266
  - 0.4101848900318146
  - 0.42175018787384033
  - 0.4372331500053406
  - 0.4442991018295288
  - 0.4668358266353607
  - 0.4012669026851654
  - 0.4099406898021698
  - 0.5060965418815613
  - 0.4168994426727295
  - 0.40631788969039917
  - 0.42839884757995605
  - 0.40307706594467163
  - 0.4268154501914978
  - 0.3997895419597626
  - 0.40415775775909424
  - 0.4055142104625702
  - 0.40664026141166687
  - 0.43380388617515564
  - 0.4101729989051819
  - 0.4069611430168152
  - 0.541803240776062
  - 0.41382500529289246
  - 0.40973973274230957
  - 0.4100220501422882
  - 0.42779842019081116
  - 0.4823402166366577
  - 0.43203797936439514
  - 0.4234570264816284
  - 0.4075198769569397
  - 0.4100339412689209
  - 0.4274832308292389
  - 0.41338568925857544
  - 0.4531034529209137
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 73 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:02:03.562453'
