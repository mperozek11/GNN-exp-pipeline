config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 20:36:29.467677'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/66/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/66/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/66/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/66/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/66/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/66/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/66/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/66/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/66/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/66/fold_4_state_dict.pt
loss_records_fold3:
  train_losses:
  - 14.292145878076553
  - 14.254339307546616
  - 14.136921063065529
  - 14.43367624282837
  - 14.340896874666214
  - 14.21186289191246
  - 14.32895340025425
  - 14.495643764734268
  - 14.12803703546524
  - 14.156686753034592
  - 14.19201123714447
  - 14.261707797646523
  - 14.08487954735756
  - 14.119161307811737
  - 14.889675214886665
  - 14.491875246167183
  - 14.339101672172546
  - 14.173247143626213
  - 14.177889227867126
  - 14.141082301735878
  - 14.156814098358154
  - 14.526824668049812
  - 14.407587677240372
  - 14.395991742610931
  - 14.457891404628754
  - 14.251896411180496
  - 14.205951914191246
  - 14.063388466835022
  - 14.528448536992073
  - 14.168809741735458
  - 14.226717337965965
  - 14.172144025564194
  - 14.176715150475502
  - 14.142131447792053
  - 14.020028531551361
  - 14.086104184389114
  - 14.319605514407158
  - 14.029390588402748
  - 14.274739891290665
  - 14.126949429512024
  - 14.059472724795341
  - 14.058754652738571
  - 14.135519251227379
  - 14.16140691936016
  - 14.429965019226074
  - 14.096764340996742
  - 14.175391137599945
  - 14.030707061290741
  - 14.17161051928997
  - 14.063730493187904
  - 14.02887199819088
  - 14.104203537106514
  - 13.944566264748573
  - 13.804155319929123
  - 13.919663041830063
  - 13.97034852206707
  - 14.115872234106064
  - 14.106101006269455
  - 14.004233852028847
  - 14.07184374332428
  - 13.97023344039917
  - 13.99086844921112
  - 13.766992583870888
  - 13.957630515098572
  - 13.997758567333221
  - 13.981761872768402
  - 13.992276817560196
  - 14.064213544130325
  - 13.854324474930763
  - 13.926754832267761
  - 13.75772026181221
  - 14.326006442308426
  - 13.848075151443481
  - 13.603584080934525
  - 13.931705862283707
  - 14.068130791187286
  - 14.02778884768486
  - 13.834632217884064
  - 13.86112329363823
  - 13.694939687848091
  - 13.915962144732475
  - 13.887411892414093
  - 13.583668202161789
  - 13.837149232625961
  - 13.9542196393013
  - 13.650031924247742
  - 13.778576582670212
  - 13.681085288524628
  - 13.780983835458755
  - 13.851655632257462
  - 14.15037116408348
  - 14.048433646559715
  - 14.115341037511826
  - 13.977098971605301
  - 14.293024808168411
  - 14.175194501876831
  - 13.803937211632729
  - 13.829530537128448
  - 13.710900962352753
  - 14.220650508999825
  validation_losses:
  - 0.40363550186157227
  - 0.38808906078338623
  - 0.3811326324939728
  - 0.37689903378486633
  - 0.3738933801651001
  - 0.37814199924468994
  - 0.3838764727115631
  - 0.4058661162853241
  - 0.378231406211853
  - 0.47277015447616577
  - 0.41847872734069824
  - 0.6207438111305237
  - 0.4503107964992523
  - 0.3876984119415283
  - 0.41738051176071167
  - 0.38141441345214844
  - 0.5364026427268982
  - 0.5237034559249878
  - 0.38018977642059326
  - 0.5175079703330994
  - 0.5437704920768738
  - 0.4058467447757721
  - 0.3897247016429901
  - 0.4102163016796112
  - 0.41990116238594055
  - 0.4834655523300171
  - 0.45123130083084106
  - 1.4325698614120483
  - 0.46467718482017517
  - 0.4700295627117157
  - 0.4235365390777588
  - 0.6275733113288879
  - 0.40152883529663086
  - 0.4021827280521393
  - 0.40343788266181946
  - 1.1696513891220093
  - 1.8098406791687012
  - 0.6211106181144714
  - 0.6944543719291687
  - 0.5197995901107788
  - 2.233409881591797
  - 0.5700829029083252
  - 0.4309707283973694
  - 0.8487024307250977
  - 0.6265221238136292
  - 0.43025079369544983
  - 0.46410584449768066
  - 0.44710659980773926
  - 0.7361321449279785
  - 0.7890437841415405
  - 0.46660566329956055
  - 0.4545946717262268
  - 0.5566680431365967
  - 0.531315267086029
  - 0.8454681634902954
  - 0.5944353342056274
  - 0.7528784871101379
  - 0.6037171483039856
  - 0.4316723346710205
  - 1.6295396089553833
  - 1.207256555557251
  - 0.4360666275024414
  - 2.105156660079956
  - 2.118886709213257
  - 0.868544340133667
  - 0.5399013161659241
  - 0.8117572665214539
  - 0.6870104670524597
  - 0.5170958042144775
  - 0.7055854201316833
  - 0.49240076541900635
  - 1.1531497240066528
  - 0.4354000687599182
  - 0.49861446022987366
  - 0.5206373929977417
  - 0.5175445079803467
  - 0.463188499212265
  - 0.7082738280296326
  - 0.8290501832962036
  - 0.8188400864601135
  - 0.8672453761100769
  - 0.9011094570159912
  - 1.1609641313552856
  - 0.7779950499534607
  - 0.6928823590278625
  - 0.9831786155700684
  - 0.9626590609550476
  - 0.592461109161377
  - 0.5234045386314392
  - 0.4569947123527527
  - 0.4350103735923767
  - 0.538680911064148
  - 0.4927830994129181
  - 0.48241910338401794
  - 0.5683296918869019
  - 0.5258445739746094
  - 0.48421505093574524
  - 0.4832178056240082
  - 0.47169917821884155
  - 0.5219855308532715
loss_records_fold4:
  train_losses:
  - 13.875756278634071
  - 13.880716666579247
  - 13.711799398064613
  - 13.679561346769333
  - 13.809674710035324
  - 13.624638065695763
  - 13.867544636130333
  - 13.667008891701698
  - 13.706275656819344
  - 13.757213816046715
  - 13.822261199355125
  - 14.296196162700653
  - 14.00812742114067
  - 14.031808465719223
  - 13.621196776628494
  - 13.72544801235199
  - 13.67053109407425
  - 13.964397460222244
  - 13.870368123054504
  - 13.785172834992409
  - 13.648412615060806
  - 14.238547563552856
  - 13.743444710969925
  - 13.920488387346268
  - 13.664155185222626
  - 13.737418070435524
  - 13.70705533027649
  - 13.759223207831383
  - 13.56838446855545
  - 13.65647055208683
  - 13.693982571363449
  - 13.749233081936836
  - 13.416046842932701
  - 13.526030912995338
  - 13.59599469602108
  - 13.595572471618652
  - 13.466627210378647
  - 13.556806147098541
  - 13.44418503344059
  - 15.009535640478134
  - 14.115272879600525
  - 13.893593221902847
  - 13.873589545488358
  - 13.649734914302826
  - 13.649937942624092
  - 13.93592931330204
  - 13.841217294335365
  - 13.589567765593529
  - 13.648532509803772
  - 13.658553168177605
  - 13.681178718805313
  - 13.400086089968681
  - 13.585496664047241
  - 13.566108971834183
  - 13.926901191473007
  - 13.416593998670578
  - 13.561750963330269
  - 13.44347070157528
  - 13.390867114067078
  - 13.573110461235046
  - 13.526200994849205
  - 13.570277065038681
  - 13.706931367516518
  - 13.588765427470207
  - 13.7874066978693
  - 13.765508338809013
  - 13.346128582954407
  - 13.47725211083889
  - 13.375222340226173
  - 13.34238637983799
  - 13.47091943025589
  - 13.393251657485962
  - 13.373588413000107
  - 13.416329026222229
  - 13.135312393307686
  - 13.497739627957344
  - 13.245832830667496
  - 13.223485350608826
  - 13.416816234588623
  - 13.266743123531342
  - 14.638416096568108
  - 14.561096370220184
  - 14.101454198360443
  - 14.021298348903656
  - 13.660587280988693
  - 13.669374793767929
  - 13.727088809013367
  - 13.62329651415348
  - 13.643185243010521
  - 13.529664188623428
  - 13.405373454093933
  - 13.467614695429802
  - 13.46682032942772
  - 13.902494430541992
  - 13.620420157909393
  - 13.32271508872509
  - 13.401639118790627
  - 13.488125026226044
  - 13.424699246883392
  - 13.348553150892258
  validation_losses:
  - 0.4541884660720825
  - 0.4706915020942688
  - 0.49188604950904846
  - 0.6092519760131836
  - 0.5015682578086853
  - 0.5237738490104675
  - 0.4680144786834717
  - 0.505326509475708
  - 0.37712177634239197
  - 0.3836601674556732
  - 0.383301705121994
  - 0.4143863618373871
  - 0.4192761480808258
  - 0.3773047924041748
  - 0.43662846088409424
  - 0.3810432255268097
  - 0.4556298553943634
  - 0.5161675214767456
  - 0.3975188434123993
  - 0.5458137392997742
  - 0.4691798686981201
  - 0.40556374192237854
  - 0.39077621698379517
  - 0.4173392653465271
  - 0.3940236270427704
  - 0.48259708285331726
  - 0.5324164032936096
  - 0.5330172777175903
  - 0.5204328298568726
  - 0.5936254262924194
  - 0.4458484947681427
  - 0.5676762461662292
  - 0.708511233329773
  - 0.9376521706581116
  - 2.2453207969665527
  - 1.2423301935195923
  - 1.4649262428283691
  - 1.5764988660812378
  - 1.65229070186615
  - 0.44477859139442444
  - 0.4310712218284607
  - 0.4252752661705017
  - 0.46720242500305176
  - 0.5213848948478699
  - 0.551518976688385
  - 0.5695849657058716
  - 0.5115769505500793
  - 0.5453174114227295
  - 0.4445914030075073
  - 0.5852925181388855
  - 0.5605854392051697
  - 0.5206820368766785
  - 0.5154639482498169
  - 0.5395068526268005
  - 0.5046994090080261
  - 0.5491408705711365
  - 0.5470368266105652
  - 0.5943855047225952
  - 0.6427395343780518
  - 0.5869954228401184
  - 0.8308027386665344
  - 0.5100436210632324
  - 0.6052289009094238
  - 0.5040743947029114
  - 0.5967328548431396
  - 0.5253338813781738
  - 0.8189120888710022
  - 0.7063432931900024
  - 0.6679570078849792
  - 0.9858987927436829
  - 0.8419432640075684
  - 0.963565468788147
  - 0.7394416332244873
  - 0.6584542393684387
  - 0.7513896822929382
  - 1.1914275884628296
  - 1.3159886598587036
  - 0.9035238027572632
  - 0.8030350804328918
  - 2.723944902420044
  - 0.6565820574760437
  - 0.67929607629776
  - 0.8726592659950256
  - 1.0608688592910767
  - 1.0235446691513062
  - 1.406978726387024
  - 0.39379438757896423
  - 1.311239242553711
  - 0.8858877420425415
  - 1.237825870513916
  - 1.1781446933746338
  - 1.2679405212402344
  - 1.2510406970977783
  - 0.926720380783081
  - 0.7899494767189026
  - 0.7674604654312134
  - 1.3999578952789307
  - 2.4273769855499268
  - 1.6902579069137573
  - 1.0906398296356201
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8113207547169812,
    0.8367697594501718]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.1911764705882353, 0.14414414414414417]'
  mean_eval_accuracy: 0.8445409158694511
  mean_f1_accuracy: 0.0670641229464759
  total_train_time: '0:06:34.812407'
