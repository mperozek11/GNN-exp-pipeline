config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:59:33.143701'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_26fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_26fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_26fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_26fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_26fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_26fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_26fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_26fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_26fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_26fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 29.737025594711305
  - 8.722045332193375
  - 4.611709856987
  - 3.1167052865028384
  - 3.9825712561607363
  - 3.057483190298081
  - 2.6084411919116977
  - 3.579286366701126
  - 3.89436993598938
  - 3.9069676995277405
  - 2.3910345137119293
  - 1.6848459124565125
  - 1.8866201281547548
  - 2.088690662384033
  - 4.022788602113724
  - 3.3821460604667664
  - 2.8932523369789127
  - 1.7060761332511902
  - 2.098675984144211
  - 3.3325300693511966
  - 1.6867707073688507
  - 1.8393119752407074
  - 2.448605662584305
  - 2.153901219367981
  - 4.691952341794968
  - 2.130921959877014
  - 6.086449205875397
  - 1.9387049555778504
  - 1.8747997105121614
  - 1.4542522609233857
  - 1.5195851892232897
  - 1.8297190189361574
  - 2.0756079077720644
  - 1.9216479420661927
  - 1.6748255252838136
  - 1.5740292370319366
  - 2.7056523621082307
  - 2.9280034959316255
  - 2.882648712396622
  - 2.0898697435855866
  - 2.7035681903362274
  - 2.710321354866028
  - 3.6398513674736024
  - 3.6309128761291505
  - 2.645215791463852
  - 1.8618570148944855
  - 1.6220455050468445
  - 1.7638907104730608
  - 1.730242842435837
  - 2.184535562992096
  - 1.93740473985672
  - 2.08553147315979
  - 1.5675133854150773
  - 1.5715061128139496
  - 1.5737379103899003
  - 1.7634760737419128
  - 2.401327955722809
  - 2.224348145723343
  - 1.7184397161006928
  - 2.536933034658432
  - 1.9538046777248383
  - 1.6479471862316133
  - 1.4431030720472338
  - 2.7134325683116916
  - 1.7203868627548218
  - 2.6092141568660736
  - 1.8335261344909668
  - 1.6203512370586397
  - 1.757995492219925
  - 1.6688388168811799
  - 2.0641035735607147
  - 1.77197984457016
  - 1.6983935415744782
  - 3.253218799829483
  - 1.920361465215683
  - 1.6593852698802949
  - 1.532457834482193
  - 1.4706990838050844
  - 1.4984065234661104
  - 1.5692274391651155
  - 1.4470359086990356
  - 1.4415777117013933
  - 1.6521358609199526
  - 1.8001377165317536
  - 1.573749452829361
  - 1.7932411015033722
  - 2.4081233203411103
  - 1.545878267288208
  - 1.5984915375709534
  - 1.6056675106287004
  - 1.4862265586853027
  - 1.875086098909378
  - 1.5797943353652955
  - 1.6005418479442597
  - 1.5564086973667146
  - 1.5635446906089783
  - 1.4619154155254366
  - 1.433674931526184
  - 1.4836710751056672
  - 1.474149352312088
  validation_losses:
  - 2.3171534538269043
  - 2.9818687438964844
  - 0.7417560815811157
  - 1.3352508544921875
  - 1.0358213186264038
  - 0.5758070945739746
  - 0.8896170258522034
  - 0.4870782494544983
  - 1.2999095916748047
  - 0.6933190226554871
  - 0.42355337738990784
  - 0.4341053068637848
  - 0.38837137818336487
  - 0.682599663734436
  - 0.4244193732738495
  - 0.6011567115783691
  - 0.4419183135032654
  - 0.3928373456001282
  - 0.4404950737953186
  - 0.5084678530693054
  - 0.4180499017238617
  - 0.5638977289199829
  - 0.4185510575771332
  - 0.3936750888824463
  - 0.4140583276748657
  - 0.4685412645339966
  - 0.8422686457633972
  - 0.38593021035194397
  - 0.3797566890716553
  - 0.3901316523551941
  - 0.39739447832107544
  - 0.4477125108242035
  - 0.9277138710021973
  - 0.49475809931755066
  - 0.5073563456535339
  - 0.3819677531719208
  - 0.45578625798225403
  - 0.3965494632720947
  - 0.4234459102153778
  - 0.7945217490196228
  - 0.6477681398391724
  - 0.4296601414680481
  - 0.7055752873420715
  - 0.4154283106327057
  - 0.4243971109390259
  - 0.3810994327068329
  - 0.3768133819103241
  - 0.42350924015045166
  - 0.6084710955619812
  - 0.4026353359222412
  - 0.3937182128429413
  - 0.37582486867904663
  - 0.4297787547111511
  - 0.3759836256504059
  - 0.7563126087188721
  - 0.4962863028049469
  - 0.3757316470146179
  - 0.381303995847702
  - 0.3917005658149719
  - 0.41710951924324036
  - 0.3817218840122223
  - 0.3853929936885834
  - 0.5054206252098083
  - 0.37166813015937805
  - 0.3773361146450043
  - 0.37307628989219666
  - 0.3797414302825928
  - 0.3739563226699829
  - 0.6206071376800537
  - 0.5538003444671631
  - 0.43857601284980774
  - 0.4017958641052246
  - 0.42467471957206726
  - 0.5295208096504211
  - 0.4593811631202698
  - 0.4461478888988495
  - 0.3838757574558258
  - 0.37488019466400146
  - 0.41637974977493286
  - 0.40178000926971436
  - 0.3713209927082062
  - 0.395579993724823
  - 0.45004206895828247
  - 0.4215555489063263
  - 0.4319981038570404
  - 0.39418327808380127
  - 0.3713107407093048
  - 0.37085065245628357
  - 0.3770049810409546
  - 0.39837077260017395
  - 0.3795343041419983
  - 0.408504456281662
  - 0.37816736102104187
  - 0.3770860433578491
  - 0.48767778277397156
  - 0.3769510090351105
  - 0.378010094165802
  - 0.3853691816329956
  - 0.3993600010871887
  - 0.42068538069725037
loss_records_fold1:
  train_losses:
  - 1.4928188860416414
  - 1.4751109302043917
  - 1.8347753822803499
  - 2.3744494020938873
  - 2.095476359128952
  - 1.5498622715473176
  - 1.5696800887584688
  - 1.4543245613574982
  - 1.5584241449832916
  - 1.5089154481887819
  - 1.5123700678348542
  - 1.80046523809433
  - 2.1254435539245606
  - 1.6295862019062044
  - 1.5225707411766054
  - 1.4971845775842667
  - 1.4545477509498597
  - 1.6625542342662811
  validation_losses:
  - 0.3935631811618805
  - 0.3984428346157074
  - 0.4659099280834198
  - 0.45805221796035767
  - 0.49338769912719727
  - 0.3989657461643219
  - 0.3955893814563751
  - 0.4166654646396637
  - 0.398241251707077
  - 0.4068835377693176
  - 0.40427660942077637
  - 0.5037517547607422
  - 0.4368937611579895
  - 0.4118938148021698
  - 0.4017542898654938
  - 0.40065523982048035
  - 0.40337538719177246
  - 0.39418384432792664
loss_records_fold2:
  train_losses:
  - 1.5740011364221573
  - 1.4959498286247255
  - 1.5467745184898378
  - 1.4832817375659944
  - 1.5113479971885682
  - 1.493430781364441
  - 1.5398015618324281
  - 1.4847958087921143
  - 1.8796711623668672
  - 1.4878838181495668
  - 1.8465816676616669
  - 1.5830674946308136
  - 1.7612464845180513
  - 1.6282066032290459
  - 1.550647097826004
  - 1.538750332593918
  - 1.8468619585037231
  - 1.6008990526199343
  - 1.5569866120815279
  - 1.6400502681732179
  - 1.4939216166734697
  - 1.6174707114696503
  - 1.462181043624878
  - 1.551591682434082
  - 1.4908601641654968
  - 1.8665217101573945
  - 1.519797420501709
  - 1.4710908263921738
  - 1.5226461172103882
  - 1.5475184082984925
  - 2.393555277585983
  - 3.3032952308654786
  - 1.7935083746910097
  - 1.4808739483356477
  - 1.577719408273697
  - 1.445074373483658
  - 1.514047169685364
  - 1.520848596096039
  - 1.5045388042926788
  - 1.6420375108718872
  - 1.5293520808219911
  - 1.6091897249221803
  - 1.5590647250413896
  - 1.4951225519180298
  - 1.8933597534894944
  - 1.5862584292888642
  - 1.604639756679535
  - 1.509878295660019
  - 1.5199043095111848
  - 1.5089610397815705
  - 1.5996589958667755
  - 1.5386084914207458
  - 1.5405020356178285
  - 1.5034403860569001
  - 1.917584776878357
  - 1.5608256965875626
  - 1.5897483080625534
  - 1.7174708008766175
  - 1.6916354835033418
  - 1.428597903251648
  - 2.0141825318336486
  - 1.9088248312473297
  - 1.6227623105049134
  - 1.5798134446144105
  - 1.54570072889328
  - 1.511375704407692
  - 1.5066310524940492
  - 1.4768683880567552
  - 1.4874850749969484
  - 1.4651869893074037
  - 1.5396512627601624
  - 1.4995867252349855
  - 1.5470003426074983
  - 1.4339432239532472
  - 1.4750227689743043
  - 1.551352220773697
  - 1.487308269739151
  - 1.4556253135204316
  - 1.4447624146938325
  - 1.4732687830924989
  - 1.517227876186371
  - 1.446140992641449
  - 1.5664388418197632
  - 1.451858860254288
  - 1.459728264808655
  - 1.520170158147812
  - 1.5282461404800416
  - 1.493816363811493
  - 1.5444699704647065
  - 1.5341086328029634
  - 1.4754763782024385
  - 1.541999113559723
  - 1.5311019599437714
  - 1.463474076986313
  - 1.648264354467392
  - 1.6174813210964203
  - 1.4790398597717287
  - 1.4467849493026734
  - 1.497343933582306
  - 1.458453118801117
  validation_losses:
  - 0.4383363425731659
  - 0.37842419743537903
  - 0.38828355073928833
  - 0.3966124653816223
  - 0.37266871333122253
  - 0.371743381023407
  - 0.37659889459609985
  - 0.3746183514595032
  - 1.1247676610946655
  - 0.3966526389122009
  - 0.3878319561481476
  - 0.37782204151153564
  - 0.37475156784057617
  - 0.40553775429725647
  - 0.4209226965904236
  - 0.3761124610900879
  - 0.3808433413505554
  - 0.39588046073913574
  - 0.37878653407096863
  - 0.3883281350135803
  - 0.3781549036502838
  - 0.38167911767959595
  - 0.467100590467453
  - 0.4521791636943817
  - 0.38190194964408875
  - 0.4762556552886963
  - 0.3712858557701111
  - 0.44724041223526
  - 0.40301206707954407
  - 2.393214464187622
  - 0.3901134729385376
  - 0.4059235751628876
  - 0.3727831244468689
  - 0.40362268686294556
  - 0.38551807403564453
  - 0.3897990584373474
  - 0.39634624123573303
  - 0.37000831961631775
  - 0.4353175461292267
  - 0.36954402923583984
  - 0.39305436611175537
  - 0.5872506499290466
  - 0.3754499554634094
  - 0.403759628534317
  - 0.4021715521812439
  - 0.3716109097003937
  - 0.43518444895744324
  - 0.425845742225647
  - 0.399752140045166
  - 0.37997162342071533
  - 0.37532293796539307
  - 0.3955692648887634
  - 0.3739488422870636
  - 0.40598294138908386
  - 0.40254509449005127
  - 0.3706169128417969
  - 0.5429234504699707
  - 0.4212517738342285
  - 0.3930419683456421
  - 0.4670974612236023
  - 0.3926513195037842
  - 0.4754849374294281
  - 0.4808272123336792
  - 0.3775097131729126
  - 0.38850682973861694
  - 0.3866083323955536
  - 0.3708767890930176
  - 0.3715619444847107
  - 0.4011112451553345
  - 0.4048321545124054
  - 0.3747316896915436
  - 0.3834291398525238
  - 0.4128779172897339
  - 0.39465808868408203
  - 0.3785753548145294
  - 0.6359009146690369
  - 0.39341142773628235
  - 0.3857549726963043
  - 0.37095192074775696
  - 0.397844523191452
  - 0.3729971945285797
  - 0.38611260056495667
  - 0.3867790997028351
  - 0.37263163924217224
  - 0.37055104970932007
  - 0.42939168214797974
  - 0.3735443353652954
  - 0.37829533219337463
  - 0.3731948137283325
  - 0.5854156017303467
  - 0.3717619776725769
  - 0.39253827929496765
  - 0.38042810559272766
  - 0.47587719559669495
  - 0.3735464811325073
  - 0.3756070137023926
  - 0.39986053109169006
  - 0.3872337341308594
  - 0.3745391070842743
  - 0.3734898269176483
loss_records_fold3:
  train_losses:
  - 1.470310801267624
  - 1.582810926437378
  - 1.4718294024467469
  - 1.4796532988548279
  - 1.5127609252929688
  - 1.4455466151237488
  - 1.5557903170585634
  - 1.5467085361480715
  - 1.4076545134186746
  - 1.468976902961731
  - 1.462862765789032
  - 1.4920512318611145
  - 1.4979071617126465
  - 1.5147082686424256
  - 1.4560099422931672
  - 1.4585325419902802
  - 1.4340781152248383
  - 1.5008150398731233
  - 1.5906716346740724
  - 1.4482002437114716
  - 1.4369776189327241
  - 1.4196283757686616
  - 1.437943547964096
  - 1.5035118341445923
  validation_losses:
  - 0.3915257751941681
  - 0.38916075229644775
  - 0.3865385353565216
  - 0.3996184170246124
  - 0.391800194978714
  - 0.5109105110168457
  - 0.4668160378932953
  - 0.3823341429233551
  - 0.3902830481529236
  - 0.37161070108413696
  - 0.37804412841796875
  - 0.4140760600566864
  - 0.37513303756713867
  - 0.3732583224773407
  - 0.38798850774765015
  - 0.4450703561306
  - 0.38061338663101196
  - 0.6192715167999268
  - 0.39686301350593567
  - 0.3997859060764313
  - 0.37786170840263367
  - 0.38130199909210205
  - 0.37710437178611755
  - 0.37890949845314026
loss_records_fold4:
  train_losses:
  - 1.498418852686882
  - 1.454755699634552
  - 1.5306441843509675
  - 1.5368440568447115
  - 1.5637904733419419
  - 1.4560289263725281
  - 1.4668997406959534
  - 1.472971820831299
  - 1.4817176252603532
  - 1.4713522732257844
  - 1.4982242166996003
  - 1.5345882058143616
  - 1.44474196434021
  - 1.4409688323736192
  - 1.438583305478096
  - 1.4396041601896288
  - 1.4293895184993746
  - 1.4951337397098543
  - 1.4429009556770325
  - 1.4409061670303345
  - 1.4834403932094575
  - 1.5097497880458832
  - 1.4276771128177643
  - 1.4596160054206848
  - 1.4887039124965669
  - 1.5167281478643417
  - 1.5484571635723114
  - 1.4751608908176423
  - 1.518045574426651
  - 1.6351351201534272
  - 1.542879617214203
  - 1.4947473227977754
  - 1.4942321538925172
  - 1.5684303283691408
  - 1.460604113340378
  - 1.4560640037059784
  - 1.445597690343857
  - 1.4691713154315948
  - 1.4544427692890167
  - 1.497485488653183
  - 1.5662876695394516
  - 1.4525142073631288
  - 1.4685789108276368
  - 1.4590066254138947
  - 1.4734884262084962
  - 1.4435081303119661
  - 1.5103337526321412
  - 1.4614972293376924
  - 1.4546777367591859
  - 1.4102472454309465
  - 1.4452780008316042
  - 1.4876103460788728
  - 1.4784066736698152
  - 1.4688347518444063
  - 1.4893405318260193
  - 1.475287687778473
  - 1.4397001981735231
  - 1.4426579356193543
  - 1.4658301174640656
  - 1.5000848293304445
  - 1.556354993581772
  - 1.738957327604294
  - 1.4749296069145204
  - 1.4561865866184236
  - 1.4529912948608399
  - 1.436477953195572
  - 1.461210083961487
  - 1.5186021625995636
  - 1.5184477984905245
  - 1.5224514484405518
  - 1.4636641412973406
  - 1.4794987320899964
  - 1.5191771745681764
  - 1.4866417229175568
  - 1.4715495824813845
  - 1.5657827615737916
  - 1.591316318511963
  - 1.5277980268001556
  - 1.4666019797325136
  - 1.4360698103904725
  - 1.5026131719350815
  - 1.4790498435497286
  - 1.443973743915558
  - 1.5812578797340393
  - 1.4152984201908112
  - 1.5257140755653382
  validation_losses:
  - 0.4449566900730133
  - 0.36554211378097534
  - 0.384209007024765
  - 0.3731059432029724
  - 0.3865090012550354
  - 0.40974631905555725
  - 0.3750821053981781
  - 0.37453338503837585
  - 0.39807865023612976
  - 0.37672823667526245
  - 0.3808266222476959
  - 0.41153421998023987
  - 0.3691205084323883
  - 0.3998635411262512
  - 0.37197867035865784
  - 0.3664589822292328
  - 0.3642778694629669
  - 0.425320029258728
  - 0.36998915672302246
  - 0.37012284994125366
  - 0.3786289393901825
  - 0.3697160482406616
  - 0.3800261914730072
  - 0.39674824476242065
  - 0.3665127754211426
  - 0.42872822284698486
  - 0.4004291594028473
  - 0.3948310315608978
  - 0.47113603353500366
  - 0.3953509032726288
  - 0.3898389935493469
  - 0.4007642865180969
  - 0.4451097846031189
  - 0.39824625849723816
  - 0.3791148066520691
  - 0.3819071352481842
  - 0.37675365805625916
  - 0.40499788522720337
  - 0.37140995264053345
  - 0.3890843391418457
  - 0.40494072437286377
  - 0.38375070691108704
  - 0.3748902678489685
  - 0.414443701505661
  - 0.3728850185871124
  - 0.406335711479187
  - 0.3690098524093628
  - 0.3834170401096344
  - 0.37241750955581665
  - 0.3759516775608063
  - 0.3808442950248718
  - 0.37773963809013367
  - 0.37373971939086914
  - 0.41709035634994507
  - 0.37160763144493103
  - 0.3657996952533722
  - 0.36812475323677063
  - 0.3694782555103302
  - 0.37025466561317444
  - 0.41107088327407837
  - 0.37606990337371826
  - 0.42948803305625916
  - 0.3903006315231323
  - 0.37216827273368835
  - 0.3894810378551483
  - 0.3760181665420532
  - 0.3799213469028473
  - 0.3723190426826477
  - 0.41654831171035767
  - 0.37894144654273987
  - 0.38256415724754333
  - 0.3815978467464447
  - 0.36950385570526123
  - 0.39134153723716736
  - 0.36875176429748535
  - 0.3918604552745819
  - 0.42376509308815
  - 0.3749621510505676
  - 0.38642093539237976
  - 0.7225510478019714
  - 0.37510761618614197
  - 0.3699895143508911
  - 0.37841248512268066
  - 0.3748968243598938
  - 0.3758053183555603
  - 0.3787310719490051
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 86 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8573883161512027]'
  fold_eval_f1: '[0.023255813953488372, 0.0, 0.0, 0.0, 0.023529411764705882]'
  mean_eval_accuracy: 0.8579270628871873
  mean_f1_accuracy: 0.00935704514363885
  total_train_time: '0:27:30.269268'
