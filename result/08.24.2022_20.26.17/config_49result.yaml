config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:32:40.543086'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_49fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_49fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_49fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_49fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_49fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_49fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_49fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_49fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_49fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_49fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.2405763387680055
  - 2.9515250980854035
  - 3.0243158400058747
  - 2.986103478074074
  - 2.9357577890157702
  - 3.030480241775513
  - 2.9801521539688114
  - 2.8626273542642595
  - 2.8195076853036882
  - 2.8938042700290683
  - 2.842639976739884
  validation_losses:
  - 0.5449807047843933
  - 0.4352293610572815
  - 0.4470187723636627
  - 0.395443320274353
  - 0.4164389967918396
  - 0.39082011580467224
  - 0.3829353153705597
  - 0.3865346610546112
  - 0.38891762495040894
  - 0.3867572247982025
  - 0.3933965265750885
loss_records_fold1:
  train_losses:
  - 2.8982756376266483
  - 2.927073299884796
  - 2.8853044122457505
  - 2.843511563539505
  - 2.861266529560089
  - 2.8950331956148148
  - 2.894345530867577
  - 2.9846601456403734
  - 2.821330550312996
  - 2.854283916950226
  - 2.8671720027923584
  - 2.8491710007190707
  validation_losses:
  - 0.39191409945487976
  - 0.4242076575756073
  - 0.3882853090763092
  - 0.38669535517692566
  - 0.3874231278896332
  - 0.4164632558822632
  - 0.3893272578716278
  - 0.3897026479244232
  - 0.3884347677230835
  - 0.3953566551208496
  - 0.39468908309936523
  - 0.3848925232887268
loss_records_fold2:
  train_losses:
  - 2.8994397401809695
  - 2.8641785830259323
  - 2.858573603630066
  - 2.868410646915436
  - 2.8355664998292927
  - 2.8002153396606446
  - 2.7983038604259494
  - 2.8026420861482624
  - 2.795401692390442
  - 2.8522135198116305
  - 2.846826082468033
  - 2.879169124364853
  - 2.8082774937152863
  - 2.8538858711719515
  - 2.772096794843674
  - 2.839685481786728
  - 2.794991630315781
  - 2.8482231825590136
  - 2.801453444361687
  - 2.8215835332870487
  - 2.8083535492420197
  - 2.804488027095795
  - 2.7815337747335436
  - 2.8649970918893817
  - 2.8244663745164873
  - 2.7901582002639773
  - 2.8446492940187458
  - 2.826633667945862
  - 2.8163706421852113
  - 2.801532745361328
  - 2.7848421931266785
  - 2.776113134622574
  - 2.7978338181972506
  - 2.827233755588532
  validation_losses:
  - 0.4006267189979553
  - 0.3859911262989044
  - 0.39246076345443726
  - 0.38486000895500183
  - 0.3882206976413727
  - 0.3830130994319916
  - 0.38674014806747437
  - 0.3982284367084503
  - 0.383188933134079
  - 0.39236152172088623
  - 0.381522536277771
  - 0.4020851254463196
  - 0.3917785882949829
  - 0.3849160671234131
  - 0.46016237139701843
  - 0.4003579318523407
  - 0.40550392866134644
  - 0.4270053505897522
  - 0.3832629323005676
  - 0.3981774151325226
  - 0.3881014287471771
  - 0.40736833214759827
  - 0.3847813010215759
  - 0.4017888009548187
  - 0.3806653320789337
  - 0.4055594801902771
  - 0.3787256181240082
  - 0.3939380645751953
  - 0.3818122148513794
  - 0.38463854789733887
  - 0.39457836747169495
  - 0.3933441638946533
  - 0.4004691243171692
  - 0.4094436764717102
loss_records_fold3:
  train_losses:
  - 2.8108563244342806
  - 2.804302522540093
  - 2.912442830204964
  - 2.8285341918468476
  - 2.822733607888222
  - 2.838554722070694
  - 2.8191858530044556
  - 2.8276465356349947
  - 2.821409299969673
  - 2.7963544875383377
  - 2.9025221168994904
  - 2.7906657189130786
  - 2.8466843187808992
  - 2.8143916845321657
  - 2.78960235118866
  - 2.772511723637581
  - 2.848055410385132
  - 2.855789625644684
  - 2.8711013138294224
  - 2.8089753061532976
  - 2.826650816202164
  - 2.8180355161428454
  - 2.8215160459280018
  - 2.809209662675858
  - 2.811875480413437
  - 2.827942427992821
  - 2.7764386117458346
  - 2.7621187776327134
  - 2.7578846335411074
  - 2.7798758268356325
  - 2.8113123536109925
  - 2.7636172771453857
  - 2.778153046965599
  - 2.8078030884265903
  - 2.7768942892551425
  - 2.795803517103195
  - 2.7591684460639954
  - 2.761199003458023
  - 2.7476657867431644
  - 2.761781054735184
  - 2.7751879751682282
  - 2.778184521198273
  - 2.753714895248413
  - 2.7611290603876117
  - 2.802272430062294
  - 2.7760718017816544
  - 2.7661695003509523
  - 2.6965474009513857
  - 2.8010884672403336
  - 2.804076164960861
  - 2.770954215526581
  - 2.738341444730759
  - 2.7543971836566925
  - 2.712805077433586
  - 2.7641699701547626
  - 2.757471495866776
  - 2.7151856631040574
  - 2.7248226642608646
  - 2.7768069297075275
  - 2.721703687310219
  - 2.7106765866279603
  - 2.746477946639061
  - 2.7363184273242953
  - 2.8442965507507325
  - 2.777521693706513
  - 2.770254421234131
  - 2.735989969968796
  - 2.7523194670677187
  - 2.7699134945869446
  - 2.7495503664016727
  - 2.7015036761760713
  - 2.7124921679496765
  - 2.7024151116609576
  - 2.767320099473
  - 2.7169015973806383
  - 2.778367078304291
  - 2.7419124811887743
  - 2.7446953386068347
  - 2.72999821305275
  - 2.714059662818909
  - 2.7715378373861315
  - 2.7228976398706437
  - 2.7280590534210205
  - 2.7908945560455325
  - 2.8582452803850176
  - 2.7582042664289474
  - 2.7020567923784258
  - 2.845539277791977
  - 2.762468805909157
  - 2.7037118881940843
  - 2.7600608617067337
  - 2.7011332690715792
  - 2.7239436298608783
  - 2.7407149553298953
  - 2.6953161716461183
  - 2.7495447397232056
  - 2.7349839001894
  - 2.7249411880970005
  - 2.7460417956113816
  - 2.770572912693024
  validation_losses:
  - 0.38957563042640686
  - 0.6467152833938599
  - 0.3616581857204437
  - 0.43208596110343933
  - 0.8722713589668274
  - 0.37753531336784363
  - 0.36991146206855774
  - 0.37252306938171387
  - 0.370464026927948
  - 1.966343641281128
  - 0.3683498501777649
  - 0.36480462551116943
  - 0.37924352288246155
  - 0.37426939606666565
  - 0.8904648423194885
  - 1.9325584173202515
  - 1.3047263622283936
  - 0.44886982440948486
  - 0.4869982600212097
  - 0.40563955903053284
  - 0.49204784631729126
  - 0.44540485739707947
  - 0.594107449054718
  - 1.906238079071045
  - 0.5462340712547302
  - 0.4391334056854248
  - 0.42454057931900024
  - 0.48405927419662476
  - 0.44023653864860535
  - 0.5518085956573486
  - 0.48212382197380066
  - 0.4628760814666748
  - 0.4332858622074127
  - 0.6792206764221191
  - 0.76877760887146
  - 0.4170057475566864
  - 0.7226080298423767
  - 2.342318058013916
  - 0.7038746476173401
  - 0.6733672022819519
  - 0.7830747961997986
  - 0.5995920300483704
  - 0.5595792531967163
  - 0.642643392086029
  - 0.7770366668701172
  - 1.5877655744552612
  - 0.4640517830848694
  - 0.6222115755081177
  - 2.2625701427459717
  - 9.801533699035645
  - 5.24658727645874
  - 4.413361549377441
  - 3.00124454498291
  - 2.0646018981933594
  - 0.8923388123512268
  - 2.3725709915161133
  - 7.88120698928833
  - 1.2190041542053223
  - 0.39144498109817505
  - 0.4885680079460144
  - 0.6157035231590271
  - 0.6655371189117432
  - 2.7269222736358643
  - 1.5734320878982544
  - 0.5581165552139282
  - 0.6676451563835144
  - 0.7342702746391296
  - 0.6214746832847595
  - 0.9700084328651428
  - 0.9271495938301086
  - 0.8176758885383606
  - 0.5391440987586975
  - 1.2028844356536865
  - 2.499350070953369
  - 0.4402103126049042
  - 0.5495970249176025
  - 0.5396095514297485
  - 0.6484683156013489
  - 1.1235744953155518
  - 18.98408317565918
  - 6.078996658325195
  - 6.345890998840332
  - 10.917763710021973
  - 10.236586570739746
  - 6.638448238372803
  - 2.1700057983398438
  - 21.833242416381836
  - 0.6004195809364319
  - 0.8593853116035461
  - 0.49967190623283386
  - 0.7617842555046082
  - 0.5801432728767395
  - 2.221181869506836
  - 2.684466600418091
  - 0.6272653341293335
  - 0.52906334400177
  - 0.4851991832256317
  - 0.5259853005409241
  - 0.5546611547470093
  - 0.7258571982383728
loss_records_fold4:
  train_losses:
  - 2.7760065615177156
  - 2.755247488617897
  - 2.7286841303110125
  - 2.7669182151556018
  - 2.7480801403522492
  - 2.728097152709961
  - 2.6976717829704286
  - 2.743602287769318
  - 2.719206702709198
  - 2.7296651661396027
  - 2.707412427663803
  - 2.7059165179729465
  - 2.675128710269928
  - 2.736672115325928
  - 2.745385780930519
  - 2.718911823630333
  - 2.7029661685228348
  - 2.688109314441681
  - 2.657935693860054
  - 2.719089645147324
  - 2.719975021481514
  - 2.700264021754265
  - 2.73857159614563
  - 2.72342586517334
  - 2.7254219114780427
  - 2.679822784662247
  - 2.6951592028141023
  - 2.715003088116646
  - 2.718400087952614
  - 2.660742908716202
  - 2.7775129914283756
  - 2.753445076942444
  - 2.7051917731761934
  - 2.694290700554848
  - 2.7433318853378297
  - 2.7580165147781375
  - 2.728396081924439
  - 2.728764507174492
  - 2.757113236188889
  - 2.704651227593422
  - 2.6478716462850573
  - 2.668503224849701
  - 2.6514992833137514
  - 2.659261754155159
  - 2.7142052173614504
  - 2.6665092140436175
  - 2.716126388311386
  - 2.697412478923798
  - 2.663149154186249
  - 2.688531839847565
  - 2.6451787114143372
  - 2.6529129207134248
  - 2.678174114227295
  - 2.6503054320812227
  - 2.676192590594292
  - 2.7071391463279726
  - 2.6947094917297365
  - 2.7270627677440644
  - 2.6791378557682037
  - 2.6810478746891024
  - 2.6812321096658707
  - 2.703140759468079
  - 2.703750598430634
  - 2.6691171109676364
  - 2.666387662291527
  - 2.663495728373528
  - 2.6379735678434373
  - 2.758459928631783
  - 2.685669645667076
  - 2.6531083405017855
  - 2.621062821149826
  - 2.652195817232132
  - 2.6416632711887362
  - 2.654088789224625
  - 2.589258447289467
  - 2.616528809070587
  - 2.580397313833237
  - 2.6038322985172275
  - 2.654467993974686
  - 2.8401514232158664
  - 2.9756321340799334
  - 2.8054711520671844
  - 2.732897233963013
  - 2.711215987801552
  - 2.8122599154710772
  - 2.7561232388019565
  - 2.721479034423828
  - 2.738460731506348
  - 2.741737833619118
  - 2.7096299409866336
  - 2.6944595873355865
  - 2.761867281794548
  - 2.676539143919945
  - 2.682163640856743
  - 2.7027303218841556
  - 2.7216245532035828
  - 2.710386145114899
  - 2.699102371931076
  - 2.728485816717148
  - 2.735269355773926
  validation_losses:
  - 0.5609990358352661
  - 0.6459828615188599
  - 0.37335315346717834
  - 0.48922210931777954
  - 0.563118040561676
  - 0.5437430143356323
  - 0.5293335318565369
  - 0.6515392661094666
  - 0.586696982383728
  - 0.6106674671173096
  - 0.5099276900291443
  - 0.6654544472694397
  - 0.6036683320999146
  - 0.5659841299057007
  - 0.5419525504112244
  - 0.632610559463501
  - 0.6009945273399353
  - 0.6586578488349915
  - 0.5848413705825806
  - 0.8036786317825317
  - 0.7922220826148987
  - 0.6878892183303833
  - 0.5935057401657104
  - 0.7523347735404968
  - 0.6603056192398071
  - 0.6179186701774597
  - 0.5946393609046936
  - 0.5615941286087036
  - 0.6154347658157349
  - 0.5718787908554077
  - 0.6218128800392151
  - 0.5343784689903259
  - 0.565756618976593
  - 0.5267370939254761
  - 0.667831301689148
  - 0.644355833530426
  - 0.6073006391525269
  - 0.579063892364502
  - 0.6075870394706726
  - 0.5682907700538635
  - 0.6282413005828857
  - 0.5852664113044739
  - 0.6139684319496155
  - 0.5831452012062073
  - 0.7021333575248718
  - 0.6798975467681885
  - 0.7402923703193665
  - 0.6671968102455139
  - 0.6451464295387268
  - 0.5683207511901855
  - 0.702030599117279
  - 0.6848788261413574
  - 0.697265625
  - 0.6696665287017822
  - 0.6534391641616821
  - 0.6183770895004272
  - 0.679685115814209
  - 0.7551076412200928
  - 0.7049607634544373
  - 0.8510744571685791
  - 0.44720321893692017
  - 0.6389235258102417
  - 0.5986464619636536
  - 0.5923771858215332
  - 0.5754659175872803
  - 0.6344156861305237
  - 0.6615152955055237
  - 0.7430799603462219
  - 0.496427446603775
  - 0.642019510269165
  - 0.6081943511962891
  - 0.7278792858123779
  - 0.7024248838424683
  - 0.5165379643440247
  - 0.8055694103240967
  - 0.7513333559036255
  - 0.6532647013664246
  - 0.6157004237174988
  - 0.4680083096027374
  - 0.3923686444759369
  - 0.6067032217979431
  - 0.40027347207069397
  - 0.43226975202560425
  - 0.3882814943790436
  - 0.42433854937553406
  - 0.4548763632774353
  - 0.3960014879703522
  - 0.4262133836746216
  - 0.42127725481987
  - 0.5744379162788391
  - 0.4269459843635559
  - 0.4366183280944824
  - 0.4428863525390625
  - 0.4077886939048767
  - 0.5201444029808044
  - 0.458743691444397
  - 0.4609278440475464
  - 1.382712483406067
  - 0.4103972911834717
  - 0.44855597615242004
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8284734133790738,
    0.8470790378006873]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.23076923076923075, 0.043010752688172046]'
  mean_eval_accuracy: 0.8500333032719727
  mean_f1_accuracy: 0.054755996691480555
  total_train_time: '0:24:43.622171'
