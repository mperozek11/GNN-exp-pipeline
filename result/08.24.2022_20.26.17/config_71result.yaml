config:
  aggregation: mean
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:05:12.398797'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_71fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_71fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_71fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_71fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_71fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_71fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_71fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_71fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_71fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_71fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 0.9890828549861909
  - 0.9650240123271943
  - 0.8545718908309937
  - 0.8263594269752503
  - 0.8975349545478821
  - 0.8467754364013672
  - 0.8463560521602631
  - 0.7964990496635438
  - 0.8480223774909974
  - 0.8269338011741638
  - 0.8858983814716339
  - 0.8292265474796295
  - 0.81322181224823
  - 0.8086671233177185
  - 0.8332940995693208
  - 0.8847388923168182
  - 0.9890968680381775
  - 0.8242700636386872
  - 0.7914580255746841
  - 0.8421987652778626
  - 0.8425987720489503
  - 0.8420390605926514
  - 0.8525398254394532
  - 0.8108645081520081
  - 0.7990550756454469
  - 0.7932159602642059
  - 0.8314716041088105
  - 0.8196168065071107
  - 0.8357928514480591
  - 0.818380755186081
  - 0.7923562556505204
  - 0.8005066931247712
  - 0.8352621912956238
  - 0.8399683713912964
  validation_losses:
  - 0.4113868176937103
  - 0.4665273129940033
  - 0.4080828130245209
  - 0.41108760237693787
  - 0.4195552170276642
  - 0.39936941862106323
  - 0.40107059478759766
  - 0.40454548597335815
  - 0.41583770513534546
  - 0.3924892544746399
  - 0.38976040482521057
  - 0.40137532353401184
  - 0.4005274772644043
  - 0.3895469307899475
  - 0.4053138196468353
  - 0.4047873020172119
  - 0.4284433424472809
  - 0.3987181782722473
  - 0.39128419756889343
  - 0.3952045440673828
  - 0.38480144739151
  - 0.39623767137527466
  - 0.3901853859424591
  - 0.3886096179485321
  - 0.3953404128551483
  - 0.38963747024536133
  - 0.3877352774143219
  - 0.4018458127975464
  - 0.3932516574859619
  - 0.3976418077945709
  - 0.39501988887786865
  - 0.3958899974822998
  - 0.3868318200111389
  - 0.39530932903289795
loss_records_fold1:
  train_losses:
  - 0.837484210729599
  - 0.8212298452854156
  - 0.8403564155101777
  - 0.8237508475780487
  - 0.797802633047104
  - 0.8425195932388306
  - 0.8325417041778564
  - 0.8663231551647187
  - 0.8356959402561188
  - 0.830833226442337
  - 0.8072687268257142
  validation_losses:
  - 0.38893428444862366
  - 0.44463402032852173
  - 0.41362082958221436
  - 0.3996778130531311
  - 0.3934197723865509
  - 0.39088940620422363
  - 0.3996785283088684
  - 0.3957586884498596
  - 0.39668938517570496
  - 0.39680108428001404
  - 0.3995448648929596
loss_records_fold2:
  train_losses:
  - 0.8346283197402955
  - 0.8204485297203065
  - 0.880879282951355
  - 0.8025836408138276
  - 0.823590487241745
  - 0.8127505421638489
  - 0.7790509641170502
  - 0.83903648853302
  - 0.8080397188663483
  - 0.8780661404132843
  - 0.8648027539253236
  - 0.8268707811832429
  - 0.8416436016559601
  - 0.825972354412079
  - 0.8538534343242645
  - 0.8223794460296632
  validation_losses:
  - 0.39047110080718994
  - 0.3847579061985016
  - 0.40781518816947937
  - 0.4069792926311493
  - 0.3942456841468811
  - 0.3977050483226776
  - 0.3865591883659363
  - 0.4268758296966553
  - 0.3919617533683777
  - 0.4388640820980072
  - 0.39170029759407043
  - 0.39560624957084656
  - 0.3964388072490692
  - 0.3908839225769043
  - 0.3953591585159302
  - 0.39455318450927734
loss_records_fold3:
  train_losses:
  - 0.8147049486637116
  - 0.8168419659137727
  - 0.7960571825504303
  - 0.8211621403694154
  - 0.7988351881504059
  - 0.8036794722080232
  - 0.9091309428215028
  - 0.8093108415603638
  - 0.8528572022914886
  - 0.8316403806209565
  - 0.8404001832008362
  - 0.8633824765682221
  - 0.7824585378170014
  - 0.8738107442855836
  - 0.8119782686233521
  - 0.8186122417449951
  - 0.8631217420101166
  - 0.8574197113513947
  - 0.8497961759567261
  - 0.9078258812427521
  - 0.8082632720470428
  - 0.8765755057334901
  - 0.8316929280757904
  - 0.9028223335742951
  - 0.8565157592296601
  - 1.0217275202274323
  - 0.8890776872634888
  - 0.8273514747619629
  validation_losses:
  - 0.3786824941635132
  - 0.3836856484413147
  - 0.38065677881240845
  - 0.38856953382492065
  - 0.3837926387786865
  - 0.386005163192749
  - 0.4031142294406891
  - 0.3890396058559418
  - 0.3885463774204254
  - 0.39925143122673035
  - 0.39716243743896484
  - 0.39495155215263367
  - 0.37874531745910645
  - 0.40020063519477844
  - 0.382466197013855
  - 0.37928664684295654
  - 0.380914568901062
  - 0.39948755502700806
  - 0.39158767461776733
  - 0.3927600383758545
  - 0.37794703245162964
  - 0.3945469260215759
  - 0.40134045481681824
  - 0.3802545368671417
  - 0.38650596141815186
  - 0.3845672011375427
  - 0.3859938383102417
  - 0.38804322481155396
loss_records_fold4:
  train_losses:
  - 0.9334576606750489
  - 0.8162100851535797
  - 0.8681096971035004
  - 0.8823456466197968
  - 0.8636117637157441
  - 0.8519480705261231
  - 1.0075669169425965
  - 0.8465741395950318
  - 0.8123683512210846
  - 0.8004919171333313
  - 0.7982728958129883
  - 0.8815167248249054
  - 0.8050187349319459
  - 0.8384088754653931
  - 0.826896619796753
  - 0.8205109775066376
  - 0.8119656622409821
  - 0.8146677494049073
  - 0.821199905872345
  - 0.8060657858848572
  - 0.8142594516277314
  - 0.8238912403583527
  - 0.813468086719513
  - 0.8249439597129822
  - 0.8544009149074555
  - 0.8007049381732941
  - 0.7961750090122224
  - 0.8214289367198945
  - 0.7726530373096466
  - 0.7989898204803467
  - 0.8477586388587952
  - 0.8149634182453156
  - 0.8200620651245117
  - 0.7775669217109681
  - 0.8203409790992737
  - 0.8067122399806976
  - 0.7813856482505799
  - 0.7723817199468613
  - 0.7734507501125336
  - 0.7864415407180787
  - 0.8269587159156799
  - 0.8054939210414886
  - 0.782353937625885
  - 0.7890851497650146
  - 0.7749480187892914
  - 0.780204564332962
  validation_losses:
  - 0.41167619824409485
  - 0.40132102370262146
  - 0.3919317126274109
  - 0.4222133457660675
  - 0.42424076795578003
  - 0.39535826444625854
  - 0.415153831243515
  - 0.44605615735054016
  - 0.4225676655769348
  - 0.4079638719558716
  - 0.3892930746078491
  - 0.3862408995628357
  - 0.4004983901977539
  - 0.41626763343811035
  - 0.4250132441520691
  - 0.38472968339920044
  - 0.391169011592865
  - 0.3764183223247528
  - 0.386661171913147
  - 0.3788876533508301
  - 0.39705991744995117
  - 0.38952627778053284
  - 0.3772594630718231
  - 0.37062570452690125
  - 0.38277319073677063
  - 0.38096556067466736
  - 0.389800101518631
  - 0.37938597798347473
  - 0.3891600966453552
  - 0.382741242647171
  - 0.39663925766944885
  - 0.3814820945262909
  - 0.3907474875450134
  - 0.4025125503540039
  - 0.38777050375938416
  - 0.3821200728416443
  - 0.3780629336833954
  - 0.4103451669216156
  - 0.3991268277168274
  - 0.4754490852355957
  - 0.3886183798313141
  - 0.3796178698539734
  - 0.3798774480819702
  - 0.3849879503250122
  - 0.3925805687904358
  - 0.37933528423309326
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 46 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:11:16.298895'
