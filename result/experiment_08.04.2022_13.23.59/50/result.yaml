config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 18:31:18.940260'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/50/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/50/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/50/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/50/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/50/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/50/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/50/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/50/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/50/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/50/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 182.44284397363663
  - 55.9762806892395
  - 31.93419973552227
  - 25.649221390485764
  - 29.219990462064743
  - 18.876133978366852
  - 25.353504106402397
  - 46.122093588113785
  - 18.917466267943382
  - 17.116616919636726
  - 34.60596165060997
  - 18.31351837515831
  - 16.80747151374817
  - 19.80523607134819
  - 31.726606458425522
  - 23.33699730038643
  - 29.964811712503433
  - 43.77127708494663
  - 28.674021899700165
  - 24.35958695411682
  - 19.018705651164055
  - 18.27510279417038
  - 15.465191766619682
  - 20.681409537792206
  - 19.40371835231781
  - 20.969625115394592
  - 23.77207201719284
  - 29.293885678052902
  - 20.62748309969902
  - 17.56381893157959
  - 15.947952061891556
  - 37.73780153691769
  - 14.445946604013443
  - 16.300695195794106
  - 15.96471931040287
  - 15.07719224691391
  - 17.485285744071007
  - 16.108651965856552
  - 18.05175346136093
  - 23.475727558135986
  - 15.978422313928604
  - 17.129763066768646
  - 15.816343396902084
  - 18.50140579044819
  - 16.195951014757156
  - 16.32089777290821
  - 18.618972167372704
  - 16.393270149827003
  - 15.052262961864471
  - 14.929603785276413
  - 15.892259076237679
  - 18.166088700294495
  - 15.337249666452408
  - 28.151291370391846
  - 19.124593317508698
  - 16.723086059093475
  - 15.14658796787262
  - 17.885182723402977
  - 16.234603375196457
  - 15.381436631083488
  - 15.7397151440382
  - 15.122656434774399
  - 14.732658296823502
  - 15.01270779967308
  - 14.683143183588982
  - 14.923262402415276
  - 15.912085115909576
  - 16.00546357035637
  - 14.41117238998413
  - 15.13547858595848
  - 14.817486494779587
  - 15.679034978151321
  - 14.12738761305809
  - 15.721955552697182
  - 18.154524832963943
  - 15.639219999313354
  - 15.423467054963112
  - 14.881676405668259
  - 15.403828084468842
  - 14.97901962697506
  - 14.29954382777214
  - 14.216356053948402
  - 15.096364364027977
  - 14.934485852718353
  - 14.034810662269592
  - 14.669781625270844
  - 14.543901026248932
  - 15.160803943872452
  - 14.692696958780289
  - 14.158439010381699
  - 14.390169575810432
  - 14.80894923210144
  - 13.971554934978485
  - 14.554904401302338
  - 14.680172741413116
  - 14.890440240502357
  - 14.108087599277496
  - 14.433607131242752
  - 14.517755776643753
  - 15.549786567687988
  validation_losses:
  - 3.1055288314819336
  - 1.1818413734436035
  - 0.498124897480011
  - 0.451909601688385
  - 0.7447561025619507
  - 0.5875471234321594
  - 1.419297456741333
  - 0.4323488771915436
  - 0.38383397459983826
  - 0.3906296193599701
  - 1.1435880661010742
  - 0.48069003224372864
  - 0.537479043006897
  - 0.4873692989349365
  - 0.5209763646125793
  - 0.3828662037849426
  - 0.9264827370643616
  - 0.5561437606811523
  - 0.43765783309936523
  - 0.4112209975719452
  - 0.45758292078971863
  - 0.3841843605041504
  - 0.6023199558258057
  - 0.37868672609329224
  - 0.3775078058242798
  - 0.4235053062438965
  - 0.3744697868824005
  - 2.289372205734253
  - 0.3828936815261841
  - 0.3957911431789398
  - 0.38221853971481323
  - 0.38340815901756287
  - 0.43822070956230164
  - 0.3890589773654938
  - 0.38472938537597656
  - 0.40885016322135925
  - 0.3993450701236725
  - 0.39564549922943115
  - 0.3767582178115845
  - 0.3966485857963562
  - 0.3924799859523773
  - 0.3871386647224426
  - 0.3965035378932953
  - 0.38840460777282715
  - 0.370602011680603
  - 0.3949081599712372
  - 0.3994957506656647
  - 0.4288492798805237
  - 0.48293790221214294
  - 0.39863476157188416
  - 0.37700018286705017
  - 0.4660891592502594
  - 0.3944278359413147
  - 0.4185495376586914
  - 0.44425469636917114
  - 0.37614792585372925
  - 0.38703930377960205
  - 0.40530678629875183
  - 0.38335299491882324
  - 0.38075342774391174
  - 0.3829978406429291
  - 0.3776305615901947
  - 0.39176079630851746
  - 0.39055129885673523
  - 0.3737975060939789
  - 0.6167582869529724
  - 0.39999333024024963
  - 0.3747982084751129
  - 0.4221359193325043
  - 0.3796235918998718
  - 0.43943414092063904
  - 0.38009899854660034
  - 0.42566031217575073
  - 0.7293916940689087
  - 0.4029114842414856
  - 0.4408239722251892
  - 0.37918707728385925
  - 0.3842097222805023
  - 0.44893741607666016
  - 0.3884970545768738
  - 0.372301310300827
  - 0.5385705232620239
  - 0.3953307271003723
  - 0.39217883348464966
  - 0.3726704716682434
  - 0.46138402819633484
  - 0.39885762333869934
  - 0.372205525636673
  - 0.37871912121772766
  - 0.39804506301879883
  - 0.433376669883728
  - 0.3743087947368622
  - 0.3792544901371002
  - 0.3710152804851532
  - 0.37418416142463684
  - 0.37159281969070435
  - 0.3817313015460968
  - 0.3891344666481018
  - 0.400798499584198
  - 0.3829111158847809
loss_records_fold1:
  train_losses:
  - 14.068460136651993
  - 14.130721509456635
  - 14.276655837893486
  - 13.912852421402931
  - 16.549656242132187
  - 14.207991421222687
  - 14.159011200070381
  - 14.145499140024185
  - 14.338506028056145
  - 14.84437283873558
  - 14.278531804680824
  - 14.131958901882172
  - 14.68334785103798
  - 14.1102564483881
  - 14.435906022787094
  - 13.848242461681366
  - 14.334171921014786
  - 14.34613911807537
  - 14.126267224550247
  - 14.065467491745949
  - 14.604509457945824
  - 14.50367945432663
  - 14.473842367529869
  - 14.402989909052849
  - 14.273725539445877
  - 14.058613628149033
  - 14.397511914372444
  - 14.673186734318733
  - 14.369335979223251
  - 13.878424599766731
  - 13.855716943740845
  - 13.863637506961823
  - 14.258031159639359
  - 13.850139364600182
  - 14.263547852635384
  - 13.8330739736557
  - 15.433392822742462
  - 14.182328224182129
  - 14.070330426096916
  - 14.121927216649055
  - 13.783311575651169
  - 13.828573405742645
  - 13.956449121236801
  - 13.894232228398323
  - 13.914784461259842
  - 13.739978462457657
  - 13.64413794875145
  - 14.446866810321808
  - 14.365878328680992
  - 13.729071334004402
  - 14.68595378100872
  - 14.026047125458717
  - 14.08004555106163
  - 14.186132073402405
  - 13.766713455319405
  - 14.182041108608246
  - 14.057740971446037
  - 14.694438457489014
  - 14.608270317316055
  - 14.4380813986063
  - 14.469358250498772
  - 15.030448704957962
  - 13.993497475981712
  - 14.050249770283699
  - 14.507189244031906
  - 13.893721804022789
  - 14.392064988613129
  - 14.242404460906982
  - 14.123781308531761
  - 13.973878845572472
  - 13.962869748473167
  - 14.520353078842163
  - 14.499706506729126
  - 13.904820412397385
  - 14.65333041548729
  - 14.012469157576561
  - 14.296129360795021
  - 13.925178721547127
  - 14.227682828903198
  - 14.054823875427246
  - 14.378152310848236
  - 14.346918061375618
  - 14.160912454128265
  - 13.89745382964611
  - 13.88503822684288
  - 14.004847154021263
  - 14.316468194127083
  - 13.903859585523605
  - 14.060627669095993
  - 13.96138171851635
  - 14.005889728665352
  - 13.965051114559174
  - 14.115193992853165
  - 13.90786799788475
  - 14.011568665504456
  - 14.325135633349419
  - 14.25397202372551
  - 14.200336426496506
  - 14.006351351737976
  - 14.148032575845718
  validation_losses:
  - 0.393776535987854
  - 0.39149048924446106
  - 0.40476474165916443
  - 0.3943137526512146
  - 0.3942929804325104
  - 0.4010099768638611
  - 0.3968623876571655
  - 0.4241373836994171
  - 0.401211142539978
  - 0.40902888774871826
  - 0.3972889482975006
  - 0.6368276476860046
  - 0.4018251895904541
  - 0.4939381778240204
  - 0.39716434478759766
  - 0.39476150274276733
  - 0.4297061562538147
  - 0.5155060887336731
  - 0.398700475692749
  - 0.3986545503139496
  - 0.4474523067474365
  - 0.41138124465942383
  - 0.47750818729400635
  - 0.3923810124397278
  - 0.46213003993034363
  - 0.3916567862033844
  - 0.41548994183540344
  - 0.41668936610221863
  - 0.4201297163963318
  - 0.396295428276062
  - 0.39386874437332153
  - 0.42343518137931824
  - 0.41887348890304565
  - 0.4084915220737457
  - 0.4157595634460449
  - 0.41581374406814575
  - 0.431454062461853
  - 0.4105501174926758
  - 0.3962880074977875
  - 0.3966011703014374
  - 0.437094122171402
  - 0.3969075381755829
  - 0.3923133611679077
  - 0.4492264986038208
  - 0.39270511269569397
  - 0.5644590854644775
  - 0.39478999376296997
  - 0.40865975618362427
  - 0.39530062675476074
  - 0.4004852771759033
  - 0.42668381333351135
  - 0.3863711655139923
  - 0.4168008267879486
  - 0.3954564034938812
  - 0.38859036564826965
  - 0.4088360071182251
  - 0.7515063285827637
  - 0.39981377124786377
  - 0.39902305603027344
  - 0.3997572660446167
  - 0.4362812936306
  - 0.39520683884620667
  - 0.39336809515953064
  - 0.4739145040512085
  - 0.3937096893787384
  - 0.4235328137874603
  - 0.4026152789592743
  - 0.3944505453109741
  - 0.39499199390411377
  - 0.39731359481811523
  - 0.4081994593143463
  - 0.39304497838020325
  - 0.41269823908805847
  - 0.3907874822616577
  - 0.4294337034225464
  - 0.3925858438014984
  - 0.390017032623291
  - 0.38804730772972107
  - 0.4543328285217285
  - 0.4178484082221985
  - 0.3934364914894104
  - 0.40122270584106445
  - 0.39012083411216736
  - 0.38763973116874695
  - 0.46972349286079407
  - 0.3905855417251587
  - 0.39674896001815796
  - 0.44243448972702026
  - 0.38879260420799255
  - 0.3904070556163788
  - 0.4119763672351837
  - 0.4017903208732605
  - 0.39000725746154785
  - 0.40069779753685
  - 0.38636574149131775
  - 0.4161320626735687
  - 0.3950057327747345
  - 0.42595458030700684
  - 0.4059494733810425
  - 0.4588170349597931
loss_records_fold2:
  train_losses:
  - 14.657047122716904
  - 14.245819300413132
  - 14.170392602682114
  - 14.538700699806213
  - 14.401817828416824
  - 13.917233377695084
  - 13.955107986927032
  - 14.000198096036911
  - 13.873110115528107
  - 14.103320926427841
  - 14.158434480428696
  - 14.029411435127258
  - 14.522243708372116
  - 14.37727965414524
  - 14.261874586343765
  - 14.102516800165176
  - 14.053540170192719
  - 15.20082899928093
  - 14.562524184584618
  - 13.98150472342968
  - 14.278841704130173
  - 14.238060161471367
  - 14.529430478811264
  - 14.55580273270607
  - 14.426313862204552
  - 14.042280048131943
  - 14.418347388505936
  - 14.475203722715378
  - 15.005883052945137
  - 13.786682724952698
  - 14.083774149417877
  - 13.598173052072525
  - 14.018296763300896
  - 14.09316010773182
  - 14.009161114692688
  - 13.989134028553963
  - 14.53023724257946
  - 16.49449387192726
  - 16.46326184272766
  - 15.464591443538666
  - 14.905051156878471
  - 21.561851024627686
  - 28.85300152003765
  - 27.28076758980751
  - 25.308728963136673
  - 18.62507790327072
  - 21.587615430355072
  - 16.991056382656097
  - 15.910957306623459
  - 17.300258561968803
  - 16.985321193933487
  - 15.361738368868828
  - 15.223907142877579
  - 15.210665583610535
  - 15.709292396903038
  - 15.577828422188759
  - 15.715159744024277
  - 15.512177780270576
  - 15.898666948080063
  - 15.907846108078957
  - 15.343998476862907
  - 15.10386610031128
  - 15.78098464012146
  - 15.511889189481735
  - 15.045994222164154
  - 14.988096684217453
  - 15.000401258468628
  - 14.705297887325287
  - 15.811482384800911
  - 15.235377058386803
  - 15.81891444325447
  - 14.819065436720848
  - 15.287716954946518
  - 15.302551329135895
  - 14.980327278375626
  - 14.927076250314713
  - 15.021930605173111
  - 14.689377531409264
  - 15.45042434334755
  - 14.83414663374424
  - 14.840905517339706
  - 15.005690932273865
  - 15.299599051475525
  - 15.12117899954319
  - 14.832635000348091
  - 14.81819199025631
  - 14.885676324367523
  - 14.59015142917633
  - 15.04395142197609
  - 15.349982842803001
  - 15.032958030700684
  - 15.14819398522377
  - 15.193013906478882
  - 15.01376137137413
  - 14.420735239982605
  - 14.510582700371742
  - 15.27579315006733
  - 15.543447047472
  - 14.608854427933693
  - 14.897238850593567
  validation_losses:
  - 0.3666323125362396
  - 0.3671882450580597
  - 0.36887410283088684
  - 0.43536290526390076
  - 0.3672848641872406
  - 0.41679245233535767
  - 0.4354998767375946
  - 0.40112775564193726
  - 0.37296852469444275
  - 0.36828312277793884
  - 0.37173810601234436
  - 0.6406522393226624
  - 0.36601170897483826
  - 0.40972456336021423
  - 0.36705467104911804
  - 0.3712756335735321
  - 0.39248308539390564
  - 0.3769442141056061
  - 0.3874337375164032
  - 0.36488524079322815
  - 0.3688925802707672
  - 0.37202927470207214
  - 0.4379664659500122
  - 0.37504565715789795
  - 0.37576618790626526
  - 0.6003120541572571
  - 0.3788253366947174
  - 0.3692223131656647
  - 0.3703353703022003
  - 0.369595468044281
  - 0.4709435999393463
  - 0.36929017305374146
  - 0.3808537423610687
  - 0.4041919708251953
  - 0.36632123589515686
  - 0.38582417368888855
  - 0.4271039366722107
  - 3.868198871612549
  - 0.38197270035743713
  - 0.3833220899105072
  - 0.38239145278930664
  - 0.38280707597732544
  - 0.8844601511955261
  - 0.5199347138404846
  - 0.4589622914791107
  - 1.655332326889038
  - 0.43367111682891846
  - 0.3867335319519043
  - 0.40043988823890686
  - 0.38571587204933167
  - 0.3879527151584625
  - 0.39122599363327026
  - 0.3970617651939392
  - 0.4204014539718628
  - 0.3806656002998352
  - 0.3931415379047394
  - 0.4585167169570923
  - 0.403091698884964
  - 0.45562902092933655
  - 0.40769442915916443
  - 0.41839316487312317
  - 0.44546231627464294
  - 0.4161292612552643
  - 0.4113864600658417
  - 0.44880905747413635
  - 0.37459129095077515
  - 0.3779211938381195
  - 0.40347155928611755
  - 0.39927932620048523
  - 0.41809460520744324
  - 0.38878604769706726
  - 0.3898502290248871
  - 0.38725075125694275
  - 0.398397296667099
  - 0.40640494227409363
  - 0.38246482610702515
  - 0.4291665852069855
  - 0.43155303597450256
  - 0.3794398903846741
  - 0.38481009006500244
  - 0.3883163630962372
  - 0.40847480297088623
  - 0.4012950658798218
  - 0.3827323913574219
  - 0.3874061107635498
  - 0.3869486451148987
  - 0.3904969096183777
  - 0.40724480152130127
  - 0.42183125019073486
  - 0.3778659701347351
  - 0.37796786427497864
  - 0.44441860914230347
  - 0.3791508078575134
  - 0.38883867859840393
  - 0.3797520101070404
  - 0.39198291301727295
  - 0.4265541732311249
  - 0.3806242346763611
  - 0.40942105650901794
  - 0.43894338607788086
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 35 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8147512864493996, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0847457627118644, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8500374293410667
  mean_f1_accuracy: 0.01694915254237288
  total_train_time: '0:08:04.690280'
