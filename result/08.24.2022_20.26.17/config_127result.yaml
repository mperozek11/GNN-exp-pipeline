config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:24:53.169909'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_127fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_127fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_127fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_127fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_127fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_127fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_127fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_127fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_127fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_127fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 11.759029197692872
  - 9.158095192909242
  - 7.085204493999481
  - 8.444256854057313
  - 7.345517945289612
  - 4.818592429161072
  - 3.0590987443923954
  - 1.8981486842036248
  - 1.5078619778156281
  - 1.602539622783661
  - 1.835264539718628
  - 1.9613806366920472
  - 1.6263457059860231
  - 4.322586911916733
  - 7.8835499644279485
  - 2.877710771560669
  - 4.209308969974518
  - 2.3456789374351503
  - 2.265283513069153
  - 2.7899142980575564
  - 1.4552579522132874
  - 2.0955380618572237
  - 1.5659270286560059
  - 1.511658424139023
  - 1.207109308242798
  - 1.19357248544693
  - 5.5191298007965095
  - 6.211569809913636
  - 3.5010654211044314
  - 1.4191903352737427
  - 1.1150917828083038
  - 1.333539253473282
  - 0.9181117832660676
  - 1.0675614058971405
  - 1.594677358865738
  - 14.98068392276764
  - 1.2573739528656007
  - 2.3668980181217196
  - 1.3374910354614258
  - 1.3528366148471833
  - 2.298940968513489
  - 25.922995185852052
  - 3.0066952228546144
  - 2.9633807063102724
  - 1.623873233795166
  - 1.8250612497329712
  - 1.7518262684345247
  - 2.633765745162964
  - 1.2114323019981386
  - 1.4872210741043093
  - 5.64250899553299
  - 1.485399603843689
  - 12.055424922704697
  - 1.8473419666290285
  - 1.1519194304943086
  - 1.2250024795532228
  - 4.564821541309357
  - 4.14256666302681
  - 1.6201855540275574
  - 1.3882677644491197
  - 1.019163089990616
  - 1.0371062099933626
  - 1.3437311112880708
  - 1.0271935880184173
  - 0.9989912211894989
  - 0.846682769060135
  - 0.9129885911941529
  - 0.9488690316677094
  - 0.8337261140346528
  - 1.0296973288059235
  - 0.9511345028877258
  - 1.1356343805789948
  - 0.8784058690071106
  - 0.8400222361087799
  - 1.009720903635025
  - 0.9825152516365052
  - 1.186481559276581
  - 0.936060917377472
  - 0.875202888250351
  - 0.9335440099239349
  - 1.2437775790691377
  - 0.8473032355308533
  - 0.9306271135807038
  - 0.8346871912479401
  - 0.8670099794864655
  - 0.8580276727676392
  validation_losses:
  - 9.038614273071289
  - 3.1369941234588623
  - 2.3055293560028076
  - 1.5747435092926025
  - 2.8166844844818115
  - 1.1805070638656616
  - 0.8348244428634644
  - 0.5090975761413574
  - 0.8049512505531311
  - 0.7440898418426514
  - 0.45931217074394226
  - 0.6360619068145752
  - 0.6596588492393494
  - 0.7470459938049316
  - 1.247976303100586
  - 0.9370749592781067
  - 1.2445039749145508
  - 0.9879372715950012
  - 0.6301065683364868
  - 0.5188730955123901
  - 0.5815660953521729
  - 0.600741446018219
  - 0.6440386176109314
  - 0.5714256167411804
  - 0.43771618604660034
  - 0.4666874408721924
  - 0.45041128993034363
  - 0.49070289731025696
  - 0.4738847017288208
  - 0.5833659768104553
  - 0.4358217418193817
  - 0.44672292470932007
  - 0.40442419052124023
  - 0.4897693395614624
  - 0.4722997844219208
  - 0.43957939743995667
  - 0.5622382760047913
  - 0.48629045486450195
  - 0.5093761086463928
  - 0.44211867451667786
  - 0.509291410446167
  - 0.6784878373146057
  - 0.5024455189704895
  - 0.5542625784873962
  - 0.4635908007621765
  - 0.5452907085418701
  - 0.568152129650116
  - 0.4979255497455597
  - 0.40917572379112244
  - 0.49941059947013855
  - 0.4484733045101166
  - 0.9078924059867859
  - 0.421558678150177
  - 0.41294461488723755
  - 0.4318794310092926
  - 0.4067685604095459
  - 0.4316275119781494
  - 0.40204256772994995
  - 0.46086055040359497
  - 0.41415736079216003
  - 0.4227807819843292
  - 0.416439950466156
  - 0.4224737584590912
  - 0.45143917202949524
  - 0.42140454053878784
  - 0.47343599796295166
  - 0.4318416714668274
  - 0.41164925694465637
  - 0.39568039774894714
  - 0.4024340510368347
  - 0.41608211398124695
  - 0.42428794503211975
  - 0.42119300365448
  - 0.4065108597278595
  - 0.43729883432388306
  - 0.4153892695903778
  - 0.40906161069869995
  - 0.4122198820114136
  - 0.39427921175956726
  - 0.4155307114124298
  - 0.400739461183548
  - 0.40665552020072937
  - 0.39502137899398804
  - 0.4024110734462738
  - 0.3942534625530243
  - 0.39732810854911804
loss_records_fold1:
  train_losses:
  - 0.8316542506217957
  - 0.875951486825943
  - 1.9441682636737825
  - 2.129993569850922
  - 0.9050726056098939
  - 0.9956894636154175
  - 0.87696573138237
  - 0.94808629155159
  - 1.0147430062294007
  - 0.9373756885528565
  - 1.0214451253414154
  validation_losses:
  - 0.4190577268600464
  - 0.42273086309432983
  - 0.9219807386398315
  - 0.4037019908428192
  - 0.40709275007247925
  - 0.4133515954017639
  - 0.40504369139671326
  - 0.41440483927726746
  - 0.41844865679740906
  - 0.40702733397483826
  - 0.4063035845756531
loss_records_fold2:
  train_losses:
  - 0.9401928722858429
  - 1.1606169939041138
  - 0.8204456210136414
  - 1.2155697524547577
  - 3.6968586027622226
  - 3.4263857901096344
  - 3.1462988317012788
  - 1.0255794286727906
  - 1.2724803268909455
  - 1.1679385244846345
  - 1.1878101885318757
  - 4.602651673555374
  - 1.8070302605628967
  - 0.9200451850891114
  - 1.5544532358646395
  - 0.9311144530773163
  - 1.0836795747280121
  - 0.9359463095664978
  - 0.9014400780200958
  - 0.8750231683254243
  - 1.0184691965579986
  - 1.1414467811584472
  - 0.8919808328151704
  - 0.9402382493019105
  - 0.8663140714168549
  - 1.0183017134666443
  - 11.200557357072832
  - 0.9154941141605377
  - 1.8191240191459657
  - 5.3744197428226474
  - 1.8102255821228028
  - 1.2423346400260926
  - 2.424005401134491
  - 1.1237462043762207
  - 0.8967568755149842
  - 2.8123134076595306
  - 3.729014712572098
  - 1.089961987733841
  - 0.8911047160625458
  - 0.8336237013339997
  - 0.845545780658722
  - 0.9520174026489259
  validation_losses:
  - 0.41321080923080444
  - 0.39201611280441284
  - 0.40197691321372986
  - 0.4018508493900299
  - 0.383760541677475
  - 0.42564937472343445
  - 0.3832528293132782
  - 0.43456730246543884
  - 0.39096692204475403
  - 0.4019128382205963
  - 0.399759978055954
  - 0.4060294032096863
  - 0.43407270312309265
  - 0.3891139030456543
  - 0.4164329171180725
  - 0.39373669028282166
  - 0.3928702771663666
  - 0.3764120936393738
  - 0.39546847343444824
  - 0.38792166113853455
  - 0.42069852352142334
  - 0.4123762249946594
  - 0.4150577187538147
  - 0.41522216796875
  - 0.3767428398132324
  - 0.3912142813205719
  - 0.3986922800540924
  - 0.40395957231521606
  - 0.3937399089336395
  - 0.4787851572036743
  - 0.42771315574645996
  - 0.3946089446544647
  - 0.4090835452079773
  - 0.4017582833766937
  - 0.4039972722530365
  - 0.476810485124588
  - 0.4053032398223877
  - 0.39582929015159607
  - 0.3785852789878845
  - 0.38502761721611023
  - 0.3908159136772156
  - 0.3801670968532562
loss_records_fold3:
  train_losses:
  - 0.9683181047439575
  - 0.922541630268097
  - 2.4133980810642246
  - 0.9516921281814575
  - 1.132374930381775
  - 0.8294209182262421
  - 5.392415791749954
  - 0.8511416077613831
  - 1.2299610376358032
  - 0.9787448287010193
  - 1.0840762674808502
  - 0.9114345610141754
  - 0.8524254381656647
  - 1.314014804363251
  - 1.5042584836483002
  - 0.9179234683513642
  - 1.0327332735061645
  - 0.873214340209961
  - 0.8306212365627289
  - 0.8634188055992127
  - 0.8481110632419586
  - 0.8214427828788757
  - 0.8521999657154083
  - 0.8615819931030274
  - 0.863110375404358
  - 0.8400529742240906
  - 0.9006439566612244
  - 1.770848697423935
  - 0.8567042589187622
  - 0.8499097704887391
  - 0.7969679802656174
  - 0.9775263905525208
  - 0.8980444014072418
  - 0.8092952460050583
  - 0.848459279537201
  - 1.114891928434372
  - 0.9012389242649079
  - 2.2384216129779815
  - 0.8213753402233124
  - 0.8207343965768814
  - 0.8415503561496736
  - 0.7936610579490662
  - 0.8481772065162659
  - 0.8258828878402711
  validation_losses:
  - 0.40524691343307495
  - 0.44113269448280334
  - 0.390076220035553
  - 0.3865324556827545
  - 0.3977784216403961
  - 0.3923027515411377
  - 0.40469151735305786
  - 0.40349894762039185
  - 0.39228516817092896
  - 0.45981982350349426
  - 0.42460814118385315
  - 0.39249637722969055
  - 0.39681071043014526
  - 0.39939045906066895
  - 0.43055078387260437
  - 0.4273652732372284
  - 0.4041142165660858
  - 0.4084165394306183
  - 0.42705726623535156
  - 0.4033280611038208
  - 0.41808390617370605
  - 0.42158016562461853
  - 0.3920542597770691
  - 0.4068472385406494
  - 0.4391378164291382
  - 0.4133313000202179
  - 0.39438745379447937
  - 0.4231511354446411
  - 0.39602231979370117
  - 0.3959086835384369
  - 0.4027917683124542
  - 0.4068397581577301
  - 0.42940038442611694
  - 0.4529443085193634
  - 0.4094924330711365
  - 0.44363588094711304
  - 0.41627004742622375
  - 0.42809316515922546
  - 0.39708781242370605
  - 0.39830851554870605
  - 0.397367000579834
  - 0.39473605155944824
  - 0.4034327566623688
  - 0.4045240879058838
loss_records_fold4:
  train_losses:
  - 0.829137134552002
  - 0.8392043471336366
  - 0.8703351259231568
  - 0.8561019539833069
  - 0.8890023410320282
  - 0.8525301456451416
  - 0.7962401568889619
  - 1.2966794431209565
  - 0.9390377044677735
  - 0.8336945593357087
  - 0.844112366437912
  - 0.8243172824382783
  - 0.8625443458557129
  - 0.8028433322906494
  - 0.9079931557178498
  - 0.9584101378917694
  - 1.1355014979839326
  - 1.00471510887146
  - 1.0912432193756103
  - 0.9737614154815675
  - 0.8818235516548157
  - 0.9069878041744233
  - 0.9301223695278168
  - 0.8439160168170929
  - 0.9003545641899109
  - 0.8253831565380096
  - 0.8434604823589326
  - 0.8395530998706818
  - 0.8452128410339356
  - 0.9384331822395325
  - 0.8667277336120606
  - 0.8032427847385407
  - 0.8916418671607972
  - 0.8568317890167236
  - 0.8503992557525635
  - 0.8259658515453339
  - 0.8084971368312837
  - 0.8441210865974427
  - 0.8283657848834992
  - 0.867046481370926
  - 0.8331855475902558
  - 0.8156310141086579
  - 0.8184373736381532
  - 0.8582602858543397
  - 0.8589480876922608
  - 0.8543339669704437
  - 0.8513668596744538
  - 0.8245999097824097
  - 0.8381987035274506
  - 0.8179072618484498
  - 0.8002452462911607
  - 0.8133320391178132
  - 0.8708802580833436
  - 0.868014121055603
  - 0.8049779415130616
  - 0.7931475013494492
  - 0.8634332358837128
  - 0.9103365778923035
  - 1.0268404364585877
  - 0.823254829645157
  - 0.8050833880901337
  - 0.8308838248252869
  - 0.8297984898090363
  - 0.8158467292785645
  - 0.8286488234996796
  - 0.820607876777649
  - 0.8859774112701416
  - 0.8949099242687226
  - 0.8698798835277558
  - 0.859455007314682
  - 0.8012514919042588
  - 1.281390517950058
  - 1.0647399067878724
  - 0.9099217832088471
  - 0.9774848222732544
  - 0.9698522388935089
  - 0.7938105762004852
  - 0.8920748233795166
  - 1.0867216110229492
  - 0.8467045724391937
  validation_losses:
  - 0.3993065059185028
  - 0.40679073333740234
  - 0.4274609684944153
  - 0.44873929023742676
  - 0.4057311713695526
  - 0.392961710691452
  - 0.4337765872478485
  - 0.43861091136932373
  - 0.4175560474395752
  - 0.4305163621902466
  - 0.4014380872249603
  - 0.40892961621284485
  - 0.43333005905151367
  - 0.39134007692337036
  - 0.43850550055503845
  - 0.4145067632198334
  - 0.41234469413757324
  - 0.4856339991092682
  - 0.4863070547580719
  - 0.5158212184906006
  - 0.5489102005958557
  - 0.4937950372695923
  - 0.48715686798095703
  - 0.43170931935310364
  - 0.4780804216861725
  - 0.4508901834487915
  - 0.48249727487564087
  - 0.44534942507743835
  - 0.394783616065979
  - 0.49272140860557556
  - 0.4549294114112854
  - 0.4869225323200226
  - 0.4810739755630493
  - 0.4450427293777466
  - 0.4283539056777954
  - 0.4088016450405121
  - 0.40771567821502686
  - 0.4408855736255646
  - 0.42737963795661926
  - 0.43625083565711975
  - 0.40544024109840393
  - 0.4597388207912445
  - 0.4511299729347229
  - 0.4513609707355499
  - 0.39288708567619324
  - 0.42941999435424805
  - 0.41404613852500916
  - 0.4320473372936249
  - 0.41660046577453613
  - 0.39722803235054016
  - 0.4402419626712799
  - 0.44217416644096375
  - 0.441159188747406
  - 0.44082528352737427
  - 0.40822234749794006
  - 0.42628732323646545
  - 0.4250139892101288
  - 0.41383275389671326
  - 0.4411316514015198
  - 0.4075946807861328
  - 0.4042373299598694
  - 0.40898647904396057
  - 0.43712136149406433
  - 0.4148571789264679
  - 0.42157062888145447
  - 0.4131181836128235
  - 0.42687928676605225
  - 0.4263665974140167
  - 0.4471503794193268
  - 0.3979724645614624
  - 0.42126962542533875
  - 0.43058890104293823
  - 0.4061453938484192
  - 0.4463442862033844
  - 0.4409152865409851
  - 0.44344931840896606
  - 0.41997283697128296
  - 0.4272041916847229
  - 0.4112911820411682
  - 0.39997947216033936
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 86 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 42 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 44 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 80 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:13:54.195789'
