config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:34:58.107756'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_22fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_22fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_22fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_22fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_22fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_22fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_22fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_22fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_22fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_22fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.95902019739151
  - 1.666824597120285
  - 1.6348140299320222
  - 1.6093154430389405
  - 1.6220103204250336
  - 1.5825635731220247
  - 1.6084995687007906
  - 1.6204273998737335
  - 1.5746424496173859
  - 1.5932967603206636
  - 1.5083622723817827
  - 1.561175322532654
  - 1.5749976933002472
  - 1.6534999787807465
  - 1.5913465976715089
  - 1.5998457849025727
  - 1.5514186322689056
  - 1.485772031545639
  - 1.5234917759895326
  - 1.54128075838089
  - 1.5865771889686586
  - 1.555819207429886
  - 1.5976054251194
  - 1.6015893816947937
  - 1.6295872509479523
  - 1.5814984500408173
  - 1.6561024963855744
  validation_losses:
  - 0.4297482669353485
  - 0.42044445872306824
  - 0.41382861137390137
  - 0.3910687565803528
  - 0.393571138381958
  - 0.4023951292037964
  - 0.400164395570755
  - 0.3914780020713806
  - 0.3861159086227417
  - 0.39956003427505493
  - 0.3924368619918823
  - 0.40005096793174744
  - 0.3947908878326416
  - 0.38399598002433777
  - 0.4006509482860565
  - 0.3826411962509155
  - 0.41081008315086365
  - 0.3844461739063263
  - 0.39079368114471436
  - 0.38783758878707886
  - 0.4279331862926483
  - 0.3948018252849579
  - 0.3926359713077545
  - 0.3941195011138916
  - 0.38729849457740784
  - 0.39222028851509094
  - 0.3906591534614563
loss_records_fold1:
  train_losses:
  - 1.544384616613388
  - 1.593277817964554
  - 1.4775430500507356
  - 1.5222379088401796
  - 1.5486045837402345
  - 1.521962594985962
  - 1.5647806227207184
  - 1.5011234164237977
  - 1.5265995383262636
  - 1.5282741308212282
  - 1.5843033373355866
  - 1.5100379824638368
  - 1.552854770421982
  - 1.564636391401291
  - 1.5314485698938372
  - 1.5924086570739746
  - 1.5646857500076294
  validation_losses:
  - 0.38560038805007935
  - 0.40304508805274963
  - 0.39247483015060425
  - 0.41012120246887207
  - 0.43236058950424194
  - 0.40655237436294556
  - 0.3906168043613434
  - 0.4151283800601959
  - 0.4002561569213867
  - 0.38716423511505127
  - 0.3992685377597809
  - 0.3887382745742798
  - 0.39605191349983215
  - 0.3978232741355896
  - 0.39962896704673767
  - 0.40708839893341064
  - 0.39657604694366455
loss_records_fold2:
  train_losses:
  - 1.6003122210502625
  - 1.556155186891556
  - 1.502752661705017
  - 1.5482867598533632
  - 1.5299472630023958
  - 1.5890791833400728
  - 1.5574820280075075
  - 1.5485629618167878
  - 1.522719818353653
  - 1.4831478893756866
  - 1.568334048986435
  - 1.5463561713695526
  - 1.5249070882797242
  - 1.5286494255065919
  - 1.5765066862106325
  - 1.6235608637332917
  - 1.536717015504837
  - 1.5405955016613007
  validation_losses:
  - 0.5665139555931091
  - 0.4916832149028778
  - 0.5342798829078674
  - 0.4191409945487976
  - 0.39046511054039
  - 0.4426233470439911
  - 0.4040578603744507
  - 0.39579495787620544
  - 0.5308677554130554
  - 0.7471927404403687
  - 0.6738374829292297
  - 0.8804661631584167
  - 0.40357473492622375
  - 0.4010974168777466
  - 0.3945213556289673
  - 0.3965008556842804
  - 0.39172640442848206
  - 0.38621827960014343
loss_records_fold3:
  train_losses:
  - 1.5468263983726502
  - 1.501099741458893
  - 1.5217392802238465
  - 1.496232408285141
  - 1.5688570380210878
  - 1.5564341008663178
  - 1.5375798344612122
  - 1.521220850944519
  - 1.5697503685951233
  - 1.5064081609249116
  - 1.5299197971820833
  - 1.5132746636867525
  - 1.5091783106327057
  - 1.5093070924282075
  - 1.5371120631694795
  - 1.5174192905426027
  - 1.5404038965702058
  - 1.5115925252437592
  - 1.5204694986343386
  - 1.482677298784256
  - 1.4742865324020387
  - 1.5285628736019135
  - 1.5348627269268036
  - 1.5402182281017305
  - 1.578003078699112
  - 1.5524426937103273
  - 1.5531863093376161
  - 1.5812632858753206
  - 1.6066867470741273
  - 1.580461096763611
  - 1.5209271311759949
  - 1.5448512613773346
  - 1.5628523588180543
  - 1.5171473503112793
  - 1.5477149188518524
  - 1.5563437402248383
  - 1.6272140860557558
  - 1.5305232346057893
  - 1.5478435754776
  - 1.5800170958042146
  - 1.5953691601753235
  - 1.541201186180115
  - 1.5362234115600586
  - 1.5405582427978517
  - 1.5930764436721803
  - 1.5297195374965669
  - 1.5440192461013795
  - 1.5242748856544495
  - 1.5232197999954225
  - 1.4976099133491516
  - 1.5001743465662003
  - 1.499360489845276
  - 1.5308602273464205
  - 1.489962124824524
  - 1.5562479913234712
  - 1.6029504239559174
  - 1.503961220383644
  - 1.5128681659698486
  - 1.5238623678684236
  - 1.526789152622223
  - 1.5198157191276551
  - 1.5200522720813752
  - 1.518508642911911
  - 1.5303473055362702
  - 1.5436907887458802
  - 1.5020946264266968
  - 1.5215164482593537
  - 1.523000353574753
  - 1.5298148810863497
  - 1.5805759847164156
  - 1.5367691934108736
  - 1.4579678237438203
  - 1.523146939277649
  - 1.496620124578476
  - 1.4953755795955659
  - 1.5123142123222353
  - 1.4975354850292206
  - 1.5100389420986176
  - 1.519956821203232
  - 1.507821574807167
  - 1.50009201169014
  - 1.520717465877533
  - 1.491347360610962
  - 1.544757342338562
  - 1.480652767419815
  - 1.4930579185485842
  - 1.5114844024181366
  - 1.5198342263698579
  - 1.4990749061107635
  - 1.5038860261440279
  - 1.4882829010486605
  - 1.54961296916008
  - 1.5238359600305558
  - 1.520626723766327
  - 1.4990851879119873
  - 1.552987736463547
  - 1.5357643961906433
  - 1.5069932222366333
  - 1.4562423050403597
  - 1.4949198722839356
  validation_losses:
  - 0.38416755199432373
  - 0.40335559844970703
  - 0.36695101857185364
  - 0.43261975049972534
  - 0.40867215394973755
  - 0.3988488018512726
  - 0.40426379442214966
  - 0.3890717923641205
  - 0.5373755097389221
  - 0.3736400902271271
  - 0.46921446919441223
  - 0.5836306214332581
  - 0.5323771834373474
  - 0.7847427129745483
  - 0.38377872109413147
  - 0.45549628138542175
  - 0.4922928810119629
  - 0.611498236656189
  - 0.4448484182357788
  - 0.38392359018325806
  - 0.7109295725822449
  - 0.6120441555976868
  - 0.3800796866416931
  - 1.7734754085540771
  - 0.5162228941917419
  - 0.4141058921813965
  - 0.5906652212142944
  - 0.4156031012535095
  - 0.6366894245147705
  - 0.7852680683135986
  - 0.38864803314208984
  - 0.3784940242767334
  - 0.3802512586116791
  - 0.3835562765598297
  - 0.3885086476802826
  - 0.41646063327789307
  - 0.39582085609436035
  - 0.4007275402545929
  - 0.41520509123802185
  - 0.4417828917503357
  - 0.5083686113357544
  - 0.7619869112968445
  - 0.6747188568115234
  - 0.6941635012626648
  - 0.4418894052505493
  - 0.46906089782714844
  - 0.49093320965766907
  - 0.5360057353973389
  - 0.5812197923660278
  - 0.6548590064048767
  - 0.4528141915798187
  - 0.38345322012901306
  - 0.5283539295196533
  - 0.40100470185279846
  - 0.47817015647888184
  - 0.6470090746879578
  - 0.49779415130615234
  - 0.5277906656265259
  - 0.5317404270172119
  - 0.5617143511772156
  - 0.5677351951599121
  - 0.5144687294960022
  - 0.6130218505859375
  - 0.6744512915611267
  - 0.8771802186965942
  - 0.8721012473106384
  - 1.0324028730392456
  - 0.3769533932209015
  - 0.6368192434310913
  - 1.1363019943237305
  - 0.8154473900794983
  - 0.6270099878311157
  - 0.6786910891532898
  - 0.6192265152931213
  - 0.6313472390174866
  - 0.5553812384605408
  - 0.5099480152130127
  - 0.4890151917934418
  - 0.4720016121864319
  - 0.5532004833221436
  - 0.6072167754173279
  - 0.8532357811927795
  - 0.7946652770042419
  - 0.5381948351860046
  - 0.4631301760673523
  - 0.48053979873657227
  - 0.5769177079200745
  - 0.6954293251037598
  - 0.45758968591690063
  - 0.5985828638076782
  - 0.5140554904937744
  - 0.5913246273994446
  - 0.5187138319015503
  - 0.5561218857765198
  - 0.5392603874206543
  - 0.613044261932373
  - 0.5521103143692017
  - 0.8964551687240601
  - 1.0776832103729248
  - 0.5952774882316589
loss_records_fold4:
  train_losses:
  - 1.4977644950151445
  - 1.508328902721405
  - 1.4586583256721497
  - 1.525689446926117
  - 1.5047156810760498
  - 1.5099154114723206
  - 1.4732769191265107
  - 1.477741187810898
  - 1.535834389925003
  - 1.5015657544136047
  - 1.5153958916664125
  - 1.5408607304096222
  - 1.5154891550540925
  - 1.5272546768188477
  - 1.5404975891113282
  - 1.5258426725864411
  - 1.5275886595249177
  - 1.508693540096283
  - 1.4988213419914247
  - 1.4587843149900437
  - 1.494189327955246
  - 1.5315764725208283
  - 1.4964556694030762
  - 1.5261498093605042
  - 1.5665406584739685
  - 1.5437878668308258
  - 1.498220080137253
  - 1.4887370705604555
  - 1.5026902556419373
  - 1.525355225801468
  - 1.499281042814255
  - 1.5403248131275178
  - 1.5698509752750398
  - 1.5733022391796112
  - 1.4998905658721924
  - 1.5759409368038177
  validation_losses:
  - 0.4157995581626892
  - 0.44395121932029724
  - 0.5304474234580994
  - 0.5223917365074158
  - 0.5581052303314209
  - 0.43705591559410095
  - 0.38398846983909607
  - 0.4181331992149353
  - 0.45286455750465393
  - 0.3819573223590851
  - 0.3685203790664673
  - 0.42749258875846863
  - 0.3883121609687805
  - 0.41608092188835144
  - 0.3878530263900757
  - 0.3949614465236664
  - 0.3741513788700104
  - 0.4691421091556549
  - 0.42055508494377136
  - 0.4262154698371887
  - 0.47011321783065796
  - 0.38852599263191223
  - 0.4060247540473938
  - 0.37203285098075867
  - 0.39822477102279663
  - 0.42629748582839966
  - 0.3801562190055847
  - 0.37384170293807983
  - 0.39736586809158325
  - 0.4460863471031189
  - 0.44133421778678894
  - 0.38699907064437866
  - 0.3840055465698242
  - 0.37407928705215454
  - 0.36962199211120605
  - 0.3754529058933258
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 36 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8456260720411664,
    0.8608247422680413]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.15094339622641506, 0.024096385542168676]'
  mean_eval_accuracy: 0.8558699227246201
  mean_f1_accuracy: 0.03500795635371674
  total_train_time: '0:16:29.379568'
