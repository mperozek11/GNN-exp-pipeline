config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:38:27.086223'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_104fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_104fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_104fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_104fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_104fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_104fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_104fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_104fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_104fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_104fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 68.1576978802681
  - 40.767345185577874
  - 31.359494177997114
  - 22.729377722367644
  - 17.326749125123026
  - 11.59073287844658
  - 21.86350436806679
  - 28.49382523447275
  - 30.07696603536606
  - 15.810114961862565
  - 17.188223001360893
  - 16.663626742362975
  - 34.201351043581965
  - 16.708415415883064
  - 8.721291795372963
  - 7.289738529920578
  - 10.237685820460321
  - 11.14839523434639
  - 8.064849370718003
  - 8.99043641090393
  - 8.420547984540462
  - 7.533985805511475
  - 7.13869498372078
  - 8.83159066438675
  - 7.673238301277161
  - 7.226463517546654
  - 6.883433932065964
  - 7.1525665611028675
  - 6.44123821258545
  - 6.640921056270599
  - 6.665089532732964
  - 7.840115098655224
  - 6.7943414747715
  - 6.697989857196808
  - 7.119850480556488
  - 6.309280768036842
  - 6.302446791529656
  - 6.005406510829926
  - 5.883700820803643
  - 5.9760532736778265
  - 5.985563746094704
  - 6.090636563301087
  - 5.706242462992669
  - 6.5920810312032705
  - 6.220801487565041
  - 6.142066010832787
  - 5.928025224804879
  - 5.8094445198774345
  - 6.119996792078019
  - 5.816328838467598
  - 6.222346910834313
  - 6.440209987759591
  - 5.9558490961790085
  - 6.1289412736892706
  - 5.787190109491348
  - 5.7159922823309905
  - 5.968191352486611
  - 6.786858886480331
  - 6.337867115437985
  - 6.019944259524346
  - 6.1244504153728485
  - 6.078728479146958
  - 5.889856326580048
  - 5.895923274755479
  - 6.354752391576767
  - 6.032291413843632
  - 6.073976963758469
  - 6.017676667869091
  - 5.967337021231652
  - 5.861454835534096
  - 7.236823645234108
  - 7.630008411407471
  - 6.429968059062958
  - 6.177186897397042
  - 5.761077296733856
  - 6.0445237390697
  - 5.838428124785423
  - 5.783843094110489
  - 6.660246416926384
  - 7.011433403193951
  - 6.2849611163139345
  - 6.999404239654542
  - 6.474750873446465
  - 6.112353149056435
  - 6.2638492554426195
  - 6.108216154575349
  - 6.029036137461663
  - 6.047928446531296
  - 6.1430151730775835
  - 6.564130353927613
  - 6.113182978332043
  - 6.030592635273933
  - 5.913998779654503
  - 5.978052568435669
  - 6.209652802348137
  - 5.960266974568367
  - 6.005361151695252
  - 5.931501016020775
  - 5.873068490624428
  - 6.0975212275981905
  validation_losses:
  - 5.319820880889893
  - 1.1151728630065918
  - 0.4317237138748169
  - 0.5535194873809814
  - 0.48267942667007446
  - 0.596760094165802
  - 1.7960262298583984
  - 0.537214994430542
  - 0.5469443202018738
  - 0.4351131319999695
  - 3.6167831420898438
  - 0.4462204575538635
  - 0.3802962899208069
  - 0.4076448678970337
  - 0.4495639204978943
  - 0.3776032030582428
  - 0.39062270522117615
  - 0.44205525517463684
  - 0.5170535445213318
  - 0.3774334192276001
  - 0.385464608669281
  - 0.38596245646476746
  - 0.3879483640193939
  - 2.1695291996002197
  - 0.40549376606941223
  - 2.0914909839630127
  - 0.4458405375480652
  - 0.4697542190551758
  - 0.4160311222076416
  - 0.39765068888664246
  - 0.3771394193172455
  - 0.3893893361091614
  - 0.3871915340423584
  - 0.4019840955734253
  - 0.4098055362701416
  - 0.3814956545829773
  - 0.37677791714668274
  - 0.3821207880973816
  - 0.42862653732299805
  - 0.387510746717453
  - 0.378316730260849
  - 0.38502609729766846
  - 0.433555543422699
  - 1.6655644178390503
  - 0.3971613347530365
  - 1.8586158752441406
  - 2.1998097896575928
  - 0.3831665813922882
  - 1.464195728302002
  - 33.5454216003418
  - 1.0811240673065186
  - 0.44331762194633484
  - 0.9518187046051025
  - 0.3756812512874603
  - 3.310399055480957
  - 8.930188179016113
  - 2.5319809913635254
  - 0.39014914631843567
  - 1.548402190208435
  - 0.4292612671852112
  - 0.3988126814365387
  - 0.6717720627784729
  - 0.4847237467765808
  - 0.39598342776298523
  - 0.39844781160354614
  - 0.3940372169017792
  - 0.4278501272201538
  - 0.3916243016719818
  - 0.390775203704834
  - 0.3912257254123688
  - 0.7142412066459656
  - 0.445780485868454
  - 0.4469638168811798
  - 6.9544525146484375
  - 0.4019112288951874
  - 0.44224944710731506
  - 0.39013323187828064
  - 584.5200805664062
  - 0.40715456008911133
  - 0.41468551754951477
  - 0.4267335832118988
  - 0.3880091607570648
  - 0.4194314777851105
  - 0.4270237982273102
  - 0.45416387915611267
  - 0.40945518016815186
  - 0.39120346307754517
  - 0.39203593134880066
  - 0.4503955543041229
  - 0.40876901149749756
  - 0.391683429479599
  - 0.3944769501686096
  - 0.4030283987522125
  - 0.4258509576320648
  - 0.6827887892723083
  - 0.4522702097892761
  - 0.4191385805606842
  - 0.47046756744384766
  - 0.7758709192276001
  - 0.5120070576667786
loss_records_fold1:
  train_losses:
  - 5.902018567919732
  - 5.809534534811974
  - 5.9002267718315125
  - 6.282580907642842
  - 6.070288357138634
  - 6.085251969099045
  - 6.037494367361069
  - 5.856779184937477
  - 5.976808373630047
  - 6.233935931324959
  - 6.009017696976662
  - 6.07451640367508
  - 5.867507615685463
  - 5.822032070159913
  - 5.7887999594211585
  - 5.8032636359333996
  - 5.904841929674149
  - 5.957367786765099
  - 5.960994121432305
  - 5.813496893644333
  - 5.8497788220644
  - 5.975353565812111
  - 5.745277410745621
  - 5.798822104930878
  - 5.815179187059403
  - 5.833968961238861
  - 5.907877212762833
  - 6.170776909589768
  - 5.967741823196412
  - 5.857104656100273
  - 5.851379501819611
  - 5.89107349216938
  - 5.958605989813805
  - 5.962345068156719
  - 5.904946732521058
  - 6.00618654191494
  - 5.96904690861702
  - 5.783772712945939
  - 6.09754687845707
  - 5.8668892890214925
  - 5.880155271291733
  - 5.83735516667366
  - 5.868643349409104
  - 5.9386899173259735
  - 6.069099867343903
  - 5.941048496961594
  - 5.814892664551735
  - 5.931912779808044
  - 5.846058550477029
  - 5.987905670702458
  - 5.858868038654328
  - 6.123842921853066
  - 5.897031030058861
  - 6.165359212458134
  - 5.833383265137673
  - 5.8974277734756475
  - 5.852901357412339
  - 5.817457711696625
  - 5.888396349549294
  - 6.272087591886521
  - 5.814840397238732
  - 5.935359218716622
  - 6.0264474004507065
  - 5.809239572286606
  - 5.895208439230919
  - 5.981272760033608
  - 6.003214570879937
  - 5.8577323943376545
  - 6.024301978945733
  validation_losses:
  - 0.4062998294830322
  - 0.8229847550392151
  - 0.4509205222129822
  - 0.4335009753704071
  - 0.4523273706436157
  - 0.4326975643634796
  - 1.017266035079956
  - 0.6155967116355896
  - 0.4686746895313263
  - 0.41624125838279724
  - 0.4199448525905609
  - 0.5377741456031799
  - 0.599516749382019
  - 163.3716278076172
  - 1167658368.0
  - 8705897136128.0
  - 17406941184.0
  - 2401179795456.0
  - 3569080074240.0
  - 2051626631168.0
  - 3226787381248.0
  - 3193531269120.0
  - 752809410560.0
  - 3197138108416.0
  - 1702048301056.0
  - 845336346624.0
  - 1745480581120.0
  - 1591263494144.0
  - 1066092.75
  - 11207471104.0
  - 0.40405920147895813
  - 1788535242752.0
  - 1682552520704.0
  - 1299519504384.0
  - 23763171147776.0
  - 3419134754816.0
  - 105962135552.0
  - 2735866118144.0
  - 980918206464.0
  - 3373746356224.0
  - 5903138422784.0
  - 7641230737408.0
  - 1698515779584.0
  - 163610312704.0
  - 1385018949632.0
  - 2998885941248.0
  - 1509270749184.0
  - 2218288218112.0
  - 1557168521216.0
  - 1389576847360.0
  - 1.0260823965072632
  - 45093352.0
  - 0.45044952630996704
  - 0.4147738516330719
  - 0.41995882987976074
  - 0.4106300473213196
  - 0.4243907630443573
  - 0.4059157371520996
  - 0.4370148181915283
  - 0.4072215259075165
  - 0.4068864583969116
  - 0.40777260065078735
  - 0.42245838046073914
  - 0.4173363149166107
  - 0.41020622849464417
  - 0.4152935743331909
  - 0.4223344027996063
  - 0.4198451042175293
  - 0.40676239132881165
loss_records_fold2:
  train_losses:
  - 5.992728978395462
  - 5.900667345523835
  - 6.157126919925213
  - 5.980635923147202
  - 5.927273717522621
  - 6.111144554615021
  - 5.99913370013237
  - 5.885186865925789
  - 6.168832971155644
  - 5.919013449549675
  - 6.08093997836113
  - 5.916953578591347
  - 6.178022995591164
  - 5.96356488764286
  - 5.9343456625938416
  - 5.90984500348568
  - 5.911915197968483
  - 10.368588715791702
  - 10.184575638175012
  - 11.87518728375435
  - 6.038468439877033
  - 6.094147831201553
  - 11.415651406347752
  - 8.724024039506913
  - 6.859341111779213
  - 6.9696202874183655
  - 6.672424140572549
  - 6.120864778757095
  - 6.4064720422029495
  - 7.401819595694542
  - 7.030507674813271
  - 6.49820386171341
  - 6.451708862185479
  - 5.9505524933338165
  - 6.943177804350853
  - 6.091214063763619
  - 6.064897179603577
  - 6.140334895253182
  - 6.077946192026139
  - 6.010378792881966
  - 6.326367726922036
  - 6.329882435500622
  - 6.410256561636925
  - 5.960917234420776
  - 6.023090234398842
  - 6.0391919106245044
  validation_losses:
  - 0.4145529270172119
  - 0.5046190023422241
  - 0.39443808794021606
  - 0.39199137687683105
  - 0.3948168158531189
  - 0.4218055307865143
  - 0.4050559103488922
  - 0.39487573504447937
  - 0.3839111924171448
  - 0.3899978995323181
  - 0.3890441656112671
  - 0.4320236146450043
  - 0.39540910720825195
  - 0.41009223461151123
  - 0.3913171589374542
  - 0.38856691122055054
  - 0.38442695140838623
  - 0.47608816623687744
  - 0.5157477855682373
  - 1181.6248779296875
  - 0.39809688925743103
  - 0.3842753767967224
  - 0.43281203508377075
  - 0.3830305337905884
  - 0.38101232051849365
  - 0.39560502767562866
  - 0.3813157379627228
  - 0.39736664295196533
  - 0.4054591655731201
  - 0.3950873911380768
  - 0.41582342982292175
  - 0.3843325674533844
  - 0.39094504714012146
  - 0.3823002576828003
  - 0.43277716636657715
  - 0.38320404291152954
  - 0.3911934792995453
  - 0.40474608540534973
  - 0.3847832977771759
  - 0.39549875259399414
  - 0.39276251196861267
  - 0.3801027834415436
  - 0.3806493580341339
  - 0.3839569389820099
  - 0.3841361999511719
  - 0.3909595012664795
loss_records_fold3:
  train_losses:
  - 5.997557982802391
  - 6.134685668349267
  - 5.937088391184807
  - 5.954006922245026
  - 6.122821664810181
  - 6.050021755695344
  - 5.962120041251183
  - 5.995731928944588
  - 5.934373363852501
  - 6.183026590943337
  - 5.999741879105568
  - 5.873788437247277
  - 5.936098703742028
  - 5.823819333314896
  - 5.853535531461239
  - 5.878401041030884
  - 5.904920655488969
  - 5.809470915794373
  - 5.906017790734769
  - 6.3003408223390585
  - 5.939281967282295
  - 5.888020113110542
  - 6.0594664275646215
  - 5.832414296269417
  - 5.912374275922776
  - 6.097049817442894
  - 6.130751356482506
  - 6.186040681600571
  - 5.924011293053628
  - 6.1172581076622015
  - 5.961452375352383
  - 5.93641312122345
  - 6.217315304279328
  - 7.158122384548188
  - 6.746519723534584
  - 5.916982227563858
  - 6.7765227377414705
  - 6.006289297342301
  - 5.94674343764782
  - 6.067157727479935
  - 6.204735955595971
  - 6.0098262459039695
  - 6.097141322493553
  - 5.9353844165802006
  - 5.995124083757401
  - 5.894457817077637
  - 6.164033964276314
  - 6.13673515021801
  - 5.835804316401482
  - 6.639037045836449
  - 5.962050652503968
  - 5.846825513243676
  - 5.878200253844262
  - 5.907261592149735
  - 5.885337778925896
  - 5.954808324575424
  - 5.806178694963456
  - 5.937965470552445
  - 5.837309896945953
  - 5.835963612794877
  - 6.04859684407711
  - 5.961378613114357
  - 6.042336651682854
  - 5.834658974409104
  - 5.880675294995308
  - 5.85085754096508
  - 6.120126152038575
  - 5.859156578779221
  - 5.747934484481812
  - 5.921439889073373
  - 5.831258934736252
  - 6.080514919757843
  - 6.070444133877754
  - 5.879696568846703
  - 6.114355155825615
  - 5.922436186671257
  - 5.82290353178978
  - 5.920928773283959
  - 5.850743818283082
  - 5.867602220177651
  - 5.985760070383549
  - 5.857374688982964
  - 6.000957836210728
  - 6.011039158701897
  - 5.985059805214405
  - 6.128943493962288
  - 6.035056349635124
  - 5.95519594848156
  - 5.880402186512947
  - 6.418793258070946
  - 6.608648982644081
  - 6.353316891193391
  - 6.064748996496201
  - 5.890776778757573
  - 5.877927055954934
  - 12.062690514326096
  - 13.109424355626107
  - 6.48868896663189
  - 6.657065460085869
  - 6.755691662430763
  validation_losses:
  - 0.4029983878135681
  - 0.44803857803344727
  - 0.39536261558532715
  - 0.3947813808917999
  - 0.5231054425239563
  - 0.5299277305603027
  - 0.40128275752067566
  - 0.4053051769733429
  - 0.3962732255458832
  - 0.44139406085014343
  - 47.46913146972656
  - 3291973.25
  - 72914152.0
  - 24482122.0
  - 0.40291595458984375
  - 14947481.0
  - 5594716.5
  - 115492488.0
  - 221481456.0
  - 734087168.0
  - 114267192.0
  - 379954272.0
  - 2066.98193359375
  - 185207.265625
  - 299208800.0
  - 175416720.0
  - 96147704.0
  - 442662688.0
  - 219094720.0
  - 0.3971039354801178
  - 0.4473736882209778
  - 0.41479265689849854
  - 0.41649550199508667
  - 0.40992552042007446
  - 0.3962438702583313
  - 0.41315957903862
  - 0.40874966979026794
  - 0.981443464756012
  - 0.404239296913147
  - 0.40917083621025085
  - 0.40218186378479004
  - 0.5206913352012634
  - 0.46535784006118774
  - 0.7941693067550659
  - 0.878271222114563
  - 2.6545443534851074
  - 0.3982408940792084
  - 0.4071853756904602
  - 0.4036696255207062
  - 2128566.75
  - 16331927552.0
  - 9130019840.0
  - 25651066880.0
  - 54675902464.0
  - 84638883840.0
  - 11707462656.0
  - 25597659136.0
  - 843117568.0
  - 22245672960.0
  - 17708857344.0
  - 1340662400.0
  - 126527479808.0
  - 4290190848.0
  - 21143941120.0
  - 152605589504.0
  - 42596511744.0
  - 1746459520.0
  - 120492793856.0
  - 102967951360.0
  - 177292624.0
  - 14097173504.0
  - 4483023872.0
  - 88910553088.0
  - 62100148224.0
  - 78202060800.0
  - 25532661760.0
  - 33648470016.0
  - 135001874432.0
  - 30874417152.0
  - 53945450496.0
  - 72228536320.0
  - 0.44452983140945435
  - 105865968.0
  - 3.161425783344333e+16
  - 0.3987787663936615
  - 0.46243518590927124
  - 0.41381195187568665
  - 0.41769179701805115
  - 0.4067031741142273
  - 0.46910595893859863
  - 45399.88671875
  - 98458016.0
  - 0.4227251708507538
  - 0.3962412476539612
  - 0.40400856733322144
  - 1.185510516166687
  - 0.44272202253341675
  - 0.4495249390602112
  - 0.41197481751441956
  - 0.40271902084350586
loss_records_fold4:
  train_losses:
  - 8.276703500747681
  - 7.22193346619606
  - 7.912966775894166
  - 6.451159577071667
  - 6.2412025272846225
  - 6.029453638195992
  - 6.685926660895348
  - 5.985946279764176
  - 6.394574826955796
  - 6.360502681136132
  - 6.187928307056428
  - 6.320927691459656
  - 6.1680744946002966
  - 6.207133549451829
  - 6.363057184219361
  - 6.248344579339028
  - 6.970278400182725
  - 7.30772774219513
  - 6.252664589881897
  - 6.039808177947998
  - 6.544013127684593
  - 6.479293125867844
  - 6.2294817745685584
  - 5.963739815354348
  - 6.163895761966706
  - 6.131861627101898
  - 6.0061155036091805
  - 6.203534850478173
  - 6.127194508910179
  - 7.4938693538308145
  - 6.1501735866069795
  - 6.0188831955194475
  - 6.11364686191082
  validation_losses:
  - 0.3893281817436218
  - 0.4812142252922058
  - 0.39847180247306824
  - 0.6977528929710388
  - 0.42783114314079285
  - 0.4803616404533386
  - 0.39128682017326355
  - 0.39163365960121155
  - 0.4562772810459137
  - 0.39979732036590576
  - 0.3921882212162018
  - 0.42702725529670715
  - 0.4095444977283478
  - 0.4033646583557129
  - 0.39665406942367554
  - 0.45094409584999084
  - 0.39567670226097107
  - 0.39468446373939514
  - 0.3951564133167267
  - 0.41296693682670593
  - 0.4794873297214508
  - 0.49440914392471313
  - 0.45613333582878113
  - 0.4165012240409851
  - 0.4032494127750397
  - 0.3954537510871887
  - 0.43250900506973267
  - 0.3933061957359314
  - 0.393436074256897
  - 0.4007165729999542
  - 0.39278820157051086
  - 0.39251646399497986
  - 0.3929303288459778
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 69 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 46 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 33 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:39:36.296740'
