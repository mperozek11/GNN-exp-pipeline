config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 11:16:02.842927'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/9/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/9/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/9/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/9/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/9/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/9/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/9/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/9/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/9/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/9/fold_4_state_dict.pt
loss_records:
  train_losses:
  - 7.852413177490234
  - 7.983713269233704
  - 7.681413322687149
  - 7.933711677789688
  - 7.920223504304886
  - 7.813574552536011
  - 7.758507341146469
  - 7.737201154232025
  - 7.760820388793945
  - 7.893157631158829
  - 8.078010350465775
  - 7.727169543504715
  - 7.698965519666672
  - 7.894808292388916
  - 7.943483620882034
  - 7.676750630140305
  - 7.788628935813904
  - 7.832640618085861
  - 7.779730886220932
  - 7.838198274374008
  - 7.959765315055847
  - 7.731251806020737
  - 7.767379611730576
  - 7.963838696479797
  - 7.962449908256531
  - 8.023518443107605
  - 8.111532241106033
  - 7.886240661144257
  - 8.071993380784988
  - 7.893492490053177
  - 7.744784086942673
  - 7.770565032958984
  - 7.800703912973404
  - 7.625755086541176
  - 8.122802138328552
  - 7.917572766542435
  - 8.105805039405823
  - 7.935434103012085
  - 7.704082369804382
  - 7.713135167956352
  - 7.791605532169342
  - 7.736966043710709
  - 7.80235281586647
  - 7.80690211057663
  - 7.854299604892731
  - 7.959731638431549
  - 7.866887986660004
  - 8.10341414809227
  - 7.722513437271118
  - 7.807965964078903
  - 7.963671654462814
  - 7.848267912864685
  - 7.916945040225983
  - 7.714722394943237
  - 8.018092036247253
  - 7.746322572231293
  - 7.7080762684345245
  - 8.369717687368393
  - 8.143593817949295
  - 7.694948613643646
  - 7.672655820846558
  - 7.864524841308594
  - 7.869024276733398
  - 7.796077251434326
  - 7.778829097747803
  - 7.645138502120972
  - 7.861671656370163
  - 7.656067848205566
  - 7.973564475774765
  - 8.081081241369247
  - 8.01068365573883
  - 7.9430239498615265
  - 7.682573139667511
  - 7.803253024816513
  - 7.898698627948761
  - 8.050710082054138
  - 7.710541486740112
  - 7.963289856910706
  - 7.915595442056656
  - 7.933842808008194
  - 7.6995545625686646
  - 7.747723937034607
  - 7.726421415805817
  - 7.683375388383865
  - 7.720737606287003
  - 7.889863312244415
  - 8.089611381292343
  - 7.8670540153980255
  - 7.883705586194992
  - 7.775090783834457
  - 8.026822000741959
  - 7.877423256635666
  - 7.83211874961853
  - 7.678386449813843
  - 8.05082592368126
  - 8.027888625860214
  - 7.831960707902908
  - 7.738423138856888
  - 7.840158075094223
  - 7.937832862138748
  validation_losses:
  - 1.7632091444556595e+17
  - 1.7074552462733476e+18
  - 5.417074437482611e+18
  - 1.6931159653797069e+19
  - 5.876478872918583e+19
  - 7.698991523920085e+19
  - 8.198460953609424e+19
  - 8.359170850780978e+19
  - 8.180207301369738e+19
  - 8.231413756898522e+19
  - 8.47410587986496e+19
  - 8.102032904244167e+19
  - 8.708417965009142e+19
  - 8.262244062941361e+19
  - 8.157669951828237e+19
  - 8.209220334594189e+19
  - 8.419547233285413e+19
  - 8.158918117428088e+19
  - 8.162398731436976e+19
  - 8.122418729432436e+19
  - 8.255668103797958e+19
  - 8.533547237681136e+19
  - 7.724248625424053e+19
  - 7.926374926590673e+19
  - 8.232603868284427e+19
  - 7.813750631143624e+19
  - 8.213759118593648e+19
  - 7.982562609597933e+19
  - 8.139714487142004e+19
  - 8.41311728928618e+19
  - 8.237040617604828e+19
  - 8.351764540456278e+19
  - 8.273206633674939e+19
  - 8.232192211130987e+19
  - 7.792955787629822e+19
  - 8.149953139419854e+19
  - 8.443075902510517e+19
  - 8.015780175287e+19
  - 7.912156921829576e+19
  - 7.979622955309911e+19
  - 8.178804324532696e+19
  - 7.87225784388084e+19
  - 8.162570255250909e+19
  - 8.004244978897676e+19
  - 7.468548840096373e+19
  - 7.491827700279647e+19
  - 7.249662463285238e+19
  - 6.771845496179366e+19
  - 6.424065570267306e+19
  - 6.101500964417359e+19
  - 5.94819957659376e+19
  - 6.554071825135541e+19
  - 5.7957149060072735e+19
  - 6.281938739019121e+19
  - 5.300753034187165e+19
  - 6.352029526657283e+19
  - 6.221713649314667e+19
  - 6.076352054857564e+19
  - 5.738937005158223e+19
  - 6.156217061062004e+19
  - 6.676135208004721e+19
  - 6.07825948762943e+19
  - 5.799397390351021e+19
  - 5.9480130994216894e+19
  - 6.327988485013635e+19
  - 6.037095531504101e+19
  - 6.523305730571764e+19
  - 6.333251187468822e+19
  - 6.011385431209489e+19
  - 6.714768967972212e+19
  - 5.3704589927556055e+19
  - 5.891951200544647e+19
  - 5.9765968833066566e+19
  - 6.537265130198008e+19
  - 5.804511878638784e+19
  - 6.199715060470776e+19
  - 6.281411413242439e+19
  - 6.1724009926135644e+19
  - 6.394754789294054e+19
  - 6.242091118214565e+19
  - 6.0498533848235114e+19
  - 6.565065182194696e+19
  - 5.701050033488318e+19
  - 5.9754810989067895e+19
  - 6.325290723283724e+19
  - 5.689906263238482e+19
  - 5.988508992281982e+19
  - 5.915837430951104e+19
  - 5.528746006104192e+19
  - 5.960633293885302e+19
  - 6.3155886326802285e+19
  - 6.406620278976361e+19
  - 5.580277917074797e+19
  - 5.899247999511219e+19
  - 5.6187181629958e+19
  - 5.939463736808754e+19
  - 6.669006854219524e+19
  - 6.139992667482541e+19
  - 5.9247601877128315e+19
  - 6.178115374445442e+19
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.14065180102915953,
    0.140893470790378]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.24661654135338348, 0.24698795180722893]'
  mean_eval_accuracy: 0.571231867399928
  mean_f1_accuracy: 0.09872089863212248
  total_train_time: '0:03:41.713716'
