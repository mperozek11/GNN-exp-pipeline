config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 13:23:59.776383'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/0/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/0/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/0/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/0/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/0/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/0/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/0/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/0/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/0/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/0/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 28.075455009937286
  - 28.06641948223114
  - 27.555096223950386
  - 28.00342670083046
  - 27.73400767147541
  - 27.456547126173973
  - 27.83421766757965
  - 28.045790448784828
  - 27.765292137861252
  - 27.507941365242004
  - 27.411973297595978
  - 28.100501149892807
  - 27.91131180524826
  - 28.17432038486004
  - 27.887050479650497
  - 28.09541606903076
  - 27.849920392036438
  - 27.932434171438217
  - 27.929883122444153
  - 27.62191942334175
  - 27.741943269968033
  - 27.679508298635483
  - 27.824695885181427
  - 27.67424564063549
  - 27.569115296006203
  - 27.788105711340904
  - 27.647572427988052
  - 27.55303494632244
  - 27.604567378759384
  - 27.419697538018227
  - 27.418015137314796
  - 27.38635566830635
  - 27.346556916832924
  - 27.538557067513466
  - 27.255882188677788
  - 27.20041361451149
  - 27.333037167787552
  - 27.47023816406727
  - 27.198602184653282
  - 27.370791628956795
  - 27.532642528414726
  - 27.247537449002266
  - 27.31819098442793
  - 27.377696573734283
  - 27.21458664536476
  - 26.907543629407883
  - 27.53534610569477
  - 27.038145765662193
  - 27.090787336230278
  - 27.303795650601387
  - 27.292098224163055
  - 27.1438899487257
  - 27.819084212183952
  - 26.98374742269516
  - 27.267545074224472
  - 27.459309086203575
  - 27.323083713650703
  - 26.999586135149002
  - 26.97349864244461
  - 26.891890734434128
  - 27.457985773682594
  - 27.325779303908348
  - 26.89125782996416
  - 27.20404352247715
  - 27.38170202076435
  - 27.371004700660706
  - 26.878772646188736
  - 26.72668945789337
  - 26.89358487725258
  - 26.88448415696621
  - 27.08838550746441
  - 27.042938008904457
  - 26.786384001374245
  - 27.39850889146328
  - 27.03910857439041
  - 26.917158618569374
  - 27.45271770656109
  - 26.975438624620438
  - 26.961382061243057
  - 27.027890652418137
  - 27.294165939092636
  - 27.034013733267784
  - 26.510284557938576
  - 26.879095390439034
  - 26.754603877663612
  - 27.429593697190285
  - 27.079469919204712
  - 27.464182138442993
  - 26.920272186398506
  - 26.88683781027794
  - 26.9724031239748
  - 26.732382908463478
  - 26.710020668804646
  - 27.05628740787506
  - 26.77053827047348
  - 27.913562580943108
  - 27.26468373835087
  - 27.006310611963272
  - 27.02612954378128
  - 26.6982531696558
  validation_losses:
  - 0.3900976777076721
  - 0.3849714994430542
  - 0.3887346088886261
  - 0.39256352186203003
  - 0.4086447060108185
  - 0.42921748757362366
  - 0.3891236484050751
  - 0.38890719413757324
  - 0.38541990518569946
  - 0.4055020809173584
  - 0.41086873412132263
  - 0.38818177580833435
  - 0.38419216871261597
  - 0.39429882168769836
  - 0.3865167200565338
  - 0.3880348205566406
  - 0.3857601583003998
  - 0.38653668761253357
  - 0.38569381833076477
  - 0.4082583487033844
  - 0.38490182161331177
  - 0.3911277949810028
  - 0.38556376099586487
  - 0.38423284888267517
  - 0.3848256766796112
  - 0.4336097836494446
  - 0.39804980158805847
  - 0.3862232565879822
  - 0.38370203971862793
  - 0.4038639962673187
  - 0.40855610370635986
  - 0.41993197798728943
  - 0.39555832743644714
  - 0.5000268220901489
  - 0.770760715007782
  - 0.40634042024612427
  - 0.8603330254554749
  - 0.44986188411712646
  - 0.6222410202026367
  - 0.44721853733062744
  - 0.4986668825149536
  - 0.425884485244751
  - 0.4547617733478546
  - 0.4252641797065735
  - 0.5400662422180176
  - 0.47610077261924744
  - 0.4368758499622345
  - 0.3861147463321686
  - 0.4852314889431
  - 0.5833882093429565
  - 0.397793173789978
  - 0.48357516527175903
  - 0.48909255862236023
  - 0.5393755435943604
  - 0.46225085854530334
  - 0.5559402108192444
  - 0.5561752915382385
  - 0.45866408944129944
  - 0.7224550247192383
  - 0.620423436164856
  - 0.5140656232833862
  - 0.5388666391372681
  - 0.516202986240387
  - 0.3867450952529907
  - 0.4800318777561188
  - 0.41037341952323914
  - 0.5440965890884399
  - 0.88609379529953
  - 0.6070119142532349
  - 0.5859584808349609
  - 0.5482692718505859
  - 0.4410659074783325
  - 0.5686062574386597
  - 0.5128130912780762
  - 0.5263638496398926
  - 0.5074406862258911
  - 0.5647079348564148
  - 0.5160353779792786
  - 0.5343355536460876
  - 0.5797393321990967
  - 0.42212873697280884
  - 0.6747899651527405
  - 0.615960955619812
  - 0.49975723028182983
  - 0.6143980026245117
  - 0.44103702902793884
  - 0.4713102877140045
  - 0.7052730917930603
  - 0.6200573444366455
  - 0.5553407073020935
  - 0.5996996760368347
  - 0.5609765648841858
  - 0.5319101214408875
  - 0.7219555974006653
  - 0.9380162358283997
  - 0.8006529808044434
  - 0.5771194696426392
  - 0.7430043816566467
  - 0.6905159950256348
  - 0.6848385334014893
loss_records_fold2:
  train_losses:
  - 26.476806357502937
  - 26.81410326063633
  - 26.738418824970722
  - 27.265785440802574
  - 26.951054945588112
  - 26.413125544786453
  - 27.109157741069794
  - 28.554586470127106
  - 27.07824808359146
  - 26.900891840457916
  - 26.5099525898695
  - 27.417334645986557
  - 26.910069167613983
  - 26.86487104743719
  - 26.50629450380802
  - 26.87259203195572
  - 27.468391925096512
  - 27.992103070020676
  - 27.027050733566284
  - 26.802653953433037
  - 26.94664493203163
  - 26.95626713335514
  - 27.13962933421135
  - 26.94162029027939
  - 26.474089726805687
  - 26.468272238969803
  - 27.183448895812035
  - 26.71283583343029
  - 26.50435020029545
  - 26.60605575144291
  - 26.775065407156944
  - 26.84820507466793
  - 26.359241664409637
  - 26.836329251527786
  - 27.09680414199829
  - 26.43917426466942
  - 26.951408490538597
  - 26.485611483454704
  - 26.93519139289856
  - 26.44219933450222
  - 26.36953428387642
  - 27.083547055721283
  - 26.710022553801537
  - 27.273741871118546
  - 26.7606480717659
  - 26.760898381471634
  - 27.386069610714912
  - 28.084660932421684
  - 27.367115661501884
  - 27.300707578659058
  - 27.080346167087555
  - 27.21177190542221
  - 26.500550761818886
  - 27.219329610466957
  - 27.299575433135033
  - 27.137843027710915
  - 26.995880380272865
  - 27.036836624145508
  - 26.693097963929176
  - 26.758513152599335
  - 26.260695680975914
  - 27.013858273625374
  - 26.67646035552025
  - 26.80959415435791
  - 27.044098496437073
  - 26.30909299850464
  - 26.569600105285645
  - 26.809179827570915
  - 26.73374778032303
  - 26.78380236029625
  - 26.56023497879505
  - 27.099015697836876
  - 26.8123739361763
  - 26.73512375354767
  - 26.679606571793556
  - 26.6573768556118
  - 26.78303872048855
  - 26.57562465965748
  - 26.83731012046337
  - 26.438108146190643
  - 26.971897825598717
  - 26.85475282371044
  - 26.510419219732285
  - 26.71107406914234
  - 26.433635398745537
  - 26.600523605942726
  - 26.53192050009966
  - 26.34708060324192
  - 26.740403562784195
  - 26.29814726114273
  - 26.266455680131912
  - 26.234112203121185
  - 26.911563009023666
  - 26.849474370479584
  - 26.566864490509033
  - 26.13702268898487
  - 26.699534684419632
  - 26.761237129569054
  - 26.643565848469734
  - 26.64375329017639
  validation_losses:
  - 0.37173840403556824
  - 1.0799477100372314
  - 0.7173847556114197
  - 1.140297293663025
  - 0.8623222708702087
  - 0.6359525322914124
  - 0.8967005610466003
  - 0.42161473631858826
  - 0.4607608914375305
  - 0.5608620047569275
  - 1.2039114236831665
  - 1.175331950187683
  - 0.6498062610626221
  - 0.8965495228767395
  - 0.8417467474937439
  - 0.9337444305419922
  - 0.43835699558258057
  - 0.6670292615890503
  - 0.5960165858268738
  - 0.5401814579963684
  - 1.161016583442688
  - 0.8538846969604492
  - 0.42278149724006653
  - 0.42450571060180664
  - 0.5423132181167603
  - 0.7943019866943359
  - 0.5025345683097839
  - 0.5146973133087158
  - 0.5365065932273865
  - 0.42875972390174866
  - 0.5224362015724182
  - 0.4651086926460266
  - 0.49813368916511536
  - 0.48590439558029175
  - 0.42921188473701477
  - 0.4768291711807251
  - 0.4496930241584778
  - 0.6316145658493042
  - 0.44577986001968384
  - 0.5065657496452332
  - 0.8782508969306946
  - 0.528256356716156
  - 0.4760092794895172
  - 0.4618658125400543
  - 0.47589123249053955
  - 0.5685850977897644
  - 0.5838620662689209
  - 0.3849751353263855
  - 0.3888607323169708
  - 0.38975077867507935
  - 0.39428025484085083
  - 0.4052673280239105
  - 0.4025711417198181
  - 1.694832682609558
  - 0.3851960003376007
  - 0.38390401005744934
  - 0.654394268989563
  - 0.389608234167099
  - 0.46308189630508423
  - 0.5634148716926575
  - 0.48821038007736206
  - 0.5412611365318298
  - 0.6697064638137817
  - 0.5061760544776917
  - 0.4994193911552429
  - 0.701423168182373
  - 0.4727233648300171
  - 0.5685498714447021
  - 1.2303557395935059
  - 0.6352756023406982
  - 0.4125659465789795
  - 0.4200020134449005
  - 0.7579821348190308
  - 0.4502354860305786
  - 0.5362728238105774
  - 0.4815753400325775
  - 0.748751699924469
  - 0.504028856754303
  - 0.6049373149871826
  - 0.47435468435287476
  - 0.5255199074745178
  - 0.5157642364501953
  - 0.49584805965423584
  - 1.0504915714263916
  - 0.4379304349422455
  - 0.4509127736091614
  - 0.48049476742744446
  - 0.4950246512889862
  - 0.5545058250427246
  - 0.9191513657569885
  - 0.6788346171379089
  - 0.4205878674983978
  - 0.6784108877182007
  - 1.1345350742340088
  - 0.42460501194000244
  - 0.8473371863365173
  - 0.40942975878715515
  - 0.5483565926551819
  - 0.4558784067630768
  - 0.5549284219741821
loss_records_fold3:
  train_losses:
  - 27.35977664589882
  - 26.917705059051514
  - 27.315021589398384
  - 27.31362445652485
  - 26.917698249220848
  - 27.18892265856266
  - 26.892325311899185
  - 26.926137641072273
  - 26.855210065841675
  - 26.6721473634243
  - 27.26402397453785
  - 26.90116484463215
  - 27.0007127225399
  - 26.58911581337452
  - 26.790689557790756
  - 26.920703008770943
  - 26.52642250061035
  - 27.051192224025726
  - 26.689927220344543
  - 27.861277014017105
  - 28.17944097518921
  - 27.68092878162861
  - 27.283933222293854
  - 27.503929749131203
  - 27.684705942869186
  - 27.10614663362503
  - 27.535951033234596
  - 27.214548096060753
  - 27.34806579351425
  - 27.310855507850647
  - 27.297636300325394
  - 27.279570430517197
  - 27.564423486590385
  - 27.360206335783005
  - 27.3348491191864
  - 27.1435918956995
  - 27.155300341546535
  - 27.465115427970886
  - 26.68626134097576
  - 28.385429590940475
  - 27.797028079628944
  - 27.445784255862236
  - 27.344542756676674
  - 27.35928425192833
  - 27.442758932709694
  - 27.450369358062744
  - 27.328378155827522
  - 27.490514487028122
  - 28.04184302687645
  - 27.2739679813385
  - 27.19651570916176
  - 27.308814823627472
  - 27.262480050325394
  - 27.25901125371456
  - 27.43118503689766
  - 27.1878300011158
  - 27.106183722615242
  - 28.011449351906776
  - 27.51273101568222
  - 27.37094083428383
  - 27.266535222530365
  - 27.076309874653816
  - 27.191333934664726
  - 26.950613856315613
  - 27.16047151386738
  - 27.280050575733185
  - 27.25713510811329
  - 27.130134984850883
  - 27.22565071284771
  - 27.249297201633453
  - 27.19874034821987
  - 27.07312400639057
  - 27.029319241642952
  - 27.822818145155907
  - 27.129138499498367
  - 27.19377252459526
  - 27.245403617620468
  - 27.301816046237946
  - 27.32276937365532
  - 27.51583258807659
  - 27.19969555735588
  - 27.05273738503456
  - 27.208943873643875
  - 27.1946180164814
  - 27.48320209980011
  - 26.86734974384308
  - 27.26676368713379
  - 27.30942238867283
  - 27.310589596629143
  - 27.110233545303345
  - 26.98936289548874
  - 26.842156931757927
  - 27.68742299079895
  - 27.367667391896248
  - 27.608613684773445
  - 27.195485770702362
  - 27.374229818582535
  - 27.24343930184841
  - 27.342920437455177
  - 27.133344680070877
  validation_losses:
  - 0.8907068967819214
  - 0.5692442655563354
  - 0.648668646812439
  - 0.7239981889724731
  - 0.9636232852935791
  - 1.327712893486023
  - 1.7257945537567139
  - 0.8031667470932007
  - 0.8032438158988953
  - 0.639857828617096
  - 0.7439468502998352
  - 0.688366174697876
  - 0.5155950784683228
  - 0.4594714045524597
  - 1.3944642543792725
  - 0.8894477486610413
  - 0.5112819671630859
  - 0.8637080192565918
  - 1.3162715435028076
  - 0.3710634410381317
  - 0.46392205357551575
  - 0.3631781339645386
  - 0.3809158504009247
  - 0.3711867928504944
  - 0.3834537863731384
  - 0.4433867633342743
  - 0.4786091148853302
  - 0.4373040497303009
  - 0.37492835521698
  - 0.3904760777950287
  - 0.6294883489608765
  - 0.38370925188064575
  - 1.0915124416351318
  - 1.4468919038772583
  - 0.5695688724517822
  - 0.721011757850647
  - 0.5934177041053772
  - 2.0486414432525635
  - 1.4398407936096191
  - 0.3667559325695038
  - 0.3646228313446045
  - 0.3699425160884857
  - 0.3635809123516083
  - 0.42593827843666077
  - 0.40504977107048035
  - 0.36812928318977356
  - 0.38341963291168213
  - 0.3703445792198181
  - 0.382228285074234
  - 0.44365984201431274
  - 0.4420659840106964
  - 0.390044242143631
  - 0.42943906784057617
  - 0.5149917602539062
  - 0.39523619413375854
  - 0.3862074315547943
  - 0.3858970105648041
  - 0.5210186243057251
  - 0.49314847588539124
  - 0.42726120352745056
  - 0.363694965839386
  - 0.3974193334579468
  - 0.368075430393219
  - 0.3603549003601074
  - 0.40863466262817383
  - 0.4125790596008301
  - 0.41912737488746643
  - 0.3625810146331787
  - 0.4134127199649811
  - 0.387664794921875
  - 0.36419734358787537
  - 0.37675365805625916
  - 0.42610812187194824
  - 0.37278348207473755
  - 0.4033975899219513
  - 0.38649970293045044
  - 0.40708738565444946
  - 0.36743462085723877
  - 0.3634471893310547
  - 0.3698401153087616
  - 0.43089741468429565
  - 0.3672960698604584
  - 0.3695591390132904
  - 0.48290398716926575
  - 0.3878898024559021
  - 0.3881182074546814
  - 0.41777557134628296
  - 0.7227378487586975
  - 0.5631182789802551
  - 0.48486244678497314
  - 0.483403742313385
  - 1.707081913948059
  - 0.3755185306072235
  - 0.3697194755077362
  - 0.4499622881412506
  - 0.48853084444999695
  - 0.49872905015945435
  - 0.3953876197338104
  - 0.41258683800697327
  - 0.40643787384033203
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8284734133790738, 0.8473413379073756, 0.8576329331046312,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.13793103448275862, 0.043010752688172046, 0.04597701149425288,
    0.0]'
  mean_eval_accuracy: 0.8500374293410667
  mean_f1_accuracy: 0.04538375973303671
  total_train_time: '0:11:09.010720'
