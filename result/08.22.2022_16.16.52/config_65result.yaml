config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:39:06.983009'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_65fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_65fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_65fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_65fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_65fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_65fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_65fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_65fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_65fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_65fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.1931530594825746
  - 3.014156991243363
  - 2.9973450720310213
  - 2.9981371700763706
  - 3.112282407283783
  - 2.9240244150161745
  - 2.8944216132164002
  - 2.838278105854988
  - 2.960078248381615
  - 2.9316615104675297
  - 2.943095821142197
  - 2.919787728786469
  - 2.9435093939304355
  - 2.873920911550522
  - 2.8490133523941044
  - 2.9067272156476975
  - 2.8707938611507418
  - 2.860763961076737
  - 2.872320824861527
  - 2.7930203408002856
  - 2.8764367282390597
  - 2.8329604029655457
  - 2.834110859036446
  - 2.8329989671707154
  validation_losses:
  - 0.4185045063495636
  - 0.4220125079154968
  - 0.3956052362918854
  - 0.3946162164211273
  - 0.39909446239471436
  - 0.3930012881755829
  - 0.42243170738220215
  - 0.38372254371643066
  - 0.3863098919391632
  - 0.46293434500694275
  - 0.38767045736312866
  - 0.40756556391716003
  - 0.3851024806499481
  - 0.3861958384513855
  - 0.40430065989494324
  - 0.38884466886520386
  - 0.38538801670074463
  - 0.40348196029663086
  - 0.3864358961582184
  - 0.38525834679603577
  - 0.38411396741867065
  - 0.3878178596496582
  - 0.3946738541126251
  - 0.3892146050930023
loss_records_fold1:
  train_losses:
  - 2.824221992492676
  - 2.8068902254104615
  - 2.8493812054395677
  - 2.857401490211487
  - 2.8121610939502717
  - 2.845561602711678
  - 2.8484252750873567
  - 2.8023002564907076
  - 2.822789812088013
  - 2.846225193142891
  - 2.828819841146469
  - 2.843130123615265
  - 2.836932411789894
  - 2.834815883636475
  - 2.804906263947487
  - 2.83755539059639
  - 2.814907628297806
  - 2.8671334743499757
  validation_losses:
  - 0.3976574242115021
  - 0.3832806944847107
  - 0.382802277803421
  - 0.3940737545490265
  - 0.3880942463874817
  - 0.3947782814502716
  - 0.40519681572914124
  - 0.38768360018730164
  - 0.40182122588157654
  - 0.42585381865501404
  - 0.39266112446784973
  - 0.41096270084381104
  - 0.38651713728904724
  - 0.3939282298088074
  - 0.39300549030303955
  - 0.3863777220249176
  - 0.38362935185432434
  - 0.38658785820007324
loss_records_fold2:
  train_losses:
  - 2.8217460542917254
  - 2.80788426399231
  - 2.811120527982712
  - 2.820524820685387
  - 2.8838445246219635
  - 2.844708347320557
  - 2.80276749432087
  - 2.855278703570366
  - 2.927666014432907
  - 2.8518967866897587
  - 2.818690770864487
  - 2.817441993951798
  - 2.837151873111725
  - 2.8344933807849886
  - 2.8066171139478686
  validation_losses:
  - 0.3880325257778168
  - 0.38425323367118835
  - 0.3896479904651642
  - 0.38526982069015503
  - 0.39077475666999817
  - 0.40331214666366577
  - 0.3873784840106964
  - 0.38472655415534973
  - 0.4053243398666382
  - 0.3942093253135681
  - 0.38379397988319397
  - 0.3880942761898041
  - 0.38762983679771423
  - 0.3849996328353882
  - 0.38496050238609314
loss_records_fold3:
  train_losses:
  - 2.8363184154033663
  - 2.8286844044923782
  - 2.839187830686569
  - 2.839478546380997
  - 2.8286310106515886
  - 2.818255287408829
  - 2.7914733946323396
  - 2.842082902789116
  - 2.8338234931230546
  - 2.823025494813919
  - 2.856048560142517
  - 2.8584329783916473
  - 2.8527816742658616
  - 2.8517445713281635
  - 2.817346924543381
  - 2.8361621737480167
  - 2.781999757885933
  - 2.8114367514848713
  - 2.7993595123291017
  - 2.7920383870601655
  - 2.8482966393232347
  - 2.8454721838235857
  - 2.7936717689037325
  - 2.7795086562633515
  - 2.7641185581684113
  - 2.8604881852865223
  - 2.809336537122727
  - 2.8049657166004183
  - 2.8437455713748934
  - 2.859828215837479
  - 2.7762795954942705
  - 2.784315758943558
  - 2.8016963571310045
  - 2.79610747396946
  - 2.8011362075805666
  - 2.8221388459205627
  - 2.743933156132698
  - 2.7618946313858035
  - 2.771375244855881
  - 2.7549397945404053
  - 2.797554472088814
  - 2.7952532052993777
  - 2.7295313239097596
  - 2.7766603499650957
  - 2.7639756679534915
  - 2.7709546953439714
  - 2.7728965878486633
  - 2.7651325285434725
  - 2.764816051721573
  - 2.74190916121006
  - 2.737783792614937
  - 2.7358708620071415
  - 2.8092936754226687
  - 2.8696279853582385
  - 2.783058887720108
  - 2.7337566375732423
  - 2.724331492185593
  - 2.768870633840561
  - 2.7559869259595873
  - 2.72706111073494
  - 2.724295836687088
  - 2.7564688086509705
  - 2.7962957262992862
  - 2.7743111431598666
  - 2.7165414154529572
  - 2.7185550510883334
  - 2.739931991696358
  - 2.74062896668911
  - 2.740879786014557
  - 2.763176140189171
  - 2.858953073620796
  - 2.748300886154175
  - 2.7664982587099076
  - 2.74702365398407
  - 2.752388110756874
  - 2.7595305621623996
  - 2.8133327066898346
  - 2.7744907230138782
  - 2.7307791471481324
  - 2.7378320395946503
  - 2.793277084827423
  - 2.7037150591611865
  - 2.7721075743436816
  - 2.826758232712746
  - 2.7829755306243897
  - 2.769916254281998
  - 2.744681823253632
  - 2.7506453633308414
  - 2.788836029171944
  - 2.7363896608352665
  - 2.724890941381455
  - 2.7789317995309832
  - 2.7718222022056582
  - 2.721501135826111
  - 2.7395680367946627
  - 2.732747048139572
  - 2.827798104286194
  - 2.764243268966675
  - 2.7178875982761386
  - 2.732616823911667
  validation_losses:
  - 0.37026292085647583
  - 0.37182262539863586
  - 0.3715924024581909
  - 0.37291812896728516
  - 0.3704170286655426
  - 0.373444527387619
  - 1.1990195512771606
  - 0.3701026439666748
  - 0.3741050660610199
  - 0.3706372082233429
  - 0.3953160345554352
  - 0.3784240484237671
  - 0.36856526136398315
  - 0.38289305567741394
  - 0.37745994329452515
  - 0.4132958948612213
  - 0.4157091975212097
  - 0.4331318140029907
  - 0.45088666677474976
  - 0.4278865158557892
  - 0.420670747756958
  - 0.5396966934204102
  - 0.4680612087249756
  - 0.45409059524536133
  - 0.4166823625564575
  - 0.4285788834095001
  - 0.3895812928676605
  - 0.5005921721458435
  - 0.4536448121070862
  - 0.38128921389579773
  - 0.40046781301498413
  - 0.4929627478122711
  - 0.3750646710395813
  - 0.4358844757080078
  - 0.42599889636039734
  - 0.5032747387886047
  - 1.0801111459732056
  - 2.6257779598236084
  - 0.40599769353866577
  - 0.8409929871559143
  - 0.48344141244888306
  - 0.8547632694244385
  - 0.3815949559211731
  - 0.533707320690155
  - 0.5988179445266724
  - 0.7570160031318665
  - 0.48096761107444763
  - 0.5593721866607666
  - 0.5874886512756348
  - 0.6445622444152832
  - 0.5723984241485596
  - 1.1876591444015503
  - 2.0032835006713867
  - 0.5023549199104309
  - 0.5566316843032837
  - 0.5644106268882751
  - 0.6336132287979126
  - 0.5057411193847656
  - 0.9932062029838562
  - 0.9745074510574341
  - 0.5709401369094849
  - 0.6076592803001404
  - 0.9404386878013611
  - 0.6122547388076782
  - 0.8026562929153442
  - 0.984794020652771
  - 1.5651366710662842
  - 0.9600341320037842
  - 1.516619324684143
  - 0.789739191532135
  - 0.6228300333023071
  - 0.48054519295692444
  - 0.8421488404273987
  - 0.7323826551437378
  - 0.4658631384372711
  - 0.6720595955848694
  - 0.4884471595287323
  - 0.9412205815315247
  - 0.5471372008323669
  - 0.6119880080223083
  - 1.2820106744766235
  - 0.7502999901771545
  - 1.079454779624939
  - 0.3988303244113922
  - 0.43463486433029175
  - 0.43381181359291077
  - 0.4152870774269104
  - 0.4583111107349396
  - 0.7334921360015869
  - 0.5889543294906616
  - 0.9270836114883423
  - 0.8050523996353149
  - 0.3944259285926819
  - 0.4188046157360077
  - 0.5215506553649902
  - 0.7864051461219788
  - 1.023944616317749
  - 0.4419479966163635
  - 0.5672914385795593
  - 0.44973689317703247
loss_records_fold4:
  train_losses:
  - 2.7703609824180604
  - 2.7600705146789553
  - 2.7262513607740404
  - 2.763489028811455
  - 2.7275521129369737
  - 2.7234849333763123
  - 2.7360668033361435
  - 2.724699914455414
  - 2.6978130221366885
  - 2.7557979196310045
  - 2.744075053930283
  - 2.7519142508506778
  - 2.779417723417282
  - 2.7768126845359804
  - 2.756816840171814
  - 2.6982453346252444
  - 2.7350353717803957
  - 2.709717479348183
  - 2.7688576161861422
  - 2.712807071208954
  - 2.705129373073578
  - 2.7269388139247894
  - 2.7335422128438953
  - 2.731438449025154
  - 2.7565559983253483
  - 2.731011670827866
  - 2.7467751622200014
  - 2.738168662786484
  - 2.7202617615461353
  - 2.7132441937923435
  - 2.8368762463331225
  - 2.7779498517513277
  - 2.73996017575264
  - 2.7621669083833695
  - 2.744207420945168
  - 2.7222158908843994
  - 2.7284125030040745
  - 2.7377653509378437
  - 2.7493264317512516
  - 2.740394034981728
  - 2.7461222589015963
  - 2.6957977175712586
  - 2.6999258518218996
  - 2.696895855665207
  - 2.7045438766479495
  - 2.699272960424423
  - 2.7382796168327332
  - 2.759198206663132
  - 2.7483847916126254
  - 2.7595118999481203
  - 2.723786026239395
  - 2.762269335985184
  - 2.7122882902622223
  - 2.6946357607841493
  - 2.705659067630768
  - 2.7175837427377703
  - 2.703074198961258
  - 2.7328746259212497
  - 2.697822028398514
  - 2.709114745259285
  - 2.6995955944061283
  - 2.7048437327146533
  - 2.687966313958168
  - 2.7513486385345463
  - 2.7334099203348163
  - 2.713669416308403
  - 2.674894216656685
  - 2.688559889793396
  - 2.699195218086243
  - 2.6784070879220963
  - 2.712562742829323
  - 2.7144232898950578
  - 2.691492030024529
  - 2.6618861198425297
  - 2.734044459462166
  - 2.6888358831405643
  - 2.688624688982964
  - 2.6859795033931735
  - 2.6959576249122623
  - 2.671609210968018
  - 2.7104172945022587
  - 2.7143819421529773
  - 2.6849697411060336
  - 2.7073858797550203
  - 2.6996559262275697
  - 2.743072524666786
  - 2.7022115498781205
  - 2.675540417432785
  - 2.6777189016342167
  - 2.6931595981121066
  - 2.7020745366811756
  - 2.662865248322487
  - 2.6720798254013065
  - 2.6915615618228914
  - 2.6580162674188617
  - 2.6409350216388705
  - 2.7756843626499177
  - 2.7093614012002947
  - 2.683921802043915
  - 2.663398814201355
  validation_losses:
  - 0.4142228960990906
  - 0.4269256591796875
  - 0.3987157940864563
  - 0.39778757095336914
  - 0.48880651593208313
  - 0.3940970301628113
  - 0.4023998975753784
  - 0.49573636054992676
  - 0.44736236333847046
  - 0.49250489473342896
  - 0.40651649236679077
  - 1.475117564201355
  - 0.6284647583961487
  - 0.4774138927459717
  - 0.4148170053958893
  - 0.4110967218875885
  - 0.46404433250427246
  - 0.36804330348968506
  - 0.44342994689941406
  - 0.39280182123184204
  - 0.4212993383407593
  - 0.3894396424293518
  - 0.6153480410575867
  - 0.5595526099205017
  - 0.5198598504066467
  - 1.0531519651412964
  - 0.4236539900302887
  - 0.3778199851512909
  - 0.44790104031562805
  - 1.148550271987915
  - 0.3966984152793884
  - 0.3853340446949005
  - 0.384548544883728
  - 0.4131544828414917
  - 0.3720457851886749
  - 0.42941194772720337
  - 0.3849671483039856
  - 0.4108119010925293
  - 2.2953410148620605
  - 0.5742238163948059
  - 1.0912314653396606
  - 0.6656626462936401
  - 0.5199349522590637
  - 0.5606502890586853
  - 1.213636040687561
  - 0.6380295753479004
  - 0.7818540930747986
  - 0.3741510212421417
  - 0.44879618287086487
  - 0.44704702496528625
  - 0.4470009505748749
  - 0.5402945280075073
  - 0.6656047105789185
  - 0.5352276563644409
  - 0.4614706337451935
  - 0.43953561782836914
  - 0.4724283814430237
  - 0.4524187445640564
  - 0.37684786319732666
  - 0.5145829319953918
  - 0.45580005645751953
  - 0.4704577326774597
  - 0.8298176527023315
  - 0.3629838228225708
  - 0.39490485191345215
  - 1.5701134204864502
  - 0.6140089631080627
  - 0.4645450711250305
  - 0.5324826836585999
  - 0.45498132705688477
  - 0.42785656452178955
  - 0.3928849399089813
  - 0.37002143263816833
  - 2.873889684677124
  - 0.40673696994781494
  - 13.693766593933105
  - 0.9297881722450256
  - 0.37852367758750916
  - 0.41191065311431885
  - 1.6613121032714844
  - 0.372030109167099
  - 0.517890453338623
  - 0.456666499376297
  - 0.3863331377506256
  - 0.45188310742378235
  - 0.3701108396053314
  - 0.3671666383743286
  - 0.39061373472213745
  - 0.5394459962844849
  - 0.43545088171958923
  - 0.5827454924583435
  - 0.4422346353530884
  - 0.5854344367980957
  - 0.4198482632637024
  - 0.6229208111763
  - 0.9052905440330505
  - 0.4581296741962433
  - 0.3730390667915344
  - 0.4478171467781067
  - 0.40243473649024963
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8336192109777015,
    0.8402061855670103]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.14159292035398233, 0.11428571428571427]'
  mean_eval_accuracy: 0.8496878923449629
  mean_f1_accuracy: 0.05117572692793933
  total_train_time: '0:23:52.384854'
