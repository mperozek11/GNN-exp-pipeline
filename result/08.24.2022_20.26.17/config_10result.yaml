config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.839705'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_10fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_10fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_10fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_10fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_10fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_10fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_10fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_10fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_10fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_10fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 30.462652397155765
  - 8.82952460050583
  - 5.138254570960999
  - 2.8158301532268526
  - 2.753448075056076
  - 2.772215336561203
  - 2.2689404368400576
  - 2.4664387166500092
  - 2.7667167246341706
  - 3.1279522299766542
  - 2.632429832220078
  - 2.0841169118881226
  - 2.1862400949001315
  - 5.008744484186173
  - 3.632443070411682
  - 2.281886237859726
  - 3.0782336711883547
  - 1.924043780565262
  - 2.0120643198490145
  - 1.9621479630470278
  - 1.8599574387073519
  - 2.1655339300632477
  - 2.2023721873760223
  - 2.1252622574567797
  - 4.355283576250076
  - 3.005545824766159
  - 3.1302440762519836
  - 3.9647132098674778
  - 2.46804832816124
  - 2.003146916627884
  - 2.5425092682242396
  - 2.9588258862495422
  - 2.762815696001053
  - 2.4498064458370212
  - 2.3585799753665926
  - 2.2606936514377596
  - 2.6276033341884615
  - 2.295455676317215
  - 2.605486917495728
  - 2.583572155237198
  - 2.72832790017128
  - 2.417959523200989
  - 3.3184116184711456
  - 2.713906002044678
  - 2.1074229776859283
  - 1.6343395322561265
  - 1.5890707910060884
  - 1.6723055213689806
  - 1.5181121826171875
  - 1.8441936343908312
  - 1.876747339963913
  - 1.837949424982071
  - 2.0026573449373246
  - 1.9637570261955262
  - 1.8023377299308778
  - 1.835439956188202
  - 2.2546133160591126
  - 2.2390132784843444
  - 1.599916422367096
  - 2.3572890013456345
  - 1.6057766497135164
  - 1.5156952798366548
  - 1.432138803601265
  - 1.6270463079214097
  - 1.7573429286479951
  - 2.4330441415309907
  - 1.749800318479538
  - 1.571934473514557
  - 1.8921657621860506
  - 1.5434127449989319
  - 2.106700837612152
  - 1.6081351697444917
  - 1.8257058322429658
  - 2.9361296176910403
  - 1.8926178455352785
  - 1.572704553604126
  - 1.662773033976555
  - 1.5023525714874268
  - 1.4776135087013245
  - 1.6387162029743196
  - 1.4818854153156282
  - 1.4607208967208862
  - 1.8345169723033905
  - 1.5686193346977235
  - 1.4578825116157532
  - 1.596505481004715
  - 2.5191698491573336
  - 1.554088979959488
  - 1.5420290231704712
  - 1.4908863365650178
  - 1.5040070414543152
  - 1.5711840212345125
  - 1.5652719616889954
  - 1.5271771490573884
  - 1.522612375020981
  - 1.4726360499858857
  - 1.448756241798401
  - 1.4352638244628908
  - 1.49521564245224
  - 1.5566534876823426
  validation_losses:
  - 3.4717459678649902
  - 2.722580909729004
  - 0.8446895480155945
  - 0.8050088286399841
  - 1.1384135484695435
  - 0.5293472409248352
  - 0.3925531804561615
  - 1.9413068294525146
  - 0.44763487577438354
  - 1.7752889394760132
  - 0.610249936580658
  - 0.5271610617637634
  - 0.5485778450965881
  - 0.6029587984085083
  - 0.6632223129272461
  - 0.4596640467643738
  - 0.3733938932418823
  - 0.4195297062397003
  - 0.37890881299972534
  - 0.37732431292533875
  - 0.7421715259552002
  - 0.413566917181015
  - 0.3829965591430664
  - 0.3775871992111206
  - 0.5384497046470642
  - 0.46959182620048523
  - 0.6988036632537842
  - 0.4947108030319214
  - 0.41043326258659363
  - 0.7924668788909912
  - 0.40618401765823364
  - 0.4868409037590027
  - 0.6139034628868103
  - 0.4494565725326538
  - 0.6014959812164307
  - 0.384099543094635
  - 0.39257368445396423
  - 0.396129846572876
  - 0.3831821382045746
  - 0.4369845688343048
  - 0.389651894569397
  - 0.37967774271965027
  - 0.4631091356277466
  - 0.43025466799736023
  - 0.3750572204589844
  - 0.3837895393371582
  - 0.4223354756832123
  - 0.42250490188598633
  - 0.4587583541870117
  - 0.37015631794929504
  - 0.37871304154396057
  - 0.401793509721756
  - 0.38784801959991455
  - 0.4318634867668152
  - 0.7835996150970459
  - 0.38003095984458923
  - 0.3895382881164551
  - 0.4400613009929657
  - 0.56192547082901
  - 0.4974914491176605
  - 0.38313186168670654
  - 0.41759148240089417
  - 0.4495648741722107
  - 0.3889821767807007
  - 0.37499651312828064
  - 0.42103227972984314
  - 0.38732242584228516
  - 0.4792637228965759
  - 0.469462126493454
  - 0.38386088609695435
  - 0.4350869655609131
  - 0.3737025260925293
  - 0.4439483880996704
  - 0.6714786291122437
  - 0.377480149269104
  - 0.4106466770172119
  - 0.4002591371536255
  - 0.3817758560180664
  - 0.42763054370880127
  - 0.37655335664749146
  - 0.4033253490924835
  - 0.38366082310676575
  - 0.40601685643196106
  - 0.3840405344963074
  - 0.38159769773483276
  - 0.5033330917358398
  - 0.3793398141860962
  - 0.3990042507648468
  - 0.44910329580307007
  - 0.3832171559333801
  - 0.37903904914855957
  - 0.3930821418762207
  - 0.3768039643764496
  - 0.4012267589569092
  - 0.42024898529052734
  - 0.37696176767349243
  - 0.3828607201576233
  - 0.38142678141593933
  - 0.461241215467453
  - 0.39198192954063416
loss_records_fold1:
  train_losses:
  - 1.587397575378418
  - 1.5041531682014466
  - 1.5053461968898774
  - 1.470535033941269
  - 1.6671561837196351
  - 1.6345287680625917
  - 1.7185376584529877
  - 1.4860734641551971
  - 1.5808634221553803
  - 1.454596358537674
  - 1.443466889858246
  - 1.547208693623543
  - 1.7506650030612947
  - 1.645581352710724
  - 1.8202547848224642
  - 1.8162685632705688
  - 1.6049642205238344
  - 1.7377883911132814
  - 1.5640721917152405
  - 1.4821715325117113
  - 1.4057575970888139
  - 1.4774757742881777
  validation_losses:
  - 0.39414891600608826
  - 0.4021492600440979
  - 0.4844045341014862
  - 0.3956255316734314
  - 0.39298805594444275
  - 0.3938848674297333
  - 0.4017440378665924
  - 0.41133925318717957
  - 0.43500271439552307
  - 0.39460498094558716
  - 0.397858589887619
  - 0.5080409049987793
  - 0.3941493332386017
  - 0.3944741189479828
  - 0.3923460841178894
  - 0.4057368040084839
  - 0.41572123765945435
  - 0.4080296754837036
  - 0.39961257576942444
  - 0.3968234956264496
  - 0.39993780851364136
  - 0.3958592116832733
loss_records_fold2:
  train_losses:
  - 1.5415365755558015
  - 1.4550841212272645
  - 1.4765385568141938
  - 1.474736350774765
  - 1.790283274650574
  - 1.497548681497574
  - 1.711827886104584
  - 1.6070955276489258
  - 1.6110909461975098
  - 1.5867280542850495
  - 1.531733012199402
  - 1.5767501950263978
  - 1.9096020668745042
  - 1.5771159172058107
  - 1.4694242238998414
  - 1.5272293746471406
  - 1.479888916015625
  - 1.579250091314316
  - 1.4716437339782715
  - 1.5025710463523865
  - 1.47860044836998
  - 1.5665397286415101
  - 1.4757347345352174
  - 1.5119424402713777
  - 1.567695212364197
  - 1.7150815665721895
  - 2.2242655813694
  - 2.939508318901062
  - 1.8490349888801576
  - 1.4950437009334565
  - 1.5532856523990632
  - 1.4600869089365007
  - 1.5206234753131866
  - 1.5397948056459427
  - 1.5107039988040925
  - 1.509320679306984
  - 1.4600863486528397
  - 1.4964335024356843
  - 1.5579588353633882
  - 1.5151807308197023
  - 1.6699986219406129
  - 1.5492327928543093
  - 1.5837157607078554
  - 1.4957911014556886
  - 1.5351525068283083
  - 1.4827884852886202
  - 1.4952081680297853
  - 1.6182536125183107
  - 1.5270694375038147
  - 1.476893711090088
  - 1.8369969487190247
  - 1.520614579319954
  - 1.5206677794456482
  - 1.5834370017051698
  - 1.5620946407318117
  - 2.231639975309372
  - 2.059004294872284
  - 1.864762616157532
  - 1.520468682050705
  - 2.1142232179641725
  - 1.580868434906006
  - 1.5537106961011888
  - 1.539534604549408
  - 1.4158429294824602
  - 1.5522607743740082
  - 1.5221519947052002
  - 1.4681362748146058
  - 1.4587186872959137
  - 1.519224685430527
  - 1.4452330321073532
  - 1.5587774574756623
  - 1.4877719521522523
  - 1.4715520441532135
  - 1.4825423181056978
  - 1.434948229789734
  - 1.487331920862198
  - 1.5712746739387513
  - 1.4539927303791047
  - 1.5169678032398224
  - 1.423505860567093
  validation_losses:
  - 0.37675607204437256
  - 0.37362274527549744
  - 0.38204866647720337
  - 0.386272132396698
  - 0.3798110783100128
  - 0.37426283955574036
  - 0.37910759449005127
  - 0.38519933819770813
  - 0.3974313735961914
  - 0.3809494972229004
  - 0.44300904870033264
  - 0.37353459000587463
  - 0.3879634439945221
  - 0.38254299759864807
  - 0.37673959136009216
  - 0.3873562812805176
  - 0.3761482834815979
  - 0.3795938789844513
  - 0.38564610481262207
  - 0.403076708316803
  - 0.37854868173599243
  - 0.3832879662513733
  - 0.3743666708469391
  - 0.45061588287353516
  - 0.37513256072998047
  - 1.26956307888031
  - 0.38064295053482056
  - 0.3801971971988678
  - 0.3733581304550171
  - 0.38486403226852417
  - 0.3730158507823944
  - 0.39644792675971985
  - 0.37679970264434814
  - 0.3773441016674042
  - 0.38373178243637085
  - 0.37603864073753357
  - 0.37731561064720154
  - 0.41037267446517944
  - 0.39457911252975464
  - 0.3752606511116028
  - 0.38471144437789917
  - 0.3778958022594452
  - 0.39702099561691284
  - 0.3955133259296417
  - 0.3788319230079651
  - 0.37543851137161255
  - 0.37705478072166443
  - 0.38722625374794006
  - 0.37307047843933105
  - 0.37392714619636536
  - 0.4055080711841583
  - 0.43139779567718506
  - 0.5380730628967285
  - 0.37486815452575684
  - 0.39267078042030334
  - 2.2521374225616455
  - 0.4091409742832184
  - 0.37853994965553284
  - 0.4469619393348694
  - 0.37563279271125793
  - 0.37605568766593933
  - 0.37857702374458313
  - 0.3758363723754883
  - 0.3795698881149292
  - 0.493533730506897
  - 0.3747524619102478
  - 0.3791085183620453
  - 0.3782314658164978
  - 0.41426119208335876
  - 0.38989415764808655
  - 0.39235398173332214
  - 0.41661542654037476
  - 0.37516480684280396
  - 0.39087772369384766
  - 0.3744753301143646
  - 0.3753696382045746
  - 0.38204026222229004
  - 0.37381961941719055
  - 0.38259124755859375
  - 0.3762347102165222
loss_records_fold3:
  train_losses:
  - 1.5057191818952562
  - 1.5066661179065706
  - 1.9765919327735901
  - 1.7268337965011598
  - 1.507703948020935
  - 1.5311158418655397
  - 1.743117666244507
  - 1.5722067654132843
  - 1.4798692047595978
  - 1.6943642616271974
  - 1.5279871225357056
  validation_losses:
  - 0.3943571448326111
  - 0.4024190902709961
  - 0.3899829089641571
  - 0.39293012022972107
  - 0.39430201053619385
  - 0.40353259444236755
  - 0.3960161507129669
  - 0.4031206965446472
  - 0.4005810022354126
  - 0.4027121067047119
  - 0.3990883529186249
loss_records_fold4:
  train_losses:
  - 1.4833811640739443
  - 1.4827583611011506
  - 1.6574973225593568
  - 1.5240295708179474
  - 1.5119800180196763
  - 1.703427278995514
  - 1.573761981725693
  - 1.5348427414894106
  - 1.4969451934099198
  - 1.549873596429825
  - 1.5761546075344086
  - 1.507238578796387
  - 1.5128574311733247
  - 1.4543868809938432
  - 1.5067402482032777
  - 1.4860883474349977
  validation_losses:
  - 0.39106103777885437
  - 0.39028313755989075
  - 0.3892519474029541
  - 0.4914652109146118
  - 0.3921407163143158
  - 0.4235295057296753
  - 0.39172685146331787
  - 0.4075661301612854
  - 0.4008982479572296
  - 0.4382166564464569
  - 0.3854829967021942
  - 0.384918212890625
  - 0.3904014229774475
  - 0.3913743495941162
  - 0.3928740322589874
  - 0.3889763653278351
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 80 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:19:05.148654'
