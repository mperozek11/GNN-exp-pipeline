config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:00:41.754879'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_27fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_27fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_27fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_27fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_27fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_27fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_27fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_27fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_27fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_27fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 20.43679828643799
  - 10.025147259235382
  - 2.2378749728202822
  - 2.101801359653473
  - 1.7961055874824525
  - 2.905175262689591
  - 1.9729211330413818
  - 2.3091020047664643
  - 2.5184511840343475
  - 1.43031005859375
  - 1.2832350850105287
  - 1.4418131351470949
  - 1.783304011821747
  - 2.73742139339447
  - 2.038943660259247
  - 1.4901866734027864
  - 1.5506305754184724
  - 1.0904949128627777
  - 1.0174997985363008
  - 1.0902104377746582
  - 0.9269444286823273
  - 0.8201946794986725
  - 0.829845416545868
  - 0.8437559068202973
  - 1.629276293516159
  - 1.285158121585846
  - 2.125202012062073
  - 3.5866965472698213
  - 1.6213989913463593
  - 1.3904155135154725
  - 3.3669343313202265
  - 1.881840217113495
  - 2.2468634963035585
  - 1.4183894872665406
  - 1.0448950827121735
  - 0.9693477630615235
  - 4.025283396244049
  - 1.3696825683116913
  - 1.1820055902004243
  - 6.614434897899628
  - 2.069905918836594
  - 1.5211594820022585
  - 2.863814115524292
  - 3.719770020246506
  - 2.446023714542389
  - 1.120154458284378
  - 1.0239846765995027
  - 1.0620296955108643
  - 1.0178609848022462
  - 1.037813201546669
  - 1.0392124474048614
  - 1.4806530058383942
  - 0.8203682482242585
  - 0.9744586050510406
  - 0.9357942461967469
  - 0.986145281791687
  - 0.8430363655090333
  - 0.7893183708190918
  - 0.9789342224597931
  - 0.9137446880340576
  - 1.1443130254745484
  - 0.8201760113239289
  - 0.7457115471363068
  - 0.7753501057624818
  - 0.8371813118457795
  - 0.834334409236908
  - 0.8440768957138062
  - 0.8475808739662171
  - 1.0224897921085359
  - 0.8437059223651886
  - 0.8720247924327851
  - 1.1204064071178437
  - 1.1723494946956634
  - 1.8968806147575379
  - 1.092525202035904
  - 1.0756031811237337
  - 0.8158883661031724
  - 0.8153515994548798
  - 0.7807499587535859
  - 0.8206398725509644
  - 0.7685040533542633
  - 0.743208196759224
  - 0.8194547891616821
  - 0.8127574741840363
  - 0.7671941816806793
  - 0.9822440326213837
  - 1.495479428768158
  - 1.4445310831069946
  - 1.5201927602291108
  - 1.1037438571453095
  - 1.0068690776824951
  - 0.9852748394012452
  - 0.9007300794124604
  - 1.0329473733901977
  - 0.9044300436973572
  - 1.0698818504810335
  - 0.7803544640541077
  - 0.7976265609264375
  - 0.8422878921031952
  - 0.8922987818717957
  validation_losses:
  - 5.534725189208984
  - 2.113222360610962
  - 2.5633671283721924
  - 1.8313920497894287
  - 1.197702407836914
  - 1.8022454977035522
  - 1.1267138719558716
  - 0.8272120356559753
  - 1.0455349683761597
  - 0.7403525710105896
  - 0.779532253742218
  - 1.6842257976531982
  - 0.5578625202178955
  - 1.4468967914581299
  - 0.5267432332038879
  - 0.5010733008384705
  - 0.5442705154418945
  - 0.4815576672554016
  - 0.5094103217124939
  - 0.4846116304397583
  - 0.45059189200401306
  - 0.38698142766952515
  - 0.3991542160511017
  - 0.39721375703811646
  - 0.39779844880104065
  - 0.535452127456665
  - 1.30165696144104
  - 0.6917967200279236
  - 0.6383283734321594
  - 0.638094961643219
  - 1.2809953689575195
  - 1.3817657232284546
  - 0.7671046853065491
  - 0.5427712202072144
  - 0.48857825994491577
  - 0.4144704341888428
  - 0.509881317615509
  - 0.43588387966156006
  - 0.561629056930542
  - 0.9561530351638794
  - 0.6534860730171204
  - 0.575705885887146
  - 0.5389640927314758
  - 0.46974143385887146
  - 0.46141454577445984
  - 0.550872802734375
  - 0.45825713872909546
  - 0.4501286447048187
  - 0.7233827710151672
  - 0.4557524621486664
  - 0.4158060848712921
  - 0.40317878127098083
  - 0.4322216212749481
  - 0.4604000151157379
  - 0.6633532047271729
  - 0.3975352346897125
  - 0.38778138160705566
  - 0.38131290674209595
  - 0.38086429238319397
  - 0.40023529529571533
  - 0.3980463445186615
  - 0.4064731001853943
  - 0.3854122757911682
  - 0.38541263341903687
  - 0.39254072308540344
  - 0.4362073242664337
  - 0.3813607394695282
  - 0.40704795718193054
  - 0.6759841442108154
  - 0.3981689214706421
  - 0.4521341025829315
  - 0.43538782000541687
  - 0.47089526057243347
  - 0.5845878720283508
  - 0.6472475528717041
  - 0.5104824900627136
  - 0.3903711438179016
  - 0.382973849773407
  - 0.39378419518470764
  - 0.3897334039211273
  - 0.38213229179382324
  - 0.3809819221496582
  - 0.4367980659008026
  - 0.3873290717601776
  - 0.3837011754512787
  - 0.5536247491836548
  - 0.4169946014881134
  - 0.4926123023033142
  - 0.7718507051467896
  - 0.43142685294151306
  - 0.4622338116168976
  - 0.43318480253219604
  - 0.41969314217567444
  - 0.3965737223625183
  - 0.818093478679657
  - 0.38687577843666077
  - 0.39229342341423035
  - 0.38869741559028625
  - 0.38426142930984497
  - 0.44348785281181335
loss_records_fold1:
  train_losses:
  - 0.8927969038486481
  - 0.8556391656398774
  - 0.8615292310714722
  - 0.8102389812469483
  - 0.8956803798675538
  - 0.811223441362381
  - 0.9834579050540925
  - 0.8323578298091889
  - 0.8387584567070008
  - 0.8362754225730896
  - 0.9635547578334809
  - 1.1696261763572693
  - 0.8710286676883698
  - 0.8736084580421448
  - 1.7945698499679565
  - 1.0625837832689287
  - 1.125324285030365
  - 1.2848657667636871
  - 1.3041697204113007
  - 0.9413190245628358
  - 0.8248563796281815
  - 0.8922544121742249
  - 0.8624658286571503
  - 0.8173263967037201
  - 1.2339051425457002
  - 0.7854846239089966
  - 0.8084310173988343
  - 0.8309326171875
  - 0.7873963832855225
  - 0.759300720691681
  - 0.771492099761963
  - 1.0577157199382783
  - 1.260743683576584
  - 1.0900457203388214
  - 1.0472453832626343
  - 0.9729220032691956
  - 0.9336241781711578
  - 0.8976793825626374
  - 1.207332617044449
  - 0.951432889699936
  - 0.8158739686012269
  - 0.8196296930313111
  - 0.7957713007926941
  - 0.7662807703018188
  - 0.8762601792812348
  - 0.8160082697868347
  - 0.823325937986374
  - 3.9415074408054354
  - 1.157886838912964
  - 0.8008174479007721
  - 1.1173022627830507
  - 4.225602543354035
  - 1.3387113511562347
  - 1.0088677406311035
  - 1.1730351388454439
  - 1.6691651463508608
  - 5.050889021158219
  - 1.6820294797420503
  - 0.980942577123642
  - 0.8913546681404114
  - 0.8111451268196106
  - 0.83378866314888
  - 0.8597599804401398
  - 1.1182777762413025
  - 1.2412243366241456
  - 1.229662472009659
  - 1.159821778535843
  - 2.9674534082412722
  - 1.8667538940906525
  - 0.9447993695735932
  - 1.0360685110092163
  validation_losses:
  - 0.4051001965999603
  - 0.41310763359069824
  - 0.39969098567962646
  - 0.4186757802963257
  - 0.410662978887558
  - 0.3957667946815491
  - 0.4187815487384796
  - 0.3984200656414032
  - 0.4112171232700348
  - 0.40755388140678406
  - 0.42396029829978943
  - 0.47094470262527466
  - 0.42670997977256775
  - 0.40099772810935974
  - 0.6477571129798889
  - 0.44580990076065063
  - 0.4311976730823517
  - 0.4131743609905243
  - 0.5259811282157898
  - 0.40348610281944275
  - 0.4034137725830078
  - 0.4264366626739502
  - 0.43096858263015747
  - 0.4096188247203827
  - 0.5130588412284851
  - 0.40284186601638794
  - 0.4530489146709442
  - 0.4173893928527832
  - 0.4217989444732666
  - 0.4037165939807892
  - 0.4263734817504883
  - 1.075106143951416
  - 0.4245450496673584
  - 0.43620043992996216
  - 0.40441301465034485
  - 0.4046212136745453
  - 0.5611768960952759
  - 0.42737820744514465
  - 0.7014459371566772
  - 0.4766559898853302
  - 0.405376136302948
  - 0.39498624205589294
  - 0.3956432342529297
  - 0.42833957076072693
  - 0.4537598490715027
  - 0.43770632147789
  - 0.404170960187912
  - 0.39789584279060364
  - 0.3978995680809021
  - 0.4093782603740692
  - 0.6134435534477234
  - 0.43061402440071106
  - 0.4143231511116028
  - 0.41675832867622375
  - 0.4822583496570587
  - 0.408498078584671
  - 0.40823328495025635
  - 0.7211552262306213
  - 0.42846131324768066
  - 0.4028831422328949
  - 0.4067564010620117
  - 0.4184725880622864
  - 0.47034189105033875
  - 0.45997968316078186
  - 0.7049262523651123
  - 0.6408247351646423
  - 0.5860835313796997
  - 0.4245416522026062
  - 0.40944981575012207
  - 0.4114193618297577
  - 0.41353487968444824
loss_records_fold2:
  train_losses:
  - 0.974144846200943
  - 2.927584445476532
  - 0.9160399705171586
  - 0.7895517401397228
  - 0.8389055967330933
  - 0.860844111442566
  - 0.7801598370075227
  - 0.8490618526935578
  - 1.0810453474521637
  - 1.0586630403995514
  - 0.8990029454231263
  - 2.065490388870239
  - 0.9839314043521882
  - 0.9429997265338899
  - 0.7426648050546647
  - 0.8205232799053193
  - 0.8713968455791474
  - 0.8447151005268098
  - 0.8622341156005859
  - 0.8185214936733246
  - 0.7703910708427429
  - 0.7643611043691636
  - 0.8126583814620972
  - 0.7809492588043213
  - 0.8054209172725678
  - 0.7470547020435334
  - 0.791904902458191
  - 0.7919169783592225
  - 0.7527856707572937
  - 0.8003882944583893
  - 0.7731048166751862
  - 0.8083868622779846
  - 1.0206067562103271
  - 0.8459207892417908
  - 0.8238573789596558
  - 0.8100500404834747
  - 0.9441038846969605
  - 0.860106772184372
  - 0.8180785357952118
  - 0.8083617806434632
  - 0.7865580439567567
  - 0.8505440533161164
  validation_losses:
  - 0.37420251965522766
  - 0.39772289991378784
  - 0.37171632051467896
  - 0.37932029366493225
  - 0.39058345556259155
  - 0.38128024339675903
  - 0.3855026960372925
  - 0.39139726758003235
  - 0.419567734003067
  - 0.38449740409851074
  - 0.4292135536670685
  - 0.4125773012638092
  - 0.37282755970954895
  - 0.3728419542312622
  - 0.37317293882369995
  - 0.5117109417915344
  - 0.38352057337760925
  - 0.3723902702331543
  - 0.3735727369785309
  - 0.38463497161865234
  - 0.4180251955986023
  - 0.37490034103393555
  - 0.39140385389328003
  - 0.3817903399467468
  - 0.3979760706424713
  - 0.37372270226478577
  - 0.3884901702404022
  - 0.39088910818099976
  - 0.3735763728618622
  - 0.38552841544151306
  - 0.3863648474216461
  - 0.3928258419036865
  - 0.46938806772232056
  - 0.3988419473171234
  - 0.3739417493343353
  - 0.41672852635383606
  - 0.3776065409183502
  - 0.3719061315059662
  - 0.3752285838127136
  - 0.3712247312068939
  - 0.37140071392059326
  - 0.38121911883354187
loss_records_fold3:
  train_losses:
  - 0.796781998872757
  - 0.8474914252758027
  - 0.8671879947185517
  - 0.8507934749126435
  - 0.9812307536602021
  - 0.8119194328784943
  - 0.885068941116333
  - 0.8710638940334321
  - 1.7094726860523224
  - 0.8779349684715272
  - 0.7830236017704011
  - 0.790398931503296
  - 0.890545266866684
  - 0.7221681147813798
  - 0.8058055520057679
  - 0.7859309434890748
  - 0.7725302159786225
  - 0.7693648040294647
  - 0.7993671536445618
  - 0.7676387071609497
  - 0.7702979624271393
  validation_losses:
  - 0.37640440464019775
  - 0.4668008089065552
  - 0.3906804919242859
  - 0.3754332363605499
  - 0.38431796431541443
  - 0.3774866759777069
  - 0.37727075815200806
  - 0.3932023048400879
  - 0.39109721779823303
  - 0.3901647627353668
  - 0.4667031764984131
  - 0.4201030433177948
  - 0.41655081510543823
  - 0.3914557695388794
  - 0.4352131187915802
  - 0.3857840299606323
  - 0.3772454857826233
  - 0.3761170208454132
  - 0.37895822525024414
  - 0.3835034668445587
  - 0.38918787240982056
loss_records_fold4:
  train_losses:
  - 0.7437645077705384
  - 0.7232069730758668
  - 0.7567111670970917
  - 0.781908404827118
  - 0.7916205942630768
  - 0.7582255125045777
  - 0.7673943042755127
  - 0.7656979441642762
  - 0.7837836623191834
  - 0.7756659090518951
  - 0.7991707503795624
  validation_losses:
  - 0.3767228424549103
  - 0.40728601813316345
  - 0.49918919801712036
  - 0.3969246745109558
  - 0.41691988706588745
  - 0.3837623596191406
  - 0.38094422221183777
  - 0.38629746437072754
  - 0.38278958201408386
  - 0.38555091619491577
  - 0.38998275995254517
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 71 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 42 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8336192109777015, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.07619047619047621, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8538110142467271
  mean_f1_accuracy: 0.015238095238095242
  total_train_time: '0:19:47.224072'
