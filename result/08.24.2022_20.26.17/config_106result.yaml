config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:53:33.905754'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_106fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_106fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_106fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_106fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_106fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_106fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_106fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_106fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_106fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_106fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 23.530051279067994
  - 11.600837481021882
  - 12.455395114421846
  - 6.895945972204209
  - 7.740063029527665
  - 4.871591544151307
  - 5.11556146144867
  - 2.9737790420651438
  - 2.6365386784076694
  - 2.4312488555908205
  - 2.9342753887176514
  - 1.6920800864696504
  - 1.7792018949985504
  - 2.6511050164699554
  - 14.772058141231538
  - 5.895761001110078
  - 4.722497487068177
  - 2.2302022516727447
  - 3.36254261136055
  - 4.916765397787095
  - 3.005811017751694
  - 4.64805897474289
  - 2.653560692071915
  - 2.7222409307956696
  - 3.913934290409088
  - 1.938653439283371
  - 5.474508833885193
  - 6.494630092382431
  - 2.5889141976833345
  - 1.8484035730361938
  - 1.6662444770336151
  - 1.7673231780529024
  - 1.8582432270050049
  - 1.7184449195861817
  - 2.0893653213977816
  - 3.173921686410904
  - 3.5078125953674317
  - 4.022151201963425
  - 2.9041342318058017
  - 2.393342137336731
  - 1.9787997901439667
  - 12.375826871395113
  - 2.173589771986008
  - 1.8936096012592316
  - 1.7814977288246157
  - 1.727733075618744
  - 1.9431335210800171
  - 2.269282537698746
  - 1.6612986207008362
  - 1.7101836740970613
  - 2.8377700567245485
  - 2.1499482929706573
  - 4.70212082862854
  - 1.9106427311897278
  - 3.674996876716614
  - 2.1980660796165465
  - 2.905940198898316
  - 4.434763461351395
  - 1.982075196504593
  - 2.987767028808594
  - 1.8143248677253725
  - 2.524880310893059
  - 1.9956830084323884
  - 2.121870052814484
  validation_losses:
  - 4.851731777191162
  - 2.7134201526641846
  - 1.6169109344482422
  - 1.2102574110031128
  - 0.5964094996452332
  - 1.0697205066680908
  - 0.6036427617073059
  - 0.8428907990455627
  - 0.6595511436462402
  - 0.4754369854927063
  - 0.5601551532745361
  - 0.42896768450737
  - 0.47017529606819153
  - 11.252580642700195
  - 0.7499339580535889
  - 0.5905002355575562
  - 3.5319695472717285
  - 1.1243295669555664
  - 0.40103578567504883
  - 0.3922838568687439
  - 0.6016795635223389
  - 0.9524653553962708
  - 0.45467549562454224
  - 0.6073707938194275
  - 0.628965437412262
  - 0.6413697600364685
  - 0.42263826727867126
  - 0.43787485361099243
  - 0.40047192573547363
  - 0.4972637891769409
  - 0.3818727731704712
  - 0.38678646087646484
  - 0.3958224356174469
  - 0.38866186141967773
  - 0.3894534707069397
  - 0.8938890099525452
  - 0.47094643115997314
  - 0.4119682312011719
  - 0.43505531549453735
  - 0.39407968521118164
  - 0.48578697443008423
  - 0.4062274992465973
  - 0.3941180408000946
  - 0.43886733055114746
  - 0.3769788444042206
  - 0.3812759220600128
  - 0.40583568811416626
  - 0.42568567395210266
  - 0.39009299874305725
  - 0.42733171582221985
  - 0.41761109232902527
  - 0.3799940049648285
  - 0.4308822453022003
  - 0.3932746350765228
  - 0.4443936347961426
  - 0.406279057264328
  - 0.3856731653213501
  - 1.043688178062439
  - 0.38907280564308167
  - 0.3916989862918854
  - 0.3911842107772827
  - 0.38652655482292175
  - 0.39291703701019287
  - 0.3863092064857483
loss_records_fold1:
  train_losses:
  - 1.793251705169678
  - 1.7684205770492554
  - 1.8683079063892365
  - 1.5009178400039673
  - 1.616226202249527
  - 2.165992331504822
  - 1.4910138249397278
  - 3.032741063833237
  - 1.6664071321487428
  - 1.5755371421575548
  - 1.51108980178833
  - 1.662572377920151
  - 1.6439698398113252
  - 1.589132195711136
  - 1.6050014197826385
  - 2.723385471105576
  - 1.6488012909889223
  - 1.958697384595871
  - 1.7079759269952774
  - 1.595664167404175
  - 1.4828203320503235
  - 1.5908634483814241
  - 1.5556530833244324
  - 1.6357132077217102
  - 2.080038321018219
  - 1.8374147713184357
  - 1.626465117931366
  - 3.5883279085159305
  - 3.15699265897274
  - 1.6886377215385437
  - 1.6471970796585085
  - 1.535426598787308
  - 1.659093225002289
  - 4.35836153626442
  - 1.9109328329563142
  - 2.167972218990326
  - 1.5919993340969087
  - 1.6325392961502077
  - 1.8761474668979645
  - 2.381386685371399
  - 1.9265910416841507
  - 2.7083093583583833
  - 2.0697075545787813
  - 1.7350832760334016
  - 1.4814375400543214
  - 1.502923381328583
  - 1.5419696986675264
  - 2.608552885055542
  - 1.5518264532089234
  - 1.5419275224208833
  - 1.7317954719066622
  - 3.4521746397018434
  - 1.474391609430313
  - 1.5192664980888368
  validation_losses:
  - 0.4026796519756317
  - 0.5685412287712097
  - 0.395139217376709
  - 0.46705588698387146
  - 0.39566653966903687
  - 0.41844797134399414
  - 0.4351000487804413
  - 0.5547776818275452
  - 0.433103084564209
  - 0.39748111367225647
  - 0.4090292453765869
  - 0.3967050611972809
  - 0.4048088788986206
  - 0.3971942663192749
  - 0.4056197702884674
  - 0.44925305247306824
  - 0.3957161605358124
  - 0.4740501642227173
  - 0.39562419056892395
  - 0.40533772110939026
  - 0.39414510130882263
  - 0.4177476763725281
  - 0.40474602580070496
  - 0.40055567026138306
  - 0.5189862251281738
  - 0.3975405693054199
  - 0.40778622031211853
  - 0.39853435754776
  - 0.3953437805175781
  - 0.4847310781478882
  - 0.40592920780181885
  - 0.4293143153190613
  - 0.39954084157943726
  - 0.43540331721305847
  - 0.6674949526786804
  - 0.41217106580734253
  - 0.44749030470848083
  - 0.3960893750190735
  - 0.4750760495662689
  - 0.552988588809967
  - 0.4146648943424225
  - 0.4363733232021332
  - 0.39978882670402527
  - 0.40019574761390686
  - 0.40454065799713135
  - 0.4319806396961212
  - 0.39852991700172424
  - 0.4266408383846283
  - 0.40094396471977234
  - 0.40008434653282166
  - 0.3986952602863312
  - 0.39763763546943665
  - 0.4013979136943817
  - 0.40909329056739807
loss_records_fold2:
  train_losses:
  - 1.526259106397629
  - 1.5734183728694917
  - 1.4850436687469484
  - 1.5688061594963074
  - 1.7232374727725983
  - 3.0808025240898136
  - 2.0010657727718355
  - 1.7448637902736666
  - 2.5411040723323826
  - 2.675076532363892
  - 1.5354273796081543
  - 2.1610889971256255
  - 1.6889692664146425
  - 1.853503340482712
  - 1.8527637600898743
  - 1.5334383487701417
  - 1.5120176017284395
  - 1.4672040283679963
  - 1.492183440923691
  - 1.4896450102329255
  - 1.499114939570427
  - 1.519615399837494
  - 1.479299956560135
  - 1.4939101397991181
  - 1.495318514108658
  - 1.574609625339508
  - 1.4900784373283387
  - 1.5775930345058442
  - 1.5074131846427918
  - 1.518661403656006
  - 2.159566676616669
  - 1.5625097334384919
  - 1.5376642048358917
  - 1.6347187519073487
  - 1.4524160563945772
  - 1.6193004071712496
  - 1.6054798990488053
  - 1.4789626896381378
  - 1.5129812479019167
  - 1.4917989730834962
  - 1.4562863051891328
  - 1.4560367941856385
  - 1.4349968850612642
  - 1.4419409036636353
  - 1.5228789508342744
  - 1.8121789813041689
  - 1.8825472414493563
  - 1.7313384890556336
  - 1.505424976348877
  - 1.5755993008613587
  - 1.622588497400284
  - 1.8200481474399568
  - 1.6379252612590791
  - 1.50620995759964
  - 1.4600483536720277
  - 2.01444793343544
  - 1.5958606243133546
  - 1.47286314368248
  validation_losses:
  - 0.40027356147766113
  - 0.38069432973861694
  - 0.3819977641105652
  - 0.4173184931278229
  - 0.3753814697265625
  - 0.3761619031429291
  - 0.3887471854686737
  - 0.39096567034721375
  - 0.39672768115997314
  - 0.3754763901233673
  - 0.3762572109699249
  - 0.5679947733879089
  - 0.40171101689338684
  - 0.3777109384536743
  - 0.37236282229423523
  - 0.3859637379646301
  - 0.3806496858596802
  - 0.37554073333740234
  - 0.38722196221351624
  - 0.37489286065101624
  - 0.44656848907470703
  - 0.40182363986968994
  - 0.38076281547546387
  - 0.37566933035850525
  - 0.3949468731880188
  - 0.3740917444229126
  - 0.4426402747631073
  - 0.37519699335098267
  - 0.38035351037979126
  - 0.3779395520687103
  - 0.40365153551101685
  - 0.37324678897857666
  - 0.39425402879714966
  - 0.4123038649559021
  - 0.3904734253883362
  - 0.41564202308654785
  - 0.374240517616272
  - 0.402461439371109
  - 0.4013160169124603
  - 0.37268900871276855
  - 0.38761425018310547
  - 0.37710654735565186
  - 0.38377633690834045
  - 0.5328364372253418
  - 0.3858894109725952
  - 0.4090103209018707
  - 0.4829244017601013
  - 0.4367174506187439
  - 0.561177670955658
  - 0.38124966621398926
  - 0.38077741861343384
  - 0.6841756701469421
  - 0.3994975686073303
  - 0.37629759311676025
  - 0.37599512934684753
  - 0.37713149189949036
  - 0.3760203421115875
  - 0.38214191794395447
loss_records_fold3:
  train_losses:
  - 1.5519702076911928
  - 1.4674490809440615
  - 1.4355911761522293
  - 1.5651094675064088
  - 1.4581479907035828
  - 1.4855866551399233
  - 1.529204970598221
  - 1.432057934999466
  - 1.4878335773944855
  - 1.539007043838501
  - 1.4637746512889862
  - 1.4705655753612519
  - 1.46240913271904
  - 1.4698479890823366
  - 1.422380954027176
  - 1.6988661289215088
  - 1.4854234963655473
  - 1.5428222060203554
  - 1.7634257197380068
  - 1.584058254957199
  - 1.514738804101944
  - 1.780789852142334
  - 1.664142829179764
  - 1.568442875146866
  - 1.6775750815868378
  - 1.5728265583515169
  - 1.5254349887371064
  - 1.5315346598625184
  - 1.732691466808319
  - 1.5656697839498521
  - 1.544410413503647
  - 1.550869280099869
  - 1.5419214606285097
  - 1.5373083472251894
  - 1.5337252020835876
  - 1.5530961871147158
  - 1.4406381949782372
  - 1.500359344482422
  - 1.476992988586426
  - 1.4857526183128358
  - 1.5113484561443329
  - 1.4892846703529359
  validation_losses:
  - 0.3997962176799774
  - 0.38082602620124817
  - 0.3853927254676819
  - 0.3872111737728119
  - 0.4064825773239136
  - 0.3808620572090149
  - 0.3817260265350342
  - 0.38917526602745056
  - 0.4479420483112335
  - 0.3853677809238434
  - 0.3816130459308624
  - 0.3788543939590454
  - 0.384518027305603
  - 0.3807224631309509
  - 0.3941180408000946
  - 0.3785095810890198
  - 0.4040713608264923
  - 0.38342905044555664
  - 0.38972750306129456
  - 0.4146060347557068
  - 0.4025546610355377
  - 0.38130486011505127
  - 0.7229709625244141
  - 0.4799107611179352
  - 0.3946272134780884
  - 0.41300591826438904
  - 0.3840535879135132
  - 0.38714978098869324
  - 0.3883773386478424
  - 0.4103684723377228
  - 0.38663819432258606
  - 0.38513273000717163
  - 0.38674789667129517
  - 0.4200587570667267
  - 0.3901212215423584
  - 0.4090045988559723
  - 0.40154343843460083
  - 0.3964366316795349
  - 0.3877546191215515
  - 0.39069458842277527
  - 0.3857190012931824
  - 0.39529839158058167
loss_records_fold4:
  train_losses:
  - 1.5018047749996186
  - 1.4916723370552063
  - 1.652387982606888
  - 1.5456753790378572
  - 1.5000675559043886
  - 1.5152333974838257
  - 1.4591750741004945
  - 1.4763476788997651
  - 1.4638065934181215
  - 1.5745932340621949
  - 1.5902872860431672
  - 1.522237277030945
  - 1.48409481048584
  - 1.514062935113907
  - 1.6328701376914978
  - 1.46809279024601
  - 1.4956009685993195
  - 1.5071520745754243
  - 1.4931111991405488
  - 1.4459953844547273
  - 1.476871767640114
  - 1.474921989440918
  - 1.4967634320259096
  - 1.606682848930359
  validation_losses:
  - 0.3801109790802002
  - 0.3826805055141449
  - 0.3878375291824341
  - 0.3995855748653412
  - 0.39167460799217224
  - 0.38280847668647766
  - 0.3861079514026642
  - 0.3999176621437073
  - 0.3922288715839386
  - 0.3895437717437744
  - 0.386015921831131
  - 0.3984813690185547
  - 0.3875662684440613
  - 0.4129312336444855
  - 0.3871650695800781
  - 0.380634069442749
  - 0.37901100516319275
  - 0.3966135084629059
  - 0.3776437044143677
  - 0.3769513964653015
  - 0.37886473536491394
  - 0.37920182943344116
  - 0.38327881693840027
  - 0.3839995563030243
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 64 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 54 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 58 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 42 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:21:44.637847'
