config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 15:52:58.607285'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/22/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/22/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/22/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/22/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/22/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/22/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/22/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/22/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/22/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/22/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 102.20073461532593
  - 41.552839517593384
  - 12.570400774478912
  - 10.967609941959381
  - 9.546971380710602
  - 12.678926527500153
  - 13.979994714260101
  - 17.185082256793976
  - 9.077404141426086
  - 13.704590559005737
  - 8.264875084161758
  - 5.248055189847946
  - 4.756129831075668
  - 9.163083583116531
  - 8.896296501159668
  - 6.163001924753189
  - 12.628200113773346
  - 9.58227252960205
  - 8.020424902439117
  - 10.426767766475677
  - 5.738424122333527
  - 6.887572526931763
  - 5.561331361532211
  - 4.396379709243774
  - 7.697163611650467
  - 6.8501730263233185
  - 13.371905595064163
  - 13.957371056079865
  - 6.44012513756752
  - 5.741481006145477
  - 11.10259153880179
  - 7.584427446126938
  - 8.270963937044144
  - 6.6394528448581696
  - 5.567890048027039
  - 4.225186288356781
  - 19.229401469230652
  - 5.1797677874565125
  - 5.0103887021541595
  - 9.944782376289368
  - 8.503365695476532
  - 5.551806688308716
  - 14.373191148042679
  - 7.304193317890167
  - 8.623296678066254
  - 4.352573901414871
  - 4.84411147236824
  - 5.242606192827225
  - 4.603287190198898
  - 4.42079758644104
  - 4.489079028367996
  - 5.845347821712494
  - 4.68869423866272
  - 10.073822438716888
  - 6.160681664943695
  - 4.4015326499938965
  - 4.463140577077866
  - 4.235381156206131
  - 5.754992723464966
  - 4.7630515396595
  - 4.8031085431575775
  - 4.204830646514893
  - 4.159175515174866
  - 4.525190204381943
  - 4.208430111408234
  - 4.376816362142563
  - 4.695197552442551
  - 4.770383775234222
  - 7.566876024007797
  - 4.39493653178215
  - 4.323507100343704
  - 4.660478055477142
  - 6.120353251695633
  - 11.5603686273098
  - 8.685634642839432
  - 11.33946281671524
  - 7.023425996303558
  - 7.0981506407260895
  - 7.0847742557525635
  - 7.9791619181633
  - 4.522729516029358
  - 4.906248942017555
  - 5.611020982265472
  - 4.777376115322113
  - 4.016322553157806
  - 8.648997902870178
  - 7.389947175979614
  - 4.737182646989822
  - 5.7019223272800446
  - 7.307332634925842
  - 4.466380193829536
  - 5.180512607097626
  - 4.744584530591965
  - 5.727879196405411
  - 4.599291145801544
  - 5.300326466560364
  - 4.002114653587341
  - 3.815574735403061
  - 4.899260848760605
  - 4.467538356781006
  validation_losses:
  - 5.3115339279174805
  - 1.1516833305358887
  - 1.245880365371704
  - 1.714313268661499
  - 1.1056699752807617
  - 2.9174320697784424
  - 1.9856740236282349
  - 1.4963024854660034
  - 0.8727210164070129
  - 1.0510408878326416
  - 0.8274754285812378
  - 0.5423058867454529
  - 0.4041445553302765
  - 0.8040724396705627
  - 0.6171461343765259
  - 0.5504779815673828
  - 0.7737439870834351
  - 0.7442979216575623
  - 0.6136175394058228
  - 0.6653074026107788
  - 0.54779052734375
  - 0.7133285999298096
  - 0.4174657166004181
  - 0.41423922777175903
  - 0.6791568398475647
  - 0.5187935829162598
  - 1.005248785018921
  - 0.6882480382919312
  - 0.4737529158592224
  - 0.5398672819137573
  - 0.7645701766014099
  - 0.4500054717063904
  - 0.6923593282699585
  - 0.4689251482486725
  - 0.4852144420146942
  - 0.5667690634727478
  - 0.9336980581283569
  - 0.48898983001708984
  - 0.4307170808315277
  - 0.42282843589782715
  - 0.454889178276062
  - 0.5659593343734741
  - 0.6784610748291016
  - 0.758586585521698
  - 0.5301459431648254
  - 0.44092825055122375
  - 0.4443461000919342
  - 0.4217146635055542
  - 0.49661773443222046
  - 0.38826993107795715
  - 0.4170630872249603
  - 0.5086973905563354
  - 0.5402968525886536
  - 0.8827503323554993
  - 0.3984529972076416
  - 0.40630409121513367
  - 0.45894527435302734
  - 0.4166348874568939
  - 0.5317699909210205
  - 0.4246619939804077
  - 0.3942635953426361
  - 0.38081344962120056
  - 0.3814118802547455
  - 0.44808316230773926
  - 0.38340139389038086
  - 0.39165470004081726
  - 0.3878055810928345
  - 0.48593586683273315
  - 1.0690128803253174
  - 0.39246371388435364
  - 0.43794679641723633
  - 0.4151889681816101
  - 0.49303126335144043
  - 0.5785083174705505
  - 1.141261339187622
  - 0.6787958145141602
  - 0.5871562361717224
  - 0.4375501871109009
  - 0.49364492297172546
  - 0.4659889340400696
  - 0.4227966070175171
  - 0.40506064891815186
  - 0.46932005882263184
  - 0.37907692790031433
  - 0.38203734159469604
  - 0.4873993992805481
  - 0.39622625708580017
  - 0.4384945034980774
  - 0.4834204912185669
  - 0.4108598828315735
  - 0.46250563859939575
  - 0.49389052391052246
  - 0.4408462643623352
  - 0.3921791613101959
  - 0.4835512340068817
  - 0.3818778097629547
  - 0.3755098581314087
  - 0.39441436529159546
  - 0.40463531017303467
  - 0.45363402366638184
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 80 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
training_metrics:
  fold_eval_accs: '[0.8542024013722127, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.0
  total_train_time: '0:01:45.159153'
