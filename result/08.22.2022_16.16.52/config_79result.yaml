config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:57:36.809492'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_79fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_79fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_79fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_79fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_79fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_79fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_79fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_79fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_79fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_79fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 9.824799251556398
  - 9.835124170780183
  - 5.234763145446777
  - 2.7119559407234193
  - 2.7208411693573
  - 2.243859100341797
  - 1.8660849690437318
  - 1.7109987616539002
  - 2.6216533660888675
  - 1.5270323753356934
  - 1.6531949281692506
  - 1.8182345509529114
  - 1.6516870498657228
  - 1.6521579742431642
  - 1.6410545468330384
  - 6.164421784877778
  - 9.44206719994545
  - 2.058075821399689
  - 1.663610064983368
  - 1.8885308146476747
  - 2.6117871403694153
  - 2.4480929136276246
  - 1.836270034313202
  - 1.209385508298874
  - 1.6768411040306093
  - 1.2338540375232698
  - 2.1589429438114167
  - 1.5616370677948
  - 1.3861524045467377
  - 0.9574630081653596
  - 1.3575636804103852
  - 0.9503932893276215
  - 0.8709908425807953
  - 2.681038671731949
  - 1.0988709211349488
  - 1.4608584463596346
  - 1.7126941978931427
  - 0.9196289479732513
  - 1.008043384552002
  - 1.087248170375824
  - 1.3339365124702454
  - 0.8970769077539444
  - 1.0014626204967498
  - 4.855693942308426
  - 1.253766793012619
  - 1.6909951388835909
  - 1.2525785207748414
  - 3.7094868779182435
  - 0.9922440648078918
  - 1.3639854788780212
  - 1.8121032238006594
  - 1.3816201269626618
  - 2.0778793811798097
  - 1.3330823183059692
  - 1.1710991501808168
  - 1.293726032972336
  - 1.8085317075252534
  - 4.4089109122753145
  - 2.0061560690402986
  - 1.1889324188232422
  - 2.0652044475078584
  - 3.7058825194835663
  - 1.5036580324172975
  - 1.9481946527957916
  - 1.016366821527481
  - 1.5119570612907411
  - 0.992077374458313
  - 1.4683440625667572
  - 1.026878786087036
  - 1.3355027616024018
  - 1.5898002564907074
  - 0.9671584963798523
  - 4.472645801305771
  - 1.0791760206222534
  - 1.0092602133750916
  - 1.026700520515442
  - 0.9129247307777405
  - 1.0314216613769531
  - 0.8625714123249054
  - 1.0100847005844116
  - 0.8900833427906036
  - 1.1011669397354127
  - 1.2379575729370118
  - 0.9888072133064271
  - 1.1009084105491638
  - 0.8983023941516877
  - 3.1394030451774597
  - 1.846201664209366
  - 1.414002412557602
  - 0.935977178812027
  - 0.8816671192646027
  - 0.8605467021465302
  - 0.8904007017612457
  - 1.0360491156578064
  - 0.8108514100313187
  - 0.9548769593238831
  - 0.920272558927536
  - 0.8924548208713532
  - 0.9793702483177186
  - 0.8073301285505295
  validation_losses:
  - 2.3937017917633057
  - 3.9480111598968506
  - 6.1922197341918945
  - 1.3459137678146362
  - 0.7950401306152344
  - 0.7248611450195312
  - 0.6909346580505371
  - 0.7146211266517639
  - 0.5455525517463684
  - 0.8902046084403992
  - 0.5062047839164734
  - 0.9686076641082764
  - 0.9340609908103943
  - 0.9144791960716248
  - 0.49131062626838684
  - 1.228752613067627
  - 0.9770432710647583
  - 1.1444756984710693
  - 0.7498384714126587
  - 0.772819995880127
  - 0.5909720063209534
  - 0.7442054748535156
  - 0.47602757811546326
  - 0.63175368309021
  - 0.5170356035232544
  - 0.6856570839881897
  - 0.4413772523403168
  - 0.8589920401573181
  - 0.3916616141796112
  - 0.40409332513809204
  - 0.4110339283943176
  - 0.3868255317211151
  - 0.4191509783267975
  - 0.4110768139362335
  - 0.38666778802871704
  - 0.40343141555786133
  - 0.5680704116821289
  - 0.40943509340286255
  - 0.4282546639442444
  - 0.44822263717651367
  - 0.4651912450790405
  - 0.49571549892425537
  - 0.43571192026138306
  - 0.46018216013908386
  - 1.5092495679855347
  - 0.66502845287323
  - 0.41127264499664307
  - 0.4143731892108917
  - 0.45484378933906555
  - 0.4377143681049347
  - 0.41396039724349976
  - 0.39821505546569824
  - 0.4420282244682312
  - 0.4171576201915741
  - 0.45872145891189575
  - 0.4881276786327362
  - 1.138822078704834
  - 0.4887837767601013
  - 0.460443377494812
  - 0.47722530364990234
  - 0.39389947056770325
  - 0.49543723464012146
  - 0.45519235730171204
  - 0.45541274547576904
  - 0.42130371928215027
  - 0.47693920135498047
  - 0.39458799362182617
  - 0.40154916048049927
  - 0.4111921191215515
  - 0.5358178615570068
  - 0.49425172805786133
  - 0.3956184387207031
  - 0.42221760749816895
  - 0.4545880854129791
  - 0.4131791591644287
  - 0.446286678314209
  - 0.4133172631263733
  - 0.40002256631851196
  - 0.49612873792648315
  - 0.4278026521205902
  - 0.4655393362045288
  - 0.40046951174736023
  - 0.3913115859031677
  - 0.45564258098602295
  - 0.3968694508075714
  - 0.4191104769706726
  - 0.39734986424446106
  - 0.41174018383026123
  - 0.4138237535953522
  - 0.3936740756034851
  - 0.4120538532733917
  - 0.4094848334789276
  - 0.44926753640174866
  - 0.46484512090682983
  - 0.3951261043548584
  - 0.39041587710380554
  - 0.41333308815956116
  - 0.39807695150375366
  - 0.39088040590286255
  - 0.4002818763256073
loss_records_fold1:
  train_losses:
  - 0.8942565858364105
  - 0.832418966293335
  - 0.8443438649177551
  - 0.8791293263435365
  - 0.9158014237880707
  - 1.0926482856273652
  - 0.8203239977359772
  - 0.8474949717521668
  - 0.8921443045139313
  - 0.8639199256896973
  - 0.7964445173740388
  - 0.8966379940509797
  - 0.8480605185031891
  - 0.8955157458782197
  - 4.401051241159439
  validation_losses:
  - 0.4077762961387634
  - 0.41015103459358215
  - 0.4006396532058716
  - 0.41211938858032227
  - 0.412540465593338
  - 0.41804659366607666
  - 0.43052685260772705
  - 0.4050106704235077
  - 0.4375289976596832
  - 0.422059029340744
  - 0.4149059057235718
  - 0.4089493751525879
  - 0.4171772003173828
  - 0.4249185621738434
  - 0.40739142894744873
loss_records_fold2:
  train_losses:
  - 1.1412855863571167
  - 0.9610666036605835
  - 0.9343535661697389
  - 0.955351859331131
  - 0.8591267019510269
  - 0.976716011762619
  - 1.9161617159843445
  - 0.9476439118385316
  - 0.9735172092914581
  - 0.8872881889343263
  - 0.8627152025699616
  - 0.844380670785904
  - 0.8400747537612916
  - 0.8532819986343384
  - 0.9565019607543945
  - 0.9113211214542389
  - 3.2818062126636507
  - 1.2929539382457733
  - 1.076818871498108
  - 3.106049954891205
  - 1.6065802931785584
  - 1.1428240239620209
  - 1.280299174785614
  - 0.9973839282989503
  - 0.9380085051059723
  - 1.3503172636032106
  - 1.2049748301506042
  - 0.9055721938610077
  - 0.9411129534244538
  - 1.030373638868332
  - 0.8632056683301926
  - 0.8549293398857117
  - 0.8823020994663239
  - 0.8903700709342957
  - 1.0709584772586822
  - 1.143596488237381
  - 0.9705216586589813
  - 1.02479590177536
  - 0.9568997323513031
  - 1.305266445875168
  - 1.0121237635612488
  - 0.8521009355783463
  - 0.899248617887497
  - 0.8551716625690461
  - 0.8581333100795746
  - 0.8336735606193543
  - 0.8683776319026948
  - 0.8547650516033173
  - 0.8861887574195862
  - 0.8777053534984589
  - 0.8325805723667146
  - 0.9703362882137299
  - 0.853228372335434
  - 0.9521851241588593
  validation_losses:
  - 0.4423997700214386
  - 0.391944020986557
  - 0.39531004428863525
  - 0.3777930438518524
  - 0.3824821710586548
  - 0.3959006667137146
  - 0.4180182218551636
  - 0.44367921352386475
  - 0.383172869682312
  - 0.4006953239440918
  - 0.4000394940376282
  - 0.3892199993133545
  - 0.3794381022453308
  - 0.3906751275062561
  - 0.4201153516769409
  - 0.3879872262477875
  - 0.4092942178249359
  - 1.3012362718582153
  - 1.4081374406814575
  - 0.5423344373703003
  - 0.6662128567695618
  - 0.8298836946487427
  - 0.6561232209205627
  - 0.44995856285095215
  - 0.7819737792015076
  - 0.7827615737915039
  - 0.4862511456012726
  - 0.38877296447753906
  - 0.4747989773750305
  - 0.550817608833313
  - 0.40233591198921204
  - 0.39826762676239014
  - 0.39512020349502563
  - 0.4135684072971344
  - 0.44685471057891846
  - 0.49414703249931335
  - 0.45747849345207214
  - 0.3846077024936676
  - 0.5197292566299438
  - 0.40542128682136536
  - 0.4130893647670746
  - 0.47861331701278687
  - 0.38013142347335815
  - 0.3885625898838043
  - 0.38366013765335083
  - 0.3922734558582306
  - 0.4006587564945221
  - 0.4215194284915924
  - 0.39023256301879883
  - 0.3878695070743561
  - 0.396665096282959
  - 0.3877872824668884
  - 0.38998571038246155
  - 0.3766045570373535
loss_records_fold3:
  train_losses:
  - 1.1039855360984803
  - 0.8981662452220918
  - 0.8736205518245698
  - 0.8656050264835358
  - 0.8040979385375977
  - 1.7493303656578065
  - 1.179938507080078
  - 0.8354517281055451
  - 0.9104809522628785
  - 0.9063632190227509
  - 0.8932764410972596
  validation_losses:
  - 0.3897963762283325
  - 0.399534672498703
  - 0.40581178665161133
  - 0.40697726607322693
  - 0.42658036947250366
  - 0.41631370782852173
  - 0.3925876021385193
  - 0.39095667004585266
  - 0.39917364716529846
  - 0.3948104977607727
  - 0.39182180166244507
loss_records_fold4:
  train_losses:
  - 1.0083540499210357
  - 1.0168257892131807
  - 0.9764457881450653
  - 0.9025776565074921
  - 0.8307284027338029
  - 0.9314930856227875
  - 0.8914338052272797
  - 0.8400789380073548
  - 0.8254945755004883
  - 0.7946897923946381
  - 0.9019511282444
  - 1.0978672087192536
  - 0.9360943496227265
  - 0.898445200920105
  - 0.9451380491256715
  - 0.8700890600681306
  - 0.9254827439785004
  - 0.885724151134491
  - 0.8658414125442505
  - 0.8781413674354553
  - 0.9426618099212647
  - 1.1600098311901093
  - 0.9939926564693451
  - 0.7957797586917877
  - 0.8294891715049744
  - 0.8923770785331726
  - 0.9205688655376435
  - 2.1364899635314942
  - 0.8835230231285096
  - 0.9322749972343445
  - 1.0519632518291473
  - 0.8181626707315446
  - 0.8462821543216705
  - 0.8497533559799195
  - 0.8672315359115601
  - 0.8509071350097657
  - 0.8300981879234315
  validation_losses:
  - 0.4598575532436371
  - 0.444480836391449
  - 0.4805513024330139
  - 0.45100849866867065
  - 0.4453389346599579
  - 0.3928550183773041
  - 0.41483235359191895
  - 0.4608110189437866
  - 0.4605759084224701
  - 0.3973207175731659
  - 0.4266696870326996
  - 0.6701334118843079
  - 0.4517708420753479
  - 0.41687244176864624
  - 0.4206054210662842
  - 0.42241182923316956
  - 0.41040921211242676
  - 0.4306972622871399
  - 0.43174803256988525
  - 0.4591164290904999
  - 0.45118415355682373
  - 0.4302024841308594
  - 0.39734429121017456
  - 0.39592480659484863
  - 0.41420477628707886
  - 0.4040725827217102
  - 0.4025319218635559
  - 0.4160662889480591
  - 0.4095021188259125
  - 0.4276071786880493
  - 0.8713919520378113
  - 0.6183524131774902
  - 0.4811747074127197
  - 0.4001844525337219
  - 0.4074396789073944
  - 0.38923341035842896
  - 0.3937608599662781
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 54 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 37 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:17:48.346378'
