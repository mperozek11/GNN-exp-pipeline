config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 08:14:38.985510'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/154/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/154/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/154/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/154/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/154/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/154/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/154/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/154/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/154/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/154/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 158.29943180084229
  - 103.04500141739845
  - 58.060501873493195
  - 77.54763442277908
  - 82.50545704364777
  - 40.03453877568245
  - 41.743574649095535
  - 32.52126944065094
  - 66.81582215428352
  - 76.95837485790253
  - 51.62841787934303
  - 38.71392484009266
  - 63.2772741317749
  - 33.19272321462631
  - 63.110563069581985
  - 126.12271454930305
  - 37.11771894991398
  - 34.97089818120003
  - 24.50966814160347
  - 19.62410444021225
  - 31.784800201654434
  - 23.403917998075485
  - 26.93733012676239
  - 20.043999314308167
  - 23.378451108932495
  - 19.33668127655983
  - 31.21711191534996
  - 21.733023330569267
  - 21.175989985466003
  - 15.980794727802277
  - 16.660255879163742
  - 19.431970208883286
  - 26.325545638799667
  - 28.278301298618317
  - 22.33797739446163
  - 16.840182185173035
  - 26.790152922272682
  - 16.37086582183838
  - 16.109753906726837
  - 28.13384698331356
  - 22.15961855649948
  - 17.54934197664261
  - 16.25291182100773
  - 23.172207221388817
  - 17.321029648184776
  - 18.803513646125793
  - 16.712717175483704
  - 20.811997190117836
  - 17.766187995672226
  - 19.49925336241722
  - 20.278380423784256
  - 25.2255756855011
  - 17.989945635199547
  - 17.054823607206345
  - 16.045928239822388
  - 15.755031198263168
  - 15.874645084142685
  - 19.076746314764023
  - 18.38942977786064
  - 16.904640853405
  - 16.721689105033875
  - 17.435222804546356
  - 18.28059294819832
  - 16.698537409305573
  - 16.542401611804962
  - 17.71828755736351
  - 16.627195179462433
  - 16.596005514264107
  - 16.063003182411194
  - 17.591937392950058
  - 16.74231345951557
  - 17.412352472543716
  - 15.67763090133667
  - 17.89761719107628
  - 17.333687514066696
  - 20.317784398794174
  - 17.666904270648956
  - 15.84202665090561
  - 15.821911633014679
  - 16.77977165579796
  - 16.56931385397911
  - 15.644737184047699
  - 17.16232618689537
  - 16.307899594306946
  - 16.144552543759346
  - 16.69274792075157
  - 17.105607718229294
  - 19.608132928609848
  - 17.246789753437042
  - 16.86088103055954
  - 16.487374246120453
  - 16.518220216035843
  - 15.996057242155075
  - 17.26803556084633
  - 16.315888911485672
  - 16.148699015378952
  - 17.70425307750702
  - 16.378657907247543
  - 16.48536755144596
  - 17.18428461253643
  validation_losses:
  - 2.0269522666931152
  - 1.2516330480575562
  - 0.4383782148361206
  - 1.0223783254623413
  - 0.8071902990341187
  - 0.8258294463157654
  - 0.4046209454536438
  - 0.5393561124801636
  - 0.415021687746048
  - 0.5907894372940063
  - 0.47474968433380127
  - 1.0486794710159302
  - 0.674604058265686
  - 0.42162075638771057
  - 0.830371618270874
  - 0.3880262076854706
  - 0.4171810448169708
  - 0.4035528302192688
  - 0.40565213561058044
  - 0.44090723991394043
  - 0.3903881311416626
  - 0.3937263786792755
  - 0.7018687129020691
  - 0.3958873152732849
  - 0.38858017325401306
  - 0.4341988265514374
  - 0.483218789100647
  - 0.4229733943939209
  - 0.3932005763053894
  - 0.3904002904891968
  - 0.3888315260410309
  - 0.39900466799736023
  - 0.40304937958717346
  - 0.4924158453941345
  - 0.42900052666664124
  - 0.4020984172821045
  - 0.43529143929481506
  - 0.39266902208328247
  - 0.39675894379615784
  - 0.42325055599212646
  - 0.4278865158557892
  - 0.39279916882514954
  - 0.48100602626800537
  - 0.4056166708469391
  - 0.41754597425460815
  - 0.3928447365760803
  - 0.4092054069042206
  - 0.4358607232570648
  - 0.40601250529289246
  - 0.46081671118736267
  - 0.40812602639198303
  - 0.38961270451545715
  - 0.44723230600357056
  - 0.42770883440971375
  - 0.41332465410232544
  - 0.39741581678390503
  - 0.4074471890926361
  - 0.42560863494873047
  - 0.4261195957660675
  - 0.39240601658821106
  - 0.40008142590522766
  - 0.4144640564918518
  - 0.4278651177883148
  - 0.42458510398864746
  - 0.41106539964675903
  - 0.48665204644203186
  - 0.4115280210971832
  - 0.4284231960773468
  - 0.40671515464782715
  - 0.40864551067352295
  - 0.4136442542076111
  - 0.40174564719200134
  - 0.41627585887908936
  - 0.41874921321868896
  - 0.40364620089530945
  - 0.5007706880569458
  - 0.414971262216568
  - 0.408310204744339
  - 0.42781490087509155
  - 0.42574813961982727
  - 0.41555362939834595
  - 0.4015049338340759
  - 0.4061669707298279
  - 0.4178908169269562
  - 0.48003000020980835
  - 0.4393034279346466
  - 2.7741570472717285
  - 0.41584905982017517
  - 0.4120251536369324
  - 0.4091011583805084
  - 0.4109603464603424
  - 0.3994482457637787
  - 0.4104198217391968
  - 0.4112628996372223
  - 0.4188539385795593
  - 0.40649357438087463
  - 0.4166203737258911
  - 0.40786629915237427
  - 0.4471922516822815
  - 0.4065057337284088
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 93 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 43 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:06:36.055684'
