config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 08:37:19.034642'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/157/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/157/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/157/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/157/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/157/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/157/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/157/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/157/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/157/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/157/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 8.43123072385788
  - 16.89005097746849
  - 32.070095509290695
  - 23.042758464813232
  - 11.15816444158554
  - 10.367420256137848
  - 14.445697873830795
  - 62.63572955131531
  - 28.998977065086365
  - 28.777753472328186
  - 25.195411443710327
  - 14.114443182945251
  - 13.900414794683456
  - 53.124547213315964
  - 68.02438431978226
  - 15.051531255245209
  - 13.387541592121124
  - 14.162904858589172
  - 19.30058467388153
  - 109.08376523852348
  - 59.108994007110596
  - 39.73423919081688
  - 21.553495347499847
  - 14.404736191034317
  - 21.886380970478058
  - 18.500475108623505
  - 16.87468132376671
  - 10.285055816173553
  - 12.58641168475151
  - 110.22738438844681
  - 12.83435806632042
  - 15.825345933437347
  - 17.7158006131649
  - 19.4442600607872
  - 14.347656548023224
  - 10.807529300451279
  - 9.594999074935913
  - 11.321530520915985
  - 8.870125442743301
  - 8.448244035243988
  - 8.756931364536285
  - 10.484182715415955
  - 12.578737139701843
  - 9.458729922771454
  - 11.639503747224808
  - 20.269316464662552
  - 11.53396126627922
  - 8.510281682014465
  - 9.467416435480118
  - 11.70263198018074
  - 11.235143601894379
  - 8.871887803077698
  - 8.414256066083908
  - 8.783267885446548
  - 10.548088163137436
  - 11.93741551041603
  - 10.578555405139923
  - 9.931258767843246
  - 11.81568792462349
  - 12.944204062223434
  - 12.780736267566681
  - 12.382738560438156
  - 14.73586717247963
  - 74.32323297858238
  - 142.14027976989746
  - 73.92911195755005
  - 23.640784978866577
  - 67.07004028558731
  - 43.202324599027634
  - 83.66600307822227
  - 23.158486485481262
  - 51.612249583005905
  - 21.338099002838135
  - 15.75926485657692
  - 252.00274753570557
  - 398.200548350811
  - 81.54280784726143
  - 37.03300082683563
  - 62.75562286376953
  - 8.420365631580353
  - 11.484678596258163
  - 17.285899132490158
  - 11.409715965390205
  - 9.58980342745781
  - 10.130511909723282
  - 11.069816052913666
  - 8.460900366306305
  - 9.472972065210342
  - 13.685006856918335
  - 12.11490711569786
  - 19.35679316520691
  - 8.670588225126266
  - 8.944276243448257
  - 9.87845003604889
  - 9.956274688243866
  - 14.188562631607056
  - 11.023722767829895
  - 9.40271970629692
  - 8.962772727012634
  - 30.96088817715645
  validation_losses:
  - 0.978759765625
  - 0.6404401659965515
  - 0.5169329047203064
  - 0.44645237922668457
  - 0.6388565897941589
  - 0.4819510877132416
  - 0.6615124344825745
  - 2.3452043533325195
  - 0.6202759146690369
  - 0.49697747826576233
  - 0.8636111617088318
  - 30.730295181274414
  - 0.8054105043411255
  - 2.6209146976470947
  - 1.3065128326416016
  - 0.6802767515182495
  - 0.5335511565208435
  - 0.9934030175209045
  - 1.0092307329177856
  - 0.6201199293136597
  - 1.6181390285491943
  - 3.7675414085388184
  - 0.874161958694458
  - 0.7604197859764099
  - 0.5191265344619751
  - 0.5024105310440063
  - 0.4552973508834839
  - 0.5127639770507812
  - 0.7573574781417847
  - 0.835338830947876
  - 6.755395889282227
  - 0.9676586389541626
  - 0.6512898206710815
  - 0.4699442684650421
  - 0.5685348510742188
  - 0.5088062286376953
  - 0.6097589135169983
  - 0.5730435252189636
  - 0.4346032738685608
  - 0.4597908854484558
  - 0.6845899224281311
  - 0.9263993501663208
  - 0.49672871828079224
  - 1.0734076499938965
  - 1.099906086921692
  - 0.9644423723220825
  - 4.352734565734863
  - 8452.1015625
  - 124118.546875
  - 153.0320281982422
  - 112.6236343383789
  - 1143459968.0
  - 0.5075916051864624
  - 1.8435282707214355
  - 411202.46875
  - 360466720.0
  - 15028633600.0
  - 726563225600.0
  - 45155790848.0
  - 1306448887808.0
  - 19448249450496.0
  - 170.47689819335938
  - 56.7679443359375
  - 3809846.5
  - 7.448503494262695
  - 8.194616317749023
  - 0.9523448944091797
  - 12.909178733825684
  - 10.638673782348633
  - 2.2854580879211426
  - 1.4109888076782227
  - 0.6237289309501648
  - 237.618896484375
  - 0.43580999970436096
  - 0.8708942532539368
  - 0.46303945779800415
  - 1.139841914176941
  - 0.5426325798034668
  - 0.4100917875766754
  - 0.46576443314552307
  - 0.5322483777999878
  - 0.8399896621704102
  - 0.49530303478240967
  - 0.42255866527557373
  - 0.4317032992839813
  - 0.444487601518631
  - 0.4345741868019104
  - 0.8688603043556213
  - 0.6109362840652466
  - 0.8383591771125793
  - 0.5115638375282288
  - 0.424384206533432
  - 0.522075891494751
  - 0.8901034593582153
  - 0.5733413100242615
  - 0.9698553681373596
  - 0.6884213089942932
  - 0.4647064208984375
  - 0.5324536561965942
  - 2230066.0
loss_records_fold4:
  train_losses:
  - 9.138786315917969
  - 12.067657172679901
  - 9.586947530508041
  - 10.274766623973846
  - 9.489887341856956
  - 10.723360240459442
  - 15.509857773780823
  - 12.023126393556595
  - 14.913105994462967
  - 11.616368591785431
  - 11.709137856960297
  - 15.43759536743164
  - 17.223235934972763
  - 15.338493764400482
  - 10.470463901758194
  - 10.568435907363892
  - 10.198750913143158
  - 9.26842412352562
  - 8.512794733047485
  - 8.948786705732346
  - 13.504594534635544
  - 10.858122736215591
  - 12.114272505044937
  - 20.02807965874672
  - 96.75541198253632
  - 16.853848427534103
  - 9.461279392242432
  - 64.67348521947861
  - 154.292264431715
  - 11.03188356757164
  - 9.899183213710785
  - 13.184818744659424
  - 11.40120580792427
  - 11.934584379196167
  - 10.142082571983337
  - 19.771289706230164
  - 30.43187814950943
  - 10.503409594297409
  - 9.103473007678986
  - 20.185282856225967
  - 24.52936565876007
  - 9.69536629319191
  - 8.52471062541008
  - 43.19626623392105
  - 27.086238771677017
  - 35.011758267879486
  - 15.876119405031204
  - 10.474571198225021
  - 37.72864651679993
  - 8.719023019075394
  - 8.83544334769249
  - 8.253227770328522
  - 8.839781820774078
  - 8.852334707975388
  - 93.34697878360748
  - 8.411776214838028
  - 10.217258244752884
  - 8.490135252475739
  - 11.338786363601685
  - 9.965390712022781
  - 9.436875343322754
  - 9.003035485744476
  - 13.82404538989067
  - 10.261699050664902
  - 10.899961292743683
  - 10.249882102012634
  - 11.040584206581116
  - 9.874011725187302
  - 9.335546314716339
  - 10.796263664960861
  - 11.37469607591629
  - 9.273186206817627
  - 12.678598910570145
  - 9.874666035175323
  - 12.680576652288437
  - 9.411295980215073
  - 9.038824141025543
  - 8.433047831058502
  - 9.797119289636612
  - 10.516127496957779
  - 10.128956139087677
  - 8.95713260769844
  - 8.98757854104042
  - 17.10085466504097
  - 10.933964908123016
  - 9.833947867155075
  - 9.958179116249084
  - 13.610243111848831
  - 9.902130365371704
  - 9.483984589576721
  - 10.57696807384491
  - 17.136229395866394
  - 17.500406503677368
  - 19.318959057331085
  - 14.41450896859169
  - 13.890715181827545
  - 10.23025792837143
  - 11.62888589501381
  - 9.809345960617065
  - 12.586191773414612
  validation_losses:
  - 0.6989328861236572
  - 0.5677633881568909
  - 0.49706947803497314
  - 0.45878270268440247
  - 0.6877224445343018
  - 0.7876589894294739
  - 0.8096659779548645
  - 0.6314356327056885
  - 0.46230974793434143
  - 0.42907509207725525
  - 0.583323061466217
  - 1.3022480010986328
  - 0.4120364785194397
  - 0.6053422689437866
  - 0.47582095861434937
  - 0.652046263217926
  - 0.40630459785461426
  - 0.43682366609573364
  - 0.43066972494125366
  - 0.8489804863929749
  - 0.6002412438392639
  - 0.6188745498657227
  - 0.8211790323257446
  - 0.4846990704536438
  - 7.147383689880371
  - 0.6493878364562988
  - 1.0719292163848877
  - 0.7677524089813232
  - 0.5807988047599792
  - 0.5936375856399536
  - 0.5303555727005005
  - 0.6578141450881958
  - 0.5938695669174194
  - 0.47543755173683167
  - 0.6219905018806458
  - 0.817064642906189
  - 0.707523763179779
  - 0.4301515519618988
  - 0.7441374063491821
  - 0.8592897653579712
  - 0.5664693117141724
  - 0.4359204173088074
  - 0.5796676874160767
  - 33.273067474365234
  - 1.1160831451416016
  - 3.2617032527923584
  - 0.5042522549629211
  - 0.7788781523704529
  - 0.4004305601119995
  - 0.476723849773407
  - 0.4580785036087036
  - 0.45842698216438293
  - 0.3942877948284149
  - 0.5184499025344849
  - 0.40670204162597656
  - 0.4065605401992798
  - 0.43519100546836853
  - 0.6129663586616516
  - 0.5090121626853943
  - 0.426994264125824
  - 0.4759497344493866
  - 0.5751211643218994
  - 0.46951231360435486
  - 0.777751624584198
  - 1.491780161857605
  - 79.01732635498047
  - 26.419862747192383
  - 44.30214309692383
  - 31.051788330078125
  - 35.047019958496094
  - 31.376157760620117
  - 43.06751251220703
  - 62.012699127197266
  - 40.32142639160156
  - 18.4156494140625
  - 18.451040267944336
  - 32.71595764160156
  - 40.30904006958008
  - 45.52932357788086
  - 206.20127868652344
  - 215.40516662597656
  - 206.12620544433594
  - 226.13002014160156
  - 220.24893188476562
  - 191.41734313964844
  - 180.79904174804688
  - 302.20220947265625
  - 250.98892211914062
  - 230.69029235839844
  - 174.32363891601562
  - 231.31292724609375
  - 289.34112548828125
  - 205.9428253173828
  - 242.42359924316406
  - 218.72976684570312
  - 238.50074768066406
  - 265.53369140625
  - 203.6369171142578
  - 246.63743591308594
  - 253.307861328125
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 53 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 59 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:04:41.524268'
