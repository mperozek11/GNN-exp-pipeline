config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.216678'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_5fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_5fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_5fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_5fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_5fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_5fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_5fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_5fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_5fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_5fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.646523267030716
  - 3.215506500005722
  - 3.142662024497986
  - 3.0919868230819705
  - 3.218745708465576
  - 3.054288107156754
  - 3.081549417972565
  - 3.2505146831274034
  - 3.1192650675773623
  - 3.0881646215915683
  - 3.0629427969455723
  validation_losses:
  - 0.41560038924217224
  - 0.4023398756980896
  - 0.39722493290901184
  - 0.4030557870864868
  - 0.41531381011009216
  - 0.3911778926849365
  - 0.396342396736145
  - 0.3905635476112366
  - 0.3991360664367676
  - 0.4008980393409729
  - 0.38448551297187805
loss_records_fold1:
  train_losses:
  - 3.052936941385269
  - 3.091127097606659
  - 3.008440935611725
  - 3.019379544258118
  - 3.063812988996506
  - 3.0416992187500003
  - 3.0179091274738314
  - 2.9103933930397035
  - 2.959994447231293
  - 3.0955643206834793
  - 3.1239001572132112
  - 3.0870332419872284
  - 3.0271446943283085
  validation_losses:
  - 0.3920576870441437
  - 0.4204493463039398
  - 0.4012164771556854
  - 0.3948879539966583
  - 0.394377201795578
  - 0.392713725566864
  - 0.40636366605758667
  - 0.4038217067718506
  - 0.3985004723072052
  - 0.39195865392684937
  - 0.3932589888572693
  - 0.39194369316101074
  - 0.3984125554561615
loss_records_fold2:
  train_losses:
  - 3.1464844763278963
  - 3.011230945587158
  - 3.033350110054016
  - 3.000429490208626
  - 3.043673723936081
  - 3.140784701704979
  - 3.0543361723423006
  - 2.982617086172104
  - 3.0237685799598695
  - 3.0910639703273777
  - 3.0180729150772097
  - 2.99055452644825
  - 2.95045462846756
  - 3.028348219394684
  - 3.075627601146698
  - 2.952043497562409
  - 3.0592307180166247
  - 3.0072623372077945
  - 3.064875251054764
  - 3.0506068587303163
  - 2.9777157783508303
  - 2.988741344213486
  - 2.944403862953186
  - 3.046229469776154
  - 2.9679217606782915
  - 3.05558277964592
  - 3.031791323423386
  - 3.0071505427360536
  - 3.100187519192696
  - 2.9159491419792176
  - 2.9723143517971042
  - 3.0240540504455566
  - 2.9193766832351686
  - 3.084136611223221
  - 3.033478891849518
  - 3.0661633849143985
  - 3.0619706213474274
  - 3.0455177605152133
  - 2.985737493634224
  - 2.966261768341065
  - 2.9896481186151505
  - 2.9643767803907397
  - 2.9569813132286074
  - 2.984073066711426
  - 3.0531273990869523
  - 3.0243118047714237
  - 3.025897237658501
  - 2.9783083975315097
  - 2.981044292449951
  - 3.0217632889747623
  - 2.9173889398574833
  - 2.993388676643372
  - 2.934521323442459
  - 2.9988340586423874
  - 2.997274208068848
  - 2.9995099663734437
  - 2.982882958650589
  - 2.937572091817856
  - 2.9849792242050173
  - 2.98217990398407
  - 2.917487853765488
  - 2.972479456663132
  - 2.961912137269974
  - 3.021766698360443
  - 3.052135115861893
  - 2.9904714643955232
  - 3.0214057803153995
  - 2.8729826629161836
  - 3.0427835285663605
  - 2.973502510786057
  - 3.050612646341324
  - 2.9214082896709446
  - 2.9391466081142426
  - 2.93953395485878
  - 2.866926628351212
  - 2.936667481064797
  - 3.0029330193996433
  - 2.9952138245105746
  - 2.928496390581131
  - 2.931499081850052
  - 3.0231302440166474
  - 2.9898500561714174
  - 2.946841260790825
  - 2.9211494565010074
  - 2.9827361702919006
  - 2.8800493597984316
  - 2.943935763835907
  - 2.9778256952762607
  - 2.9447638511657717
  - 2.951907476782799
  - 2.9719528526067736
  - 2.984090855717659
  - 2.9682859241962434
  - 2.984323889017105
  - 2.9502660155296327
  - 2.8904543220996857
  - 2.900912725925446
  - 2.9845322698354724
  - 2.9437289088964462
  - 2.8915230989456178
  validation_losses:
  - 0.3885818123817444
  - 0.3968120217323303
  - 0.41446030139923096
  - 0.4137532711029053
  - 0.7478703856468201
  - 0.3897194564342499
  - 0.41648828983306885
  - 0.43041783571243286
  - 0.420102059841156
  - 0.7970091700553894
  - 0.40861767530441284
  - 0.416993647813797
  - 0.9360233545303345
  - 0.4024820327758789
  - 0.6010056734085083
  - 1.5650653839111328
  - 0.468699187040329
  - 0.3918745815753937
  - 0.38904207944869995
  - 0.39427706599235535
  - 0.4833330810070038
  - 0.3887254297733307
  - 0.45123013854026794
  - 0.5637599229812622
  - 0.5116989612579346
  - 0.39093446731567383
  - 0.5317762494087219
  - 0.38454899191856384
  - 0.4347972273826599
  - 0.4173049330711365
  - 0.47035178542137146
  - 0.4749836027622223
  - 0.9644903540611267
  - 0.8128670454025269
  - 1.191746711730957
  - 0.4577353596687317
  - 0.4173057973384857
  - 0.47322097420692444
  - 0.5154000520706177
  - 0.4234270453453064
  - 0.5211554169654846
  - 0.4565061330795288
  - 0.5201177597045898
  - 0.4067685902118683
  - 0.3911355435848236
  - 0.3878849148750305
  - 0.3891138434410095
  - 0.38236311078071594
  - 0.39411070942878723
  - 0.3856847584247589
  - 0.3904852867126465
  - 0.3994804918766022
  - 0.3818890154361725
  - 2.320815086364746
  - 0.3903217315673828
  - 0.4360014796257019
  - 0.5001372694969177
  - 0.5927245020866394
  - 0.4750392436981201
  - 0.4272131025791168
  - 0.585067868232727
  - 0.48241931200027466
  - 0.7087031602859497
  - 0.49192872643470764
  - 0.511806309223175
  - 0.5044068694114685
  - 0.8021810054779053
  - 0.4895593822002411
  - 0.5206891894340515
  - 0.4599590003490448
  - 0.390643835067749
  - 0.6194315552711487
  - 0.6401228308677673
  - 0.6750993132591248
  - 1.0269924402236938
  - 0.9642122983932495
  - 1.6646654605865479
  - 0.3978501856327057
  - 0.39054256677627563
  - 0.3800681531429291
  - 0.3911409378051758
  - 0.3780291974544525
  - 0.3834037482738495
  - 0.3768056631088257
  - 2.0204808712005615
  - 0.4901658594608307
  - 0.39785975217819214
  - 0.40252885222435
  - 0.9232054352760315
  - 0.7654942870140076
  - 0.6378877758979797
  - 0.38268065452575684
  - 0.5279138088226318
  - 0.6877186894416809
  - 0.835896372795105
  - 0.6856913566589355
  - 0.5863603353500366
  - 0.39463573694229126
  - 0.541102945804596
  - 1.001394271850586
loss_records_fold3:
  train_losses:
  - 2.9955447852611545
  - 2.9500961661338807
  - 3.012944993376732
  - 3.00459058880806
  - 2.9484862983226776
  - 2.899832355976105
  - 2.998984330892563
  - 3.0105740606784823
  - 2.953893464803696
  - 2.8675851166248325
  - 2.928533971309662
  - 2.9415273427963258
  - 2.9553709864616398
  - 2.9857120454311374
  - 2.9204558432102203
  - 2.951162087917328
  - 2.9939661741256716
  - 2.987968307733536
  - 2.9839958012104035
  - 2.953668022155762
  - 2.9421960175037385
  - 2.9331595599651337
  - 2.9693048715591432
  - 2.926907202601433
  - 2.9947981238365173
  - 2.9674552500247957
  - 2.9441062390804293
  - 3.0092863738536835
  - 2.9249902009963993
  - 2.9370053112506866
  - 2.9529065996408463
  - 2.976900553703308
  - 2.9306086957454682
  - 2.956037622690201
  - 3.0222849547863007
  - 3.0298632979393005
  - 3.0263700604438784
  - 2.924752795696259
  - 2.9745042771101
  - 3.0634668648242953
  - 3.02967666387558
  - 2.921243792772293
  - 2.9683021277189257
  - 2.882208698987961
  - 2.8923118263483047
  - 3.0343244940042498
  - 2.8996754586696625
  - 2.881471997499466
  - 2.9824751019477844
  - 3.057307827472687
  - 2.8909237027168277
  - 2.9220224380493165
  - 2.946923589706421
  - 2.9316902220249177
  - 2.985736346244812
  - 2.920259153842926
  - 2.920927253365517
  - 2.9911260277032854
  - 2.9850674599409106
  - 2.9998554438352585
  - 2.99905064702034
  - 2.8741909444332125
  - 2.998122972249985
  - 2.9320376694202426
  - 2.9285407781600954
  - 3.023219656944275
  - 2.946100848913193
  - 2.961155116558075
  - 2.8844829946756363
  - 2.962389659881592
  - 2.946973955631256
  - 2.9407878398895266
  - 2.9706824958324436
  - 2.819915020465851
  - 2.9285824537277225
  - 2.935833215713501
  - 3.0324730902910235
  - 2.969734972715378
  - 2.8978712022304536
  - 2.9468225985765457
  - 2.9335999816656115
  - 2.9607220828533176
  - 2.9088595092296603
  - 2.8972960114479065
  - 2.9133037626743317
  - 3.0707516044378282
  - 2.954115915298462
  - 2.859062820672989
  - 2.9261941581964495
  - 2.897591003775597
  - 2.9566378712654116
  - 2.931860619783402
  - 2.9259659707546235
  - 2.914437758922577
  - 2.950917196273804
  - 2.9226126700639727
  - 2.9144750833511353
  - 2.8551623970270157
  - 2.942355209589005
  - 2.8622726619243624
  validation_losses:
  - 6.676011562347412
  - 2.8557205200195312
  - 0.9347577095031738
  - 1.0095555782318115
  - 1.875405192375183
  - 2.931928873062134
  - 3.5640742778778076
  - 0.6866844892501831
  - 0.8645718097686768
  - 1.1706578731536865
  - 0.534635603427887
  - 0.6320222020149231
  - 0.8580303192138672
  - 1.7618502378463745
  - 0.6604153513908386
  - 0.5518432259559631
  - 0.5011267066001892
  - 0.7633016109466553
  - 0.6305009126663208
  - 0.7845075130462646
  - 0.5330434441566467
  - 0.5420157313346863
  - 0.6384201645851135
  - 0.5077110528945923
  - 0.4304015040397644
  - 0.3780001699924469
  - 0.4087497293949127
  - 0.41583192348480225
  - 0.5604113936424255
  - 0.7680747509002686
  - 0.7282611727714539
  - 1.0406638383865356
  - 0.5612691640853882
  - 0.4903286099433899
  - 0.9090157747268677
  - 0.5783170461654663
  - 1.0466758012771606
  - 1.7020478248596191
  - 1.8426541090011597
  - 0.9485840201377869
  - 4.442950248718262
  - 0.662935733795166
  - 0.6237130165100098
  - 0.6028132438659668
  - 0.4393233060836792
  - 0.5608887672424316
  - 0.769589900970459
  - 0.6491711735725403
  - 0.5624895691871643
  - 0.6147513389587402
  - 2.228982448577881
  - 1.815657138824463
  - 0.560302197933197
  - 1.1104265451431274
  - 0.6220799684524536
  - 0.727547287940979
  - 0.6018392443656921
  - 0.5447096824645996
  - 0.6942992210388184
  - 0.5910252332687378
  - 0.5296846628189087
  - 0.5880261659622192
  - 1.0596294403076172
  - 0.5983903408050537
  - 0.5740309357643127
  - 0.46641474962234497
  - 0.5191404223442078
  - 1.1006556749343872
  - 0.9990354180335999
  - 1.098905086517334
  - 0.4564428925514221
  - 0.42640218138694763
  - 0.4746554493904114
  - 0.3965567946434021
  - 0.6068006753921509
  - 0.6770851016044617
  - 0.4045303463935852
  - 0.6655840277671814
  - 0.37340062856674194
  - 0.6855781674385071
  - 1.0323989391326904
  - 2.775217056274414
  - 0.6862241625785828
  - 1.327677845954895
  - 3.247621536254883
  - 1.1979697942733765
  - 1.2354555130004883
  - 1.8355103731155396
  - 1.984550952911377
  - 1.577655553817749
  - 5.085424900054932
  - 0.9987371563911438
  - 0.7901738882064819
  - 0.9565163850784302
  - 1.0691883563995361
  - 1.1875379085540771
  - 0.45696404576301575
  - 1.109221339225769
  - 0.643879234790802
  - 0.7541939616203308
loss_records_fold4:
  train_losses:
  - 2.962170332670212
  - 2.9435518711805346
  - 2.9505894392728806
  - 2.969047075510025
  - 2.9421934425830845
  - 2.938838648796082
  - 2.9660154312849047
  - 2.9537216424942017
  - 2.929887515306473
  - 2.896881502866745
  - 2.888421112298966
  - 2.979180335998535
  - 2.9029387652873995
  - 2.9421189188957215
  - 3.011148935556412
  - 2.9292241871356968
  - 2.946139889955521
  - 2.924780195951462
  - 2.9462929606437687
  - 2.86621208190918
  - 2.9405619323253633
  - 2.9715275645256045
  - 2.9853104531764987
  - 2.9367804348468782
  - 2.9399588465690614
  - 2.9691402435302736
  - 2.966924893856049
  - 2.885673749446869
  - 2.929398822784424
  - 2.8773013532161715
  - 2.9613397240638735
  - 2.922410333156586
  - 2.9381221383810043
  - 2.8978004813194276
  - 2.8597642809152606
  - 2.963911861181259
  - 2.9645296871662143
  - 2.9167800843715668
  - 2.888080820441246
  - 2.951854294538498
  - 2.910668504238129
  - 2.890847197175026
  - 2.9075457692146305
  - 2.8870755910873416
  - 2.825101262331009
  - 2.8853220641613007
  - 2.9562848567962647
  - 2.8514430820941925
  - 3.016334939002991
  - 2.936438447237015
  - 2.9118429124355316
  - 2.884519273042679
  - 2.9286370873451233
  - 2.9175700068473818
  - 3.110537534952164
  - 3.0871927678585056
  - 2.8976571917533875
  - 2.998772919178009
  - 2.89252864420414
  - 2.9378791689872745
  - 2.8696934759616854
  - 2.973425582051277
  - 2.9420659095048904
  - 2.9301821768283847
  - 2.9018493950366975
  - 2.8776014089584354
  - 2.888030856847763
  - 2.9575061708688737
  - 2.8460581779479983
  - 2.8125600427389146
  - 2.86919596195221
  - 2.94927174448967
  - 2.975936281681061
  - 2.95817024409771
  - 2.871175980567932
  - 2.84976903796196
  - 2.9565353244543076
  - 2.915170589089394
  - 3.0240477561950687
  - 2.917302519083023
  - 2.9314690232276917
  - 2.8459671795368195
  - 2.9016816794872287
  - 2.8441691577434542
  - 2.8488247811794283
  - 2.9784007102251056
  - 2.9539622843265536
  - 2.8102726459503176
  - 2.878412088751793
  - 2.8385313987731937
  - 2.970720419287682
  - 3.0393234074115756
  - 2.943268406391144
  - 2.9989767998456958
  - 3.0316942512989047
  - 2.8748841643333436
  - 2.940330529212952
  - 2.996605318784714
  - 2.969001865386963
  - 3.077480709552765
  validation_losses:
  - 0.48926451802253723
  - 0.615896463394165
  - 0.4582706391811371
  - 0.43654757738113403
  - 0.5394062995910645
  - 0.4461769759654999
  - 0.5907638072967529
  - 0.5522125363349915
  - 0.4018731117248535
  - 0.41838982701301575
  - 0.396537721157074
  - 0.37187516689300537
  - 0.6044519543647766
  - 0.46444764733314514
  - 0.6328115463256836
  - 0.4912656843662262
  - 0.44605553150177
  - 0.3757947087287903
  - 0.4159330725669861
  - 0.46819260716438293
  - 0.4621492028236389
  - 0.5042570233345032
  - 0.478441059589386
  - 0.5355196595191956
  - 0.41763103008270264
  - 0.47776558995246887
  - 0.39827999472618103
  - 0.4821235239505768
  - 0.41877827048301697
  - 0.4472377300262451
  - 0.4697285592556
  - 0.5177916288375854
  - 0.4418221712112427
  - 0.5937559008598328
  - 0.6811812520027161
  - 0.4517098665237427
  - 0.388446569442749
  - 0.49035823345184326
  - 0.5676181316375732
  - 0.5205630660057068
  - 0.5003270506858826
  - 0.46134182810783386
  - 0.4663187265396118
  - 0.5112001299858093
  - 0.5853742361068726
  - 0.48896145820617676
  - 0.5600355863571167
  - 0.3839031159877777
  - 0.6540582776069641
  - 0.6144747138023376
  - 0.6525307893753052
  - 0.4965050518512726
  - 0.39355045557022095
  - 0.5192170739173889
  - 0.3817850351333618
  - 0.381706178188324
  - 0.39782729744911194
  - 0.37457871437072754
  - 0.3831460177898407
  - 0.5261717438697815
  - 0.5189341306686401
  - 0.46902719140052795
  - 0.5320112705230713
  - 0.5734682083129883
  - 0.4598034918308258
  - 0.47195467352867126
  - 0.5059967637062073
  - 0.5958306789398193
  - 0.5611119270324707
  - 0.6357398629188538
  - 0.5119420886039734
  - 0.4466155469417572
  - 0.6164314150810242
  - 0.42442238330841064
  - 0.46026548743247986
  - 0.573628842830658
  - 0.6124377846717834
  - 0.5648435354232788
  - 0.532929003238678
  - 0.6081644892692566
  - 0.6287592053413391
  - 0.5032002925872803
  - 0.438810795545578
  - 0.5433424711227417
  - 0.5668904781341553
  - 0.4722840189933777
  - 0.5052267909049988
  - 0.5111889243125916
  - 0.4911300837993622
  - 0.4500753879547119
  - 0.42695388197898865
  - 0.38350263237953186
  - 0.4226481318473816
  - 0.5401010513305664
  - 0.41487181186676025
  - 0.42048436403274536
  - 0.48564091324806213
  - 0.41122621297836304
  - 0.43967410922050476
  - 0.41786831617355347
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8387650085763293, 0.8421955403087479,
    0.8436426116838488]'
  fold_eval_f1: '[0.0, 0.0, 0.12962962962962962, 0.22033898305084748, 0.09900990099009901]'
  mean_eval_accuracy: 0.8479738053556376
  mean_f1_accuracy: 0.08979570273411522
  total_train_time: '0:28:44.281610'
