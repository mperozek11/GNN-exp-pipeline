config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:51:36.356242'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_33fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_33fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_33fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_33fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_33fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_33fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_33fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_33fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_33fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_33fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.2609997153282166
  - 2.92542799115181
  - 2.996559363603592
  - 2.933640030026436
  - 2.888034015893936
  - 2.9700659096241
  - 3.1167061746120455
  - 2.8617690771818163
  - 2.86003033220768
  - 2.8377232015132905
  - 2.8218706369400026
  - 2.861353886127472
  - 2.87459996342659
  - 2.897768920660019
  - 2.8171027660369874
  - 2.891408181190491
  - 2.961022299528122
  - 2.882809090614319
  - 2.8396948158741
  - 2.8443186163902285
  - 2.8906801760196688
  - 2.8587261140346527
  - 2.8199464082717896
  - 2.8421833634376528
  - 2.8274612694978716
  - 2.810240599513054
  validation_losses:
  - 0.40090104937553406
  - 0.4087180495262146
  - 0.39650458097457886
  - 0.3886796832084656
  - 0.3871505558490753
  - 0.40263864398002625
  - 0.3853442370891571
  - 0.3850759267807007
  - 0.39630159735679626
  - 0.3848952353000641
  - 0.3987196087837219
  - 0.38688915967941284
  - 0.3901679813861847
  - 0.40424415469169617
  - 0.38774484395980835
  - 0.42119771242141724
  - 0.38514965772628784
  - 0.38603460788726807
  - 0.3880898654460907
  - 0.40043941140174866
  - 0.3889927566051483
  - 0.38540202379226685
  - 0.3881051242351532
  - 0.39254242181777954
  - 0.3915162682533264
  - 0.3915226459503174
loss_records_fold1:
  train_losses:
  - 2.8466986924409867
  - 2.8802400618791584
  - 2.8035469949245453
  - 2.8064848303794863
  - 2.839740085601807
  - 2.8533041119575504
  - 2.9091498970985414
  - 2.8847040593624116
  - 2.8161430060863495
  - 2.812063437700272
  - 2.801054233312607
  validation_losses:
  - 0.39332395792007446
  - 0.38420113921165466
  - 0.38464027643203735
  - 0.38782310485839844
  - 0.3870874345302582
  - 0.3848364055156708
  - 0.3872759938240051
  - 0.3892190158367157
  - 0.3930049538612366
  - 0.39608943462371826
  - 0.39450132846832275
loss_records_fold2:
  train_losses:
  - 2.7995566576719284
  - 2.8055054247379303
  - 2.797826534509659
  - 2.8471238374710084
  - 2.7962649375200272
  - 2.843717283010483
  - 2.818840479850769
  - 2.820746099948883
  - 2.7754941791296006
  - 2.840252071619034
  - 2.8439204484224323
  - 2.820167189836502
  - 2.8372477263212206
  - 2.8392915189266206
  - 2.7866581171751026
  - 2.8314454644918445
  - 2.771057987213135
  - 2.7704238533973697
  - 2.8198534786701206
  - 2.850817769765854
  - 2.79792976975441
  - 2.8113577634096147
  - 2.8193660378456116
  validation_losses:
  - 0.3862968683242798
  - 0.3859706223011017
  - 0.390532523393631
  - 0.42045319080352783
  - 0.3830054700374603
  - 0.3872324228286743
  - 0.39061814546585083
  - 0.4045008420944214
  - 0.40810489654541016
  - 0.42113232612609863
  - 0.4728016257286072
  - 0.40113067626953125
  - 0.38008755445480347
  - 0.39888742566108704
  - 0.5470216870307922
  - 0.3965742588043213
  - 4.09976053237915
  - 2.2792675495147705
  - 1.6800119876861572
  - 0.4738491177558899
  - 0.44378024339675903
  - 0.3991977274417877
  - 0.3828430771827698
loss_records_fold3:
  train_losses:
  - 2.794662290811539
  - 2.811337924003601
  - 2.8348742067813877
  - 2.8816991299390793
  - 2.8625640690326692
  - 2.8335156083107
  - 2.842173913121224
  - 2.8729448646306994
  - 2.802496013045311
  - 2.8565265417099
  - 2.8186280310153964
  - 2.82517366707325
  - 2.8182196736335756
  - 2.8373971164226535
  - 2.8460468977689746
  - 2.8933121085166933
  - 2.8162481486797333
  - 2.861797380447388
  - 2.8169101357460025
  - 2.8172763705253603
  - 2.8250303804874424
  - 2.796505251526833
  - 2.821487247943878
  - 2.7989218086004257
  - 2.760152652859688
  - 2.8097668915987017
  - 2.7638128250837326
  - 2.810988807678223
  - 2.7744743108749392
  - 2.7692382782697678
  - 2.789474231004715
  - 2.78714359998703
  - 2.810507678985596
  - 2.770701038837433
  - 2.767953717708588
  - 2.7677445203065876
  - 2.7956084281206133
  - 2.804182264208794
  - 2.7925604552030565
  - 2.7827990442514423
  - 2.7882235229015353
  - 2.74497472345829
  - 2.800820338726044
  - 2.7786671757698063
  - 2.719698888063431
  - 2.7723199754953387
  - 2.8341022580862045
  - 2.786713230609894
  - 2.7402901589870456
  - 2.7859623730182648
  - 2.749250701069832
  - 2.7500435918569566
  - 2.738788291811943
  - 2.727563425898552
  - 2.7173058807849886
  - 2.798214092850685
  - 2.766315641999245
  - 2.7629913091659546
  - 2.7710696399211887
  - 2.7792182207107547
  - 2.9219094634056093
  - 2.8159888565540316
  - 2.7777866214513782
  - 2.79267800450325
  - 2.756209683418274
  - 2.7877341449260715
  - 2.771470293402672
  - 2.7377923846244814
  - 2.733419191837311
  - 2.7172843098640445
  - 2.7650000005960464
  - 2.7278186559677127
  - 2.721342706680298
  - 2.7257497251033787
  - 2.7307985305786135
  - 2.763428118824959
  - 2.732985255122185
  - 2.745150086283684
  - 2.6852958261966706
  - 2.722301381826401
  - 2.7255464255809785
  - 2.809490603208542
  - 2.9956436395645145
  - 2.7901302337646485
  - 2.9346352577209474
  - 2.7815876424312593
  - 2.7569392323493958
  - 2.8144018024206163
  - 2.738436281681061
  - 2.7722581893205644
  - 2.745831671357155
  - 2.720618724822998
  - 2.749361327290535
  - 2.6819235712289813
  - 2.7350507318973545
  - 2.7535748124122623
  - 2.7202565103769305
  - 2.7633914530277255
  - 2.7376282632350923
  - 2.7731991648674015
  validation_losses:
  - 0.6165787577629089
  - 0.5154125094413757
  - 0.6174810528755188
  - 0.3895858824253082
  - 0.4333881437778473
  - 0.5349628329277039
  - 0.9508218169212341
  - 0.4982605576515198
  - 0.8739079833030701
  - 0.488331139087677
  - 0.7837028503417969
  - 0.4444575011730194
  - 0.44694575667381287
  - 0.5516864061355591
  - 2.5133090019226074
  - 0.6481032371520996
  - 6.107987403869629
  - 0.46349334716796875
  - 0.5270423293113708
  - 0.853400707244873
  - 0.612022876739502
  - 0.6252334713935852
  - 1.8622008562088013
  - 1.3343229293823242
  - 0.5475245118141174
  - 0.3937646448612213
  - 0.5867588520050049
  - 0.5964179635047913
  - 0.48420917987823486
  - 0.46589595079421997
  - 0.523862898349762
  - 0.5157052874565125
  - 0.4196329414844513
  - 0.5137858390808105
  - 0.5529646873474121
  - 0.857142984867096
  - 0.4855291247367859
  - 0.49596238136291504
  - 0.5360262989997864
  - 0.5115218162536621
  - 0.5282719135284424
  - 0.7470773458480835
  - 0.5743281841278076
  - 0.46230730414390564
  - 0.6067901849746704
  - 0.6072636246681213
  - 0.6488556265830994
  - 0.6869027614593506
  - 0.6397358775138855
  - 4.727607250213623
  - 1.9013917446136475
  - 0.5878807306289673
  - 0.5165516138076782
  - 0.6328259110450745
  - 0.6441779136657715
  - 0.4640751779079437
  - 0.49082526564598083
  - 0.45920848846435547
  - 0.4295580983161926
  - 0.4823942482471466
  - 0.4214172065258026
  - 0.43432214856147766
  - 0.5239441394805908
  - 0.4934062361717224
  - 0.4814738631248474
  - 0.4280793368816376
  - 0.5102161169052124
  - 0.5440624952316284
  - 0.4661760628223419
  - 0.6230098605155945
  - 0.7912791967391968
  - 0.4590367376804352
  - 0.48847123980522156
  - 0.535719633102417
  - 0.47327253222465515
  - 0.40345901250839233
  - 0.5363441705703735
  - 0.3796869218349457
  - 0.412975937128067
  - 0.5671008229255676
  - 0.5185422897338867
  - 0.47342416644096375
  - 0.4967558979988098
  - 0.4033617079257965
  - 0.3900861144065857
  - 0.44101718068122864
  - 0.4015101194381714
  - 0.48382848501205444
  - 0.40532252192497253
  - 0.4039211869239807
  - 0.41460052132606506
  - 0.4886331856250763
  - 0.3881774842739105
  - 0.40517300367355347
  - 0.44982287287712097
  - 0.4577365815639496
  - 0.5940980911254883
  - 0.42157235741615295
  - 0.5105602145195007
  - 0.5645664930343628
loss_records_fold4:
  train_losses:
  - 2.7375699281692505
  - 2.718748813867569
  - 2.753432729840279
  - 2.6904535412788393
  - 2.701817232370377
  - 2.6929008007049564
  - 2.7317827701568604
  - 2.710676148533821
  - 2.7307495862245563
  - 2.7008594542741777
  - 2.706630861759186
  - 2.793970248103142
  - 2.7478208333253864
  - 2.757209539413452
  - 2.742140752077103
  - 2.705593577027321
  validation_losses:
  - 0.398599237203598
  - 0.409037321805954
  - 0.42097795009613037
  - 0.38786330819129944
  - 0.4537041187286377
  - 0.3888968527317047
  - 0.4731447100639343
  - 0.3624568581581116
  - 0.371367871761322
  - 0.4074065685272217
  - 0.4033640921115875
  - 0.40261781215667725
  - 0.40095818042755127
  - 0.40027424693107605
  - 0.38401275873184204
  - 0.3634742498397827
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8524871355060034,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.10416666666666667, 0.0]'
  mean_eval_accuracy: 0.8572415459791456
  mean_f1_accuracy: 0.020833333333333336
  total_train_time: '0:16:28.713112'
