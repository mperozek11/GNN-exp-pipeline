config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.838033'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_9fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_9fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_9fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_9fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_9fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_9fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_9fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_9fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_9fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_9fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 36.58917044401169
  - 12.016488927602769
  - 7.239407780766488
  - 6.202888369560242
  - 4.733765375614166
  - 3.640691962838173
  - 7.163619446754456
  - 10.0411099255085
  - 4.506795936822892
  - 4.392432475090027
  - 6.5063703000545505
  - 4.511603969335556
  - 3.6807698190212252
  - 6.7847993791103365
  - 6.25932766199112
  - 6.059369134902955
  - 4.246506902575493
  - 5.424127048254014
  - 6.435226926207543
  - 8.925041818618775
  - 3.5094610393047336
  - 3.787308394908905
  - 3.373425489664078
  - 3.889623922109604
  - 3.336658251285553
  - 3.3261746168136597
  - 4.281869500875473
  - 4.913827994465828
  - 3.425569608807564
  - 3.023786109685898
  - 2.911040276288986
  - 4.519718033075333
  - 3.337439024448395
  - 3.6749689102172853
  - 3.2092542350292206
  - 2.8766593486070633
  - 5.549515318870545
  - 4.123881101608276
  - 4.574893167614937
  - 3.30791651904583
  - 3.1999276101589205
  - 3.43327753841877
  - 5.379143232107163
  - 5.866413319110871
  - 3.0616049945354464
  - 3.096862915158272
  - 3.441862851381302
  - 2.8919104099273683
  - 2.868552267551422
  - 3.1740584254264834
  - 3.4240441858768467
  - 2.984949117898941
  - 2.93992994427681
  - 3.3451600193977358
  - 3.3965177834033966
  - 3.22034550011158
  - 2.986231702566147
  - 3.129687687754631
  - 3.064285871386528
  - 3.405813080072403
  - 3.6415526419878006
  - 3.0501634359359744
  - 2.8049996107816697
  - 2.9025614678859712
  - 2.891862684488297
  - 3.0071136116981507
  - 3.009034752845764
  - 3.184656110405922
  - 3.0315029561519626
  - 3.184103685617447
  - 3.013741245865822
  - 3.1250172913074494
  - 2.9999976694583896
  - 3.1361351907253265
  - 2.8922077089548113
  - 3.0767627120018006
  - 3.042512753605843
  - 3.0940904170274734
  - 2.9331141471862794
  - 2.918616303801537
  - 2.8005596816539766
  - 2.870575261116028
  - 2.948048174381256
  - 2.958502465486527
  - 2.817916598916054
  - 2.909585011005402
  - 2.8625804096460343
  - 2.8660465598106386
  - 2.9018552362918855
  - 2.8726413875818255
  - 2.9692366391420366
  - 2.9793092727661135
  - 2.825437116622925
  - 2.9202756822109226
  - 2.9117855668067936
  - 2.8182815432548525
  - 2.8022519409656526
  - 2.8968036711215976
  - 2.8972714960575106
  - 3.0016159206628803
  validation_losses:
  - 6.0566630363464355
  - 0.4813902676105499
  - 0.4570854902267456
  - 0.44670140743255615
  - 0.5101490020751953
  - 0.38257721066474915
  - 0.5652285814285278
  - 0.41708046197891235
  - 0.45568692684173584
  - 0.568825900554657
  - 0.7900255918502808
  - 0.3828205168247223
  - 0.506170392036438
  - 0.48008930683135986
  - 0.4125889539718628
  - 0.42927953600883484
  - 0.4247753322124481
  - 0.5862067341804504
  - 0.3998302221298218
  - 0.4037962555885315
  - 0.384132981300354
  - 0.4103534519672394
  - 0.4371901750564575
  - 0.3793378472328186
  - 0.4068451225757599
  - 0.4023056626319885
  - 0.4272058606147766
  - 0.3786217272281647
  - 0.37891122698783875
  - 0.39484402537345886
  - 0.38611525297164917
  - 0.37842223048210144
  - 0.4693130552768707
  - 0.38256293535232544
  - 0.3981069326400757
  - 0.42217761278152466
  - 0.4816643297672272
  - 0.5654973983764648
  - 0.3856300413608551
  - 0.4049443304538727
  - 0.3768939971923828
  - 1.0050042867660522
  - 1.2977955341339111
  - 0.4329129755496979
  - 0.38931477069854736
  - 0.37910595536231995
  - 0.4081694185733795
  - 0.37720397114753723
  - 0.4486290216445923
  - 0.467369019985199
  - 0.3832429349422455
  - 0.38813847303390503
  - 0.3786889612674713
  - 0.3795974850654602
  - 0.5214184522628784
  - 0.378754198551178
  - 0.39628762006759644
  - 0.3851792514324188
  - 0.469985693693161
  - 0.4878197908401489
  - 0.37660089135169983
  - 0.38945165276527405
  - 0.38483405113220215
  - 0.48876461386680603
  - 0.3889963626861572
  - 0.7767820954322815
  - 0.4172908365726471
  - 0.3891693651676178
  - 0.4180719256401062
  - 0.38692474365234375
  - 0.42425596714019775
  - 0.37963154911994934
  - 0.4031795263290405
  - 0.3687053620815277
  - 0.4090505540370941
  - 0.39586880803108215
  - 0.4018611013889313
  - 0.4461650252342224
  - 0.41525912284851074
  - 0.380085289478302
  - 0.45685485005378723
  - 0.8038233518600464
  - 0.4475747346878052
  - 0.4118942320346832
  - 0.42137616872787476
  - 0.39453208446502686
  - 0.3679061532020569
  - 0.42351585626602173
  - 0.49847978353500366
  - 0.8114017844200134
  - 0.4751409590244293
  - 0.38260501623153687
  - 0.47141456604003906
  - 0.37408438324928284
  - 0.41235870122909546
  - 0.44446685910224915
  - 0.4477986693382263
  - 0.4231952130794525
  - 0.41110339760780334
  - 0.5546963810920715
loss_records_fold1:
  train_losses:
  - 2.762603771686554
  - 2.7575826883316044
  - 2.855496656894684
  - 2.754217737913132
  - 2.8406139045953753
  - 2.873549979925156
  - 2.7939624547958375
  - 2.825767195224762
  - 2.8445082634687426
  - 2.970633029937744
  - 2.9252343624830246
  - 2.857389980554581
  - 2.920550537109375
  - 2.917101722955704
  - 2.8309046030044556
  - 2.7910813182592396
  - 2.8346845984458926
  - 2.816641718149185
  - 3.012604743242264
  - 2.8636891305446626
  - 2.80667274594307
  - 2.8357778549194337
  - 2.8315278828144077
  - 2.8078264355659486
  - 2.8276573061943058
  - 2.7885822117328645
  - 2.859820041060448
  - 2.9438777685165407
  - 2.9592031031847004
  - 2.9159062892198566
  - 2.800295960903168
  - 2.8119308799505234
  - 2.909003531932831
  - 2.7638126105070118
  - 2.913606840372086
  - 2.815900129079819
  - 3.13730965256691
  - 2.8527047246694566
  - 2.817578262090683
  - 2.8713200449943543
  - 2.7580194175243378
  - 2.7689154475927356
  - 2.7771297693252563
  - 2.7870533019304276
  - 2.783122256398201
  - 2.7844913035631182
  - 2.7567787170410156
  - 2.844938072562218
  - 2.8849231690168384
  - 2.8458920359611515
  - 3.3456349432468415
  - 3.0333878576755526
  - 3.6471354544162753
  - 3.6036880671977998
  - 2.917588821053505
  - 2.999622642993927
  - 2.962998050451279
  - 2.945706430077553
  validation_losses:
  - 0.4422571361064911
  - 0.38927140831947327
  - 0.43185970187187195
  - 0.4139940142631531
  - 0.5185680389404297
  - 0.5061435103416443
  - 0.40469422936439514
  - 0.4101622700691223
  - 0.4378955662250519
  - 0.5147640109062195
  - 0.40597227215766907
  - 0.565505862236023
  - 0.3990327715873718
  - 0.39830711483955383
  - 0.478717178106308
  - 0.413662314414978
  - 0.4494408071041107
  - 0.6259554028511047
  - 0.6498683094978333
  - 0.46703290939331055
  - 0.49669376015663147
  - 0.5267346501350403
  - 0.5611068606376648
  - 0.46421927213668823
  - 0.5663124322891235
  - 0.4516296982765198
  - 0.45306330919265747
  - 0.7270963788032532
  - 0.3987778127193451
  - 0.404343843460083
  - 0.42894497513771057
  - 0.39730897545814514
  - 0.6636974215507507
  - 0.4031713902950287
  - 0.4214896559715271
  - 0.3956134617328644
  - 0.42902377247810364
  - 0.423478364944458
  - 0.42695489525794983
  - 0.43566006422042847
  - 0.4217384159564972
  - 0.4115595519542694
  - 0.5061289072036743
  - 0.43316519260406494
  - 0.501545250415802
  - 1.5901131629943848
  - 0.5443092584609985
  - 0.7249649167060852
  - 2.4275693893432617
  - 0.5034904479980469
  - 0.474364310503006
  - 0.7871177792549133
  - 0.42205810546875
  - 0.4073689877986908
  - 0.40461158752441406
  - 0.40261465311050415
  - 0.40294012427330017
  - 0.40411460399627686
loss_records_fold2:
  train_losses:
  - 2.984119236469269
  - 2.917893880605698
  - 3.166697743535042
  - 3.7975505232810978
  - 3.134216630458832
  - 2.933333003520966
  - 3.3598153531551365
  - 3.0313752561807634
  - 2.9084961235523226
  - 2.858301728963852
  - 2.9018520176410676
  - 2.8321340531110764
  - 2.8702969163656236
  validation_losses:
  - 0.38074377179145813
  - 0.39793407917022705
  - 0.3847642242908478
  - 30.215320587158203
  - 0.3776167929172516
  - 0.39527297019958496
  - 0.42994698882102966
  - 0.379980206489563
  - 0.37972548604011536
  - 0.3853195011615753
  - 0.3880959749221802
  - 0.39026471972465515
  - 0.388191819190979
loss_records_fold3:
  train_losses:
  - 2.8713150531053544
  - 2.895558008551598
  - 3.042705348134041
  - 2.928198766708374
  - 2.905902397632599
  - 3.2196746677160264
  - 5.092283940315247
  - 4.943320047855377
  - 3.743581393361092
  - 2.9867927461862567
  - 3.106799298524857
  - 3.3560537040233616
  - 3.151263448596001
  - 3.459241870045662
  - 3.050768178701401
  - 3.185165184736252
  - 2.9346247136592867
  - 3.038475471735001
  - 3.096075251698494
  - 2.972620552778244
  - 3.2462525308132175
  - 3.042762511968613
  - 3.0929968476295473
  - 3.0317251205444338
  - 3.1344517648220065
  - 2.910243585705757
  - 3.1339152395725254
  - 2.9820260584354403
  - 3.23564470410347
  - 3.094622564315796
  - 3.022127085924149
  - 3.047940376400948
  - 2.990889245271683
  - 3.2166840106248857
  - 3.0904660463333133
  - 2.9344145298004154
  - 2.9395999372005464
  validation_losses:
  - 0.38660210371017456
  - 0.4420222043991089
  - 302.44183349609375
  - 0.40094640851020813
  - 0.4076150357723236
  - 1213.22900390625
  - 3.7233614921569824
  - 0.4086207449436188
  - 0.3966929614543915
  - 0.40612322092056274
  - 0.3962349593639374
  - 0.41888484358787537
  - 0.39290398359298706
  - 0.422107070684433
  - 0.3941383361816406
  - 0.4061165153980255
  - 0.39716655015945435
  - 0.40230995416641235
  - 0.39721444249153137
  - 0.40177270770072937
  - 0.42777442932128906
  - 0.39645475149154663
  - 0.3966842591762543
  - 0.41077229380607605
  - 0.40563878417015076
  - 0.3979175090789795
  - 0.3987790048122406
  - 0.39436832070350647
  - 0.44867393374443054
  - 0.3972886800765991
  - 0.4158041477203369
  - 0.39500316977500916
  - 0.3962025046348572
  - 0.3977677524089813
  - 0.398464560508728
  - 0.39814040064811707
  - 0.39770469069480896
loss_records_fold4:
  train_losses:
  - 2.9959330737590792
  - 2.987235087156296
  - 3.0498762845993044
  - 2.946113654971123
  - 2.988037878274918
  - 2.9804797112941745
  - 3.020362371206284
  - 3.0504266291856768
  - 3.0310291707515717
  - 2.9593803584575653
  - 3.0182888478040697
  - 3.001512533426285
  - 2.996089655160904
  - 3.0027494311332705
  - 2.971536034345627
  - 2.9516584038734437
  - 2.9487950921058657
  validation_losses:
  - 1278004.625
  - 235633936.0
  - 0.41159212589263916
  - 0.39394232630729675
  - 0.410845011472702
  - 0.4005921483039856
  - 0.40835222601890564
  - 0.40993326902389526
  - 0.4319116473197937
  - 0.3943740129470825
  - 0.40967461466789246
  - 0.391384094953537
  - 0.3979896008968353
  - 0.3942863643169403
  - 0.391154408454895
  - 0.40024763345718384
  - 0.39210179448127747
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 58 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 37 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:19:59.655467'
