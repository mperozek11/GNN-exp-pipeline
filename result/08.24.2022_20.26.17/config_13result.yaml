config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.858471'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_13fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_13fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_13fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_13fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_13fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_13fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_13fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_13fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_13fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_13fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 51.88012483119965
  - 9.48037206530571
  - 8.527342540025712
  - 6.3021264553070075
  - 8.79025949239731
  - 5.807202470302582
  - 9.858327889442444
  - 10.304034897685051
  - 7.312046676874161
  - 5.014647728204728
  - 6.135496419668198
  - 4.350457864999771
  - 6.550934433937073
  - 6.137775355577469
  - 5.87764436006546
  - 5.056563395261765
  - 5.184842443466187
  - 10.165465593338013
  - 6.655587333440781
  - 8.019613999128342
  - 5.632860615849495
  - 4.362363553047181
  - 4.528650033473968
  - 6.6350525528192525
  - 5.6107126653194435
  - 6.955148524045945
  - 7.820709925889969
  - 5.2767819464206696
  - 3.932079291343689
  - 3.313417077064514
  - 3.398918652534485
  - 6.759382146596909
  - 4.014615869522095
  - 4.407825982570649
  - 4.02042899131775
  - 3.4926831126213074
  - 5.5015769600868225
  - 5.589236688613892
  - 4.241239738464356
  - 3.5313344776630404
  - 3.5943317472934724
  - 3.4843923926353457
  - 3.7517050445079807
  - 4.648203825950623
  - 4.093002599477768
  - 3.224539923667908
  - 4.917204177379609
  - 5.484923815727234
  - 3.653114801645279
  - 3.882204329967499
  - 3.570203495025635
  - 3.564246416091919
  - 3.640424430370331
  - 3.371985983848572
  - 3.5319737225770953
  - 3.300006812810898
  - 3.577275085449219
  - 3.6923528939485553
  - 3.281673437356949
  - 3.86286723613739
  - 3.5101883947849277
  - 3.251551938056946
  - 3.2518932431936265
  - 3.29604389667511
  - 3.4687430739402774
  - 3.380986544489861
  - 3.377564597129822
  - 3.331688377261162
  - 3.3197806000709535
  - 3.2843105077743533
  - 3.5159009397029877
  - 3.4308165073394776
  - 3.15557177066803
  - 3.2807497799396517
  - 3.1802134156227115
  - 3.0926822245121004
  - 3.065799054503441
  - 3.2083652317523956
  - 3.193030959367752
  - 3.3077668964862825
  - 3.1438757061958316
  - 3.3649520576000214
  - 3.2383654415607452
  - 3.263559114933014
  - 3.189029848575592
  - 3.450443679094315
  - 3.123251670598984
  - 3.2528968095779422
  - 3.249381756782532
  - 3.1905624479055406
  - 3.1215479493141176
  - 3.210007864236832
  - 3.1146211564540867
  - 3.2025445222854616
  - 3.272357201576233
  - 3.2029177248477936
  - 3.2269657373428347
  - 3.2711486458778385
  - 3.298006641864777
  - 3.3478779911994936
  validation_losses:
  - 1.4087473154067993
  - 0.6115012764930725
  - 0.3921852707862854
  - 0.40923964977264404
  - 0.900070071220398
  - 0.5983057618141174
  - 0.46821168065071106
  - 0.4942111074924469
  - 0.5051363706588745
  - 0.5004470348358154
  - 0.6999058127403259
  - 0.48723456263542175
  - 0.5072110891342163
  - 0.5871658325195312
  - 0.6172643899917603
  - 0.44333985447883606
  - 0.5130685567855835
  - 0.4031563103199005
  - 0.4976966977119446
  - 0.4190952479839325
  - 0.4065217077732086
  - 0.421802818775177
  - 0.49948975443840027
  - 0.4657977521419525
  - 0.44271156191825867
  - 0.5699262022972107
  - 0.44418802857398987
  - 0.4259815812110901
  - 0.4136906862258911
  - 0.464883029460907
  - 0.4079272747039795
  - 0.42137572169303894
  - 0.5007976293563843
  - 0.40993577241897583
  - 0.4113921523094177
  - 0.4035482406616211
  - 0.406217485666275
  - 0.40071427822113037
  - 0.4421755373477936
  - 0.4195721447467804
  - 0.3945797383785248
  - 0.42251795530319214
  - 0.42875322699546814
  - 0.4274941682815552
  - 0.41609370708465576
  - 0.4043505787849426
  - 1.107312560081482
  - 0.3955772817134857
  - 0.3937141001224518
  - 0.40664950013160706
  - 0.40187230706214905
  - 0.4102361500263214
  - 0.42067474126815796
  - 0.40102019906044006
  - 0.4329015910625458
  - 0.4012276530265808
  - 0.41090548038482666
  - 0.4009738564491272
  - 0.43045398592948914
  - 0.4145495891571045
  - 0.41714438796043396
  - 0.42453473806381226
  - 0.4191572368144989
  - 0.4143494665622711
  - 0.4392721354961395
  - 0.4176827073097229
  - 0.4096391499042511
  - 0.43486979603767395
  - 0.4176059067249298
  - 0.40272095799446106
  - 0.6312181949615479
  - 0.4206082820892334
  - 0.40685680508613586
  - 0.4071276783943176
  - 0.4267766773700714
  - 0.41706153750419617
  - 0.4219382703304291
  - 0.4068165421485901
  - 0.4005832076072693
  - 0.4161781668663025
  - 0.4123062491416931
  - 0.4025394916534424
  - 0.43469512462615967
  - 0.4096137285232544
  - 0.4723479449748993
  - 0.4349493682384491
  - 0.40963900089263916
  - 0.41962796449661255
  - 0.4453414976596832
  - 0.4281393587589264
  - 0.4359951615333557
  - 0.4174273610115051
  - 0.46128225326538086
  - 0.4089886248111725
  - 0.40846842527389526
  - 0.4502255320549011
  - 0.42324119806289673
  - 0.44447383284568787
  - 0.42096325755119324
  - 0.4155905842781067
loss_records_fold1:
  train_losses:
  - 3.163750720024109
  - 3.3326094925403598
  - 3.4721087038517
  - 3.1872870504856112
  - 3.312880277633667
  - 3.3008822202682495
  - 3.187667280435562
  - 3.151245903968811
  - 3.1971260726451876
  - 3.1488695561885836
  - 3.280467623472214
  - 3.2976815819740297
  - 3.4061059653759003
  - 3.2672072052955627
  - 3.1711087822914124
  - 3.1509658813476564
  - 3.1825717836618423
  - 3.340322917699814
  - 3.3546532273292544
  - 3.223679143190384
  - 3.2066364824771885
  - 3.1585790663957596
  - 3.9993750154972076
  - 3.8697514295578004
  - 3.2937147200107577
  - 3.3314881026744843
  - 3.3882970571517945
  - 3.4284931838512422
  - 3.82742879986763
  - 4.421738016605377
  - 3.6856216490268707
  - 3.4629744768142703
  - 3.6566063821315766
  - 3.282222306728363
  - 3.467213246226311
  - 3.2708802700042727
  - 3.3688605546951296
  - 3.4178097486495975
  - 3.19779963195324
  - 3.350209960341454
  - 3.191500508785248
  - 3.1826434254646303
  - 3.1705096006393436
  - 3.1812345385551453
  - 3.2513765156269074
  - 3.248022496700287
  - 3.103811663389206
  - 3.1821339905261996
  - 3.2669322401285172
  - 3.150142860412598
  - 3.278721070289612
  - 3.273909878730774
  - 3.2079220324754716
  - 3.2521545559167864
  - 3.1823274314403536
  - 3.13294672369957
  - 3.2187802582979206
  - 3.145276659727097
  - 3.1769928216934207
  - 3.153729009628296
  - 3.1590410351753238
  - 3.1847532451152802
  - 3.1437251925468446
  - 3.1813072323799134
  - 3.3248416006565096
  - 3.2213728427886963
  - 3.2722579896450044
  - 3.2689697325229647
  - 3.2216340363025666
  - 3.2657305479049685
  - 3.205769130587578
  - 3.2549628973007203
  - 3.2481205701828006
  - 3.285866278409958
  - 3.1743853926658634
  - 3.205619066953659
  - 3.161184048652649
  - 3.202359426021576
  - 3.2191071927547457
  - 3.190294283628464
  - 3.288598394393921
  - 3.2991460740566256
  - 3.1790470421314243
  - 3.16065211892128
  - 3.148883426189423
  - 3.251969426870346
  - 3.1989916384220125
  - 3.1735274255275727
  - 3.0985664963722233
  - 3.1946022391319278
  - 3.2144692033529285
  - 3.179596090316773
  - 3.237923020124436
  - 3.1648019909858705
  - 3.2129907965660096
  - 3.245639252662659
  - 3.2587086439132693
  - 3.30849581360817
  - 3.180060231685639
  - 3.1777272403240207
  validation_losses:
  - 0.4271599352359772
  - 0.5435140132904053
  - 0.42679813504219055
  - 0.4697059392929077
  - 0.4101632833480835
  - 0.4555540680885315
  - 0.4193373918533325
  - 0.4680768549442291
  - 0.41967785358428955
  - 0.4118121266365051
  - 0.41240155696868896
  - 0.40810635685920715
  - 0.4281916916370392
  - 0.4182018041610718
  - 0.4262528121471405
  - 0.4209001660346985
  - 0.4140760004520416
  - 0.4307422339916229
  - 0.44076016545295715
  - 0.44791415333747864
  - 0.4291439652442932
  - 0.4650305211544037
  - 0.48991715908050537
  - 0.4202713072299957
  - 0.43961235880851746
  - 0.5361047983169556
  - 0.4176577031612396
  - 0.40636301040649414
  - 13.234078407287598
  - 0.46827608346939087
  - 0.4165211319923401
  - 0.45258447527885437
  - 0.4260638356208801
  - 0.4177049398422241
  - 0.4461517333984375
  - 0.43506768345832825
  - 0.417248398065567
  - 0.43788543343544006
  - 0.4085763990879059
  - 0.4438208043575287
  - 0.43197402358055115
  - 0.41278955340385437
  - 0.42181628942489624
  - 0.4615475535392761
  - 0.43114686012268066
  - 0.4162396788597107
  - 0.4159180521965027
  - 0.40814152359962463
  - 0.4324895441532135
  - 0.43021854758262634
  - 0.4880443513393402
  - 0.4064936935901642
  - 0.5196300148963928
  - 16922.166015625
  - 68226.296875
  - 0.429267555475235
  - 0.42811068892478943
  - 0.4316500127315521
  - 0.42398801445961
  - 0.43689364194869995
  - 0.4125659763813019
  - 0.43699905276298523
  - 717.7872314453125
  - 127853.9453125
  - 3385.818359375
  - 2250143.0
  - 33863460.0
  - 0.4437756836414337
  - 0.48009324073791504
  - 0.44470399618148804
  - 0.4786539375782013
  - 0.42143765091896057
  - 0.4443505108356476
  - 0.421429842710495
  - 0.41609111428260803
  - 0.42762455344200134
  - 0.4524509608745575
  - 0.4161151051521301
  - 0.4207148849964142
  - 0.4552265405654907
  - 0.4248890280723572
  - 0.42695561051368713
  - 0.41247183084487915
  - 0.4249039888381958
  - 0.4764636754989624
  - 0.4153091311454773
  - 0.4143555462360382
  - 0.4095366299152374
  - 0.43853482604026794
  - 0.48507583141326904
  - 0.4462379813194275
  - 0.43762537837028503
  - 0.4049398899078369
  - 0.42748919129371643
  - 0.42552223801612854
  - 0.4256296455860138
  - 0.42758384346961975
  - 0.4219202995300293
  - 0.4395257532596588
  - 0.44266852736473083
loss_records_fold2:
  train_losses:
  - 3.2818481504917147
  - 3.20742062330246
  - 3.3084533989429477
  - 3.2912402510643006
  - 3.2925853431224823
  - 3.239694219827652
  - 3.2538265109062197
  - 3.2687994956970217
  - 3.2035941779613495
  - 3.248935276269913
  - 3.2042815744876862
  - 3.2130179047584537
  - 3.3040169656276706
  - 3.2090218991041186
  - 3.287187844514847
  - 3.2317955374717715
  - 3.238720190525055
  - 3.322461187839508
  - 3.251468425989151
  - 3.3727360486984255
  - 3.2696702897548677
  - 3.418481776118279
  - 3.2417326390743257
  - 3.170421981811524
  - 3.1794373095035553
  - 3.2518036246299746
  - 3.2214246034622196
  - 3.224355012178421
  - 3.277631747722626
  - 3.2317536532878877
  - 3.284522092342377
  - 3.2909284949302675
  - 3.318827211856842
  - 3.4729936540126802
  - 3.266404688358307
  - 3.2511106818914417
  - 3.3989581763744354
  - 3.5205146908760074
  - 3.265840005874634
  - 3.175465166568756
  - 3.3842898279428484
  - 3.183601921796799
  - 3.2378289043903354
  - 3.2440189838409426
  - 3.2588922798633577
  - 3.1860700726509097
  - 3.213411657512188
  - 3.3167716324329377
  - 3.30605046749115
  - 3.4266671508550646
  - 3.3753078520298008
  - 3.21635764837265
  - 3.232913571596146
  - 3.3916344344615936
  - 3.300775441527367
  - 3.2120425701141357
  - 3.25507847070694
  - 3.209065574407578
  - 3.2091314375400546
  - 3.277495020627976
  - 3.202654802799225
  - 3.372281560301781
  - 3.204722684621811
  - 3.2148380875587463
  - 3.330527883768082
  - 3.2443334877491
  - 3.302286380529404
  - 3.2087248444557193
  - 3.4628566682338717
  - 3.275948575139046
  - 3.354987233877182
  - 3.2554146349430084
  - 3.47259818315506
  - 3.3959820926189423
  - 3.1538919836282733
  - 3.3008611619472505
  - 3.248038572072983
  - 3.2515854239463806
  - 3.359659516811371
  - 3.188289445638657
  - 3.260817611217499
  - 3.2442637145519257
  - 3.122030252218247
  - 3.2604835569858555
  - 3.390918099880219
  - 3.2254105448722843
  - 3.2073933809995654
  - 3.2264708220958713
  - 3.384345400333405
  - 3.4820005238056186
  - 3.274952578544617
  - 3.2164447367191316
  - 3.340027153491974
  - 3.1944419682025913
  - 3.2242784559726716
  - 3.0985229492187503
  - 3.2582963436841967
  validation_losses:
  - 0.42018505930900574
  - 0.3975113332271576
  - 0.40437400341033936
  - 0.4971387982368469
  - 0.408680260181427
  - 0.4167420268058777
  - 0.4145774841308594
  - 0.3917858600616455
  - 0.39424630999565125
  - 0.41513898968696594
  - 0.41937455534935
  - 0.4763498902320862
  - 0.4038902819156647
  - 0.41435369849205017
  - 0.40807053446769714
  - 0.436471551656723
  - 0.3989500105381012
  - 0.408956378698349
  - 0.4600141942501068
  - 0.40608906745910645
  - 0.44896960258483887
  - 0.39424073696136475
  - 0.40654686093330383
  - 0.4182484447956085
  - 0.4102816879749298
  - 0.4174481928348541
  - 0.4238715171813965
  - 0.4066692888736725
  - 0.4224230647087097
  - 0.40143880248069763
  - 0.39803722500801086
  - 0.4067038297653198
  - 0.41725608706474304
  - 0.39583197236061096
  - 0.40770334005355835
  - 0.4037703275680542
  - 0.4006933271884918
  - 0.412608802318573
  - 0.3965543508529663
  - 0.41429269313812256
  - 0.40150177478790283
  - 0.42184120416641235
  - 0.3926140367984772
  - 0.4241984784603119
  - 0.401250422000885
  - 0.4066317677497864
  - 0.41060763597488403
  - 0.4081711769104004
  - 0.42264312505722046
  - 0.41470223665237427
  - 0.39683419466018677
  - 0.41947898268699646
  - 0.4062511622905731
  - 0.4210728704929352
  - 0.4057616889476776
  - 0.4015372693538666
  - 0.44118285179138184
  - 0.39983612298965454
  - 0.40533018112182617
  - 0.4020056128501892
  - 0.4045237898826599
  - 0.40364158153533936
  - 0.41553953289985657
  - 0.4466540813446045
  - 0.4081130027770996
  - 0.4156302809715271
  - 0.41798678040504456
  - 0.4067094027996063
  - 0.40780165791511536
  - 0.4238938093185425
  - 0.4069560170173645
  - 0.43179813027381897
  - 0.39986059069633484
  - 0.43388882279396057
  - 0.398813396692276
  - 0.43576326966285706
  - 0.4307320713996887
  - 0.491275817155838
  - 0.4248540997505188
  - 0.40527084469795227
  - 0.4090808033943176
  - 0.396296888589859
  - 0.4168735444545746
  - 0.4361100196838379
  - 0.4014016389846802
  - 0.4167727530002594
  - 0.41552987694740295
  - 0.407293438911438
  - 0.40239036083221436
  - 0.411024808883667
  - 0.45217227935791016
  - 0.42376530170440674
  - 0.4178599715232849
  - 0.413639098405838
  - 0.40515613555908203
  - 0.4125153124332428
  - 0.4022383391857147
loss_records_fold3:
  train_losses:
  - 3.25210474729538
  - 3.40878199338913
  - 3.243976140022278
  - 3.374099034070969
  - 3.255032107234001
  - 3.101069444417954
  - 3.4357179492712024
  - 3.2514928907155993
  - 3.265443402528763
  - 3.2553866416215897
  - 3.268034088611603
  - 5.031941851973534
  - 4.315468144416809
  - 5.0365362703800205
  - 3.6567606747150423
  - 3.353133806586266
  - 3.4947237581014634
  - 3.373713231086731
  - 3.301552611589432
  - 3.938552433252335
  - 3.6250100612640384
  - 3.1940980076789858
  - 3.238893848657608
  - 3.172668302059174
  - 3.2300384759902956
  validation_losses:
  - 0.40505078434944153
  - 0.41621720790863037
  - 0.4134197533130646
  - 0.4937575161457062
  - 0.40691500902175903
  - 0.41347619891166687
  - 0.4173406660556793
  - 0.42225903272628784
  - 0.4581977427005768
  - 0.4075848162174225
  - 5455.0029296875
  - 0.4590132236480713
  - 0.4857989251613617
  - 0.40710970759391785
  - 0.4184802174568176
  - 0.42438405752182007
  - 0.40355733036994934
  - 0.4112411141395569
  - 0.44838637113571167
  - 0.41181808710098267
  - 0.40834882855415344
  - 0.41050875186920166
  - 0.4011143445968628
  - 0.4029527008533478
  - 0.41271257400512695
loss_records_fold4:
  train_losses:
  - 3.346309706568718
  - 3.337445849180222
  - 3.3326107397675515
  - 3.637283104658127
  - 3.296523490548134
  - 3.242709437012673
  - 3.3511551916599274
  - 3.203651174902916
  - 3.3001791119575503
  - 3.2024668157100677
  - 3.2125634849071503
  - 3.3292395353317263
  - 3.812609845399857
  - 3.459218204021454
  - 3.2954926788806915
  - 3.1686641693115236
  - 3.1833527207374575
  - 3.232162290811539
  - 3.1091485381126405
  - 3.29421266913414
  - 3.219675782322884
  - 3.310563158988953
  - 3.094688653945923
  - 3.2033512473106387
  - 3.2467628896236422
  validation_losses:
  - 0.41973239183425903
  - 0.413705974817276
  - 0.45752012729644775
  - 0.4241393506526947
  - 0.4061788022518158
  - 0.41133224964141846
  - 0.40045103430747986
  - 0.3972525894641876
  - 0.4133843183517456
  - 0.42155662178993225
  - 0.4270106554031372
  - 0.4065094590187073
  - 0.5035364031791687
  - 0.4135059416294098
  - 0.4106356203556061
  - 0.41483214497566223
  - 0.3983944058418274
  - 0.4166874885559082
  - 0.4952384829521179
  - 0.43106552958488464
  - 0.42520251870155334
  - 0.40662848949432373
  - 0.40217041969299316
  - 0.40023282170295715
  - 0.40653786063194275
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 97 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:30:54.758454'
