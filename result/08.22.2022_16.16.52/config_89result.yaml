config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:12:17.943241'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_89fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_89fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_89fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_89fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_89fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_89fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_89fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_89fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_89fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_89fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 51.42830129861832
  - 8.461683869361877
  - 6.9808459222316745
  - 7.263311332464219
  - 13.926809868216516
  - 7.1875729143619544
  - 6.735487240552903
  - 6.86846849322319
  - 11.306867337226869
  - 14.690318435430527
  - 5.174092400074006
  - 6.284236666560173
  - 6.208803689479828
  - 6.230567276477814
  - 5.647963926196098
  - 6.979177534580231
  - 9.033428618311882
  - 7.449583148956299
  - 3.1941422879695893
  - 3.3357661694288256
  - 6.808809143304825
  - 3.6773661226034164
  - 3.9562793314456943
  - 4.230042207241058
  - 6.10396112203598
  - 3.3884127259254457
  - 4.2654377877712255
  - 3.553206914663315
  - 3.7420535147190095
  - 3.1816509842872622
  - 3.529781955480576
  - 4.322619956731796
  - 4.336511781811715
  - 3.713226252794266
  - 3.5118466049432757
  - 3.1816743671894074
  - 4.074402791261673
  - 3.0473220467567446
  - 3.0404456317424775
  - 3.5157718837261203
  - 4.522426909208298
  - 2.916868555545807
  - 2.8809902250766757
  - 3.8398638278245927
  - 3.2171684205532074
  - 3.3193276643753054
  - 3.023802709579468
  - 3.6006106346845628
  - 3.1422797679901127
  - 3.1105866938829423
  - 2.914092266559601
  - 3.5366834938526157
  - 3.0155125707387924
  - 2.99181969165802
  - 3.15611686706543
  - 3.072963106632233
  - 3.2374453604221345
  - 3.8487301349639895
  - 3.3521399557590486
  - 3.373245418071747
  - 3.043239408731461
  - 3.0917527943849565
  - 3.152396255731583
  - 2.995785242319107
  - 3.0271432220935823
  - 2.986877176165581
  - 3.014522817730904
  - 3.0706960886716845
  - 2.978204828500748
  - 3.162337499856949
  - 2.9973524034023287
  - 2.96392662525177
  - 2.9689785569906237
  - 3.007934653759003
  - 3.097740697860718
  validation_losses:
  - 1.2342147827148438
  - 1.22488534450531
  - 0.7632044553756714
  - 0.9454403519630432
  - 0.6443578600883484
  - 0.7976967692375183
  - 0.5727248191833496
  - 0.49566391110420227
  - 0.4471887946128845
  - 0.4953649044036865
  - 0.41229870915412903
  - 0.4610480070114136
  - 0.3880535662174225
  - 0.42731958627700806
  - 0.4696866273880005
  - 0.6969131231307983
  - 0.41027647256851196
  - 0.3865959346294403
  - 0.38750919699668884
  - 0.44159695506095886
  - 0.4013943374156952
  - 0.3959721028804779
  - 0.41525810956954956
  - 0.41890811920166016
  - 0.385818213224411
  - 0.3901040554046631
  - 0.38364607095718384
  - 0.4042714834213257
  - 0.38738247752189636
  - 0.38566017150878906
  - 0.426777720451355
  - 0.3910306394100189
  - 0.3982595205307007
  - 0.39013776183128357
  - 0.39284807443618774
  - 0.39608234167099
  - 0.4069492518901825
  - 0.3978498876094818
  - 0.4136675000190735
  - 0.38803935050964355
  - 0.3866947889328003
  - 0.3864765167236328
  - 0.3938809633255005
  - 0.3941650688648224
  - 0.41026759147644043
  - 0.3899598717689514
  - 0.38818442821502686
  - 0.39942845702171326
  - 0.40584081411361694
  - 0.38579291105270386
  - 0.3939521312713623
  - 0.43138787150382996
  - 0.42018529772758484
  - 0.40466123819351196
  - 0.42928946018218994
  - 0.39825865626335144
  - 0.7009443640708923
  - 0.4097795784473419
  - 0.38808172941207886
  - 0.38995596766471863
  - 0.3900601863861084
  - 0.39185184240341187
  - 0.4046177268028259
  - 0.3933122754096985
  - 0.3902745842933655
  - 0.39568832516670227
  - 0.4002600908279419
  - 0.39101317524909973
  - 0.5396044254302979
  - 0.45968058705329895
  - 0.39117035269737244
  - 0.3929157555103302
  - 0.38442009687423706
  - 0.3932206630706787
  - 0.40158066153526306
loss_records_fold1:
  train_losses:
  - 2.922611391544342
  - 2.9302603960037232
  - 2.9208430469036104
  - 2.944851016998291
  - 3.025788325071335
  - 2.93853012919426
  - 2.8828766822814944
  - 2.874375039339066
  - 2.9905873447656632
  - 3.8753451794385914
  - 3.1501934170722965
  - 3.203264200687409
  - 2.9541568517684937
  - 2.9194898843765262
  - 3.0844004958868028
  - 2.8925293385982513
  - 2.9240577638149263
  - 3.0263014078140262
  - 2.9521251857280735
  - 2.9732416927814485
  - 2.896855592727661
  - 2.984971672296524
  - 2.9361381739377976
  - 2.916361939907074
  - 2.9634780138731003
  - 3.017461210489273
  - 2.8763449370861056
  - 2.894368433952332
  - 3.0782968491315845
  - 3.09610088467598
  - 2.944416153430939
  - 2.9610569655895236
  - 3.000891661643982
  - 3.0583409607410434
  - 2.9472598075866703
  validation_losses:
  - 0.40660080313682556
  - 0.40487417578697205
  - 0.3988450765609741
  - 0.4204178750514984
  - 0.41365209221839905
  - 0.40078359842300415
  - 0.40152642130851746
  - 0.40753117203712463
  - 0.41103097796440125
  - 0.45495858788490295
  - 0.4115470051765442
  - 0.4046056866645813
  - 0.40609824657440186
  - 0.44764864444732666
  - 0.40479809045791626
  - 0.40900057554244995
  - 0.44614025950431824
  - 0.4063641130924225
  - 0.4215027987957001
  - 0.43761029839515686
  - 0.4387838542461395
  - 0.4128491282463074
  - 0.44665005803108215
  - 0.4062753915786743
  - 0.42522111535072327
  - 0.40572795271873474
  - 0.4121088981628418
  - 0.40382370352745056
  - 0.4771822988986969
  - 0.41748082637786865
  - 0.4129928946495056
  - 0.40262410044670105
  - 0.40861257910728455
  - 0.40674906969070435
  - 0.4063187539577484
loss_records_fold2:
  train_losses:
  - 3.038425022363663
  - 3.0014480590820316
  - 3.0139456331729892
  - 2.970699390769005
  - 2.976853179931641
  - 2.989787071943283
  - 3.0789058148860935
  - 4.11493858397007
  - 3.6045426309108737
  - 3.1394048422575
  - 3.5472433745861056
  - 3.234449401497841
  - 3.0876063048839573
  - 3.0571181118488315
  - 3.0718670040369034
  - 3.202957195043564
  - 2.9982521295547486
  - 3.0003392457962037
  - 2.9793895483016968
  - 3.0066114723682404
  - 3.0352124869823456
  - 3.042620372772217
  - 3.0243488073349
  - 3.0022879362106325
  - 3.034544861316681
  - 2.9460927665233614
  - 2.966638073325157
  - 3.0545638471841814
  - 3.042416602373123
  - 2.9719011783599854
  - 3.0874501317739487
  - 3.018108254671097
  - 2.945221090316773
  - 2.9077181696891787
  - 2.960301268100739
  - 3.000963395833969
  - 3.041743212938309
  - 3.0618428587913513
  - 2.966244724392891
  - 2.9574942231178287
  - 2.9538413107395174
  - 3.0128615260124207
  - 2.9856980353593827
  - 2.941806930303574
  - 2.976748263835907
  - 2.948897135257721
  - 2.941322907805443
  - 2.9978721737861633
  - 3.0811725676059725
  - 3.00973978638649
  - 2.9499567210674287
  validation_losses:
  - 0.41505008935928345
  - 0.39038947224617004
  - 0.3829365074634552
  - 0.669719398021698
  - 0.38373687863349915
  - 0.3862740695476532
  - 0.3812192976474762
  - 6.431684494018555
  - 0.38013502955436707
  - 0.40577399730682373
  - 0.4169761836528778
  - 0.38934579491615295
  - 0.3830534517765045
  - 0.3859099745750427
  - 0.40109771490097046
  - 0.3879378139972687
  - 0.396467000246048
  - 0.38208359479904175
  - 0.39619526267051697
  - 0.38999444246292114
  - 0.38385069370269775
  - 0.4212293326854706
  - 0.40724360942840576
  - 0.39955785870552063
  - 0.38624370098114014
  - 0.38719454407691956
  - 0.38783007860183716
  - 0.4088347554206848
  - 0.3838321268558502
  - 0.3864060938358307
  - 0.3825073838233948
  - 0.4018021821975708
  - 0.3885815143585205
  - 0.3863774538040161
  - 0.38232454657554626
  - 0.3883260488510132
  - 0.42415398359298706
  - 0.3864002823829651
  - 0.38347992300987244
  - 0.42143678665161133
  - 0.38927024602890015
  - 0.38902872800827026
  - 0.38317373394966125
  - 0.3821248412132263
  - 0.3921459913253784
  - 0.38433367013931274
  - 0.39022451639175415
  - 0.3957188129425049
  - 0.39093446731567383
  - 0.3833787739276886
  - 0.3846491277217865
loss_records_fold3:
  train_losses:
  - 2.93891399204731
  - 2.9606914043426515
  - 2.9647989571094513
  - 2.9552466303110125
  - 2.9401304304599765
  - 3.0007109463214876
  - 2.963102838397026
  - 2.991538563370705
  - 2.9491315841674806
  - 3.026381725072861
  - 3.02765389084816
  - 2.967052155733109
  - 2.999405723810196
  - 2.960498869419098
  - 2.9507941931486132
  - 2.980985420942307
  - 2.9122068285942078
  - 2.92125957608223
  - 2.9164065390825273
  - 3.0340121746063233
  - 2.9638950139284135
  - 2.9602900445461273
  - 2.959618067741394
  - 2.973462364077568
  - 3.2702465057373047
  - 3.253386849164963
  - 3.0601574718952183
  - 3.0085675895214083
  - 2.956125244498253
  - 2.9338742673397067
  - 3.0118243873119357
  - 3.0116270542144776
  - 3.0843632489442827
  - 2.979582649469376
  - 2.9741549342870712
  - 3.1141554176807404
  - 3.044460192322731
  - 3.0333047568798066
  - 2.9700080037117007
  validation_losses:
  - 0.39529430866241455
  - 0.39661818742752075
  - 0.39589962363243103
  - 0.3976285755634308
  - 0.44105586409568787
  - 0.40244606137275696
  - 0.41547736525535583
  - 0.4015854001045227
  - 0.4049546420574188
  - 0.4037707448005676
  - 0.5478025674819946
  - 2.3492393493652344
  - 0.39812561869621277
  - 0.397285133600235
  - 0.4437667429447174
  - 0.39952322840690613
  - 0.39638033509254456
  - 0.39571326971054077
  - 0.41408783197402954
  - 0.40198951959609985
  - 8.220273971557617
  - 0.3970346450805664
  - 0.3992283344268799
  - 0.4104902744293213
  - 0.4009038209915161
  - 0.39329662919044495
  - 0.39329344034194946
  - 0.4535309672355652
  - 0.3956451714038849
  - 0.39903193712234497
  - 0.40728262066841125
  - 0.4089510440826416
  - 0.43723633885383606
  - 0.3933693468570709
  - 0.3947419822216034
  - 0.40145012736320496
  - 0.40517646074295044
  - 0.3968329429626465
  - 0.3975931704044342
loss_records_fold4:
  train_losses:
  - 2.9633008539676666
  - 2.97183957695961
  - 3.0666073232889177
  - 3.0195106625556947
  - 3.072412884235382
  - 3.000851321220398
  - 2.978666284680367
  - 3.016185846924782
  - 3.0353442788124085
  - 3.1292642742395405
  - 2.963456451892853
  - 2.9498396277427674
  - 2.9143683910369873
  - 3.0406855285167698
  - 2.999367505311966
  validation_losses:
  - 0.3901554048061371
  - 0.3932703137397766
  - 0.46902307868003845
  - 0.39383313059806824
  - 0.4180198311805725
  - 0.3936479091644287
  - 0.39001330733299255
  - 0.39450347423553467
  - 0.4448508620262146
  - 0.39096716046333313
  - 0.3983713984489441
  - 0.39065566658973694
  - 0.39626604318618774
  - 0.392241507768631
  - 0.39815351366996765
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 75 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 35 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:19:42.720906'
