config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:25:34.060029'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_59fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_59fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_59fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_59fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_59fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_59fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_59fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_59fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_59fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_59fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 19.115082168579104
  - 6.987775039672852
  - 12.708945918083192
  - 6.566051578521729
  - 2.75696918964386
  - 2.7268455743789675
  - 2.551009213924408
  - 1.6448075503110886
  - 1.4838411837816239
  - 1.0642858922481537
  - 1.3457285404205324
  - 1.0652185320854188
  - 0.9904119431972505
  - 1.0431555151939393
  - 1.1173810899257661
  - 1.4645679354667664
  - 1.2152480006217958
  - 1.3597171664237977
  - 1.1633018732070923
  - 0.9474838793277741
  - 1.0074592113494873
  - 0.9106943726539612
  - 1.4520488381385803
  - 5.0450136244297035
  - 1.6431894183158875
  - 0.8214269936084748
  - 0.8617364704608917
  - 0.9569200336933137
  - 0.9326967656612397
  - 0.7882138133049011
  - 0.8438033401966095
  - 2.3206580519676208
  - 0.8328115046024323
  - 0.9274642765522003
  - 0.9200383186340333
  - 0.8744629383087159
  - 1.7525954127311707
  - 2.3359510958194734
  - 1.7648530811071397
  - 1.9207419812679292
  - 2.892282783985138
  - 2.8183630585670474
  - 3.769164049625397
  - 1.5343952775001526
  - 1.864304292201996
  - 0.9118047893047333
  - 0.9146899640560151
  - 0.8473343312740327
  - 0.9277529478073121
  - 0.8980901181697846
  - 0.9991534590721131
  - 0.9196484327316284
  - 1.0101500749588013
  - 0.8739577353000642
  - 1.0156582474708558
  - 1.027859115600586
  - 0.8402340531349183
  - 0.7979464411735535
  - 0.9067519843578339
  - 0.8534169256687165
  - 0.809666383266449
  - 0.8099079430103302
  - 0.9000670284032822
  - 1.1430970907211304
  - 1.292238211631775
  - 1.317081117630005
  - 0.9630999326705934
  - 0.9141898930072785
  - 1.9035329639911653
  - 1.102130651473999
  - 0.9762821435928345
  - 0.8204565435647965
  - 0.9404425084590913
  - 0.9649868071079255
  - 0.8368318259716034
  - 1.0405384302139282
  - 0.9092802584171296
  - 0.8126266062259675
  - 0.8273142814636231
  - 0.7554862916469575
  - 2.7856043815612797
  - 1.3396780312061312
  - 1.268364602327347
  - 0.9926878035068513
  - 1.0781332075595855
  - 1.01925488114357
  - 0.9985756993293763
  - 0.9190197110176087
  - 1.053558522462845
  - 1.0024268090724946
  - 1.1099365830421448
  - 0.8921424329280854
  - 3.9944378674030308
  - 1.0495682418346406
  - 0.9032401740550995
  - 0.9187172234058381
  - 0.8706762939691544
  validation_losses:
  - 4.350207328796387
  - 5.010838508605957
  - 5.4822211265563965
  - 2.067037343978882
  - 1.5621328353881836
  - 1.6633154153823853
  - 0.7784081697463989
  - 0.648223340511322
  - 0.42543739080429077
  - 0.6009306907653809
  - 0.41372352838516235
  - 0.5831671953201294
  - 0.42551153898239136
  - 0.4090023338794708
  - 0.5009421110153198
  - 0.47040805220603943
  - 0.4621559679508209
  - 0.64194655418396
  - 0.40037426352500916
  - 0.4351424276828766
  - 0.4102709889411926
  - 0.4581628739833832
  - 0.8207876086235046
  - 0.39923393726348877
  - 0.5007884502410889
  - 0.414411723613739
  - 0.49996137619018555
  - 0.43377748131752014
  - 0.39402878284454346
  - 0.3782532513141632
  - 0.37698879837989807
  - 0.38956308364868164
  - 0.4076369106769562
  - 0.40807101130485535
  - 0.3989326059818268
  - 0.40045344829559326
  - 1.0083246231079102
  - 0.8130768537521362
  - 1.4122505187988281
  - 0.6543229818344116
  - 0.9846593737602234
  - 1.3177683353424072
  - 0.6482874155044556
  - 0.5629929900169373
  - 0.4365077316761017
  - 0.46707817912101746
  - 0.5076274275779724
  - 0.3908092677593231
  - 0.4302060604095459
  - 0.38257232308387756
  - 0.5399265289306641
  - 0.3834170699119568
  - 0.40759775042533875
  - 0.41216805577278137
  - 0.4949612021446228
  - 0.392455130815506
  - 0.38448596000671387
  - 0.40718528628349304
  - 0.3955294191837311
  - 0.3888394236564636
  - 0.38049444556236267
  - 0.4012158513069153
  - 0.37952136993408203
  - 0.3806622624397278
  - 0.42253971099853516
  - 0.3909485936164856
  - 0.4144241511821747
  - 0.42106735706329346
  - 0.4474395215511322
  - 0.3843109905719757
  - 0.4171293377876282
  - 0.4022369086742401
  - 0.5579511523246765
  - 0.3820854723453522
  - 0.459268718957901
  - 0.3954334259033203
  - 0.42206642031669617
  - 0.3843691647052765
  - 0.44197726249694824
  - 0.3906128704547882
  - 0.37853869795799255
  - 0.39221835136413574
  - 0.41581639647483826
  - 0.4430600702762604
  - 0.415598064661026
  - 0.4337383508682251
  - 0.3836795687675476
  - 0.40444323420524597
  - 0.400321900844574
  - 0.41622114181518555
  - 0.46932747960090637
  - 0.45344626903533936
  - 0.4019165337085724
  - 0.408817857503891
  - 0.39555823802948
  - 0.3809739947319031
  - 0.3860038220882416
loss_records_fold1:
  train_losses:
  - 0.836156839132309
  - 0.8339536368846894
  - 3.3222447574138645
  - 1.6272484540939331
  - 0.9679672420024872
  - 0.9294762551784516
  - 0.9335606276988984
  - 0.8308281838893891
  - 0.757958871126175
  - 0.8323337256908417
  - 0.85670205950737
  - 0.7551151424646378
  - 0.8895439624786378
  - 0.754482987523079
  - 1.8657057940959931
  - 1.1408406913280487
  - 1.989238041639328
  - 1.4927681744098664
  - 0.9396821737289429
  - 0.9728519678115846
  - 1.1356095373630524
  - 0.9037999629974366
  - 0.9286194682121277
  - 0.790930786728859
  - 0.7775781452655792
  - 0.7721016556024551
  - 2.824683314561844
  - 1.1560502767562866
  - 0.9883228182792664
  - 0.9698481291532517
  - 4.448809438943863
  - 4.913667643070221
  - 1.0189573884010314
  - 1.104922139644623
  - 0.9276122093200684
  - 0.7942456543445587
  - 0.8002316951751709
  - 0.775544747710228
  - 1.446538668870926
  - 0.7637396275997163
  - 1.2362453997135163
  - 1.5474964559078217
  - 0.8924664497375489
  - 0.8048547953367233
  - 0.8508264988660813
  - 0.8563422620296479
  - 0.8507913649082184
  - 0.8029256105422974
  - 0.7684930384159089
  - 0.783211761713028
  - 0.7921524465084077
  validation_losses:
  - 0.41036853194236755
  - 0.4027896225452423
  - 0.41215330362319946
  - 0.3953576385974884
  - 0.5071340203285217
  - 0.4801446497440338
  - 0.44645819067955017
  - 0.4193512201309204
  - 0.40228933095932007
  - 0.432935893535614
  - 0.4262872338294983
  - 0.44299453496932983
  - 0.4007154107093811
  - 0.3971951901912689
  - 0.48501771688461304
  - 0.41002020239830017
  - 0.4097704589366913
  - 0.39650988578796387
  - 0.3974411189556122
  - 0.4526274800300598
  - 0.41317442059516907
  - 0.5104687213897705
  - 0.40863609313964844
  - 0.41253477334976196
  - 0.3988266885280609
  - 0.3913949429988861
  - 0.3913915157318115
  - 0.4334120452404022
  - 0.4289446771144867
  - 0.4055253267288208
  - 0.4204402267932892
  - 0.3975759744644165
  - 0.4840508997440338
  - 0.41191527247428894
  - 0.4011062681674957
  - 0.42274701595306396
  - 0.40540796518325806
  - 0.39627814292907715
  - 0.39567509293556213
  - 0.5024849772453308
  - 0.5237175822257996
  - 0.639295220375061
  - 0.4117831289768219
  - 0.4226609766483307
  - 0.4488259553909302
  - 0.44557985663414
  - 0.4219032824039459
  - 0.39995506405830383
  - 0.4078584313392639
  - 0.4034547507762909
  - 0.3991372585296631
loss_records_fold2:
  train_losses:
  - 0.9511365771293641
  - 0.8113654613494874
  - 0.7512975543737412
  - 0.7478124976158143
  - 0.7716380059719086
  - 0.8156132996082306
  - 0.8544579446315765
  - 1.1999772846698762
  - 0.9719585418701172
  - 0.7733116388320923
  - 0.8304890930652619
  - 0.8889396667480469
  - 0.8072930753231049
  - 0.8209734082221986
  - 0.8347388446331024
  - 2.463525760173798
  - 1.083250629901886
  - 0.8208662450313569
  - 0.7890245974063874
  - 0.789547485113144
  - 0.7883189260959625
  - 0.7632607012987137
  - 0.7727757453918458
  - 0.7689233303070069
  - 0.7587109774351121
  - 0.8667086124420167
  - 0.8171781897544861
  - 0.8152975201606751
  - 0.8411522507667542
  - 1.3627985417842865
  - 0.7598982423543931
  - 0.7836884081363679
  - 0.9709339737892151
  - 0.7560215890407562
  - 0.7881019592285157
  - 0.8164680898189545
  - 1.0700814306735993
  - 0.9078995823860169
  - 0.7946587800979614
  - 0.7675932049751282
  - 0.8101112663745881
  - 0.7536542892456055
  - 0.8118301510810852
  - 0.7672417998313904
  - 0.8024046778678895
  - 1.0421535074710846
  - 1.0545069038867951
  - 0.803256380558014
  - 0.8326772749423981
  - 0.7717342197895051
  - 0.8009939432144165
  - 0.7855636775493622
  - 0.8685103952884674
  - 0.7897708773612977
  - 0.9799953639507294
  - 0.8182797372341156
  - 0.7579254150390625
  validation_losses:
  - 0.382593035697937
  - 0.38262537121772766
  - 0.3810912072658539
  - 0.3806372284889221
  - 0.3910501003265381
  - 0.38703954219818115
  - 0.4089423418045044
  - 0.49246495962142944
  - 0.3797111213207245
  - 0.37730976939201355
  - 0.4347230792045593
  - 0.4276459217071533
  - 0.3937141001224518
  - 0.37775903940200806
  - 0.38661471009254456
  - 0.4262740910053253
  - 0.38278576731681824
  - 0.38282063603401184
  - 0.4338493049144745
  - 0.4005059599876404
  - 0.4234841763973236
  - 0.3766869008541107
  - 0.383681982755661
  - 0.4071595072746277
  - 0.3756667971611023
  - 0.4050343334674835
  - 0.4219144284725189
  - 0.37655043601989746
  - 0.37851616740226746
  - 0.3938446342945099
  - 0.3853982388973236
  - 0.3831351101398468
  - 0.42748937010765076
  - 0.3785055875778198
  - 0.37812668085098267
  - 0.42923057079315186
  - 0.3868052661418915
  - 0.37984320521354675
  - 0.37664511799812317
  - 0.4111463129520416
  - 0.39735767245292664
  - 0.37892070412635803
  - 0.38434305787086487
  - 0.3744180202484131
  - 0.37474703788757324
  - 0.40221554040908813
  - 0.3744211792945862
  - 0.38005873560905457
  - 0.37355300784111023
  - 0.3727303147315979
  - 0.4071287512779236
  - 0.3949030339717865
  - 0.3839050829410553
  - 0.3898770809173584
  - 0.373323529958725
  - 0.3806208372116089
  - 0.37437689304351807
loss_records_fold3:
  train_losses:
  - 0.804962581396103
  - 0.7557053744792939
  - 0.7383308708667755
  - 0.794839608669281
  - 0.7810741841793061
  - 0.7358582556247711
  - 0.8006563067436219
  - 1.0462980210781099
  - 0.8781659483909607
  - 0.8274578273296357
  - 0.8485528588294984
  - 0.820732069015503
  - 1.1440978527069092
  - 1.2965466141700746
  - 0.7559024631977082
  - 0.7958378016948701
  - 0.7996239125728608
  - 0.8403006553649903
  - 0.7920324921607972
  - 0.8160284757614136
  - 0.7734502196311951
  - 0.7535312652587891
  - 0.7709312200546266
  - 0.9733887612819672
  - 0.8547012448310852
  - 0.8277758240699769
  - 0.7873771220445633
  - 0.7722497642040254
  validation_losses:
  - 0.4080706536769867
  - 0.3847576677799225
  - 0.38136252760887146
  - 0.39813920855522156
  - 0.3843957483768463
  - 0.38985005021095276
  - 0.40657874941825867
  - 0.40185800194740295
  - 0.390913188457489
  - 0.4222795367240906
  - 0.38223955035209656
  - 0.39889654517173767
  - 0.4037947952747345
  - 0.38205596804618835
  - 0.37802034616470337
  - 0.3777031898498535
  - 0.40072163939476013
  - 0.37738198041915894
  - 0.3880322277545929
  - 0.3817022442817688
  - 0.3764561712741852
  - 0.38748347759246826
  - 0.3960978090763092
  - 0.39136406779289246
  - 0.38229113817214966
  - 0.3834208846092224
  - 0.3911869525909424
  - 0.38414424657821655
loss_records_fold4:
  train_losses:
  - 0.777856582403183
  - 0.7392037481069565
  - 0.7702536284923553
  - 0.9434195220470429
  - 0.8749415695667268
  - 0.803191500902176
  - 0.8528290033340454
  - 0.831747967004776
  - 0.8327728986740113
  - 0.9079336762428284
  - 0.8247261881828308
  - 0.7918966889381409
  - 0.7793303549289704
  - 0.7851688385009766
  - 0.7420451283454895
  - 0.749170446395874
  validation_losses:
  - 0.3945775032043457
  - 0.38314762711524963
  - 0.40058159828186035
  - 0.3943767249584198
  - 0.38750576972961426
  - 0.3824634552001953
  - 0.3824450373649597
  - 0.3902236223220825
  - 0.38845643401145935
  - 0.3990586996078491
  - 0.39469724893569946
  - 0.4012042284011841
  - 0.3877205550670624
  - 0.3893900513648987
  - 0.38649001717567444
  - 0.3861452043056488
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 97 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 57 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:21:01.186079'
