config:
  aggregation: mean
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:28:34.868876'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_87fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_87fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_87fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_87fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_87fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_87fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_87fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_87fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_87fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_87fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 0.9610481142997742
  - 0.9558254301548005
  - 0.8696422666311264
  - 0.8320010662078858
  - 0.8795093953609467
  - 0.850808721780777
  - 0.8774395048618318
  - 0.8092596709728241
  - 0.846705186367035
  - 0.8293398022651672
  - 0.8939602315425873
  - 0.8157801687717439
  - 0.8081329464912415
  - 0.8110311269760132
  - 0.8337633073329926
  - 0.8619941055774689
  - 1.012633091211319
  - 0.8307727694511414
  - 0.7842563658952714
  - 0.8406995177268982
  - 0.8486685156822205
  - 0.842841398715973
  - 0.8464062690734864
  - 0.8140068352222443
  - 0.8030253112316132
  - 0.8017570734024049
  - 0.8317180812358856
  - 0.8212850272655488
  - 0.842326420545578
  - 0.8178046464920045
  - 0.8001230537891388
  - 0.8046135485172272
  - 0.8256534755229951
  - 0.8248811542987824
  validation_losses:
  - 0.3996923565864563
  - 0.48454371094703674
  - 0.4694810211658478
  - 0.4487018883228302
  - 0.4038313329219818
  - 0.40023720264434814
  - 0.4026561677455902
  - 0.4200112521648407
  - 0.41026806831359863
  - 0.40321919322013855
  - 0.38924044370651245
  - 0.42715126276016235
  - 0.42448875308036804
  - 0.38908064365386963
  - 0.4139806032180786
  - 0.40409985184669495
  - 0.4224587678909302
  - 0.4034324884414673
  - 0.39034774899482727
  - 0.3935169577598572
  - 0.38436180353164673
  - 0.4010791778564453
  - 0.3953765630722046
  - 0.39108380675315857
  - 0.39582860469818115
  - 0.388213187456131
  - 0.38890567421913147
  - 0.39966025948524475
  - 0.39527222514152527
  - 0.38862451910972595
  - 0.39634934067726135
  - 0.3912782371044159
  - 0.3882884085178375
  - 0.3922288119792938
loss_records_fold1:
  train_losses:
  - 0.8150326073169709
  - 0.8296896159648895
  - 0.8306701838970185
  - 0.817562621831894
  - 0.7905326902866364
  - 0.832673841714859
  - 0.826236093044281
  - 0.8509335577487946
  - 0.8359172940254211
  - 0.8265129745006562
  - 0.8140499413013459
  validation_losses:
  - 0.3903410732746124
  - 0.3939979672431946
  - 0.40468382835388184
  - 0.3958493173122406
  - 0.39139243960380554
  - 0.3902430832386017
  - 0.3963371515274048
  - 0.3919795751571655
  - 0.39057597517967224
  - 0.3994263708591461
  - 0.393378347158432
loss_records_fold2:
  train_losses:
  - 0.8397048592567444
  - 0.8203699827194214
  - 0.8845843076705933
  - 0.8194238424301148
  - 0.8300396323204041
  - 0.827095240354538
  - 0.8299607336521149
  - 1.0125437378883362
  - 0.8487500905990601
  - 0.8884404420852662
  - 0.8942325592041016
  - 0.8551460206508636
  - 0.8521428585052491
  - 0.8372930645942689
  - 0.8554200530052185
  - 0.8271557688713074
  validation_losses:
  - 0.3901902735233307
  - 0.3853737413883209
  - 0.4061046242713928
  - 0.4020455777645111
  - 0.39023229479789734
  - 0.386005699634552
  - 0.40458017587661743
  - 0.4019131660461426
  - 0.3977908194065094
  - 0.41245415806770325
  - 0.3875489830970764
  - 0.39721429347991943
  - 0.3931218087673187
  - 0.38845041394233704
  - 0.39632049202919006
  - 0.3909824788570404
loss_records_fold3:
  train_losses:
  - 0.7962535738945008
  - 0.8138163983821869
  - 0.7986622273921967
  - 0.8243611335754395
  - 0.8179958522319795
  - 0.8651763737201691
  - 0.9107662618160248
  - 0.8225621163845063
  - 0.8402394354343414
  - 0.8399287879467011
  - 0.8417293787002564
  validation_losses:
  - 0.38027918338775635
  - 0.3868487477302551
  - 0.37759333848953247
  - 0.3871917724609375
  - 0.3959854245185852
  - 0.38290613889694214
  - 0.38423407077789307
  - 0.3849170207977295
  - 0.3872174620628357
  - 0.3917863667011261
  - 0.39932894706726074
loss_records_fold4:
  train_losses:
  - 0.8076037168502808
  - 0.8180735528469086
  - 0.7932602286338807
  - 0.8414886355400086
  - 0.7814646482467652
  - 0.8012144565582275
  - 0.8304591834545136
  - 0.8204739511013032
  - 0.8635725915431977
  - 0.8127050280570984
  - 0.8409023880958557
  - 0.7988554120063782
  - 0.8209208726882935
  - 0.8204978168010713
  - 0.8215838372707367
  - 0.8031653821468354
  - 0.8018308639526368
  - 0.9044830858707429
  - 0.8005925118923187
  - 0.8528378307819366
  - 0.8477562189102174
  - 0.8598598718643189
  - 0.8214419484138489
  - 0.8951041579246521
  - 0.8634634494781495
  - 0.8256743013858796
  - 0.8046082317829133
  - 0.8033373773097993
  - 0.9507433533668519
  - 0.81437606215477
  - 0.832087904214859
  - 0.8401353776454926
  - 0.8489836692810059
  - 0.8127038359642029
  - 0.8231551051139832
  - 0.8243786573410035
  - 0.8152497053146363
  - 0.8178197383880615
  - 0.8302656829357148
  - 0.8132196605205536
  - 0.8224271655082703
  - 0.8558049201965332
  - 0.8005471527576447
  - 0.7975918471813203
  - 0.8260992705821991
  - 0.7749110102653504
  - 0.8055571556091309
  - 0.8466303586959839
  - 0.8298546016216278
  - 0.8219571173191071
  - 0.781291139125824
  - 0.821285605430603
  - 0.8081423044204712
  - 0.7738900840282441
  validation_losses:
  - 0.38546502590179443
  - 0.39147114753723145
  - 0.37584853172302246
  - 0.3814140856266022
  - 0.4075799882411957
  - 0.39544597268104553
  - 0.4123506247997284
  - 0.38973268866539
  - 0.39880886673927307
  - 0.391169935464859
  - 0.4185313284397125
  - 0.4024660587310791
  - 0.39466923475265503
  - 0.3889201879501343
  - 0.4009002447128296
  - 0.39829885959625244
  - 0.38268470764160156
  - 0.3938041925430298
  - 0.3888622224330902
  - 0.3861697316169739
  - 0.41815492510795593
  - 0.41427600383758545
  - 0.3845553994178772
  - 0.39007359743118286
  - 0.4008283019065857
  - 0.4021703600883484
  - 0.4025585949420929
  - 0.3907860815525055
  - 0.3881320655345917
  - 0.41215094923973083
  - 0.4148578643798828
  - 0.40857237577438354
  - 0.383076012134552
  - 0.3869517147541046
  - 0.3780430555343628
  - 0.3969283401966095
  - 0.37881454825401306
  - 0.3871921896934509
  - 0.38301190733909607
  - 0.38157936930656433
  - 0.3756048381328583
  - 0.3908773362636566
  - 0.38660550117492676
  - 0.3907499611377716
  - 0.3772067427635193
  - 0.3919995427131653
  - 0.3880321681499481
  - 0.3993871808052063
  - 0.3850902318954468
  - 0.39155617356300354
  - 0.388171911239624
  - 0.3798448443412781
  - 0.36791178584098816
  - 0.37588226795196533
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 54 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:10:25.393020'
