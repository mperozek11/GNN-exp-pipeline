config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:12:44.291041'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_90fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_90fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_90fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_90fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_90fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_90fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_90fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_90fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_90fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_90fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 26.607208275794985
  - 5.802326023578644
  - 3.4455332040786746
  - 2.111940395832062
  - 5.356649482250214
  - 2.378593397140503
  - 2.407818704843521
  - 2.512034557759762
  - 4.518785381317139
  - 2.3935485780239105
  - 2.0691073417663577
  - 4.502684217691422
  - 2.35815948843956
  - 2.0493593096733096
  - 2.447090631723404
  - 5.279737198352814
  - 6.814056342840195
  - 4.1969358086586
  - 1.627201497554779
  - 1.7432731151580811
  - 1.7974225878715515
  - 1.758823573589325
  - 2.7017366707324983
  - 2.323254156112671
  - 2.5295559227466584
  - 2.35348858833313
  - 2.599764883518219
  - 1.7900479912757874
  - 2.525861215591431
  - 1.6446812152862549
  - 1.7603339701890945
  - 2.4676940858364107
  - 2.039317321777344
  - 2.6691845595836643
  - 1.8565875619649888
  - 1.834992790222168
  - 1.6185724794864655
  - 1.7390494048595428
  - 2.30174897313118
  - 2.3780290991067887
  - 5.441477727890015
  - 1.750692841410637
  - 1.6830835521221161
  - 3.6800254464149478
  - 1.7698113083839417
  - 1.555859097838402
  - 2.143749099969864
  - 4.810403794050217
  - 1.847711205482483
  - 1.8844550490379335
  - 1.7304983913898468
  - 2.468439328670502
  - 1.8494988530874252
  - 1.6124427497386933
  - 1.5789386570453645
  - 1.7637036621570588
  - 2.3645327985286713
  - 1.8273234128952027
  - 2.0813424944877625
  - 1.8345706462860107
  - 2.4116377949714662
  - 2.231339967250824
  - 4.962117755413056
  - 4.054933404922486
  - 1.6053079545497895
  - 1.5815235018730165
  - 1.5580864310264588
  - 1.8631372570991518
  - 2.042145508527756
  - 1.914945435523987
  - 2.052206075191498
  - 1.55349098443985
  - 3.2771283388137817
  - 1.6585323959589005
  - 2.3772170543670654
  - 1.7487829804420472
  - 1.5488847970962525
  - 1.7620684802532196
  - 1.525649827718735
  - 1.8816222250461578
  - 1.5564009666442873
  - 1.5149330019950868
  - 1.7918186962604523
  - 2.235675096511841
  - 1.8742413699626923
  - 1.621933811903
  - 2.411761832237244
  - 1.988045608997345
  - 1.4760042667388917
  - 1.5126249194145203
  - 1.476287990808487
  - 1.5212526679039002
  - 1.4943874537944795
  validation_losses:
  - 8.265599250793457
  - 0.6047191023826599
  - 0.4224526584148407
  - 0.5824872255325317
  - 0.4758200943470001
  - 0.6452497839927673
  - 1.4774606227874756
  - 0.4753940999507904
  - 0.4540158808231354
  - 1.040128231048584
  - 1.4852783679962158
  - 0.5586205720901489
  - 0.41457465291023254
  - 0.47193723917007446
  - 0.4117393493652344
  - 0.7470805644989014
  - 0.8656840324401855
  - 0.5054666996002197
  - 0.38835617899894714
  - 0.7521003484725952
  - 0.4982377290725708
  - 0.5911936163902283
  - 0.4029351472854614
  - 0.5136281251907349
  - 0.5039681792259216
  - 0.3887886106967926
  - 0.3853430151939392
  - 0.6710675358772278
  - 0.4145641028881073
  - 0.38258644938468933
  - 0.5910471677780151
  - 0.4819343388080597
  - 0.42138102650642395
  - 0.4368739426136017
  - 0.40308481454849243
  - 0.4412096440792084
  - 0.4271433651447296
  - 0.39476385712623596
  - 0.47848641872406006
  - 0.5445861220359802
  - 0.4035293459892273
  - 0.38505735993385315
  - 0.4227708578109741
  - 0.3859034776687622
  - 0.384423166513443
  - 0.3837408125400543
  - 0.39343729615211487
  - 0.44075801968574524
  - 0.3868374824523926
  - 0.3765922784805298
  - 0.3859153091907501
  - 0.4084791839122772
  - 0.3805062174797058
  - 0.4170505106449127
  - 0.5893381237983704
  - 0.3894786238670349
  - 0.3937150239944458
  - 0.40107741951942444
  - 0.39212578535079956
  - 0.4713541567325592
  - 0.38322877883911133
  - 0.4309265613555908
  - 0.39078524708747864
  - 0.3982499837875366
  - 0.4098889231681824
  - 0.3902859091758728
  - 0.3923342525959015
  - 0.4595176875591278
  - 0.3948034346103668
  - 0.4067670702934265
  - 0.39705851674079895
  - 0.3882339596748352
  - 0.38615739345550537
  - 0.3888401687145233
  - 0.3901742398738861
  - 0.40043744444847107
  - 0.38823845982551575
  - 0.3867722451686859
  - 0.3932434618473053
  - 0.4098236560821533
  - 0.39017295837402344
  - 0.3833104372024536
  - 0.3831075429916382
  - 0.39808711409568787
  - 0.4065365493297577
  - 0.3842974901199341
  - 0.40001553297042847
  - 0.3876526653766632
  - 0.3901091516017914
  - 0.3945106863975525
  - 0.3830964267253876
  - 0.38692909479141235
  - 0.3836022615432739
loss_records_fold1:
  train_losses:
  - 1.627556347846985
  - 1.4648703038692474
  - 1.445432311296463
  - 1.5941843271255494
  - 1.4726398408412935
  - 1.7797433197498322
  - 1.5267740309238436
  - 1.4512724518775941
  - 1.4357201337814331
  - 1.4194831371307375
  - 1.520794713497162
  validation_losses:
  - 0.42523661255836487
  - 0.3956446945667267
  - 0.4256606996059418
  - 0.4016607701778412
  - 0.4103963077068329
  - 0.4103340208530426
  - 0.4024212956428528
  - 0.39849400520324707
  - 0.4001636505126953
  - 0.39750176668167114
  - 0.40350422263145447
loss_records_fold2:
  train_losses:
  - 1.5332690119743349
  - 1.5824353843927383
  - 1.504485148191452
  - 1.902772918343544
  - 1.6603638470172883
  - 1.6821438014507295
  - 1.5982872545719147
  - 1.5673780143260956
  - 1.55020409822464
  - 1.4730304196476938
  - 2.3706135153770447
  - 1.538766860961914
  - 1.4914934873580934
  - 1.4947477579116821
  - 1.50228850543499
  - 1.5417702734470369
  - 1.5044799089431764
  - 1.6027787327766418
  - 1.5092401683330536
  - 1.4882624089717866
  - 1.4878862321376802
  - 1.5800998568534852
  - 1.5336569786071779
  - 1.4592930018901826
  - 1.4408512502908708
  - 1.5050392627716065
  - 1.4851407766342164
  - 1.5310921370983124
  - 1.604368805885315
  - 1.5163844287395478
  - 2.5438122749328613
  - 1.7228517174720765
  - 1.4721056997776032
  - 1.9099756360054017
  - 2.6994122922420503
  - 1.5785360872745515
  - 2.422463685274124
  - 1.7135855317115785
  - 1.798741948604584
  - 1.4837777614593506
  - 1.4828115463256837
  - 1.5007865846157076
  - 1.5348865151405335
  - 1.4638484239578249
  - 1.467594349384308
  - 1.4781345069408418
  - 1.442084550857544
  - 1.4892562568187715
  - 1.4599071681499483
  - 1.4671088606119156
  - 1.5065761387348175
  - 1.4554841101169587
  - 1.4856469929218292
  - 1.47770015001297
  - 1.4566098272800447
  validation_losses:
  - 0.5414316058158875
  - 0.3757571280002594
  - 0.38364145159721375
  - 0.455532431602478
  - 0.4030380845069885
  - 0.459881067276001
  - 0.461859792470932
  - 0.39015308022499084
  - 0.3800620436668396
  - 0.37865662574768066
  - 0.3930169641971588
  - 0.5927019715309143
  - 0.3772335946559906
  - 0.3764328062534332
  - 0.41964104771614075
  - 0.4005714952945709
  - 0.4285810887813568
  - 0.39422571659088135
  - 0.37562569975852966
  - 0.3815464973449707
  - 0.3936848044395447
  - 0.3772241771221161
  - 0.38157445192337036
  - 0.44964611530303955
  - 0.4046838581562042
  - 0.3991239070892334
  - 0.3882237672805786
  - 0.3750559985637665
  - 0.46132907271385193
  - 0.3767358064651489
  - 0.38329216837882996
  - 0.37699437141418457
  - 0.37412360310554504
  - 0.3927270472049713
  - 0.37980756163597107
  - 0.3847273588180542
  - 0.8960856199264526
  - 0.3996874988079071
  - 0.3758482336997986
  - 0.373952180147171
  - 0.3729056417942047
  - 0.3938049376010895
  - 0.3821108043193817
  - 0.3855069577693939
  - 0.38885298371315
  - 0.44987377524375916
  - 0.3732486665248871
  - 0.37855905294418335
  - 0.4018232226371765
  - 0.375321626663208
  - 0.3734648823738098
  - 0.3760858178138733
  - 0.38514280319213867
  - 0.37605834007263184
  - 0.3745364248752594
loss_records_fold3:
  train_losses:
  - 1.5154742121696474
  - 1.4500793397426606
  - 1.4742922484874725
  - 1.538482701778412
  - 2.0446821033954623
  - 1.6047014594078064
  - 1.5195036232471466
  - 1.516742467880249
  - 1.7528281927108766
  - 2.046022292971611
  - 1.560928511619568
  - 1.5994339346885682
  - 1.4653032660484315
  - 1.5048140823841096
  - 1.4725351572036744
  - 2.183834779262543
  - 1.5172518074512482
  - 1.51042857170105
  - 1.494352686405182
  - 1.4713415741920473
  - 1.469622439146042
  - 1.5453111648559572
  validation_losses:
  - 0.3805660307407379
  - 0.39616748690605164
  - 0.3789112865924835
  - 0.37769901752471924
  - 0.38284972310066223
  - 0.37491315603256226
  - 0.46776214241981506
  - 0.38123539090156555
  - 0.4021278917789459
  - 0.3891734480857849
  - 0.41049396991729736
  - 0.37943223118782043
  - 0.38832420110702515
  - 0.42058613896369934
  - 0.38160231709480286
  - 0.4336077868938446
  - 0.4145721197128296
  - 0.3904845118522644
  - 0.3860354423522949
  - 0.395704060792923
  - 0.39976203441619873
  - 0.38838624954223633
loss_records_fold4:
  train_losses:
  - 1.5012470662593842
  - 1.5102321207523346
  - 1.444452738761902
  - 1.4482312500476837
  - 1.551393610239029
  - 1.5200031936168672
  - 1.4732162684202195
  - 1.5813497006893158
  - 1.5180512368679047
  - 1.507006648182869
  - 1.5523624300956727
  - 1.4634523689746857
  - 1.4727848768234253
  - 1.436193484067917
  - 1.458063578605652
  - 1.466266077756882
  - 1.580992478132248
  - 1.5145298182964326
  - 1.4782761693000794
  - 1.5198432087898255
  - 1.4984297037124634
  - 1.534359195828438
  - 1.5722995519638063
  - 1.5022435903549196
  - 1.51907234787941
  - 1.5075350403785706
  - 2.3652931809425355
  - 1.5773768961429597
  - 1.4950924515724182
  - 1.6190304279327394
  - 1.5176071465015413
  - 1.4953990101814272
  validation_losses:
  - 0.3777966797351837
  - 0.4121514558792114
  - 0.3818967640399933
  - 0.37447744607925415
  - 0.41934934258461
  - 0.391073614358902
  - 0.3738996386528015
  - 0.4299236834049225
  - 0.3793177306652069
  - 0.4043366611003876
  - 0.38218286633491516
  - 0.3853832483291626
  - 0.3789481520652771
  - 0.37706753611564636
  - 0.4079890847206116
  - 0.42164960503578186
  - 0.5399643182754517
  - 0.3788001537322998
  - 0.3901778757572174
  - 0.3720408082008362
  - 0.3834978938102722
  - 0.39493465423583984
  - 0.3764113783836365
  - 0.400522917509079
  - 0.3761044144630432
  - 0.394598126411438
  - 0.3812572658061981
  - 0.3762016296386719
  - 0.37333786487579346
  - 0.37205538153648376
  - 0.381529301404953
  - 0.3871307373046875
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 93 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 55 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 32 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.023255813953488372, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.004651162790697674
  total_train_time: '0:18:14.721829'
