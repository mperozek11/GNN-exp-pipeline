config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 07:45:21.548015'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/152/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/152/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/152/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/152/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/152/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/152/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/152/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/152/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/152/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/152/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 354.40380784869194
  - 206.47208261489868
  - 151.48781204223633
  - 189.6636199504137
  - 169.7654954791069
  - 120.6961370408535
  - 133.8181229531765
  - 120.97870225459337
  - 87.31903582811356
  - 128.1386710256338
  - 58.9502771794796
  - 63.57539214938879
  - 56.09543390572071
  - 64.70269440114498
  - 51.22834034264088
  - 43.95223289728165
  - 42.87074992060661
  - 36.08617681264877
  - 37.46040418744087
  - 36.783937245607376
  - 32.60248053073883
  - 34.979064494371414
  - 34.4121316075325
  - 35.634256675839424
  - 35.00537332892418
  - 34.61731897294521
  - 32.70740991830826
  - 34.586170099675655
  - 35.009509578347206
  - 35.64547747373581
  - 36.7297767996788
  - 32.395723447203636
  - 33.06664781272411
  - 31.112891122698784
  - 38.46406778693199
  - 35.20743988454342
  - 36.26190114021301
  - 33.122768610715866
  - 33.73663388192654
  - 35.48997722566128
  - 32.65867719054222
  - 31.64279356598854
  - 32.30032414197922
  - 33.45517352223396
  - 32.16444467008114
  - 34.30298286676407
  - 33.693330869078636
  - 32.66477061808109
  - 32.74176152050495
  - 31.204595759510994
  - 31.635109037160873
  - 32.158671438694
  - 32.85043066740036
  - 32.27199748158455
  - 33.62283782660961
  - 31.83945642411709
  - 32.390770599246025
  - 32.53332258760929
  - 35.31870302557945
  - 37.787943959236145
  - 39.57574115693569
  - 35.8770112991333
  - 34.25852030515671
  - 32.69447189569473
  - 33.06648863852024
  - 31.97061252593994
  - 32.057290367782116
  - 31.321519896388054
  - 32.315074518322945
  - 33.4203465282917
  - 31.637533217668533
  - 32.19756641983986
  - 31.77295894920826
  - 31.571717381477356
  - 32.4945043772459
  - 35.12134063243866
  - 32.45067575573921
  - 31.968808010220528
  - 32.8491967022419
  - 34.435387596488
  - 33.430298417806625
  - 32.23705135285854
  - 31.55984303355217
  - 32.551561281085014
  - 33.25182628631592
  - 32.77050593495369
  - 31.72154702246189
  - 33.268016546964645
  - 31.717895969748497
  - 32.183751314878464
  - 31.29352216422558
  - 31.853302001953125
  - 32.65228119492531
  - 33.19744561612606
  - 32.19907231628895
  - 33.897580742836
  - 32.40754732489586
  - 38.861998692154884
  - 36.54186089336872
  - 36.43079096078873
  validation_losses:
  - 1.7295879125595093
  - 1.8800712823867798
  - 0.5741304159164429
  - 0.8287258744239807
  - 0.5069279670715332
  - 0.6933952569961548
  - 0.46364709734916687
  - 0.4433903098106384
  - 0.4669436514377594
  - 0.47568032145500183
  - 0.40513357520103455
  - 0.4124777019023895
  - 0.43554171919822693
  - 0.4031737744808197
  - 0.4136618673801422
  - 0.4316026270389557
  - 0.41735970973968506
  - 0.480472594499588
  - 0.41137784719467163
  - 0.3948674201965332
  - 0.39637625217437744
  - 0.3963913917541504
  - 0.466450572013855
  - 0.4172156751155853
  - 0.4328238070011139
  - 0.44895902276039124
  - 0.4108840227127075
  - 0.4075463116168976
  - 0.7947474122047424
  - 0.5552608966827393
  - 0.44418320059776306
  - 0.40213537216186523
  - 0.4195115566253662
  - 0.4325731098651886
  - 0.5297150015830994
  - 0.46869152784347534
  - 0.4185924232006073
  - 0.40812158584594727
  - 0.432521790266037
  - 0.4303036332130432
  - 0.4132956862449646
  - 0.4048883318901062
  - 0.4212033748626709
  - 0.4125611484050751
  - 0.8506420254707336
  - 0.46657273173332214
  - 0.45188474655151367
  - 0.4091067314147949
  - 0.4252775311470032
  - 0.41184201836586
  - 0.409251868724823
  - 0.4603902995586395
  - 0.40852686762809753
  - 0.4246719479560852
  - 0.42774149775505066
  - 0.39903515577316284
  - 0.4099714756011963
  - 0.5230440497398376
  - 6.316716194152832
  - 1.4950515031814575
  - 1.3688743114471436
  - 6050.6484375
  - 0.4154165983200073
  - 0.45980820059776306
  - 0.44605204463005066
  - 0.4295021593570709
  - 0.41019684076309204
  - 0.4025215208530426
  - 0.44714340567588806
  - 0.4294244050979614
  - 0.4492605924606323
  - 0.4153079688549042
  - 0.41290825605392456
  - 0.41835978627204895
  - 0.5979381203651428
  - 0.4166446328163147
  - 0.4216940701007843
  - 0.41079387068748474
  - 0.42129644751548767
  - 0.417941689491272
  - 0.40758445858955383
  - 0.4110870659351349
  - 0.4223410189151764
  - 0.40643447637557983
  - 0.4689171016216278
  - 0.448469340801239
  - 0.4115166664123535
  - 0.4104527533054352
  - 0.4195702075958252
  - 0.44016143679618835
  - 0.4607200622558594
  - 0.4587413966655731
  - 0.4091174006462097
  - 0.4081212282180786
  - 0.41051363945007324
  - 0.43125584721565247
  - 0.42486730217933655
  - 0.5036988258361816
  - 0.40893974900245667
  - 0.39288070797920227
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 42 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 61 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:10:18.625546'
