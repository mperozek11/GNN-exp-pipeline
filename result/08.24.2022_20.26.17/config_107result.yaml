config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:54:00.185581'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_107fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_107fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_107fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_107fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_107fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_107fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_107fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_107fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_107fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_107fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 10.427894830703735
  - 4.003288102149964
  - 2.5581005573272706
  - 3.2619258761405945
  - 3.094571352005005
  - 7.1995129704475405
  - 6.045663928985596
  - 6.842338311672211
  - 3.861471557617188
  - 2.0783663034439086
  - 1.8334046423435213
  - 1.314442402124405
  - 0.9671711146831513
  - 2.7978163063526154
  - 3.25068553686142
  - 2.050792706012726
  - 2.6020244836807254
  - 1.6396268010139465
  - 1.7533846616744997
  - 1.905383861064911
  - 1.2834806740283966
  - 3.455197900533676
  - 2.4446137845516205
  - 1.6725955843925477
  - 1.1632443130016328
  - 1.3525215268135071
  - 3.97874253988266
  - 2.628707653284073
  - 3.7498820781707765
  - 1.395737498998642
  - 1.4619705259799958
  - 1.7836611509323121
  - 1.3555365681648255
  - 1.121149045228958
  - 1.4412442803382874
  - 6.143678605556488
  - 1.5279103338718416
  - 1.9378550946712494
  - 1.2515891671180726
  - 1.2302944123744965
  - 1.1758201241493225
  - 4.231225752830506
  - 2.2739773988723755
  - 3.10948275923729
  - 1.2948681592941285
  - 1.2997549414634706
  - 1.1352105021476746
  - 2.8826307117938996
  - 2.319071054458618
  - 1.8766594648361208
  - 3.50788334608078
  - 1.7105623722076417
  - 1.7174020648002626
  - 3.4959428012371063
  - 1.2190337717533113
  - 1.8925099849700928
  - 2.6800836205482486
  - 5.441677004098892
  - 1.2551936924457552
  - 1.4812592953443529
  - 1.410169678926468
  - 1.0517810583114624
  - 1.2746395349502564
  - 0.9889260828495026
  - 0.9727788031101228
  - 0.8705692231655121
  - 0.858913242816925
  - 0.8753252565860749
  - 0.852169531583786
  - 0.8564133286476135
  validation_losses:
  - 7.2017364501953125
  - 2.8428256511688232
  - 6.602695465087891
  - 1.511247992515564
  - 2.3483762741088867
  - 3.441786050796509
  - 5.17103385925293
  - 1.3919191360473633
  - 0.8159207105636597
  - 0.9072643518447876
  - 0.6432064771652222
  - 0.4685984253883362
  - 0.6926480531692505
  - 2.0372533798217773
  - 1.6941032409667969
  - 0.5306720733642578
  - 0.6137410998344421
  - 0.7628412246704102
  - 0.502804160118103
  - 0.6914598941802979
  - 0.4874250888824463
  - 0.6433927416801453
  - 0.9795145988464355
  - 0.9567945599555969
  - 2.242597818374634
  - 0.4380335807800293
  - 0.47679221630096436
  - 1.0065524578094482
  - 0.594933032989502
  - 1.0312660932540894
  - 0.5107528567314148
  - 0.4467317759990692
  - 0.45079028606414795
  - 0.53476482629776
  - 0.46683213114738464
  - 0.5865243077278137
  - 0.5331717729568481
  - 0.601203203201294
  - 0.48036348819732666
  - 0.38759884238243103
  - 0.5805606245994568
  - 0.5960144996643066
  - 0.4287360906600952
  - 0.5087865591049194
  - 0.810370922088623
  - 0.4209558665752411
  - 0.4187989830970764
  - 0.6566338539123535
  - 2.3011207580566406
  - 0.519548773765564
  - 0.6186120510101318
  - 0.49452418088912964
  - 0.7566601634025574
  - 0.8415545225143433
  - 0.45904338359832764
  - 0.4697972536087036
  - 0.4251880645751953
  - 0.42663612961769104
  - 0.4779201149940491
  - 0.38895970582962036
  - 0.3974158465862274
  - 0.4182485044002533
  - 0.3955392837524414
  - 0.4080177843570709
  - 0.3860843777656555
  - 0.39147916436195374
  - 0.39160794019699097
  - 0.3945750296115875
  - 0.39658641815185547
  - 0.38597139716148376
loss_records_fold1:
  train_losses:
  - 0.9060367166996003
  - 0.887339586019516
  - 0.7887260138988496
  - 0.7679692327976227
  - 0.7762803256511689
  - 0.868767648935318
  - 0.8022815704345704
  - 0.8341091573238373
  - 0.8126962065696717
  - 0.8299495935440064
  - 0.9659817636013032
  - 0.8469812333583833
  - 0.8807132452726365
  - 0.9890925586223602
  - 0.9676648437976838
  - 0.9116130530834199
  validation_losses:
  - 0.3998168706893921
  - 0.3993786871433258
  - 0.3996741771697998
  - 0.39871084690093994
  - 0.40292567014694214
  - 0.3976874351501465
  - 0.41550779342651367
  - 0.40037816762924194
  - 0.40210187435150146
  - 0.4537737965583801
  - 0.39786750078201294
  - 0.3975607752799988
  - 0.40422898530960083
  - 0.4007292687892914
  - 0.3993801474571228
  - 0.3971995711326599
loss_records_fold2:
  train_losses:
  - 0.8548007547855377
  - 0.9580625832080841
  - 0.8164854168891907
  - 1.0954995036125184
  - 0.8213604331016541
  - 0.8597810685634614
  - 0.8435784578323364
  - 0.8546679735183716
  - 0.9081698298454285
  - 0.7960135340690613
  - 1.3803111910820007
  - 0.9248515367507935
  - 1.045741254091263
  - 0.8778255879878998
  - 0.9549787163734437
  - 1.3535075664520264
  - 3.8358858764171604
  - 1.3848638236522675
  - 1.0152150213718414
  - 1.2829764783382416
  - 1.0109027743339538
  - 1.2354401230812073
  - 4.663245505094529
  - 1.2873502850532532
  - 1.11996186375618
  - 1.534640944004059
  - 0.9319521546363831
  - 1.1085993885993959
  - 0.8703020572662354
  - 0.9558234691619873
  - 0.8442889332771302
  - 0.8703038513660432
  - 0.9232699990272523
  - 0.8483024179935456
  - 0.8062741100788117
  - 0.8164331138134003
  - 0.8538885116577148
  - 1.6422782242298126
  - 1.0682294547557831
  - 2.1427229523658755
  - 4.047206670045853
  - 3.2035514354705814
  - 1.7592158734798433
  - 1.1400343835353852
  - 1.3383920907974245
  - 1.8942556023597719
  - 2.2859150350093844
  - 1.5842252850532532
  - 1.1177095293998718
  - 0.8412738919258118
  - 0.8727600574493408
  - 0.8193830788135529
  - 0.9302450060844422
  - 0.8222857713699341
  - 0.7474081426858903
  - 0.879928743839264
  validation_losses:
  - 0.3870070278644562
  - 0.3964564800262451
  - 0.42942410707473755
  - 0.4050010144710541
  - 0.38349854946136475
  - 0.38836053013801575
  - 0.38859933614730835
  - 0.39107444882392883
  - 0.39058172702789307
  - 0.4542154371738434
  - 0.39372897148132324
  - 0.44452962279319763
  - 0.39054861664772034
  - 0.4048862159252167
  - 0.3840145170688629
  - 0.40301409363746643
  - 0.4071948528289795
  - 0.38811782002449036
  - 0.4067980647087097
  - 0.39738398790359497
  - 0.38592395186424255
  - 0.394714891910553
  - 0.39795684814453125
  - 0.4912961721420288
  - 0.4089444577693939
  - 0.3917362689971924
  - 0.4077097475528717
  - 0.41375479102134705
  - 0.3888441026210785
  - 0.38559362292289734
  - 0.39461320638656616
  - 0.411306768655777
  - 0.40288129448890686
  - 0.39789149165153503
  - 0.3868466317653656
  - 0.38594111800193787
  - 0.39350375533103943
  - 0.41644829511642456
  - 0.38234153389930725
  - 0.3857174813747406
  - 0.38168567419052124
  - 0.38124826550483704
  - 0.38414064049720764
  - 0.5345945954322815
  - 0.5753337144851685
  - 0.725307285785675
  - 0.41242262721061707
  - 0.4423826038837433
  - 0.38505369424819946
  - 0.41422533988952637
  - 0.38468822836875916
  - 0.38317427039146423
  - 0.38100674748420715
  - 0.3806476593017578
  - 0.38260751962661743
  - 0.38127070665359497
loss_records_fold3:
  train_losses:
  - 0.7728329539299011
  - 0.7851953983306885
  - 0.7817956626415253
  - 2.1698092758655547
  - 1.013970959186554
  - 2.47092245221138
  - 1.1658066809177399
  - 1.1501378357410432
  - 1.0076397836208344
  - 0.8596057415008546
  - 1.5648718476295471
  - 1.985423505306244
  - 0.9847947478294373
  - 0.8278491079807282
  - 0.882347983121872
  - 0.7850993156433106
  - 0.9989116907119752
  - 0.9600598812103271
  - 0.9430828124284745
  - 0.9174265325069428
  - 0.8511098265647888
  - 0.941655844449997
  - 0.9510772824287415
  - 0.9662491500377656
  - 1.2913320660591125
  - 0.807796972990036
  - 0.7894958555698395
  - 0.7457031607627869
  - 0.8582511007785798
  - 0.8276678264141083
  - 0.8482945442199707
  - 0.9047382175922394
  - 1.0908766388893127
  - 0.9151150643825532
  - 1.6620749950408937
  - 1.1210641741752625
  - 0.866321361064911
  - 0.8534129679203034
  - 0.7839137047529221
  - 0.7688641786575318
  - 0.8473028242588043
  - 0.7598965108394623
  validation_losses:
  - 0.42594271898269653
  - 0.39099249243736267
  - 0.3907061517238617
  - 0.38969701528549194
  - 0.38593676686286926
  - 0.4018593430519104
  - 0.40915200114250183
  - 0.4000813364982605
  - 0.40632912516593933
  - 0.4852244257926941
  - 0.4599265158176422
  - 0.46225619316101074
  - 0.4076431393623352
  - 0.39552927017211914
  - 0.393897145986557
  - 0.4052550196647644
  - 0.3959556519985199
  - 0.5207826495170593
  - 0.44325169920921326
  - 0.3908941149711609
  - 0.4212113618850708
  - 0.3976688086986542
  - 0.42613428831100464
  - 0.40824827551841736
  - 0.39846041798591614
  - 0.41217759251594543
  - 0.4029591977596283
  - 0.39602720737457275
  - 0.436068594455719
  - 0.42692533135414124
  - 0.47422701120376587
  - 0.40118643641471863
  - 0.4324495196342468
  - 0.46840181946754456
  - 0.45306187868118286
  - 1.5675128698349
  - 0.40583622455596924
  - 0.3927254378795624
  - 0.39450803399086
  - 0.3921552896499634
  - 0.3942766487598419
  - 0.3947858214378357
loss_records_fold4:
  train_losses:
  - 0.7825502455234528
  - 0.7959265470504762
  - 0.7768181562423706
  - 0.806353110074997
  - 0.8038207173347474
  - 0.7653651118278504
  - 1.0404720604419708
  - 0.8547798335552216
  - 0.7945219159126282
  - 0.7655314028263093
  - 0.762283194065094
  - 0.8011829257011414
  - 0.7590393364429474
  - 0.8504742205142976
  - 0.8229547619819642
  - 0.9707013010978699
  - 0.9941854357719422
  - 1.847116804122925
  - 1.1197547376155854
  - 0.830166095495224
  - 0.9425288021564484
  - 1.1648936152458191
  - 0.7906230688095093
  - 0.8481050670146942
  - 0.7783053755760193
  - 0.7852380514144898
  validation_losses:
  - 0.4078007638454437
  - 0.4127641022205353
  - 0.407747745513916
  - 0.3996394872665405
  - 0.4015350937843323
  - 0.40186557173728943
  - 0.41188448667526245
  - 0.42247918248176575
  - 0.4148327708244324
  - 0.4448360800743103
  - 0.41715186834335327
  - 0.44747835397720337
  - 0.42477476596832275
  - 0.4257572889328003
  - 0.4202752411365509
  - 0.39675065875053406
  - 0.5468171834945679
  - 0.7151088714599609
  - 0.4966996908187866
  - 0.5133057236671448
  - 0.4738876521587372
  - 0.45917415618896484
  - 0.4561273753643036
  - 0.45159468054771423
  - 0.4290499985218048
  - 0.42983171343803406
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 70 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 56 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 42 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:18:04.654635'
