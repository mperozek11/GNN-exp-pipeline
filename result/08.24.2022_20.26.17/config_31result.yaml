config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:01:55.931178'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_31fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_31fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_31fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_31fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_31fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_31fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_31fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_31fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_31fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_31fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 22.3548641204834
  - 7.91711151599884
  - 5.146765542030335
  - 3.045161187648773
  - 4.086893260478973
  - 2.4262781977653507
  - 1.8892255783081056
  - 1.9378300189971924
  - 1.3824934542179108
  - 1.2702544987201692
  - 1.3168114304542542
  - 1.0708994507789613
  - 1.0426582455635072
  - 4.851796334981919
  - 1.751057243347168
  - 1.949666291475296
  - 1.9570500612258912
  - 1.4905568301677705
  - 1.1516714930534364
  - 1.3594039499759676
  - 1.0590131521224977
  - 1.040526658296585
  - 1.1158399224281312
  - 1.164232861995697
  - 4.821871775388718
  - 2.4548216581344606
  - 8.205550438165664
  - 4.962168037891388
  - 1.6473925471305848
  - 1.6118720769882202
  - 1.0870632380247116
  - 1.7196312785148622
  - 2.980698525905609
  - 1.4231341719627382
  - 1.1921929121017456
  - 1.1192429959774017
  - 5.174266958236695
  - 1.9742679476737977
  - 1.4552074909210206
  - 15.501382929086686
  - 1.324427992105484
  - 1.185530209541321
  - 3.68963053226471
  validation_losses:
  - 5.631953716278076
  - 2.6168110370635986
  - 1.511666178703308
  - 2.015749454498291
  - 1.2806687355041504
  - 1.209012508392334
  - 0.5561740398406982
  - 0.5649053454399109
  - 0.6917783617973328
  - 0.4056450128555298
  - 0.7484016418457031
  - 0.48121243715286255
  - 0.4342897832393646
  - 0.4981335699558258
  - 0.6289427876472473
  - 1.3594536781311035
  - 0.5470448732376099
  - 0.6059663891792297
  - 0.5382726192474365
  - 0.6002348065376282
  - 0.4379364252090454
  - 0.47556522488594055
  - 0.4501460790634155
  - 0.43556562066078186
  - 0.4702468812465668
  - 0.43051081895828247
  - 0.8554428219795227
  - 0.6368175148963928
  - 1.6517704725265503
  - 0.698290228843689
  - 0.4990147650241852
  - 0.4884534180164337
  - 0.4718518555164337
  - 0.3850611746311188
  - 0.40996918082237244
  - 0.607894241809845
  - 0.9063867330551147
  - 0.6361067295074463
  - 0.49789780378341675
  - 0.45259392261505127
  - 0.4276050925254822
  - 0.42435529828071594
  - 0.4045906960964203
loss_records_fold1:
  train_losses:
  - 1.3532412588596345
  - 1.1344568967819215
  - 1.0027809351682664
  - 1.1607434749603271
  - 0.9086606323719025
  - 1.1213448226451874
  - 1.165639293193817
  - 1.2270493149757387
  - 0.9570254325866699
  - 0.8278927683830262
  - 1.5101670742034914
  - 0.9880755722522736
  - 1.0635680913925172
  - 0.8470622718334199
  - 0.8566524267196656
  - 0.8382998585700989
  - 0.9410796225070954
  - 0.960971349477768
  - 1.009542989730835
  - 0.7814544767141343
  - 1.1726990640163422
  - 0.8827281653881074
  - 0.7979818999767304
  - 0.7978778541088105
  - 0.8434666216373444
  - 0.8412178337574006
  - 0.8711424112319947
  - 0.8658700823783875
  - 0.7925747752189637
  - 0.8239627540111543
  - 0.8691817104816437
  - 0.8082602173089981
  - 1.1145032465457916
  - 1.1030777215957641
  - 1.055011761188507
  - 1.721693903207779
  - 1.3495942533016205
  - 1.6549695849418642
  - 1.1926562130451204
  - 0.9428686380386353
  - 0.8780853241682053
  - 1.0345123171806336
  - 0.9482015073299408
  - 1.071466737985611
  - 4.634471493959427
  - 1.0482480347156524
  - 0.9533281683921815
  - 1.3665194511413574
  - 1.0019059538841248
  - 0.990562653541565
  - 0.9908871173858643
  - 1.1249301314353943
  - 0.8339880585670472
  - 0.8660918116569519
  - 0.9736970663070679
  - 0.8937277495861053
  - 1.2416664481163027
  - 0.8220949590206147
  - 0.90685892701149
  - 0.9231346249580383
  - 0.8870381891727448
  - 1.1454076051712037
  - 1.0022909104824067
  - 0.9831790626049042
  - 0.8612178146839142
  - 0.8672990858554841
  - 0.8279016017913818
  - 0.8311676859855652
  - 0.9325961112976074
  validation_losses:
  - 0.4131721258163452
  - 0.42151930928230286
  - 0.42993491888046265
  - 0.40140995383262634
  - 0.4042876064777374
  - 0.5082939267158508
  - 0.41569727659225464
  - 0.4328879415988922
  - 0.42143499851226807
  - 0.4026522934436798
  - 0.41741782426834106
  - 0.45041486620903015
  - 0.41517481207847595
  - 0.436073899269104
  - 0.4121616780757904
  - 0.3957497477531433
  - 0.41833847761154175
  - 0.4070455729961395
  - 0.41410374641418457
  - 0.43511298298835754
  - 0.4247494339942932
  - 0.3929150104522705
  - 0.41272416710853577
  - 0.3999343812465668
  - 0.4041508436203003
  - 0.3975657820701599
  - 0.41153326630592346
  - 0.4081098437309265
  - 0.3972632884979248
  - 0.39961108565330505
  - 0.4421856999397278
  - 0.4042113423347473
  - 0.4028908908367157
  - 0.4673546850681305
  - 0.4736597239971161
  - 0.451543390750885
  - 0.4837120473384857
  - 0.7042180895805359
  - 0.4546932280063629
  - 0.45869287848472595
  - 0.4094332754611969
  - 0.43119722604751587
  - 0.4648210406303406
  - 0.40897998213768005
  - 0.42301633954048157
  - 0.421634703874588
  - 0.4110417664051056
  - 0.4919976592063904
  - 0.42862823605537415
  - 0.4208083748817444
  - 0.4049062132835388
  - 0.3925512731075287
  - 0.4069516956806183
  - 0.408511221408844
  - 0.49447396397590637
  - 0.4261094033718109
  - 0.4088066518306732
  - 0.403560996055603
  - 0.40566498041152954
  - 0.4019082188606262
  - 0.5178515315055847
  - 0.41297322511672974
  - 0.4273097813129425
  - 0.416003555059433
  - 0.4152787923812866
  - 0.40500056743621826
  - 0.4091731011867523
  - 0.4030759334564209
  - 0.4054659903049469
loss_records_fold2:
  train_losses:
  - 0.9008379518985749
  - 0.9581385552883148
  - 0.899426156282425
  - 0.8437687575817109
  - 0.8642980039119721
  - 1.0289530336856842
  - 0.9487873554229737
  - 0.8775239169597626
  - 0.8500957190990448
  - 0.876042515039444
  - 0.8163928091526031
  - 0.8862987220287324
  - 0.8925812363624573
  - 0.922088348865509
  - 0.8482011556625366
  - 0.9165259599685669
  - 1.240144431591034
  - 0.8154364049434663
  - 0.8307910561561584
  - 0.8339407920837403
  - 0.8924554824829102
  - 0.9441491305828095
  - 0.8716129839420319
  - 0.8394526064395905
  - 0.8523799657821656
  - 0.9817140102386475
  - 0.8177906036376954
  - 2.7807621479034426
  - 1.232526695728302
  - 1.7473674774169923
  - 0.9077553272247315
  - 3.1235623180866243
  - 1.52428195476532
  - 1.352979600429535
  - 0.9474719643592835
  - 4.810053390264511
  - 1.4388237357139588
  - 2.7580874502658848
  - 1.1584779143333435
  - 1.142390924692154
  - 0.9443086981773376
  - 0.9736312687397004
  - 0.9745880126953126
  - 0.9223557233810425
  - 1.1742917478084565
  validation_losses:
  - 0.4152190387248993
  - 0.39412206411361694
  - 0.4046274721622467
  - 0.4126090109348297
  - 0.38975414633750916
  - 0.4927855134010315
  - 0.3866516053676605
  - 0.3888168931007385
  - 0.3883238732814789
  - 0.3856981694698334
  - 0.38958901166915894
  - 0.40635737776756287
  - 0.393718421459198
  - 0.4116748571395874
  - 0.39201438426971436
  - 0.4088161289691925
  - 0.3947421908378601
  - 0.39617016911506653
  - 0.3909256160259247
  - 0.3952503204345703
  - 0.417104572057724
  - 0.412572979927063
  - 0.384356826543808
  - 0.3910829722881317
  - 0.3842740058898926
  - 0.39463290572166443
  - 0.40078818798065186
  - 0.40558910369873047
  - 1.5162180662155151
  - 0.47276219725608826
  - 0.4171302020549774
  - 0.4224031865596771
  - 0.5879421830177307
  - 0.4298594295978546
  - 0.41455844044685364
  - 0.40256592631340027
  - 0.5925803780555725
  - 0.4100726246833801
  - 0.49329012632369995
  - 0.4137949049472809
  - 0.3991258144378662
  - 0.40056556463241577
  - 0.3964312970638275
  - 0.38914769887924194
  - 0.397010862827301
loss_records_fold3:
  train_losses:
  - 0.8798176348209381
  - 0.8812687397003174
  - 1.0503372251987457
  - 0.8519369542598725
  - 1.9342814445495606
  - 5.101896578073502
  - 1.3311548292636872
  - 0.8352301985025407
  - 0.8661385238170625
  - 0.832576659321785
  - 0.9015872180461884
  - 0.8700587034225464
  - 0.9213743090629578
  - 1.0331706404685974
  - 0.8265368521213532
  validation_losses:
  - 0.4051888883113861
  - 0.40187835693359375
  - 0.40167662501335144
  - 0.4075080454349518
  - 0.418435275554657
  - 0.41018417477607727
  - 0.42573535442352295
  - 0.4025576114654541
  - 0.5030636787414551
  - 0.4235416054725647
  - 0.3949686586856842
  - 0.4040484130382538
  - 0.40739375352859497
  - 0.40777331590652466
  - 0.4116407334804535
loss_records_fold4:
  train_losses:
  - 0.8454829752445221
  - 1.2744280874729157
  - 0.8368690371513368
  - 0.8250062942504883
  - 0.9578565418720246
  - 0.9583780765533447
  - 0.8530343055725098
  - 0.9933975815773011
  - 0.8651944994926453
  - 1.1268218815326692
  - 0.8802186131477356
  - 0.8523089051246644
  - 0.8635692358016969
  - 0.8486820578575135
  - 0.8711803257465363
  - 0.8637820899486542
  - 0.8501015663146974
  - 0.843819272518158
  - 0.8268035590648651
  - 0.8350464224815369
  - 1.6630644023418428
  - 1.874076396226883
  - 0.9013485550880432
  - 0.9670379519462586
  - 0.9036539852619172
  - 0.8449513077735902
  - 0.8578051388263703
  - 0.9396070539951324
  - 0.8642407298088074
  - 0.836005562543869
  - 0.8871206820011139
  - 3.4460080802440647
  - 0.8709545373916626
  - 0.8371622264385223
  - 0.8885367333889008
  - 0.8383634924888611
  - 0.8429294049739838
  - 1.1908794403076173
  - 0.9185477077960968
  - 0.8322907477617264
  - 0.8566719412803651
  - 0.8501747906208039
  - 0.8288037776947021
  - 0.8359832048416138
  - 0.8554392457008362
  - 0.8256567120552063
  - 1.3435314059257508
  - 0.9182012975215912
  - 1.1358230888843537
  - 0.8852569222450257
  - 1.0316145718097687
  - 0.8558616936206818
  - 0.8218331634998322
  - 0.833865487575531
  - 0.8172994285821915
  - 0.8764604926109314
  - 0.8501408636569977
  - 0.8608294725418091
  - 0.8777361452579499
  - 1.0508695840835571
  - 0.8406809747219086
  - 0.8492764472961426
  validation_losses:
  - 0.41720911860466003
  - 0.4380829930305481
  - 0.4625397324562073
  - 0.40906599164009094
  - 0.40951186418533325
  - 0.40964412689208984
  - 0.4261389374732971
  - 0.4378272593021393
  - 0.48103538155555725
  - 0.4180488884449005
  - 0.46619051694869995
  - 0.4557608366012573
  - 0.4000687003135681
  - 0.4235461354255676
  - 0.4510319232940674
  - 0.40120190382003784
  - 0.42817866802215576
  - 0.44194838404655457
  - 0.4355008006095886
  - 0.4439947009086609
  - 0.43118196725845337
  - 0.4346127510070801
  - 0.4527013897895813
  - 0.4900267422199249
  - 0.5068025588989258
  - 0.5556851625442505
  - 0.418494313955307
  - 0.39121124148368835
  - 0.45472344756126404
  - 0.4115065336227417
  - 0.4628186821937561
  - 0.4639844298362732
  - 0.4097527265548706
  - 0.4183206856250763
  - 0.40946274995803833
  - 0.41927990317344666
  - 0.4298345446586609
  - 0.4373651444911957
  - 0.41593948006629944
  - 0.42633676528930664
  - 0.4335576295852661
  - 0.40953296422958374
  - 0.4374709725379944
  - 0.42865055799484253
  - 0.41825684905052185
  - 0.41164395213127136
  - 0.42804282903671265
  - 0.4507196843624115
  - 0.42534521222114563
  - 0.40859776735305786
  - 0.45069974660873413
  - 0.4125504195690155
  - 0.4404080808162689
  - 0.4579046368598938
  - 0.4282090663909912
  - 0.44746676087379456
  - 0.4387129545211792
  - 0.44143038988113403
  - 0.4164181053638458
  - 0.4175574779510498
  - 0.40776383876800537
  - 0.411332905292511
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 43 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 69 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 62 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.023809523809523808]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0047619047619047615
  total_train_time: '0:18:53.901829'
