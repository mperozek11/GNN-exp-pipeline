config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 10:15:54.105139'
fold_0_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_6fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_6fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_6fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_6fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_6fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_6fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_6fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_6fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_6fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_6fold_4_state_dict.pt
loss_records:
  train_losses:
  - 14.168300539255142
  - 14.253894433379173
  - 14.04331162571907
  - 13.968691676855087
  - 14.213301926851273
  - 14.16206642985344
  - 14.240144312381744
  - 14.194276690483093
  - 13.896764144301414
  - 13.823904618620872
  - 13.770531728863716
  - 14.062887459993362
  - 13.82888349890709
  - 13.94517096877098
  - 13.914414063096046
  - 13.955117762088776
  - 14.097177147865295
  - 13.898715019226074
  - 14.171671330928802
  - 14.080670446157455
  - 13.950684815645218
  - 13.770743429660797
  - 13.796347737312317
  - 13.847497820854187
  - 13.949711337685585
  - 13.835642993450165
  - 13.574070736765862
  - 13.734598964452744
  - 13.622952550649643
  - 13.686722353100777
  - 13.916782960295677
  - 13.892403244972229
  - 13.76810285449028
  - 13.652554154396057
  - 13.731964811682701
  - 13.663004457950592
  - 13.77334401011467
  - 13.678352043032646
  - 13.716857060790062
  - 13.967744216322899
  - 13.988369479775429
  - 14.008004248142242
  - 14.050721943378448
  - 13.443920493125916
  - 13.694194197654724
  - 13.975626468658447
  - 13.44140538573265
  - 13.910346418619156
  - 13.722469314932823
  - 13.705191031098366
  - 14.184899374842644
  - 14.19382257759571
  - 13.662761807441711
  - 13.590717107057571
  - 13.826852083206177
  - 13.651194781064987
  - 13.70546904206276
  - 13.78785189986229
  - 13.562208741903305
  - 13.757499396800995
  - 13.942180201411247
  - 13.584356382489204
  - 13.585543975234032
  - 13.850931316614151
  - 13.86705070734024
  - 13.409352228045464
  - 13.714269384741783
  - 13.60489647090435
  - 13.99811489880085
  - 13.806483089923859
  - 13.845284774899483
  - 13.654327645897865
  - 13.703685447573662
  - 13.741492807865143
  - 13.655786722898483
  - 13.848606377840042
  - 13.792055547237396
  - 13.529132664203644
  - 14.044788271188736
  - 13.595910042524338
  - 13.67022742331028
  - 13.636997148394585
  - 13.327158018946648
  - 13.966294392943382
  - 13.765250653028488
  - 13.672323793172836
  - 13.702874571084976
  - 13.765396192669868
  - 13.53939414024353
  - 13.439092829823494
  - 13.747901320457458
  - 13.606578782200813
  - 13.787884891033173
  - 13.429861903190613
  - 13.434701174497604
  - 13.736335501074791
  - 13.283403426408768
  - 13.748677760362625
  - 13.663035988807678
  - 13.306871443986893
  validation_losses:
  - 0.38953539729118347
  - 0.3868334889411926
  - 0.39628133177757263
  - 0.42111870646476746
  - 0.3816338777542114
  - 0.3815560042858124
  - 0.4218039810657501
  - 0.3810274004936218
  - 0.4014914035797119
  - 0.4815811216831207
  - 0.4168853759765625
  - 0.45657601952552795
  - 1.0415252447128296
  - 0.5585396885871887
  - 0.5859569907188416
  - 0.41559621691703796
  - 0.6558284163475037
  - 0.42495524883270264
  - 0.38479676842689514
  - 0.3941274881362915
  - 0.44563448429107666
  - 0.3901042640209198
  - 0.43508049845695496
  - 0.4981681704521179
  - 0.47196221351623535
  - 0.5545629262924194
  - 0.6261788606643677
  - 0.4971730411052704
  - 0.5703437924385071
  - 0.38170158863067627
  - 0.4868631064891815
  - 0.46983423829078674
  - 0.6014383435249329
  - 0.42060327529907227
  - 0.5931798815727234
  - 0.5663933753967285
  - 0.44880443811416626
  - 0.4476741552352905
  - 0.5032601952552795
  - 0.41803470253944397
  - 0.45098984241485596
  - 0.4300891160964966
  - 0.5219234824180603
  - 0.7453660368919373
  - 0.8239657282829285
  - 0.9764363765716553
  - 1.349591612815857
  - 0.6505414247512817
  - 0.7673184275627136
  - 0.6150630712509155
  - 0.40294814109802246
  - 0.40611496567726135
  - 0.5343475341796875
  - 0.6069713234901428
  - 0.6818737387657166
  - 0.6042240858078003
  - 0.5089510083198547
  - 0.6050722599029541
  - 0.5732781887054443
  - 0.5429161190986633
  - 0.47382649779319763
  - 0.6101619005203247
  - 0.7305589318275452
  - 0.711759626865387
  - 0.760886013507843
  - 0.7840781211853027
  - 0.7166597247123718
  - 0.7556653022766113
  - 0.6194405555725098
  - 0.8204383850097656
  - 0.8191648721694946
  - 0.5611969232559204
  - 0.825724720954895
  - 0.6462633609771729
  - 0.5247988104820251
  - 0.8212339878082275
  - 0.4870609641075134
  - 0.5266411900520325
  - 0.4780530631542206
  - 0.618674635887146
  - 0.9613441228866577
  - 2.0296177864074707
  - 1.9844169616699219
  - 1.1321985721588135
  - 0.633718729019165
  - 0.9926959276199341
  - 1.502388834953308
  - 0.7221356630325317
  - 0.6337923407554626
  - 0.948052704334259
  - 0.6835867762565613
  - 0.6983924508094788
  - 0.602648913860321
  - 0.7074991464614868
  - 0.6512957811355591
  - 0.6349881887435913
  - 0.5187737345695496
  - 0.5701753497123718
  - 0.5892075300216675
  - 0.6955078840255737
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 69 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8130360205831904, 0.8576329331046312, 0.8610634648370498,
    0.8608247422680413]'
  fold_eval_f1: '[0.0, 0.11382113821138211, 0.023529411764705882, 0.047058823529411764,
    0.024096385542168676]'
  mean_eval_accuracy: 0.8500380187795088
  mean_f1_accuracy: 0.04170115180953368
  total_train_time: '0:11:21.580458'
