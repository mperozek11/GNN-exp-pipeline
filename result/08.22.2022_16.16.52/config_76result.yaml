config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:54:34.625365'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_76fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_76fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_76fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_76fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_76fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_76fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_76fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_76fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_76fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_76fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 76.11081218123437
  - 48.20488167107106
  - 30.838625562191012
  - 31.740027895569803
  - 43.78622769415379
  - 37.93045725971461
  - 24.797761872410774
  - 17.587395183742046
  - 31.73281511962414
  - 21.607562386989596
  - 12.02820689380169
  - 14.983419254422188
  - 12.323322597146035
  - 12.007159948348999
  - 12.278622016310692
  - 10.41310329437256
  - 10.948402082920076
  - 8.69766769707203
  - 7.681870239973069
  - 7.683259499073029
  - 7.949443370103836
  - 7.608450302481652
  - 6.692824617028236
  - 6.616780003905297
  - 7.454584851861
  - 6.987839451432229
  - 6.927101129293442
  - 7.261222764849663
  - 7.144652891159058
  - 6.894692397117615
  - 6.907113364338875
  - 6.397420826554299
  - 6.557468464970589
  - 6.336655792593956
  - 7.030139756202698
  - 6.554292619228363
  - 6.6227610379457476
  - 6.708178243041039
  - 6.768384042382241
  - 7.0815420180559165
  - 6.559295958280564
  - 6.443231576681137
  - 6.488687136769295
  - 6.730166524648666
  - 6.421947512030602
  - 6.853227132558823
  - 7.446403723955155
  - 7.219830510020256
  - 8.24043899178505
  - 6.616657820343971
  - 6.717996114492417
  - 6.557962602376938
  - 6.5793245315551765
  - 6.449627631902695
  - 6.824931426346303
  - 6.323294693231583
  - 6.585041841864586
  - 6.536300301551819
  - 6.439155161380768
  - 6.828195902705193
  - 6.607173112034798
  - 6.732836276292801
  - 6.700825190544129
  - 6.771094000339509
  - 6.606141829490662
  - 6.414652532339097
  - 6.429512906074525
  - 6.47933806180954
  - 7.246350294351578
  - 6.682616677880287
  - 6.326028108596802
  - 6.459358179569245
  - 6.432524228096009
  - 6.389931297302247
  - 6.5141593486070635
  - 7.3985108792781835
  - 11.051299566030503
  - 7.158292049169541
  - 7.2995834171772005
  - 7.088874658942223
  - 6.752132526040078
  - 6.469802701473236
  - 6.297292783856392
  - 6.626141783595085
  - 7.139744314551354
  - 6.589138823747636
  - 6.439060348272324
  - 6.65860123038292
  - 6.411927095055581
  - 6.409150794148445
  - 6.257215014100075
  - 6.3824301123619085
  - 6.5263106524944305
  - 6.718526002764702
  - 6.470229408144951
  - 6.845750558376313
  - 6.466381633281708
  - 6.67213414311409
  - 6.580925861001015
  - 6.450387051701546
  validation_losses:
  - 7.06043004989624
  - 0.8776487112045288
  - 0.536146879196167
  - 0.6329800486564636
  - 3.2968602180480957
  - 1.1475977897644043
  - 0.5011270046234131
  - 0.46261823177337646
  - 0.5136556029319763
  - 0.5567595958709717
  - 0.4140138626098633
  - 0.4492843449115753
  - 0.4552455246448517
  - 0.5528317093849182
  - 0.6068882942199707
  - 0.45003724098205566
  - 0.4421023428440094
  - 0.543847918510437
  - 0.4271196722984314
  - 0.4228312075138092
  - 0.3956405222415924
  - 0.4321858584880829
  - 0.49282968044281006
  - 0.40720701217651367
  - 0.40933656692504883
  - 0.43941035866737366
  - 0.3938750922679901
  - 0.4126502275466919
  - 0.4354390501976013
  - 0.5928502678871155
  - 0.4344216585159302
  - 0.38622838258743286
  - 0.4063282907009125
  - 0.42384690046310425
  - 0.48743751645088196
  - 0.44008615612983704
  - 0.42314383387565613
  - 0.4099283516407013
  - 0.45493757724761963
  - 0.4256976842880249
  - 0.4114929735660553
  - 0.4381483793258667
  - 0.41851791739463806
  - 0.4141020178794861
  - 0.41613587737083435
  - 0.46401146054267883
  - 0.5917076468467712
  - 0.40927812457084656
  - 1.5313928127288818
  - 0.4061766266822815
  - 0.4126456379890442
  - 0.4619447588920593
  - 0.40740329027175903
  - 0.4208517074584961
  - 0.4174210727214813
  - 0.3975740075111389
  - 0.4074775278568268
  - 0.45056435465812683
  - 0.40265125036239624
  - 0.9525725841522217
  - 0.41396939754486084
  - 1.031653881072998
  - 0.4377308189868927
  - 1.4101370573043823
  - 90612.9453125
  - 0.43312543630599976
  - 0.4095933437347412
  - 0.4144424498081207
  - 154867.84375
  - 0.4332447350025177
  - 0.44979310035705566
  - 5031989.0
  - 55.168697357177734
  - 0.41853979229927063
  - 0.5980860590934753
  - 25.828025817871094
  - 0.42224007844924927
  - 0.41058385372161865
  - 0.4208122491836548
  - 0.41918882727622986
  - 0.40766391158103943
  - 0.4120712876319885
  - 0.42323222756385803
  - 0.42084386944770813
  - 0.435977578163147
  - 0.44780731201171875
  - 0.4123559892177582
  - 0.4112705886363983
  - 16.199132919311523
  - 0.4397216737270355
  - 0.4594283401966095
  - 0.45980003476142883
  - 0.4091099202632904
  - 0.4162744879722595
  - 0.41084739565849304
  - 0.43252015113830566
  - 0.4244704842567444
  - 0.43325942754745483
  - 0.4119889438152313
  - 0.4040127396583557
loss_records_fold1:
  train_losses:
  - 6.5212819457054145
  - 6.443893551826477
  - 6.251727133989334
  - 7.420248007774354
  - 7.0842403918504715
  - 6.467662197351456
  - 6.441458907723427
  - 6.967276656627655
  - 6.513768288493157
  - 6.770754036307335
  - 6.536316823959351
  - 6.301909223198891
  - 6.354443624615669
  - 6.428956294059754
  - 6.708379310369492
  - 6.39469578564167
  - 6.555266630649567
  - 6.901312685012818
  - 6.3714437097311025
  - 6.471287199854851
  - 6.440877094864845
  - 6.310351446270943
  - 6.423501750826836
  - 6.431325998902321
  - 6.473144152760506
  - 6.27398536503315
  - 6.2070877939462665
  - 6.5517552345991135
  - 6.594881701469422
  - 6.394659554958344
  - 6.374543496966362
  - 6.2946110546588905
  - 6.304266318678856
  - 6.6206241905689245
  - 6.445287394523621
  - 6.43414463698864
  - 6.665037244558334
  - 6.47126613855362
  - 6.4238792866468435
  - 6.461986181139946
  - 6.470481562614442
  - 8.11886725127697
  - 7.065810334682465
  - 6.676613396406174
  - 6.553749924898148
  - 6.483096626400948
  - 6.533222416043282
  - 6.338823464512825
  - 6.372951963543892
  - 6.588587722182274
  - 6.4040739923715595
  - 6.639092525839806
  - 6.309827369451523
  - 6.419334107637406
  - 6.870172742009164
  - 6.590020823478699
  - 6.511857098340989
  - 6.28509096801281
  - 6.278306689858437
  - 6.395109343528748
  - 6.47676520049572
  - 6.901611936092377
  - 6.3046541929245
  - 6.414936324954033
  - 6.422514563798905
  - 6.44796257019043
  - 6.744419512152672
  - 6.569419133663178
  - 6.378931948542595
  - 6.962731218338013
  - 6.243405884504319
  - 6.3294387638568885
  - 6.62769530415535
  - 6.769999301433564
  - 6.646117101609708
  - 6.995224398374558
  - 6.4979244649410255
  - 6.526411640644074
  - 6.405117034912109
  - 6.349475234746933
  - 6.3464466601610185
  - 6.493643617630005
  - 6.695113629102707
  - 6.331907090544701
  - 6.598013851046563
  - 6.588761162757874
  - 6.407539439201355
  - 6.558396247029305
  - 6.298769396543503
  - 6.4058073431253435
  - 6.550750425457955
  - 6.3285868048667915
  - 6.424405559897423
  - 6.290580880641937
  - 6.358050695061684
  - 6.540438857674599
  - 6.538342815637589
  - 6.457121369242668
  - 6.3485652118921285
  - 6.579318451881409
  validation_losses:
  - 0.44823309779167175
  - 0.42304274439811707
  - 0.4544523358345032
  - 0.4632550776004791
  - 1.273010015487671
  - 0.4372444152832031
  - 0.4211224317550659
  - 321.25103759765625
  - 3405.5380859375
  - 22154.0
  - 0.42866066098213196
  - 0.46721601486206055
  - 0.4185577929019928
  - 0.4243902862071991
  - 0.44679996371269226
  - 0.4554467499256134
  - 0.47273707389831543
  - 0.4328084886074066
  - 0.44627922773361206
  - 0.4129572808742523
  - 0.47281670570373535
  - 0.45796024799346924
  - 0.48895809054374695
  - 0.4984723627567291
  - 0.4357631802558899
  - 0.4173325300216675
  - 0.4057583212852478
  - 0.4761468768119812
  - 0.4627313017845154
  - 0.4721467196941376
  - 0.4133237302303314
  - 0.4425022304058075
  - 0.41807132959365845
  - 0.4401899576187134
  - 0.491849422454834
  - 0.4948466718196869
  - 0.4191173315048218
  - 0.42623764276504517
  - 0.4372609853744507
  - 0.4334580898284912
  - 0.4145587384700775
  - 0.4521103799343109
  - 0.48621055483818054
  - 0.46139031648635864
  - 0.41395166516304016
  - 0.46213024854660034
  - 0.44146469235420227
  - 0.42555728554725647
  - 0.43376100063323975
  - 0.415201872587204
  - 0.4117433726787567
  - 0.4562545418739319
  - 703.8045654296875
  - 250006.703125
  - 1287798.625
  - 2186711.75
  - 553479.75
  - 1069387.5
  - 360896.09375
  - 347221.21875
  - 2144394.0
  - 1133791.75
  - 1079727.0
  - 232482.421875
  - 673903.375
  - 6539421.5
  - 3062909.25
  - 1705882.0
  - 1670191.75
  - 1430179.875
  - 1277487.625
  - 1942966.5
  - 2050958.375
  - 507330.375
  - 1043549.5625
  - 2197443.5
  - 724354.75
  - 3854070.0
  - 3050865.75
  - 839317.4375
  - 3606161.5
  - 1219946.0
  - 1067314.125
  - 955949.5625
  - 482756.71875
  - 1289406.0
  - 2366300.0
  - 926851.3125
  - 1473343.25
  - 4178350.5
  - 631571.8125
  - 7654340.5
  - 7174019.5
  - 6585762.0
  - 3021386.25
  - 4610659.5
  - 149865.8125
  - 994019.125
  - 938660.9375
  - 2208043.25
loss_records_fold2:
  train_losses:
  - 7.011641111969948
  - 6.659311470389366
  - 6.7279949456453325
  - 6.53622761964798
  - 6.375531166791916
  - 6.425135779380799
  - 6.620345115661621
  - 6.579912993311883
  - 6.900745484232903
  - 6.63361531496048
  - 6.588808238506317
  - 6.348252749443055
  - 7.941241443157196
  - 7.806051775813103
  - 6.820924225449563
  - 7.266153964400292
  - 6.824350276589394
  - 6.546485576033593
  - 6.514861908555031
  - 6.532681477069855
  - 7.5190901368856435
  - 7.091318736970425
  - 7.2557249262928964
  - 6.433757087588311
  - 6.380208650231362
  - 6.655675059556962
  - 6.834262037277222
  - 6.608959147334099
  - 6.469954609870911
  - 6.526577743887902
  - 6.553764286637307
  - 6.4250501692295074
  - 6.39162358045578
  - 6.801716375350953
  - 6.757863450050355
  - 6.5424137145280845
  - 6.5865416646003725
  - 6.574753260612488
  - 6.74388512969017
  - 6.640583097934723
  - 6.356027740240098
  - 6.6901995807886125
  - 6.541700306534768
  - 6.350186517834664
  - 6.561757412552834
  - 6.8493345975875854
  - 6.603455981612206
  - 6.467475008964539
  - 6.406179437041283
  - 6.305310826003552
  - 6.929460048675537
  - 6.372532969713212
  - 6.86902138888836
  - 6.356736791133881
  - 6.533034858107567
  - 6.913975885510445
  - 6.30145366191864
  - 6.8466901838779455
  - 6.560294297337532
  - 6.496433603763581
  - 6.950129815936089
  - 6.720919066667557
  - 6.3672134488821035
  - 6.667664089798928
  - 6.602378255128861
  - 6.997563832998276
  - 6.4462736517190935
  - 6.6345780283212665
  - 6.701561656594277
  - 6.810337343811989
  - 6.604860192537308
  - 6.440465417504311
  - 6.406796425580978
  - 6.442413330078125
  - 6.508056947588921
  - 6.276462912559509
  - 6.864082831144334
  - 6.6441424101591116
  - 6.3538641989231115
  - 6.316762906312943
  - 6.626440379023553
  - 6.797340536117554
  - 6.556314414739609
  - 6.650101828575135
  - 6.878649261593819
  - 6.629212859272958
  - 6.417492413520813
  - 6.858106955885887
  - 6.814560917019844
  - 6.7241919368505485
  - 6.602534601092339
  - 6.468002071976662
  - 6.243440648913384
  - 6.586599183082581
  - 6.41598544716835
  - 6.538129901885987
  - 6.604685845971108
  - 6.756222915649414
  - 6.730432757735253
  - 6.416070342063904
  validation_losses:
  - 6260130.0
  - 6533036.0
  - 12016784.0
  - 1749171.0
  - 10732451.0
  - 5579060.5
  - 3266650.25
  - 16311621.0
  - 16550155.0
  - 2168347.0
  - 0.8480555415153503
  - 0.7982494235038757
  - 0.42688921093940735
  - 492.62554931640625
  - 65.30309295654297
  - 30101.373046875
  - 3706.255126953125
  - 4210.75634765625
  - 11.838251113891602
  - 644.82861328125
  - 3677.840087890625
  - 46887820.0
  - 277.37847900390625
  - 183.41357421875
  - 17.579113006591797
  - 351.95416259765625
  - 192.6690216064453
  - 2065.927490234375
  - 8813.986328125
  - 1042.7476806640625
  - 9489.2236328125
  - 426.50048828125
  - 501.2165222167969
  - 1144.0584716796875
  - 84.67206573486328
  - 68.88286590576172
  - 816.5189208984375
  - 435.9104309082031
  - 310.8061218261719
  - 126.95294189453125
  - 553.3113403320312
  - 510.4205627441406
  - 585.1139526367188
  - 114.2608871459961
  - 717.0474243164062
  - 72.73909759521484
  - 659.7640380859375
  - 1239.2176513671875
  - 459.9187927246094
  - 367.7539367675781
  - 92.31873321533203
  - 26.836383819580078
  - 298.9346008300781
  - 899.393798828125
  - 581.8154907226562
  - 93.78291320800781
  - 1279.2431640625
  - 160.52259826660156
  - 4365.34130859375
  - 530.1143188476562
  - 354.6405334472656
  - 640.1007690429688
  - 479.1287536621094
  - 303.9607849121094
  - 753.710205078125
  - 53.34133529663086
  - 648.7642211914062
  - 119.73695373535156
  - 2010.3079833984375
  - 1154.5439453125
  - 763.6572265625
  - 137.86361694335938
  - 85.53980255126953
  - 187.41932678222656
  - 581.8228759765625
  - 539.490478515625
  - 942.5792236328125
  - 722.7456665039062
  - 436.3131103515625
  - 473.98284912109375
  - 543.7980346679688
  - 1583.190185546875
  - 792.0567626953125
  - 757.7311401367188
  - 355.4599304199219
  - 622.19970703125
  - 1170.210205078125
  - 2103.421142578125
  - 3312.11865234375
  - 863.1417236328125
  - 301.7070007324219
  - 994.2479858398438
  - 3254.868896484375
  - 1837.4593505859375
  - 2487.8330078125
  - 285.66064453125
  - 2165.781494140625
  - 2598.65771484375
  - 910.5067749023438
  - 1905.5096435546875
loss_records_fold3:
  train_losses:
  - 6.328610056638718
  - 6.261357209086419
  - 6.395107963681221
  - 6.483336845040322
  - 6.411504569649697
  - 6.503109365701675
  - 6.329108187556267
  - 6.505802676081657
  - 6.553364944458008
  - 6.534636245667935
  - 6.718460968136788
  - 6.515314847230911
  - 6.5017392367124565
  - 6.405409488081933
  - 6.436270153522492
  - 6.414463502168656
  - 6.442845144867897
  - 6.464155486226082
  - 6.435434967279434
  - 6.642891204357148
  - 6.4548588514328005
  - 6.442351752519608
  - 6.3228727102279665
  - 6.370849549770355
  - 6.495949333906174
  - 6.409961959719658
  - 6.2759641945362095
  - 6.622349268198014
  - 6.553422522544861
  - 6.54384286403656
  - 6.36378388106823
  - 6.421076419949532
  - 6.453993406891823
  - 6.5218583166599275
  - 6.532106047868729
  - 6.2428815215826035
  - 6.625357174873352
  - 6.643242585659028
  - 6.670003691315651
  - 6.5681336462497715
  - 6.492477181553841
  - 6.329804311692715
  - 6.459080833196641
  - 6.496843868494034
  - 6.644469013810158
  - 6.5140813291072845
  - 6.351685139536858
  - 6.2964842289686205
  - 6.526722386479378
  - 6.959196650981903
  - 6.487368494272232
  - 6.849136951565743
  - 6.292930379509926
  - 6.3668314784765245
  - 6.422200372815133
  - 6.410790067911148
  - 6.491169199347496
  - 6.463594314455986
  - 6.475250187516213
  - 6.277713650465012
  - 6.382088723778725
  - 6.483479022979736
  - 6.348225617408753
  - 6.485936686396599
  - 6.595413073897362
  - 6.666474384069443
  - 7.010583052039147
  - 6.295889081060887
  - 6.529049676656723
  - 6.5836880773305895
  - 6.5431134551763535
  - 6.329442158341408
  - 6.446107739210129
  - 6.488729941844941
  - 6.39452702999115
  - 6.658140960335732
  - 6.51009042263031
  - 6.5887821316719055
  - 6.772043371200562
  - 6.372390976548195
  - 6.474597063660622
  - 6.5496516764163974
  - 6.581560444831848
  - 6.962287893891335
  - 6.774803140759468
  - 6.399119025468827
  - 6.448371207714081
  - 6.403325131535531
  - 6.192550995945931
  - 6.5147984564304355
  - 6.491419041156769
  - 6.3860814422369
  - 6.4126710325479515
  - 6.44859123826027
  - 6.718439763784409
  - 7.010103249549866
  - 6.531642365455628
  - 6.61088134944439
  - 6.79550538957119
  - 6.606531822681427
  validation_losses:
  - 1467.834716796875
  - 727.4177856445312
  - 1121.37841796875
  - 2255.14501953125
  - 817.044189453125
  - 2014.973876953125
  - 7637.39111328125
  - 631.1104736328125
  - 1398.9384765625
  - 2453.934814453125
  - 3729.416015625
  - 569.871826171875
  - 3781.3134765625
  - 2989.57763671875
  - 3730.343994140625
  - 2474.953857421875
  - 2091.60595703125
  - 3844.577392578125
  - 850.8825073242188
  - 772.998046875
  - 8951.3369140625
  - 3605.71630859375
  - 6260.34814453125
  - 2611.708984375
  - 968.4212036132812
  - 1172.9898681640625
  - 2279.29296875
  - 632.7224731445312
  - 1624.1314697265625
  - 2293.710205078125
  - 509.69940185546875
  - 891.3124389648438
  - 1443.0399169921875
  - 2109.50634765625
  - 2139.3359375
  - 707.1486206054688
  - 2606.453857421875
  - 556.0888061523438
  - 1036.602294921875
  - 666.7681274414062
  - 1598.8394775390625
  - 1133.719970703125
  - 1717.498046875
  - 665.7965698242188
  - 2790.776611328125
  - 1550.6593017578125
  - 842.3482055664062
  - 3465.660400390625
  - 985.9907836914062
  - 5070.8603515625
  - 12504.2080078125
  - 0.45503154397010803
  - 0.45471253991127014
  - 0.4997618496417999
  - 0.4452908933162689
  - 0.39687126874923706
  - 0.4276147186756134
  - 0.47278356552124023
  - 0.4174659550189972
  - 0.42894700169563293
  - 0.46710583567619324
  - 0.43276071548461914
  - 0.4478263556957245
  - 0.46396905183792114
  - 0.42499271035194397
  - 0.4470648467540741
  - 0.404176265001297
  - 0.43086037039756775
  - 0.4207978844642639
  - 0.49839603900909424
  - 0.4224036931991577
  - 0.4350462555885315
  - 0.41172465682029724
  - 0.4006756544113159
  - 0.44067686796188354
  - 0.429119735956192
  - 0.45576879382133484
  - 0.6784394979476929
  - 0.4211917519569397
  - 0.42069363594055176
  - 0.42384740710258484
  - 0.41203615069389343
  - 0.4939064383506775
  - 0.42175498604774475
  - 0.4217087924480438
  - 0.40945756435394287
  - 0.41041654348373413
  - 0.4118068814277649
  - 0.4349229633808136
  - 0.47403013706207275
  - 0.42336156964302063
  - 0.4110660254955292
  - 0.4541480541229248
  - 0.42356258630752563
  - 0.4897422790527344
  - 0.5570477843284607
  - 0.4305327534675598
  - 0.4506928324699402
  - 0.4328407347202301
  - 0.404626727104187
loss_records_fold4:
  train_losses:
  - 6.423534247279168
  - 6.462556156516076
  - 6.417588582634926
  - 6.366291347146035
  - 6.5714364707469946
  - 6.459049189090729
  - 6.415024264156819
  - 6.386033058166504
  - 6.381764158606529
  - 6.416737797856332
  - 6.517229065299034
  - 6.9714182883501055
  - 6.537755352258682
  - 6.646239903569222
  - 6.5467046976089485
  - 6.5801029235124595
  - 6.3628238290548325
  - 6.534397029876709
  - 6.348706376552582
  - 6.5732086390256885
  - 6.2795575082302095
  - 6.438474243879319
  - 6.4870524376630785
  - 6.469989165663719
  - 6.323100352287293
  - 6.482680448889733
  - 6.573327803611756
  - 6.325159257650376
  - 6.583144819736481
  - 6.483183455467224
  - 6.706191068887711
  - 6.550648292899132
  - 6.394985839724541
  - 6.326756075024605
  - 6.663610765337944
  - 6.777588909864426
  - 6.357262510061265
  - 6.74243156015873
  - 6.799937546253204
  - 6.489522877335549
  - 6.311696827411652
  - 6.583532610535622
  - 6.2219167232513435
  - 6.515217572450638
  - 6.339026284217835
  - 6.572222468256951
  - 6.506161791086197
  - 6.365621793270112
  - 6.610255259275437
  - 6.635020907223225
  - 7.288293486833573
  - 6.374526950716973
  - 6.332035180926323
  - 6.487658908963204
  - 6.674147853255272
  - 6.516600501537323
  - 7.635788202285767
  - 7.879142981767655
  - 6.487316527962685
  - 6.4117150694131855
  - 6.400640773773194
  - 6.399150270223618
  - 6.716940060257912
  - 6.467177978157998
  - 6.682680028676987
  - 6.406445294618607
  - 6.615772414207459
  - 6.51148556470871
  - 6.7310656130313875
  - 6.575423905253411
  - 6.697430995106697
  - 6.470841938257218
  - 6.758121979236603
  - 6.502332958579064
  - 6.441449809074403
  validation_losses:
  - 0.42193275690078735
  - 0.41927796602249146
  - 88452898816.0
  - 0.4048421084880829
  - 0.40124768018722534
  - 0.4965558350086212
  - 35678351360.0
  - 138445553664.0
  - 20699676672.0
  - 0.41011741757392883
  - 0.48048722743988037
  - 0.41436922550201416
  - 4718877.5
  - 0.4517662823200226
  - 0.4437808692455292
  - 0.4210687577724457
  - 20188600320.0
  - 0.40533727407455444
  - 288441827328.0
  - 0.3993281126022339
  - 0.40059730410575867
  - 0.42283377051353455
  - 0.4244577884674072
  - 0.40467530488967896
  - 0.4224105179309845
  - 0.41854143142700195
  - 586355441664.0
  - 0.415539413690567
  - 0.4680662453174591
  - 4455682560.0
  - 0.41250312328338623
  - 186187923456.0
  - 1176109580288.0
  - 0.41070136427879333
  - 0.4345579445362091
  - 0.43116632103919983
  - 257432666112.0
  - 0.5092445015907288
  - 51537309696.0
  - 0.40629762411117554
  - 0.41122016310691833
  - 0.4068200886249542
  - 0.4035617709159851
  - 0.40225502848625183
  - 913593466880.0
  - 0.4106764793395996
  - 0.40490466356277466
  - 0.4150650203227997
  - 73417736192.0
  - 1270117302272.0
  - 0.4076361060142517
  - 0.4174664616584778
  - 0.4365845322608948
  - 0.41655877232551575
  - 0.4325617551803589
  - 0.40470752120018005
  - 2359008.25
  - 802183936.0
  - 0.41401973366737366
  - 18055.462890625
  - 52070.1015625
  - 65608.8359375
  - 335533309952.0
  - 369896980480.0
  - 353837187072.0
  - 369897013248.0
  - 405648474112.0
  - 803721408.0
  - 11476984135680.0
  - 1429519204352.0
  - 759702093824.0
  - 755308756992.0
  - 748619694080.0
  - 0.43156060576438904
  - 0.40636348724365234
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 75 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8524871355060034, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.06818181818181818, 0.0, 0.0]'
  mean_eval_accuracy: 0.8575845991523874
  mean_f1_accuracy: 0.013636363636363636
  total_train_time: '0:49:25.712903'
