config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:50:09.567391'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_72fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_72fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_72fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_72fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_72fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_72fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_72fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_72fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_72fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_72fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 46.53636904358864
  - 28.493641689419746
  - 24.943931973725558
  - 25.59883273839951
  - 24.416951218247416
  - 13.842533597350121
  - 12.936871674656869
  - 12.197146259248257
  - 18.396816669404508
  - 25.025952818989754
  - 10.72610220015049
  - 9.759250976890327
  - 7.988100531697274
  - 9.095113071799279
  - 9.352319866418839
  - 7.205360943078995
  - 6.832764908671379
  - 6.8254713952541355
  - 6.30753428041935
  - 6.854286223649979
  - 6.575393798947335
  - 6.2689975455403335
  - 6.062510222196579
  - 6.206087040901185
  - 7.103445813059807
  - 6.3780839294195175
  - 6.2985715001821525
  - 6.3328480452299125
  - 6.182803341746331
  - 5.903861439228058
  - 6.106712579727173
  - 5.668030923604966
  - 5.855321192741394
  - 6.165667839348316
  - 6.188543853163719
  - 5.874793139100075
  - 5.914859449863434
  - 5.769714683294296
  - 5.994946163892746
  - 5.900034520030022
  - 5.953237262368202
  - 5.7865006059408195
  - 5.72070241868496
  - 5.7614820331335075
  - 5.8652069717645645
  - 6.021930891275407
  - 6.0279516071081165
  - 6.123333159089089
  - 5.858715631067753
  - 5.977863875031471
  - 5.905296283960343
  - 5.898239305615426
  - 5.90205080807209
  - 5.93740665614605
  - 5.871545335650445
  - 6.114985063672066
  - 5.996653628349304
  - 5.947327607870102
  - 6.362713927030564
  - 6.302790400385857
  - 6.192179298400879
  - 6.066651424765587
  - 5.958971539139748
  - 5.838455420732498
  - 5.974741968512536
  - 5.856479120254517
  - 5.8671975344419485
  - 5.893800297379494
  - 6.10549122095108
  - 5.9430796891450886
  - 6.01229448914528
  - 5.979191228747368
  - 5.915856719017029
  - 7.644337910413743
  - 8.402433052659035
  - 7.300534662604332
  - 6.275640678405762
  - 5.988408169150353
  - 6.002720406651497
  - 6.328831382095814
  - 5.99281225502491
  - 5.909908375144005
  - 5.965259391069413
  - 6.368903812766075
  - 7.1119154542684555
  - 5.9551515042781835
  - 6.043435803055764
  - 5.90582614839077
  - 6.077743649482727
  - 5.953170216083527
  validation_losses:
  - 1.9552956819534302
  - 0.8152536153793335
  - 0.4284455180168152
  - 0.5525442957878113
  - 0.44149085879325867
  - 0.6327795386314392
  - 0.4787951707839966
  - 0.4033692181110382
  - 0.5060904026031494
  - 1.463172197341919
  - 0.39296847581863403
  - 0.4725353419780731
  - 0.3878699243068695
  - 0.43691039085388184
  - 0.40559452772140503
  - 0.3921371400356293
  - 0.8355473875999451
  - 0.38261497020721436
  - 0.44327422976493835
  - 0.38614898920059204
  - 0.4048106074333191
  - 0.5322880148887634
  - 0.4374644160270691
  - 0.44196024537086487
  - 0.5802088379859924
  - 0.3836255967617035
  - 0.4163669943809509
  - 0.3845692276954651
  - 0.38701221346855164
  - 0.38691678643226624
  - 0.3830195367336273
  - 0.47301384806632996
  - 0.38030534982681274
  - 0.45479798316955566
  - 0.5032473206520081
  - 0.7715792059898376
  - 0.4512072801589966
  - 0.49469393491744995
  - 0.41560545563697815
  - 0.5766973495483398
  - 0.6073864698410034
  - 0.49299490451812744
  - 0.7564214468002319
  - 0.7123804688453674
  - 0.8077434301376343
  - 0.7499149441719055
  - 0.5447250604629517
  - 0.5075204372406006
  - 0.39126262068748474
  - 0.39620259404182434
  - 0.4456865191459656
  - 0.39032837748527527
  - 0.5480058789253235
  - 0.5379666090011597
  - 0.48430952429771423
  - 0.4073777496814728
  - 0.39020785689353943
  - 0.42829763889312744
  - 0.42531082034111023
  - 0.42917513847351074
  - 0.3954407870769501
  - 0.4157397747039795
  - 0.5262805223464966
  - 0.39979079365730286
  - 0.38853737711906433
  - 0.41135984659194946
  - 0.3912157118320465
  - 0.39620649814605713
  - 0.9666264653205872
  - 0.400468647480011
  - 0.40296679735183716
  - 0.3928825259208679
  - 0.39216965436935425
  - 0.39072656631469727
  - 0.42809993028640747
  - 0.39016902446746826
  - 0.3942657709121704
  - 0.39074984192848206
  - 0.46117788553237915
  - 0.4138903021812439
  - 0.4028446078300476
  - 419053.40625
  - 0.40014776587486267
  - 0.4358426034450531
  - 0.3896956741809845
  - 0.3899765610694885
  - 0.3928129971027374
  - 0.39091336727142334
  - 0.3992758095264435
  - 0.39049410820007324
loss_records_fold1:
  train_losses:
  - 5.907097572088242
  - 5.792079338431359
  - 5.8589678615331655
  - 5.886736562848092
  - 5.765989384055138
  - 5.821215066313744
  - 5.887731000781059
  - 6.045591205358505
  - 5.938757905364037
  - 5.824478462338448
  - 5.989513286948204
  - 5.816722190380097
  - 5.895158141851425
  - 5.890833497047424
  - 6.593508341908455
  - 6.610165515542031
  - 6.176870051026345
  - 6.889300897717476
  - 6.139842526614666
  - 5.984566441178322
  - 5.916028752923012
  - 5.950218495726586
  - 5.832583332061768
  - 5.977887627482414
  - 5.805771425366402
  - 5.875047278404236
  - 5.9616064667701725
  - 6.034277096390724
  - 5.82524051964283
  - 5.890830662846565
  - 6.171684187650681
  - 5.8527190595865255
  - 5.82308450639248
  - 5.8653102129697805
  - 5.796064698696137
  - 5.775605928897858
  - 5.77153754234314
  - 6.019526162743569
  - 5.868929368257523
  - 5.942243859171867
  - 5.985530549287796
  - 5.7491145014762886
  - 5.789165151119232
  - 5.9604539424180984
  - 5.831699612736703
  - 6.000054101645947
  - 6.044605243206025
  - 5.8286201626062395
  - 5.956944251060486
  - 5.929449796676636
  - 5.818190601468086
  - 5.894480073451996
  - 5.964035177230835
  - 5.895398214459419
  - 5.845679038763047
  - 5.8055161982774734
  - 5.871706914901734
  - 5.843271261453629
  - 5.932943522930145
  - 6.623031651973725
  - 5.9327982574701315
  - 5.921451234817505
  - 5.70462019443512
  - 6.021259650588036
  - 6.035673126578331
  - 5.781761491298676
  - 6.002546003460885
  - 5.926053628325462
  - 5.8924796938896185
  - 5.993988254666329
  - 5.97951979637146
  - 5.830902233719826
  - 5.7477608859539036
  - 5.8615417957305915
  - 5.870496147871018
  - 5.828364729881287
  - 6.150000593066216
  - 6.327605715394021
  - 6.406458389759064
  - 12.982468724250793
  - 7.0989749282598495
  - 6.900873395800591
  - 6.451247957348824
  - 6.458921828866005
  - 6.083753237128258
  - 6.10508269071579
  - 5.972200787067414
  - 6.2129593878984455
  - 5.841090306639671
  - 6.009365993738175
  - 5.776966458559037
  - 5.947438716888428
  - 5.98966767191887
  - 5.895226836204529
  - 5.98253964483738
  - 6.0355376452207565
  - 5.839787238836289
  - 5.939050073921681
  - 6.032079577445984
  - 5.856248527765274
  validation_losses:
  - 0.42561519145965576
  - 0.4530983567237854
  - 0.41479671001434326
  - 0.40673062205314636
  - 0.4248519539833069
  - 0.4026426076889038
  - 0.40920937061309814
  - 0.4600885808467865
  - 0.4503529369831085
  - 0.40665945410728455
  - 0.4223540127277374
  - 0.45167315006256104
  - 0.42683854699134827
  - 0.419925719499588
  - 12.372986793518066
  - 47.7055778503418
  - 2.2418832778930664
  - 0.44934210181236267
  - 12.616439819335938
  - 28465528.0
  - 815234944.0
  - 1319554816.0
  - 462330368.0
  - 995358656.0
  - 461853408.0
  - 436105408.0
  - 36221.6328125
  - 367915840.0
  - 627169024.0
  - 707362560.0
  - 605828544.0
  - 4127.791015625
  - 379817728.0
  - 764185920.0
  - 2140802176.0
  - 357569472.0
  - 1108450176.0
  - 491263264.0
  - 521762240.0
  - 2747069440.0
  - 2412242432.0
  - 474328448.0
  - 646810752.0
  - 2409266688.0
  - 722302016.0
  - 1092554.625
  - 2016106112.0
  - 38052.70703125
  - 195168608.0
  - 108586688.0
  - 988.5206909179688
  - 7954861.5
  - 2313404160.0
  - 777681024.0
  - 542156352.0
  - 630433408.0
  - 1845443456.0
  - 428737920.0
  - 773773440.0
  - 553986176.0
  - 251.7428741455078
  - 37318216.0
  - 439267936.0
  - 586118656.0
  - 1731220096.0
  - 2705901056.0
  - 610421504.0
  - 871066496.0
  - 324971840.0
  - 224563552.0
  - 2315254272.0
  - 612351232.0
  - 1247914240.0
  - 176561.34375
  - 372786592.0
  - 4564432384.0
  - 2849528320.0
  - 0.40525978803634644
  - 0.4102376401424408
  - 0.42658117413520813
  - 0.4123798608779907
  - 0.43411850929260254
  - 0.40233930945396423
  - 0.42731204628944397
  - 0.45888638496398926
  - 0.41241148114204407
  - 0.40780457854270935
  - 0.42895540595054626
  - 0.41728654503822327
  - 0.441583514213562
  - 0.41373172402381897
  - 0.45813313126564026
  - 0.40364760160446167
  - 0.40673038363456726
  - 0.4226694107055664
  - 0.40698519349098206
  - 0.4031524360179901
  - 0.4206905663013458
  - 0.4418282210826874
  - 0.41449543833732605
loss_records_fold2:
  train_losses:
  - 5.953936281800271
  - 6.197174915671349
  - 5.887267601490021
  - 6.307309877872467
  - 6.224666574597359
  - 6.174622297286987
  - 6.050186896324158
  - 6.045484322309495
  - 5.914793646335602
  - 6.082774451375008
  - 6.228855016827584
  - 6.057832890748978
  - 6.032914111018181
  - 6.193149733543397
  - 5.961269998550415
  - 5.9774273365736015
  - 6.031940612196923
  - 5.910994204878808
  - 6.065735530853272
  - 6.147447404265404
  - 5.998998585343362
  - 5.867463609576226
  - 5.969289022684098
  - 6.0195496648550035
  validation_losses:
  - 0.38727906346321106
  - 0.38549166917800903
  - 0.3872585892677307
  - 0.47388744354248047
  - 0.3929324150085449
  - 0.39848944544792175
  - 0.4099280834197998
  - 0.42302703857421875
  - 0.4322706460952759
  - 44.63494873046875
  - 0.3825533390045166
  - 0.38646891713142395
  - 76591440.0
  - 0.40577954053878784
  - 0.3925269842147827
  - 0.39558008313179016
  - 0.3998055160045624
  - 0.45114728808403015
  - 0.4032563865184784
  - 0.40562668442726135
  - 0.39069756865501404
  - 0.3892853260040283
  - 0.38624075055122375
  - 0.3912205100059509
loss_records_fold3:
  train_losses:
  - 5.874790737032891
  - 5.865105909109116
  - 5.92834612429142
  - 5.900556559860707
  - 5.896980726718903
  - 5.8588407963514335
  - 6.031000190973282
  - 5.993504402041435
  - 5.902865245938301
  - 5.922630113363266
  - 5.868196198344231
  - 6.030737844109535
  - 5.89491682946682
  - 5.840056973695756
  - 5.961165204644203
  - 5.782694080471993
  - 5.914456823468209
  - 5.905040560662747
  - 6.056739611923695
  - 6.002730497717858
  - 6.2533104673028
  - 6.13821848332882
  - 5.9584135800600055
  - 5.86621208190918
  - 5.904663780331612
  - 5.879771554470063
  - 6.40695201009512
  - 5.962228566408157
  - 6.196777105331421
  - 5.958306470513344
  - 5.875419390201569
  - 6.021881514787674
  - 5.914912897348405
  - 5.862376096844674
  - 5.9667581796646125
  - 5.960126796364785
  - 5.872772791981697
  validation_losses:
  - 0.42517733573913574
  - 0.400305837392807
  - 0.41840851306915283
  - 0.44980326294898987
  - 0.4091327488422394
  - 0.39924588799476624
  - 0.47361230850219727
  - 0.40152108669281006
  - 0.39756977558135986
  - 0.4112691581249237
  - 0.39894476532936096
  - 0.4277248978614807
  - 0.4167930781841278
  - 0.4072084426879883
  - 0.402072936296463
  - 0.4408743381500244
  - 0.4014224112033844
  - 0.4135441780090332
  - 0.40396690368652344
  - 0.4042609632015228
  - 0.44729024171829224
  - 0.412586510181427
  - 0.40836217999458313
  - 0.39927756786346436
  - 0.3983592987060547
  - 0.45284757018089294
  - 0.40082767605781555
  - 0.39805999398231506
  - 0.45185333490371704
  - 0.39795300364494324
  - 0.4123593866825104
  - 0.41729363799095154
  - 0.41730746626853943
  - 0.3985365927219391
  - 0.40613436698913574
  - 0.4066624939441681
  - 0.40190622210502625
loss_records_fold4:
  train_losses:
  - 5.968020755052567
  - 6.162700581550599
  - 5.980467438697815
  - 5.981265406310559
  - 6.103500178456307
  - 5.908454179763794
  - 5.8706379562616355
  - 5.945032986998559
  - 6.0066488742828374
  - 6.017158168554307
  - 5.9515827864408495
  - 5.99637817144394
  - 5.959111049771309
  - 5.933155956864358
  - 5.866344127058984
  - 5.909309777617455
  - 5.986289963126183
  - 5.958732101321221
  - 6.0395905703306205
  - 5.97146492600441
  - 6.129180158674718
  - 6.375278018414974
  - 6.147417199611664
  - 5.8995585098862655
  - 5.97960253059864
  - 5.981954234838486
  - 5.893240812420846
  - 6.117886379361153
  - 6.011493450403214
  - 5.960472312569618
  - 5.907976381480694
  - 5.894592693448067
  - 6.184356242418289
  - 5.8090447127819065
  - 5.91020407229662
  - 6.0085992336273195
  - 5.900863152742386
  - 5.914758697152138
  - 6.078029239177704
  - 6.1146092265844345
  - 5.936387133598328
  - 6.0098247051239015
  - 5.8571249634027485
  - 5.873364245891572
  - 5.933529496192932
  - 5.997044011950493
  - 5.954710112512112
  validation_losses:
  - 0.3915903866291046
  - 0.3960493803024292
  - 0.3930510878562927
  - 0.39147505164146423
  - 0.39383238554000854
  - 0.3914576768875122
  - 0.3910289406776428
  - 0.44100260734558105
  - 0.40256792306900024
  - 0.4207996428012848
  - 0.3914540112018585
  - 0.423175573348999
  - 0.42025813460350037
  - 0.3925606310367584
  - 0.3983234465122223
  - 0.4157296121120453
  - 0.4304921329021454
  - 0.3962061107158661
  - 0.39179810881614685
  - 0.410387247800827
  - 0.39424726366996765
  - 0.5285474061965942
  - 0.3988645374774933
  - 0.41164880990982056
  - 0.39229172468185425
  - 0.40717679262161255
  - 0.39136549830436707
  - 0.3922092914581299
  - 0.3971927762031555
  - 0.3944672644138336
  - 0.4154994487762451
  - 0.40511220693588257
  - 0.3913145363330841
  - 0.4187009036540985
  - 0.3975823223590851
  - 0.3958489000797272
  - 0.3910146653652191
  - 0.40828970074653625
  - 0.39758437871932983
  - 0.39163315296173096
  - 0.4115752577781677
  - 0.39220038056373596
  - 0.40077582001686096
  - 0.3993212580680847
  - 0.4050717055797577
  - 0.39132755994796753
  - 0.39912834763526917
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 90 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 37 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 47 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:30:39.919105'
