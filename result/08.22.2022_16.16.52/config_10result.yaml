config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.245438'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_10fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_10fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_10fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_10fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_10fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_10fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_10fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_10fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_10fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_10fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 30.48813102245331
  - 8.119126588106155
  - 6.464247888326645
  - 3.295558649301529
  - 2.8480673134326935
  - 3.1305373132228853
  - 2.2846193611621857
  - 2.7332307279109957
  - 3.1104942202568058
  - 3.1914490640163424
  - 2.3826096236705783
  - 2.245100736618042
  - 1.7120113134384156
  - 2.958034294843674
  - 2.00460689663887
  - 1.7332627952098847
  - 3.4462587118148806
  - 2.2067149341106416
  - 1.9839284718036652
  - 2.3994521260261537
  - 1.9575872123241425
  - 2.382047712802887
  - 2.4249954521656036
  - 1.8228781759738923
  - 4.273859369754791
  - 2.8374929785728455
  - 9.962967306375504
  - 1.9006574541330339
  - 1.7827752888202668
  - 1.5494726300239563
  - 1.8281921423971654
  - 1.8979927718639376
  - 1.7264866709709168
  - 1.5874203443527222
  - 1.8225541174411775
  - 1.711968743801117
  - 1.8655063807964325
  - 1.7318428575992586
  - 2.192876082658768
  - 3.406821131706238
  - 2.4661560237407687
  - 2.4933036029338838
  - 2.6411986708641053
  - 4.081585800647736
  - 2.4831092774868013
  - 2.5352437108755113
  - 2.3487976789474487
  - 2.826782608032227
  - 1.7024650275707245
  - 2.3311683595180512
  - 1.8196290373802186
  - 1.751912146806717
  - 1.885297602415085
  - 1.8799767136573793
  - 2.4112165153026583
  - 2.312875270843506
  - 2.5988010466098785
  - 2.324368476867676
  - 2.067352879047394
  - 2.1756163030862807
  - 1.630020833015442
  - 1.552897357940674
  - 1.6555414438247682
  - 1.6500956535339357
  - 1.766412079334259
  - 1.8616509914398194
  - 1.522565805912018
  - 1.5686436593532562
  - 1.6095495343208315
  - 1.5609919130802155
  - 2.1088964700698853
  - 1.5103512346744539
  - 1.7304353773593903
  - 2.5843427181243896
  - 1.7308878600597382
  - 1.5825516343116761
  - 1.5978619664907456
  - 1.5271143436431887
  - 1.5898874282836915
  - 1.8183645129203798
  - 1.5380398392677308
  - 1.5032230108976365
  - 1.6180871605873108
  - 1.6039403975009918
  - 1.4937654912471772
  validation_losses:
  - 1.032294511795044
  - 3.232438325881958
  - 2.34708309173584
  - 1.946227788925171
  - 0.5385311245918274
  - 0.7006955742835999
  - 0.9024924635887146
  - 0.4943632185459137
  - 0.6113436222076416
  - 1.4282070398330688
  - 0.712831437587738
  - 0.40546396374702454
  - 1.5203062295913696
  - 0.4357212781906128
  - 0.4549385905265808
  - 0.43962010741233826
  - 0.48012474179267883
  - 0.4432097375392914
  - 0.5655204653739929
  - 0.4645861089229584
  - 0.42923930287361145
  - 0.6249954104423523
  - 0.40080589056015015
  - 0.4230799973011017
  - 0.396404504776001
  - 0.4401613771915436
  - 0.6354628205299377
  - 0.551338791847229
  - 0.39288094639778137
  - 0.45891472697257996
  - 0.67384934425354
  - 0.4568500816822052
  - 0.5504693388938904
  - 0.43197041749954224
  - 1.273350715637207
  - 0.3914166986942291
  - 0.8338287472724915
  - 0.3807702958583832
  - 0.4390649199485779
  - 0.4225672781467438
  - 0.4763888418674469
  - 0.5051224231719971
  - 0.6593536734580994
  - 0.3757501542568207
  - 0.5087421536445618
  - 0.4538480043411255
  - 0.4445396661758423
  - 0.4120039939880371
  - 0.43834203481674194
  - 0.3927380442619324
  - 0.38674384355545044
  - 0.3873034119606018
  - 0.40958842635154724
  - 0.4141121208667755
  - 1.2315911054611206
  - 0.468705415725708
  - 0.4668215811252594
  - 0.5251173377037048
  - 0.39351657032966614
  - 0.488525390625
  - 0.38527020812034607
  - 0.3891949951648712
  - 0.41341716051101685
  - 0.41210711002349854
  - 0.38215965032577515
  - 0.37866050004959106
  - 0.40058404207229614
  - 0.38765302300453186
  - 0.6311248540878296
  - 0.3796156346797943
  - 0.38462039828300476
  - 0.3834793269634247
  - 0.4682483971118927
  - 0.4038770794868469
  - 0.4015566408634186
  - 0.3907267451286316
  - 0.389192134141922
  - 0.3923984467983246
  - 0.41896480321884155
  - 0.38903555274009705
  - 0.389912486076355
  - 0.39740487933158875
  - 0.3952799141407013
  - 0.38942039012908936
  - 0.3986744284629822
loss_records_fold1:
  train_losses:
  - 1.571382486820221
  - 1.5897353172302247
  - 2.141027957201004
  - 1.6895944833755494
  - 1.5405395686626435
  - 1.8054232954978944
  - 1.6433813750743866
  - 1.49113489985466
  - 1.6194614350795746
  - 1.7394528329372407
  - 1.4854626655578613
  - 1.5813717544078827
  - 1.6204742789268494
  - 1.5553169786930086
  - 1.8339398920536043
  - 1.6348067820072174
  - 1.5927347600460053
  - 1.5234273970127106
  - 1.5326177775859833
  - 1.7660742044448854
  - 1.6797515690326692
  validation_losses:
  - 0.40527433156967163
  - 0.4047988951206207
  - 0.40329158306121826
  - 0.4023532569408417
  - 0.4202728271484375
  - 0.4847317039966583
  - 0.40079349279403687
  - 0.4003983736038208
  - 0.4197457432746887
  - 0.4013897180557251
  - 0.3997451663017273
  - 0.40288788080215454
  - 0.46616458892822266
  - 0.4135110378265381
  - 0.51893550157547
  - 0.4173317551612854
  - 0.4113042950630188
  - 0.41764160990715027
  - 0.4050220251083374
  - 0.4069141149520874
  - 0.3996886909008026
loss_records_fold2:
  train_losses:
  - 1.647254240512848
  - 1.5862965643405915
  - 2.2361169636249545
  - 1.6914602100849152
  - 1.8823195934295656
  - 2.230992275476456
  - 1.597651106119156
  - 1.5920302480459214
  - 1.704402244091034
  - 2.515363186597824
  - 1.6845662117004396
  validation_losses:
  - 0.3871789872646332
  - 0.38446810841560364
  - 0.38074639439582825
  - 0.42177659273147583
  - 0.37989747524261475
  - 0.3848033845424652
  - 0.37940895557403564
  - 0.37978944182395935
  - 0.3844600021839142
  - 0.3806684911251068
  - 0.38178497552871704
loss_records_fold3:
  train_losses:
  - 1.9260980129241945
  - 1.7041302233934403
  - 1.8734683632850648
  - 1.7409509479999543
  - 1.4949409902095796
  - 1.532220494747162
  - 1.587270998954773
  - 1.4626788079738617
  - 1.4972257107496263
  - 1.5737792015075684
  - 1.7779467135667801
  - 1.9218682765960695
  - 1.9403707504272463
  - 1.7109892070293427
  - 2.607832819223404
  - 1.9498530328273773
  - 1.7090941309928895
  - 1.6845553815364838
  - 1.6595497250556948
  - 1.5397951185703278
  - 1.5361659646034242
  - 1.458834058046341
  - 1.492707908153534
  - 1.5518830180168153
  - 1.4477005660533906
  - 1.45214002430439
  - 1.468736505508423
  - 1.6993272304534912
  - 1.680933576822281
  - 1.4794574916362764
  - 1.5459775388240815
  - 2.166090738773346
  - 1.597010236978531
  - 1.7303278982639314
  - 2.732103252410889
  - 1.567650055885315
  validation_losses:
  - 0.47682246565818787
  - 0.39778438210487366
  - 0.391257107257843
  - 0.39307740330696106
  - 0.3964785635471344
  - 0.4143534004688263
  - 0.39667555689811707
  - 0.38664814829826355
  - 0.3990117907524109
  - 0.5962186455726624
  - 0.47715499997138977
  - 0.41918322443962097
  - 0.6410001516342163
  - 3.205519437789917
  - 0.4657094478607178
  - 0.4168531596660614
  - 0.5474970936775208
  - 1.169331669807434
  - 0.4340727925300598
  - 0.37531235814094543
  - 0.38412052392959595
  - 0.3908160626888275
  - 0.3811897337436676
  - 0.42514002323150635
  - 0.3933246433734894
  - 0.38372471928596497
  - 0.4111667275428772
  - 0.38296380639076233
  - 0.39053574204444885
  - 0.41678738594055176
  - 0.40890175104141235
  - 0.3865654468536377
  - 0.39147070050239563
  - 0.3928697109222412
  - 0.38300204277038574
  - 0.38113299012184143
loss_records_fold4:
  train_losses:
  - 1.6306874990463258
  - 1.5330622375011445
  - 1.5530317306518555
  - 1.5291001081466675
  - 1.4423431366682054
  - 1.5667001426219942
  - 1.4656012952327728
  - 1.491452342271805
  - 1.5484933853149414
  - 1.4865382075309754
  - 1.4418346017599106
  - 1.4322149604558945
  - 1.5462320268154146
  - 1.5372093677520753
  validation_losses:
  - 0.3940429091453552
  - 0.43138444423675537
  - 0.3799223303794861
  - 0.4055381715297699
  - 0.38334864377975464
  - 0.38995790481567383
  - 0.37254828214645386
  - 0.4491746127605438
  - 0.4089183509349823
  - 0.3968934714794159
  - 0.39121827483177185
  - 0.3788328468799591
  - 0.3868374228477478
  - 0.3900793790817261
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 85 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 36 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:13:47.143582'
