config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 16:55:37.093776'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/30/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/30/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/30/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/30/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/30/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/30/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/30/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/30/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/30/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/30/fold_4_state_dict.pt
loss_records_fold4:
  train_losses:
  - 6.1010719537734985
  - 5.260389983654022
  - 18.272439748048782
  - 4.179595738649368
  - 4.735699653625488
  - 4.388174533843994
  - 4.452811032533646
  - 4.227949321269989
  - 4.799045920372009
  - 4.457992315292358
  - 4.114999622106552
  - 4.143129587173462
  - 4.741051286458969
  - 4.441125571727753
  - 4.104751408100128
  - 4.481276988983154
  - 4.241664379835129
  - 4.020834147930145
  - 4.389217674732208
  - 4.322169631719589
  - 4.172033280134201
  - 4.403014838695526
  - 4.391747891902924
  - 7.367398381233215
  - 4.2336234748363495
  - 4.074097663164139
  - 4.467184543609619
  - 4.093197613954544
  - 4.304584711790085
  - 6.1374295353889465
  - 4.413419842720032
  - 4.230453640222549
  - 4.302219092845917
  - 4.19961753487587
  - 4.423973381519318
  - 5.245380222797394
  - 4.085407942533493
  - 4.297212928533554
  - 4.27242574095726
  - 4.209530621767044
  - 4.348892658948898
  - 4.152191758155823
  - 5.9831660985946655
  - 4.3004449009895325
  - 4.364442229270935
  - 4.407821536064148
  - 4.179152488708496
  - 4.053003340959549
  - 4.154449909925461
  - 4.241140156984329
  - 4.157865643501282
  - 5.963454335927963
  - 4.125236928462982
  - 7.669185757637024
  - 3.999589905142784
  - 4.4819358587265015
  - 5.255249470472336
  - 25.745375245809555
  - 4.755444139242172
  - 9.572068870067596
  - 4.3745777904987335
  - 5.072836488485336
  - 4.246635913848877
  - 18.16478106379509
  - 4.166879713535309
  - 4.210879161953926
  - 4.662624567747116
  - 4.3334483206272125
  - 4.482292145490646
  - 4.469453066587448
  - 4.004518672823906
  - 6.921319961547852
  - 4.525486499071121
  - 4.221189558506012
  - 4.2397236824035645
  - 4.246150493621826
  - 4.296423465013504
  - 4.315344154834747
  - 4.227825462818146
  - 4.188702970743179
  - 4.171542972326279
  - 4.150322079658508
  - 14.215033948421478
  - 4.4835304617881775
  - 5.129682242870331
  - 4.673669755458832
  - 4.376414358615875
  - 4.290831029415131
  - 4.380837261676788
  - 4.465114891529083
  - 4.290167033672333
  - 4.169784039258957
  - 4.338915109634399
  - 13.925616264343262
  - 4.335229367017746
  - 4.127962231636047
  - 4.320035994052887
  - 4.1765619814395905
  - 4.193237662315369
  - 5.865465849637985
  validation_losses:
  - 0.4626449644565582
  - 0.45306065678596497
  - 0.4666973650455475
  - 0.4403602182865143
  - 0.4431076943874359
  - 0.42837923765182495
  - 0.4051118493080139
  - 0.39743536710739136
  - 0.4700944721698761
  - 0.40827858448028564
  - 0.45774322748184204
  - 0.44787120819091797
  - 0.455742746591568
  - 0.40984928607940674
  - 0.4743416905403137
  - 0.4208539128303528
  - 0.4068567752838135
  - 0.4489920139312744
  - 0.3943605124950409
  - 0.4072685241699219
  - 0.41545090079307556
  - 0.42141053080558777
  - 0.42457032203674316
  - 0.4302346706390381
  - 0.44865521788597107
  - 0.41289862990379333
  - 0.4325384497642517
  - 0.424750417470932
  - 0.4561518132686615
  - 0.4389680325984955
  - 0.39404988288879395
  - 0.3943978548049927
  - 0.42821013927459717
  - 0.44893255829811096
  - 0.4419809579849243
  - 0.42973458766937256
  - 0.4039781093597412
  - 0.39404192566871643
  - 0.42620861530303955
  - 0.41349363327026367
  - 0.4016403257846832
  - 0.43563053011894226
  - 0.42911529541015625
  - 0.39869266748428345
  - 0.4046969711780548
  - 0.42938652634620667
  - 0.4115068018436432
  - 0.40577325224876404
  - 0.3990647792816162
  - 0.4188561737537384
  - 0.4080947935581207
  - 0.3992643356323242
  - 0.3929455578327179
  - 0.4394189417362213
  - 0.474890798330307
  - 0.4793406128883362
  - 0.4299072325229645
  - 0.46663427352905273
  - 0.4356231689453125
  - 0.4020659625530243
  - 0.424623042345047
  - 0.47858211398124695
  - 0.4471869170665741
  - 0.47357848286628723
  - 0.4781516194343567
  - 0.40896114706993103
  - 0.40234559774398804
  - 0.4150664806365967
  - 0.43308600783348083
  - 0.44506028294563293
  - 0.4737851917743683
  - 0.4119517505168915
  - 0.4481150805950165
  - 0.44674938917160034
  - 0.4010791480541229
  - 0.4166080951690674
  - 0.44608792662620544
  - 0.40026625990867615
  - 0.4158139228820801
  - 0.4298219084739685
  - 0.4208918511867523
  - 0.43749701976776123
  - 0.41709575057029724
  - 0.40253376960754395
  - 0.42336106300354004
  - 0.4480166733264923
  - 0.4689575433731079
  - 0.49556997418403625
  - 0.41146910190582275
  - 0.3923758566379547
  - 0.4308978319168091
  - 0.40714308619499207
  - 0.4460119903087616
  - 0.4418899118900299
  - 0.399563729763031
  - 0.40438035130500793
  - 0.40317386388778687
  - 0.4140273630619049
  - 0.4168424606323242
  - 0.42015841603279114
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 80 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 9 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 10 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582701160604291
  mean_f1_accuracy: 0.0
  total_train_time: '0:01:26.777199'
