config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:13:44.075438'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_76fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_76fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_76fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_76fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_76fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_76fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_76fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_76fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_76fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_76fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 91.59919251203537
  - 45.504923003911976
  - 40.583998416364196
  - 36.125914350152016
  - 37.688321161270146
  - 18.283985322713853
  - 20.523740199208262
  - 17.794261068105698
  - 21.24503674209118
  - 20.421599331498147
  - 8.877650174498559
  - 10.849308928847314
  - 10.907592719793321
  - 9.841169154644014
  - 10.372021752595902
  - 7.900487780570984
  - 10.798340466618539
  - 8.316764736175537
  - 7.573737531900406
  - 7.694756042957306
  - 6.924302330613137
  - 7.000912138819695
  - 6.896054729819298
  - 6.722286239266396
  - 7.441159278154373
  - 7.22014319896698
  - 7.146509340405465
  - 7.428811004757882
  - 7.136671161651612
  - 6.911601257324219
  - 6.934820702672005
  - 6.62247371673584
  - 6.563764968514443
  - 6.317007771134377
  - 7.089881631731988
  - 7.396510726213456
  - 6.8611170858144765
  - 6.779556393623352
  - 6.672196891903877
  - 7.010494375228882
  - 6.524033707380295
  - 6.348501262068749
  - 6.44587733745575
  - 6.702213025093079
  - 6.418750351667405
  - 6.848737359046936
  - 6.693777757883073
  - 6.529812899231911
  - 6.540049144625664
  - 6.237732353806496
  - 6.52483931183815
  - 6.499674171209335
  - 6.718816328048707
  - 6.8913403987884525
  - 6.978372326493264
  - 6.448383232951165
  - 6.578872776031495
  - 6.532988139986992
  - 6.444039613008499
  - 6.861605259776116
  - 7.577089375257493
  - 7.168061721324921
  - 6.935518151521683
  - 7.52579597234726
  - 6.811100262403489
  - 6.565917381644249
  - 6.609303134679795
  - 6.319678068161011
  - 6.518892198801041
  - 6.7341045349836355
  - 6.377636486291886
  - 6.539985352754593
  - 6.394404622912408
  - 6.302387812733651
  - 6.505948376655579
  - 7.029332551360131
  - 6.524226951599122
  - 6.400257343053818
  - 6.586940944194794
  - 6.880207851529121
  - 6.686394086480141
  - 6.4229602277278905
  - 6.317000406980515
  - 6.537945136427879
  - 7.011787706613541
  - 7.6741071313619615
  - 7.917774066329002
  - 8.01066496372223
  - 6.899748852849007
  - 6.477704349160195
  - 6.396126827597619
  - 6.445766523480415
  - 6.687049821019173
  - 6.754618164896965
  - 6.4352301388978965
  - 6.876717585325242
  - 6.521799233555794
  - 6.713421407341958
  - 6.579224959015846
  - 6.430670541524887
  validation_losses:
  - 0.7607507705688477
  - 1.123028039932251
  - 0.5648941993713379
  - 0.6449567079544067
  - 0.47854384779930115
  - 0.49212968349456787
  - 0.5413779616355896
  - 0.7808867692947388
  - 0.43365365266799927
  - 0.49533411860466003
  - 0.40898239612579346
  - 0.43193501234054565
  - 0.4283222258090973
  - 0.6887655258178711
  - 0.43918681144714355
  - 0.4347502291202545
  - 0.4177371859550476
  - 0.4990719258785248
  - 0.4078088104724884
  - 0.40757232904434204
  - 0.3930678367614746
  - 0.6616562008857727
  - 0.45791172981262207
  - 0.4169755280017853
  - 0.4354713261127472
  - 0.46519237756729126
  - 0.41011470556259155
  - 0.4155135154724121
  - 0.4312720000743866
  - 0.49894702434539795
  - 0.42429500818252563
  - 0.39926856756210327
  - 0.41972050070762634
  - 0.4361579120159149
  - 0.5014380812644958
  - 0.48643481731414795
  - 0.4224875569343567
  - 0.4100731909275055
  - 0.4383715093135834
  - 0.4337030053138733
  - 0.413176566362381
  - 0.40761318802833557
  - 0.421516090631485
  - 0.4123077094554901
  - 0.41669711470603943
  - 0.45571351051330566
  - 0.4531348645687103
  - 0.4087127447128296
  - 0.42368754744529724
  - 1.0717754364013672
  - 0.40684422850608826
  - 0.46105921268463135
  - 0.4658505916595459
  - 0.43041732907295227
  - 0.4134933054447174
  - 0.3974745571613312
  - 0.41072043776512146
  - 0.45060816407203674
  - 0.4056245982646942
  - 3.6262831687927246
  - 0.42055100202560425
  - 0.41999515891075134
  - 0.41335445642471313
  - 0.4492414891719818
  - 0.4473379850387573
  - 0.4294304847717285
  - 0.41022390127182007
  - 0.4001636207103729
  - 0.45146822929382324
  - 0.4234222173690796
  - 0.470938116312027
  - 0.41744616627693176
  - 0.4136882424354553
  - 0.41801917552948
  - 0.5957396030426025
  - 0.4141770005226135
  - 0.41815340518951416
  - 0.412209689617157
  - 17.857215881347656
  - 0.417817085981369
  - 92.57398986816406
  - 5908973.0
  - 370706944.0
  - 0.40455371141433716
  - 0.4854826033115387
  - 0.6260802149772644
  - 0.41331222653388977
  - 0.4115215539932251
  - 0.41701167821884155
  - 0.44695794582366943
  - 0.452237069606781
  - 0.44302263855934143
  - 0.4101286828517914
  - 0.406929075717926
  - 0.41165459156036377
  - 0.43161019682884216
  - 0.4240150451660156
  - 0.4283190667629242
  - 0.4118245840072632
  - 0.40388813614845276
loss_records_fold1:
  train_losses:
  - 6.514344796538353
  - 6.434467568993568
  - 6.267776316404343
  - 6.456622427701951
  - 6.301264908909798
  - 6.386071529984474
  - 6.450853791832924
  - 6.256605315208436
  - 6.2575158983469015
  - 6.827570781111717
  - 6.456898713111878
  - 6.2973928481340415
  - 6.396209585666657
  - 6.390342009067536
  - 6.66047135591507
  - 6.412066397070885
  - 6.563417965173722
  - 6.945836210250855
  - 6.375150144100189
  - 6.505777254700661
  - 6.478301379084588
  - 6.317112022638321
  - 6.461096090078354
  - 6.4506111502647405
  - 6.4714961618185045
  - 6.287095242738724
  - 6.266563880443574
  - 6.537541714310646
  - 6.606189209222794
  - 6.381808555126191
  - 6.390672963857651
  - 6.320205783843995
  - 6.307207527756692
  - 6.649116909503937
  - 6.5062935799360275
  - 6.466212829947472
  - 6.661676305532456
  - 6.472068214416504
  - 6.450929147005081
  - 6.453073367476463
  - 6.489680826663971
  - 6.597692334651947
  validation_losses:
  - 0.44808703660964966
  - 0.42082878947257996
  - 0.45386257767677307
  - 0.41661199927330017
  - 0.40902310609817505
  - 0.4372909367084503
  - 0.42164158821105957
  - 11565.33984375
  - 5507.39306640625
  - 0.44027867913246155
  - 0.4318862855434418
  - 0.4672620892524719
  - 0.42034396529197693
  - 0.42462876439094543
  - 0.4509446918964386
  - 0.4588395357131958
  - 0.478962779045105
  - 0.43526941537857056
  - 0.44959208369255066
  - 0.4136619567871094
  - 0.48478999733924866
  - 0.451816588640213
  - 0.49857449531555176
  - 0.5058867335319519
  - 0.4383623003959656
  - 0.41700392961502075
  - 0.40895789861679077
  - 0.4760163426399231
  - 0.4657430648803711
  - 0.4711388647556305
  - 0.4164181351661682
  - 0.4413471221923828
  - 0.41961410641670227
  - 0.442573219537735
  - 0.4836585521697998
  - 0.5058850646018982
  - 0.42030590772628784
  - 0.4293927550315857
  - 0.43624746799468994
  - 0.4348377585411072
  - 0.41673511266708374
  - 0.42059001326560974
loss_records_fold2:
  train_losses:
  - 6.554069834947587
  - 6.631683364510536
  - 6.59293039739132
  - 6.505599480867386
  - 6.417112186551094
  - 6.815209302306176
  - 6.623423975706101
  - 6.723350542783738
  - 6.513822206854821
  - 6.826383253932
  - 6.589002412557602
  - 6.569494470953941
  - 6.4630855351686485
  - 6.635085818171501
  - 6.309915265440941
  - 6.639991420507432
  - 6.629438534379005
  - 6.695936137437821
  - 6.488629847764969
  - 6.477430075407028
  - 6.44597633779049
  - 6.425637376308441
  - 6.557853770256043
  - 7.1026964575052265
  - 6.434518206119538
  - 6.561876487731934
  - 6.446627992391587
  - 6.672635912895203
  - 6.599818927049637
  - 7.842817544937134
  - 6.59998329281807
  - 6.658135724067688
  - 6.8827930957078935
  - 6.339039909839631
  - 6.487458503246308
  - 6.452914792299271
  - 6.312094420194626
  - 6.3907368898391725
  - 6.317030268907548
  - 6.562493461370469
  - 6.544875004887581
  - 6.982487078011037
  - 6.9827980458736425
  - 6.50007666349411
  - 6.7629846185445786
  - 6.483545282483101
  - 6.387098893523216
  - 6.392967373132706
  - 6.415886920690537
  - 6.3777746886014945
  - 6.293722540140152
  - 6.518327499926091
  - 6.685126712918282
  - 7.339691612124444
  - 6.819813627004624
  - 6.310927245020867
  - 6.458657401800156
  - 6.525552666187287
  - 6.68964641392231
  - 6.630256143212319
  - 6.756609466671944
  - 6.505022513866425
  - 6.37867084145546
  - 6.475803399085999
  - 6.614859822392464
  - 6.574561157822609
  - 6.88645807504654
  - 6.573496809601784
  - 6.5793975263834
  - 7.557492563128472
  - 10.63813042640686
  - 7.192328292131425
  validation_losses:
  - 0.4356454610824585
  - 0.43631407618522644
  - 0.4480528235435486
  - 0.40678685903549194
  - 0.41124799847602844
  - 0.41714155673980713
  - 0.4352569878101349
  - 0.43708381056785583
  - 0.40143194794654846
  - 0.4320403039455414
  - 0.420326828956604
  - 0.4092886447906494
  - 0.4760410487651825
  - 0.4512292146682739
  - 0.39297935366630554
  - 0.43821802735328674
  - 0.46115270256996155
  - 0.4058261811733246
  - 0.40766048431396484
  - 0.4048626124858856
  - 0.42082536220550537
  - 0.4292586147785187
  - 0.396602064371109
  - 0.4149363040924072
  - 0.4128992259502411
  - 0.39532470703125
  - 0.4102821350097656
  - 0.4110569655895233
  - 0.40846869349479675
  - 2.2570362091064453
  - 422689.78125
  - 0.43575626611709595
  - 0.39912450313568115
  - 0.43387743830680847
  - 0.39758533239364624
  - 0.40419235825538635
  - 0.4054611623287201
  - 0.3883918821811676
  - 0.4098772704601288
  - 0.40817299485206604
  - 0.4067581593990326
  - 0.5528838634490967
  - 0.4063867926597595
  - 0.4513492286205292
  - 0.41979119181632996
  - 0.40972164273262024
  - 0.42781537771224976
  - 0.4240973889827728
  - 0.4206472933292389
  - 0.40140366554260254
  - 0.42656373977661133
  - 0.39176851511001587
  - 0.424752414226532
  - 0.43185681104660034
  - 0.4147692918777466
  - 0.4041593670845032
  - 0.4560842216014862
  - 0.40563103556632996
  - 0.40758824348449707
  - 0.41046273708343506
  - 0.4583466649055481
  - 0.41136854887008667
  - 0.4134523570537567
  - 0.43666765093803406
  - 0.42409420013427734
  - 0.47807979583740234
  - 0.4678705930709839
  - 0.4022635817527771
  - 0.4094923734664917
  - 0.4162319600582123
  - 0.40822967886924744
  - 0.4062686860561371
loss_records_fold3:
  train_losses:
  - 6.686408883333207
  - 6.539492344856263
  - 6.4997516751289375
  - 6.7604647994041445
  - 6.526875764131546
  - 6.328159430623055
  - 6.870014718174935
  - 6.709641790390015
  - 6.530475026369095
  - 6.36894201040268
  - 6.4087219655513765
  - 6.540017628669739
  - 6.242948111891747
  - 6.591211563348771
  - 6.668510761857033
  - 6.334011787176133
  - 6.579732775688171
  - 6.568375033140183
  - 6.954638603329659
  validation_losses:
  - 0.5588427186012268
  - 0.4127886891365051
  - 0.42871180176734924
  - 0.41744181513786316
  - 0.4287264049053192
  - 0.42366692423820496
  - 0.4621403217315674
  - 0.4605811536312103
  - 0.4188960790634155
  - 437062008832.0
  - 0.4199223518371582
  - 0.42675551772117615
  - 0.44868582487106323
  - 0.4110684394836426
  - 0.4145587384700775
  - 0.4159663915634155
  - 0.42003050446510315
  - 0.4123055040836334
  - 0.4141596257686615
loss_records_fold4:
  train_losses:
  - 6.388756921887398
  - 6.573244094848633
  - 6.56578050851822
  - 6.38998461663723
  - 6.747550806403161
  - 6.646510484814645
  - 7.551385211944581
  - 6.539645433425903
  - 6.535014799237252
  - 6.510930991172791
  - 6.405438941717148
  - 6.372487518191338
  - 6.5875213801860815
  - 6.366180646419526
  - 6.3016746848821645
  validation_losses:
  - 0.4268728196620941
  - 0.4240748882293701
  - 0.42945122718811035
  - 0.4412013292312622
  - 0.4215158522129059
  - 0.4496479034423828
  - 0.4065540134906769
  - 0.42040225863456726
  - 0.45548465847969055
  - 0.41071584820747375
  - 0.4075143337249756
  - 0.4105519950389862
  - 0.4106665551662445
  - 0.4047076106071472
  - 0.40482157468795776
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 42 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 72 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:25:45.200550'
