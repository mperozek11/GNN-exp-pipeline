config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:21:40.846008'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_82fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_82fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_82fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_82fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_82fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_82fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_82fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_82fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_82fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_82fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.7026743948459626
  - 1.549641364812851
  - 1.4951031863689423
  - 1.4674911856651307
  - 1.5265363037586213
  - 1.5192646980285645
  - 1.505532366037369
  - 1.4653882354497911
  - 1.5530861854553224
  - 1.5147998809814454
  - 1.5499307036399843
  - 1.5387156188488007
  - 1.4630922555923462
  - 1.4501387894153597
  - 1.4737371146678926
  - 1.5419251143932344
  - 1.5415628254413605
  - 1.479106765985489
  - 1.4508532762527466
  - 1.4487630844116213
  - 1.4758296966552735
  validation_losses:
  - 0.4423987865447998
  - 0.4065319299697876
  - 0.3958907425403595
  - 0.3879632353782654
  - 0.394247442483902
  - 0.39193883538246155
  - 0.40429747104644775
  - 0.38993528485298157
  - 0.3906904458999634
  - 0.42466849088668823
  - 0.4067932963371277
  - 0.3946826756000519
  - 0.3912040591239929
  - 0.3891872763633728
  - 0.41931864619255066
  - 0.4082818031311035
  - 0.41242560744285583
  - 0.38593295216560364
  - 0.3853701055049896
  - 0.38531261682510376
  - 0.39026328921318054
loss_records_fold1:
  train_losses:
  - 1.4746336281299592
  - 1.4862548887729645
  - 1.4275158524513245
  - 1.4524867296218873
  - 1.430289939045906
  - 1.4456696748733522
  - 1.4890064716339113
  - 1.4419468104839326
  - 1.5160797476768495
  - 1.4426994621753693
  - 1.4306426048278809
  - 1.4964633941650392
  - 1.52133224606514
  - 1.5195903539657594
  - 1.4469464659690858
  - 1.4470640122890472
  - 1.4666039526462555
  - 1.4357097297906876
  - 1.4465261220932009
  - 1.4360260725021363
  - 1.4940168499946596
  - 1.451824802160263
  - 1.448444449901581
  - 1.4387856900691987
  - 1.4267172873020173
  validation_losses:
  - 0.40854835510253906
  - 0.40812617540359497
  - 0.3880764842033386
  - 0.4149371087551117
  - 0.3904726803302765
  - 0.38748809695243835
  - 0.4171763062477112
  - 0.39674386382102966
  - 0.40767765045166016
  - 0.671074390411377
  - 0.39237257838249207
  - 0.41326838731765747
  - 0.4462355673313141
  - 0.3890969455242157
  - 0.3951604962348938
  - 0.38823550939559937
  - 0.38943347334861755
  - 0.38987722992897034
  - 0.4108971059322357
  - 0.38780567049980164
  - 0.3892335593700409
  - 0.3892987370491028
  - 0.3895193040370941
  - 0.39749768376350403
  - 0.3996591567993164
loss_records_fold2:
  train_losses:
  - 1.4680359601974489
  - 1.5046896278858186
  - 1.4642289102077486
  - 1.4700873136520387
  - 1.469762420654297
  - 1.4339517891407014
  - 1.4465252101421358
  - 1.4494947314262392
  - 1.478480339050293
  - 1.4603390365839006
  - 1.4405108153820039
  - 1.5428676903247833
  - 1.4497500628232958
  - 1.4742459535598755
  - 1.4444953918457033
  - 1.46288440823555
  - 1.4235004246234895
  - 1.4507589757442476
  - 1.414523831009865
  - 1.4507803261280061
  validation_losses:
  - 0.3959541320800781
  - 0.39446285367012024
  - 0.39243459701538086
  - 0.38511326909065247
  - 0.3821982443332672
  - 0.3830971121788025
  - 0.37956374883651733
  - 0.39322730898857117
  - 0.4325462579727173
  - 0.38772451877593994
  - 0.38656678795814514
  - 0.38555458188056946
  - 0.38989028334617615
  - 0.40293678641319275
  - 0.3906328082084656
  - 0.38284170627593994
  - 0.3829863667488098
  - 0.38009122014045715
  - 0.37552008032798767
  - 0.38358595967292786
loss_records_fold3:
  train_losses:
  - 1.4509592294692994
  - 1.4929093539714815
  - 1.434597146511078
  - 1.4922889530658723
  - 1.4668559670448305
  - 1.4716769635677338
  - 1.4679698169231417
  - 1.4289636373519898
  - 1.4890557527542114
  - 1.475229263305664
  - 1.4282133221626283
  - 1.45978262424469
  - 1.493333488702774
  - 1.4754746675491335
  - 1.4936306715011598
  - 1.4113269031047821
  - 1.5270637243986132
  - 1.4480734825134278
  - 1.4912931740283968
  - 1.4980436086654665
  - 1.55745747089386
  - 1.497817713022232
  - 1.4437849283218385
  - 1.4081351161003113
  - 1.4702174961566925
  validation_losses:
  - 0.36779889464378357
  - 0.38797158002853394
  - 0.374837726354599
  - 0.37356048822402954
  - 0.37006330490112305
  - 0.3800520896911621
  - 0.41304099559783936
  - 0.3745679259300232
  - 0.38574573397636414
  - 0.39190492033958435
  - 0.39316409826278687
  - 0.4066339135169983
  - 0.4031572639942169
  - 0.3714537024497986
  - 0.3696242570877075
  - 0.3999309241771698
  - 0.3675072491168976
  - 0.36914506554603577
  - 0.44461894035339355
  - 0.400056928396225
  - 0.37356066703796387
  - 0.3748222887516022
  - 0.38091790676116943
  - 0.36741626262664795
  - 0.3753582835197449
loss_records_fold4:
  train_losses:
  - 1.4908102452754974
  - 1.4653558790683747
  - 1.4439879298210145
  - 1.4286465048789978
  - 1.4449723362922668
  - 1.4387583553791048
  - 1.4243269681930544
  - 1.4343826472759247
  - 1.4210089087486268
  - 1.4530466318130495
  - 1.417739111185074
  - 1.4261639535427095
  - 1.4281596362590792
  - 1.4212646543979646
  validation_losses:
  - 0.3748224079608917
  - 0.3849128782749176
  - 0.386415034532547
  - 0.37719789147377014
  - 0.3866092562675476
  - 0.38467806577682495
  - 0.3805038630962372
  - 0.42082175612449646
  - 0.3827309012413025
  - 0.3912016749382019
  - 0.3811453878879547
  - 0.37777402997016907
  - 0.37704482674598694
  - 0.3854196071624756
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8524871355060034, 0.8593481989708405, 0.8593481989708405,
    0.8556701030927835]'
  fold_eval_f1: '[0.0, 0.022727272727272728, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8568973139290197
  mean_f1_accuracy: 0.004545454545454545
  total_train_time: '0:09:06.534767'
