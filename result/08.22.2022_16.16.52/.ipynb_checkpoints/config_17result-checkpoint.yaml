config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:28:19.950424'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_17fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_17fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_17fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_17fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_17fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_17fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_17fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_17fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_17fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_17fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.2899504840373996
  - 3.142758733034134
  - 2.9546192198991776
  - 2.904047611355782
  - 2.930914828181267
  - 2.8169014126062395
  - 2.835936796665192
  - 2.932635059952736
  - 2.9334794878959656
  - 2.8850162774324417
  - 2.8943404376506807
  - 2.8139762163162234
  - 2.8208312422037127
  - 2.834774523973465
  - 2.8571910470724107
  - 2.868971973657608
  - 2.8308002680540088
  - 2.8779018193483354
  - 2.8635438650846483
  - 2.816939026117325
  - 2.8340313881635666
  - 2.8208128899335865
  - 2.8357611268758776
  - 2.830109587311745
  - 2.8629758656024933
  - 3.036329960823059
  - 2.954070895910263
  - 3.047761130332947
  - 2.8936890393495562
  - 2.8402305066585543
  - 2.789656063914299
  - 2.852844321727753
  - 2.825098142027855
  - 2.8458053827285767
  - 2.8330504298210144
  - 2.8003415346145633
  - 2.8760753214359287
  - 2.8124308198690415
  - 2.833624452352524
  - 2.802197989821434
  - 2.7451348066329957
  - 2.8153401732444765
  - 2.7591798245906833
  - 2.786358004808426
  - 2.7676301062107087
  - 2.7794351041316987
  - 2.8567585349082947
  - 2.7549924463033677
  - 2.7502570897340775
  - 2.7655206680297852
  - 2.773150604963303
  - 2.858580321073532
  - 2.7408332556486132
  - 2.7841354429721834
  - 2.800511804223061
  - 2.8416673958301546
  - 2.771919843554497
  - 2.7462768822908403
  - 2.735077625513077
  - 2.7467226415872577
  - 2.8125708580017093
  - 2.7868853330612184
  - 2.7573747634887695
  - 2.753273043036461
  - 2.751147294044495
  - 2.772320792078972
  - 2.7595483362674713
  - 2.8175308585166934
  - 2.812872949242592
  - 2.7743071615695953
  - 2.7778232932090763
  - 2.7758368074893953
  - 2.746606680750847
  - 2.783234253525734
  - 2.743031105399132
  - 2.734456050395966
  - 2.7833948701620104
  - 2.74428326189518
  - 2.723698168992996
  - 2.7563734710216523
  - 2.7420062661170963
  - 2.7065053403377535
  - 2.7872890502214434
  - 2.766320684552193
  - 2.717850145697594
  - 2.7626738250255585
  - 2.7532952666282657
  - 2.7352529346942904
  - 2.747451123595238
  - 2.7360095202922823
  - 2.723231774568558
  - 2.745216274261475
  - 2.742135629057884
  - 2.733843740820885
  - 2.75086345076561
  - 2.75014810860157
  - 2.6962895691394806
  - 2.6904859423637393
  - 2.7584053695201876
  - 2.734731832146645
  validation_losses:
  - 0.5474228858947754
  - 0.41648444533348083
  - 0.4238431453704834
  - 0.38914400339126587
  - 0.4932476580142975
  - 0.38779306411743164
  - 0.39091944694519043
  - 0.39365941286087036
  - 0.40239986777305603
  - 0.41503703594207764
  - 0.38490402698516846
  - 0.38456520438194275
  - 0.3986998200416565
  - 0.3843404948711395
  - 0.395403116941452
  - 0.41535064578056335
  - 0.4013654291629791
  - 0.3829038739204407
  - 0.38110801577568054
  - 0.42462173104286194
  - 0.5345821380615234
  - 0.39286479353904724
  - 0.4739760160446167
  - 0.6215779781341553
  - 0.3896922171115875
  - 0.3959786295890808
  - 0.39664214849472046
  - 0.38546937704086304
  - 0.38213685154914856
  - 0.4020414650440216
  - 0.37758877873420715
  - 0.37381836771965027
  - 0.385673463344574
  - 0.4131121337413788
  - 0.3756198585033417
  - 0.378733366727829
  - 0.4087753891944885
  - 0.4417552947998047
  - 0.3886546194553375
  - 0.3837888240814209
  - 0.41131508350372314
  - 0.5587620139122009
  - 0.8679325580596924
  - 1.380911946296692
  - 0.3928831219673157
  - 0.6278868317604065
  - 0.40620720386505127
  - 0.5018913745880127
  - 0.3746033310890198
  - 0.3920539617538452
  - 0.3849143087863922
  - 0.41558343172073364
  - 0.38003066182136536
  - 0.4239084720611572
  - 0.48294875025749207
  - 0.42672669887542725
  - 0.4107119143009186
  - 0.40900135040283203
  - 0.4450278580188751
  - 0.5559973120689392
  - 0.38559862971305847
  - 0.42563295364379883
  - 0.40481099486351013
  - 0.3948215842247009
  - 0.4725956320762634
  - 0.49115824699401855
  - 0.393147736787796
  - 0.383539080619812
  - 0.5188235640525818
  - 0.385978102684021
  - 0.4500671327114105
  - 0.415740042924881
  - 0.4423387348651886
  - 0.4394317865371704
  - 0.4319780766963959
  - 0.47624465823173523
  - 0.4163020849227905
  - 0.40074390172958374
  - 0.49877071380615234
  - 0.45178893208503723
  - 0.3958819508552551
  - 0.4224824905395508
  - 0.48684772849082947
  - 0.397002249956131
  - 0.39624014496803284
  - 0.4074113667011261
  - 0.437115341424942
  - 0.49996933341026306
  - 0.5197243094444275
  - 0.4012899398803711
  - 0.422279953956604
  - 0.4367978274822235
  - 0.44221171736717224
  - 0.4370432496070862
  - 0.40759533643722534
  - 0.4331469237804413
  - 0.44259336590766907
  - 0.41894564032554626
  - 0.591787576675415
  - 0.4590110778808594
loss_records_fold1:
  train_losses:
  - 2.710748553276062
  - 2.737029618024826
  - 2.750465670228005
  - 2.733378916978836
  - 2.7663203448057176
  - 2.7113079130649567
  - 2.733920395374298
  - 2.710045775771141
  - 2.72856425344944
  - 2.712597703933716
  - 2.715422081947327
  - 2.7217052161693576
  - 2.701998615264893
  - 2.7172393143177036
  - 2.7205470621585848
  - 2.6505388349294665
  - 2.751901635527611
  - 2.7107735544443132
  - 2.6715565741062166
  - 2.6846224755048755
  - 2.6722308188676838
  - 2.6924273282289506
  - 2.706909489631653
  - 2.6964001148939136
  - 2.8498314321041107
  - 2.7429245948791507
  - 2.713792082667351
  - 2.720596730709076
  - 2.749598088860512
  - 2.7169272005558014
  - 2.6999251544475555
  - 2.693379852175713
  - 2.6981029599905018
  - 2.6981718927621845
  - 2.7428806483745576
  - 2.6876638472080234
  - 2.6773572593927386
  - 2.7404134422540665
  - 2.729864826798439
  - 2.7095785558223726
  - 2.661168071627617
  - 2.698791536688805
  - 2.693178689479828
  - 2.6338437229394915
  - 2.701892077922821
  - 2.8351525843143466
  - 2.7124823927879333
  - 2.733741134405136
  - 2.6886419713497163
  - 2.662179496884346
  - 2.6969302117824556
  - 2.7196273744106296
  - 2.6643492579460144
  - 2.7166097044944766
  - 2.7115075409412386
  - 2.686743322014809
  - 2.6990759253501895
  - 2.7173678040504456
  - 2.7000006616115573
  - 2.6582198381423954
  - 2.6896345734596254
  - 2.667330276966095
  - 2.685093539953232
  - 2.6284469544887545
  - 2.64887033700943
  - 2.6937934428453447
  - 2.662054270505905
  - 2.7751520931720735
  - 2.684354317188263
  - 2.6874506145715715
  - 2.670580652356148
  - 2.6656250685453418
  - 2.670197466015816
  - 2.6450936555862428
  - 2.6946296393871307
  - 2.613569161295891
  - 2.661279329657555
  - 2.6898051500320435
  - 2.6643676936626437
  - 2.652156108617783
  - 2.6867302119731904
  - 2.687253433465958
  - 2.6957148611545563
  - 2.6761026769876484
  - 2.6659270465373996
  - 2.6522374123334886
  - 2.681128215789795
  - 2.6316577136516575
  - 2.6518687069416047
  - 2.6794372856616975
  - 2.656650295853615
  - 2.646855038404465
  - 2.7117421984672547
  - 2.616686937212944
  - 2.6274228870868686
  - 2.7286003202199938
  - 2.6869061797857285
  - 2.728056311607361
  - 2.682740363478661
  - 2.68800086081028
  validation_losses:
  - 0.48641127347946167
  - 0.3840641379356384
  - 0.5469783544540405
  - 0.5748535394668579
  - 0.4986155331134796
  - 0.5591409206390381
  - 0.549862265586853
  - 1.5174070596694946
  - 0.817200779914856
  - 0.9308282732963562
  - 0.8113880157470703
  - 1.2403905391693115
  - 0.6749111413955688
  - 0.5603112578392029
  - 0.5174782276153564
  - 0.7255848050117493
  - 0.9880008101463318
  - 0.7597300410270691
  - 0.7816552519798279
  - 0.6488382816314697
  - 0.6965929865837097
  - 0.6386818289756775
  - 0.8931286334991455
  - 1.8412647247314453
  - 0.5396294593811035
  - 0.7814251780509949
  - 0.7701789736747742
  - 1.1523816585540771
  - 1.1793240308761597
  - 0.3738420605659485
  - 0.8108199238777161
  - 0.5331056714057922
  - 0.5298962593078613
  - 0.6143526434898376
  - 0.38939136266708374
  - 0.48869603872299194
  - 0.7859534621238708
  - 0.6945897340774536
  - 0.5971779227256775
  - 0.7278115153312683
  - 0.7465059161186218
  - 0.6904646754264832
  - 0.9033831357955933
  - 0.6935755014419556
  - 0.423770546913147
  - 0.4416016638278961
  - 0.5292865037918091
  - 0.5292394757270813
  - 0.7199820280075073
  - 1.0064401626586914
  - 1.0148992538452148
  - 0.7808482050895691
  - 0.5471863746643066
  - 1.0673877000808716
  - 0.5394724607467651
  - 1.2990567684173584
  - 1.2645814418792725
  - 0.4836583435535431
  - 0.44388139247894287
  - 0.6296961903572083
  - 0.7889400124549866
  - 0.5665110349655151
  - 1.0530346632003784
  - 0.6321352124214172
  - 0.7613627910614014
  - 1.5859416723251343
  - 0.4251643419265747
  - 0.6512038707733154
  - 0.7506322264671326
  - 0.9567802548408508
  - 0.4715939462184906
  - 0.48150333762168884
  - 0.8220409750938416
  - 0.6115598082542419
  - 0.6630383729934692
  - 0.46120479702949524
  - 0.5954610705375671
  - 0.859344482421875
  - 0.7625365853309631
  - 0.8602376580238342
  - 0.8781704902648926
  - 0.913790225982666
  - 0.588581919670105
  - 0.9105297327041626
  - 0.9235590696334839
  - 0.8900450468063354
  - 0.5816739797592163
  - 0.698575496673584
  - 0.5517387986183167
  - 0.765261173248291
  - 0.7046658396720886
  - 1.1268681287765503
  - 0.789415180683136
  - 0.7928494811058044
  - 0.721887469291687
  - 1.5063930749893188
  - 1.0762134790420532
  - 1.6475640535354614
  - 1.1827386617660522
  - 0.8223814368247986
loss_records_fold2:
  train_losses:
  - 2.717005887627602
  - 2.6936767548322678
  - 2.6894363284111025
  - 2.650714468955994
  - 2.7271329492330554
  - 2.68371120095253
  - 2.657709187269211
  - 2.7062505453825
  - 2.646347898244858
  - 2.7364610165357592
  - 2.6210918307304385
  - 2.628463423252106
  - 2.6550420373678207
  - 2.6683128833770753
  - 2.6691915690898895
  - 2.6403513580560687
  - 2.6070306539535526
  - 2.6744681477546695
  - 2.601689672470093
  - 2.709834107756615
  - 2.6344612032175068
  - 2.623674649000168
  - 2.650952598452568
  - 2.5917160838842395
  - 2.6299741327762605
  - 2.5976930022239686
  - 2.6412304371595385
  - 2.656079882383347
  - 2.648166346549988
  - 2.578971871733666
  - 2.6639716863632206
  - 2.6001537382602695
  - 2.6193132609128953
  - 2.6027109026908875
  - 2.6784719765186313
  - 2.664686241745949
  - 2.5983285039663317
  - 2.632891780138016
  - 2.6248626470565797
  - 2.6562394201755524
  - 2.6110769987106326
  - 2.6307815074920655
  - 2.6269102871418
  - 2.60933111011982
  - 2.6135047495365145
  - 2.58203459084034
  - 2.5937245160341265
  - 2.630518209934235
  - 2.580361172556877
  - 2.5956179022789003
  - 2.571821999549866
  - 2.915540462732315
  - 2.92535582780838
  - 2.6970070868730547
  - 2.6852967739105225
  - 2.640977066755295
  - 2.637399357557297
  - 2.581489607691765
  - 2.6483305752277375
  - 2.6032527148723603
  - 2.606221178174019
  - 2.559868556261063
  - 2.634473851323128
  - 2.635234773159027
  - 2.6526220738887787
  - 2.614262670278549
  - 2.6781620264053347
  - 2.604624623060227
  - 2.6514865040779116
  - 2.6626466453075412
  - 2.6346620559692386
  - 2.637474620342255
  - 2.5984688669443132
  - 2.6276528716087344
  - 2.625224715471268
  - 2.6295281976461413
  - 2.6077622413635257
  - 2.6120835661888124
  - 2.642152497172356
  - 2.5739262461662293
  - 2.6412036001682284
  - 2.627464380860329
  - 2.613729932904244
  - 2.600058060884476
  - 2.5834262013435367
  - 2.632879263162613
  - 2.5993415832519533
  - 2.612769028544426
  - 2.6012410610914234
  - 2.583093205094338
  - 2.5813844442367557
  - 2.6487058073282244
  - 2.582819378376007
  - 2.6275414258241656
  - 2.5161779791116716
  - 2.58131967484951
  - 2.572605213522911
  - 2.5656624495983125
  - 2.553192669153214
  - 2.5839739382267
  validation_losses:
  - 0.5662398338317871
  - 0.4502723813056946
  - 1.0211330652236938
  - 0.8034452795982361
  - 0.6686587333679199
  - 0.45025038719177246
  - 0.9034974575042725
  - 0.6158925890922546
  - 0.7047810554504395
  - 0.8235780596733093
  - 0.6297961473464966
  - 0.6812731623649597
  - 0.7802239656448364
  - 0.7193383574485779
  - 0.5565869212150574
  - 0.5933942198753357
  - 0.8271769881248474
  - 0.7197104692459106
  - 0.8115596771240234
  - 0.9054763317108154
  - 1.1163891553878784
  - 0.5771482586860657
  - 1.3429378271102905
  - 0.9025824666023254
  - 0.41578054428100586
  - 0.6693974137306213
  - 0.6008923649787903
  - 0.6501052379608154
  - 0.9791731238365173
  - 0.8292146325111389
  - 2.889486789703369
  - 0.6784152984619141
  - 0.6771314144134521
  - 1.3287326097488403
  - 0.972904622554779
  - 1.2915621995925903
  - 0.6908144950866699
  - 0.6894663572311401
  - 0.7268529534339905
  - 0.6573983430862427
  - 0.6277047395706177
  - 0.5996077656745911
  - 0.5555130839347839
  - 0.6704166531562805
  - 0.8213495016098022
  - 0.7816661596298218
  - 0.725683867931366
  - 0.6202208399772644
  - 0.8134530186653137
  - 0.8281885385513306
  - 0.6344567537307739
  - 0.4130633771419525
  - 0.3944416642189026
  - 0.42501041293144226
  - 0.540550947189331
  - 0.5202080011367798
  - 0.5214468240737915
  - 0.5249050855636597
  - 0.47533145546913147
  - 0.4593817889690399
  - 0.5378534197807312
  - 0.7394453287124634
  - 0.5651755332946777
  - 0.49723637104034424
  - 0.5324615240097046
  - 0.5640414953231812
  - 0.4860159456729889
  - 0.7752650380134583
  - 0.45121145248413086
  - 0.4685993492603302
  - 0.5465492010116577
  - 0.5583888292312622
  - 0.5818578004837036
  - 0.6982275247573853
  - 0.6322627663612366
  - 0.5276525020599365
  - 0.5253189206123352
  - 0.5783668756484985
  - 0.7385185956954956
  - 0.6240600943565369
  - 0.5118066668510437
  - 0.5749732851982117
  - 0.581489086151123
  - 0.5761936902999878
  - 0.5314300060272217
  - 0.6380913257598877
  - 0.6049155592918396
  - 0.669517993927002
  - 0.5267423391342163
  - 0.5656424164772034
  - 0.6629741191864014
  - 0.61333829164505
  - 0.47942373156547546
  - 0.496677428483963
  - 0.5183818340301514
  - 0.7135055065155029
  - 0.7936421632766724
  - 0.7543128132820129
  - 1.3677922487258911
  - 0.6192853450775146
loss_records_fold3:
  train_losses:
  - 2.7055380553007127
  - 2.6635404378175735
  - 2.6392273724079134
  - 2.6781601995229725
  - 2.6248177498579026
  - 2.7575252622365953
  - 2.6639893829822543
  - 2.6581437766551974
  - 2.6245632708072666
  - 2.6100500136613847
  - 2.5999439239501956
  - 2.625663083791733
  - 2.581799799203873
  - 2.6071752250194553
  - 2.586268597841263
  - 2.6138934761285784
  - 2.6219779014587403
  - 2.65469044148922
  - 2.6171492964029315
  - 2.611270704865456
  - 2.5943799257278446
  - 2.5944017618894577
  - 2.624693048000336
  - 2.6113727688789368
  - 2.6488021790981295
  - 2.5955481797456743
  - 2.603693974018097
  - 2.629360184073448
  - 2.601605474948883
  - 2.6588154762983325
  - 2.6694387048482895
  - 2.6056630969047547
  - 2.5895489245653156
  - 2.596297407150269
  - 2.6556495040655137
  - 2.5886132031679154
  - 2.6128061652183536
  - 2.5696687161922456
  - 2.615355882048607
  - 2.6199019193649296
  - 2.5695341497659685
  - 2.6021286070346834
  - 2.6156790256500244
  - 2.6094896972179416
  - 2.5919168442487717
  - 2.6479566484689716
  - 2.6443873137235645
  - 2.564712107181549
  - 2.614979583024979
  - 2.672364076972008
  - 2.578690338134766
  - 2.6246363520622253
  - 2.587898200750351
  - 2.567698204517365
  - 2.607330483198166
  - 2.56954140663147
  - 2.620664444565773
  - 2.6816833436489107
  - 3.1148525059223178
  - 2.8282983481884005
  - 2.8340212166309358
  - 2.815212380886078
  - 2.7699049413204193
  - 2.783211323618889
  - 2.760621163249016
  - 2.723157286643982
  - 2.6650711566209795
  - 2.750430923700333
  - 2.664028823375702
  - 2.7059285908937456
  - 2.64163176715374
  - 2.628493213653565
  - 2.607455289363861
  - 2.617561730742455
  - 2.589046755433083
  - 2.6455887794494632
  - 2.6837923467159275
  - 2.615230768918991
  - 2.602087336778641
  - 2.6491470336914062
  - 2.598611479997635
  - 2.6102709442377092
  - 2.559135702252388
  - 2.6152813315391543
  - 2.622580680251122
  - 2.5800066083669666
  - 2.6177634447813034
  - 2.5539649903774264
  - 2.6001336544752123
  - 2.549254220724106
  - 2.6392130047082905
  - 2.6034750014543535
  - 2.5872476249933243
  - 2.6001102149486544
  - 2.615838879346848
  - 2.6003708064556124
  - 2.626958826184273
  - 2.519513794779778
  - 2.557407656311989
  - 2.758523461222649
  validation_losses:
  - 3.426736831665039
  - 2.769198417663574
  - 4.226496696472168
  - 3.200900077819824
  - 5.591153621673584
  - 2.270517110824585
  - 0.49638888239860535
  - 1.5519077777862549
  - 0.8710488080978394
  - 1.485285997390747
  - 0.8798319697380066
  - 0.48813721537590027
  - 0.5367305278778076
  - 0.4675447940826416
  - 0.5578787326812744
  - 0.6112079620361328
  - 0.545288622379303
  - 0.5303143858909607
  - 0.5069398283958435
  - 0.48174190521240234
  - 0.48844268918037415
  - 0.5602682828903198
  - 0.5671213269233704
  - 0.8827194571495056
  - 0.7938113212585449
  - 0.8486951589584351
  - 0.8913078308105469
  - 0.5450568199157715
  - 0.6530649065971375
  - 0.6495002508163452
  - 0.5087639689445496
  - 0.7015239596366882
  - 0.6332430839538574
  - 1.2248122692108154
  - 1.4753855466842651
  - 1.4129239320755005
  - 1.1055588722229004
  - 0.908877968788147
  - 0.6269146203994751
  - 0.6880484223365784
  - 0.6610823273658752
  - 0.5561543703079224
  - 0.6330724954605103
  - 0.5616860389709473
  - 0.6574441194534302
  - 0.47318020462989807
  - 0.5369863510131836
  - 0.5117795467376709
  - 0.8026625514030457
  - 0.7547393441200256
  - 0.46720486879348755
  - 0.8202582001686096
  - 1.3676704168319702
  - 0.8526966571807861
  - 0.585619330406189
  - 0.9471299648284912
  - 0.6919599175453186
  - 0.9176716804504395
  - 1.0667544603347778
  - 0.5168066620826721
  - 0.8549867272377014
  - 0.4473782181739807
  - 1.1414587497711182
  - 0.6322991251945496
  - 1.8362367153167725
  - 1.389399766921997
  - 1.0557224750518799
  - 1.4985582828521729
  - 0.8159219622612
  - 0.42290645837783813
  - 0.7630064487457275
  - 0.41631731390953064
  - 0.58280348777771
  - 0.6674220561981201
  - 0.580942690372467
  - 0.42877015471458435
  - 0.4453158676624298
  - 0.4576658010482788
  - 0.4786454439163208
  - 0.5049410462379456
  - 0.5909855365753174
  - 0.5241912007331848
  - 0.8180431723594666
  - 0.5196317434310913
  - 0.44866907596588135
  - 0.4766412377357483
  - 0.7340624332427979
  - 0.6205317974090576
  - 0.7506648898124695
  - 0.5621536374092102
  - 0.5582020282745361
  - 0.7570411562919617
  - 0.614764928817749
  - 0.6375810503959656
  - 0.5903152227401733
  - 0.8772283792495728
  - 0.4827767312526703
  - 0.5148283243179321
  - 0.43578818440437317
  - 0.5780476927757263
loss_records_fold4:
  train_losses:
  - 2.8484239399433138
  - 2.765056160092354
  - 2.7194340556859973
  - 2.7991310864686967
  - 2.7394169539213182
  - 2.731258526444435
  - 2.703560096025467
  - 2.6803128123283386
  - 2.667413887381554
  - 2.6687749177217484
  - 2.671986472606659
  - 2.679095256328583
  - 2.6843914091587067
  - 2.6043393313884735
  - 2.707355093955994
  - 2.645201086997986
  - 2.9470443934202195
  - 2.8240796804428103
  - 2.7857238054275513
  - 2.7939438819885254
  - 2.7497008681297306
  - 2.7771891832351687
  - 2.724579781293869
  - 2.696238958835602
  - 2.650840124487877
  - 2.713296261429787
  - 2.6692228466272354
  - 2.706721067428589
  - 2.608866883814335
  - 2.625413638353348
  - 2.6896363437175754
  - 2.6109856575727464
  - 2.701565107703209
  - 2.6773216366767887
  - 2.6758010864257815
  - 2.6313095629215244
  - 2.676212838292122
  - 2.6660424917936325
  - 2.728642785549164
  - 2.774543502926827
  - 2.758863139152527
  - 2.7040859043598178
  - 2.7443717420101166
  - 2.669157195091248
  - 2.7148817449808123
  - 2.6469317078590393
  - 2.6946244597435
  - 2.6295172035694123
  - 2.635555312037468
  - 2.687031638622284
  - 2.6721688300371174
  - 2.635527789592743
  - 2.637150025367737
  - 2.625646167993546
  - 2.631573283672333
  - 2.651072707772255
  - 2.714747554063797
  - 2.5982985585927967
  - 2.598221018910408
  - 2.6785000741481784
  - 2.5963474214076996
  - 2.53592079281807
  - 2.6454434245824814
  - 2.6272193759679796
  - 2.6140250980854036
  - 2.6539879173040393
  - 2.6634570509195328
  - 2.6570647537708285
  - 2.642238655686379
  - 2.650089013576508
  - 2.640213054418564
  - 2.642352372407913
  - 2.6275870084762576
  - 2.6211595803499224
  - 2.642416712641716
  - 2.594761294126511
  - 2.5888534963130954
  - 2.636233851313591
  - 2.6492434203624726
  - 2.5637915551662447
  - 2.620256692171097
  - 2.6048990458250048
  - 2.5420359998941424
  - 2.6193485379219057
  - 2.5906426280736925
  - 2.5269321128726006
  - 2.645506286621094
  - 2.600491759181023
  - 2.590573266148567
  - 2.5817390114068988
  - 2.6521777868270875
  - 2.5340922415256504
  - 2.5174644827842716
  - 2.659947544336319
  - 2.5784975349903108
  - 2.5749871283769608
  - 3.091572937369347
  - 2.734796899557114
  - 2.681785184144974
  - 2.6747513830661775
  validation_losses:
  - 0.3578380346298218
  - 0.39720872044563293
  - 0.5440757274627686
  - 0.6176572442054749
  - 0.4600231945514679
  - 0.45948195457458496
  - 0.46277564764022827
  - 0.5177948474884033
  - 0.5489538311958313
  - 0.5074974298477173
  - 0.45131051540374756
  - 0.509412944316864
  - 0.46628427505493164
  - 0.4061051309108734
  - 0.5428940057754517
  - 0.5875153541564941
  - 0.3833783268928528
  - 0.3861832916736603
  - 0.3748612701892853
  - 0.36931803822517395
  - 0.36790892481803894
  - 0.38809850811958313
  - 0.3791881501674652
  - 0.5039174556732178
  - 0.5526502728462219
  - 0.41771209239959717
  - 0.45000120997428894
  - 0.46822628378868103
  - 0.4265431761741638
  - 0.40254873037338257
  - 0.3892035186290741
  - 0.5364710092544556
  - 0.4720977544784546
  - 0.5518744587898254
  - 0.5577816367149353
  - 0.5379084944725037
  - 0.531420111656189
  - 0.559206485748291
  - 0.4592044949531555
  - 0.3908085227012634
  - 0.3811248540878296
  - 0.4267939329147339
  - 0.41606414318084717
  - 0.36536145210266113
  - 0.43384885787963867
  - 0.41760432720184326
  - 0.427047461271286
  - 0.37037843465805054
  - 0.6455231308937073
  - 0.6381177306175232
  - 0.5493506789207458
  - 0.7112130522727966
  - 0.6277868747711182
  - 0.6540914177894592
  - 0.4864712357521057
  - 0.3744042217731476
  - 0.6371394991874695
  - 0.887344241142273
  - 0.7218171954154968
  - 0.8396226167678833
  - 0.851431131362915
  - 0.4287038743495941
  - 0.7852080464363098
  - 0.4773520827293396
  - 0.47189971804618835
  - 0.881805956363678
  - 0.678579568862915
  - 0.5469510555267334
  - 0.5640565752983093
  - 0.554169774055481
  - 0.5250203609466553
  - 0.5988218188285828
  - 0.5854804515838623
  - 0.6490548849105835
  - 0.5945934653282166
  - 0.6354619264602661
  - 0.558562695980072
  - 0.4603996276855469
  - 0.5819190144538879
  - 0.6046019792556763
  - 0.5741318464279175
  - 0.5332786440849304
  - 0.5253381729125977
  - 0.5038211345672607
  - 0.7221463322639465
  - 0.6150224804878235
  - 0.5367817878723145
  - 0.6248838305473328
  - 0.8195834159851074
  - 0.6300647854804993
  - 0.5061740279197693
  - 0.6993218064308167
  - 0.585769534111023
  - 0.6203473806381226
  - 0.5935478210449219
  - 0.4255845844745636
  - 0.5928197503089905
  - 0.4398379921913147
  - 0.3987519145011902
  - 0.5617188215255737
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8284734133790738, 0.79073756432247, 0.8130360205831904, 0.8439108061749572,
    0.7920962199312714]'
  fold_eval_f1: '[0.18032786885245905, 0.19736842105263158, 0.29677419354838713, 0.1651376146788991,
    0.22929936305732485]'
  mean_eval_accuracy: 0.8136508048781925
  mean_f1_accuracy: 0.21378149223794035
  total_train_time: '0:44:31.542546'
