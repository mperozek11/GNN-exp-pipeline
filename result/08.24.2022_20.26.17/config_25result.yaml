config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:58:05.110836'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_25fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_25fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_25fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_25fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_25fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_25fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_25fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_25fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_25fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_25fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 36.52890232205391
  - 5.986632323265076
  - 7.867078286409378
  - 6.048357892036439
  - 4.778950136899948
  - 3.9352854013442995
  - 6.292116141319275
  - 14.872646033763885
  - 5.257102927565575
  - 4.61537829041481
  - 8.16985610127449
  - 3.763511598110199
  - 3.855819410085678
  - 4.621160835027695
  - 7.097079014778138
  - 5.729220914840699
  - 5.240231812000275
  - 8.957700669765472
  - 8.977474492788316
  - 7.154262620210648
  - 4.161996781826019
  - 3.303677719831467
  - 3.6087104797363283
  - 4.166901034116745
  - 3.364904320240021
  - 3.5084594607353212
  - 4.698454475402833
  - 4.334859842061997
  - 3.7810406267642978
  - 3.076829725503922
  - 2.9109532237052917
  - 4.072716420888901
  validation_losses:
  - 2.655480146408081
  - 0.6896990537643433
  - 1.0507705211639404
  - 0.5491851568222046
  - 1.0068919658660889
  - 0.6551138758659363
  - 2.8528225421905518
  - 1.7531952857971191
  - 0.41199991106987
  - 0.4780062139034271
  - 0.7074635028839111
  - 0.39232388138771057
  - 0.5186726450920105
  - 0.4281242787837982
  - 0.4022292196750641
  - 0.44930407404899597
  - 0.4780159890651703
  - 0.9383929371833801
  - 0.5746927857398987
  - 0.37843891978263855
  - 0.3889161944389343
  - 0.41150587797164917
  - 0.42289525270462036
  - 0.38167816400527954
  - 0.459375262260437
  - 0.6979808211326599
  - 0.48190832138061523
  - 0.39065825939178467
  - 0.39724406599998474
  - 0.3989187180995941
  - 0.38951146602630615
  - 0.3799133598804474
loss_records_fold1:
  train_losses:
  - 3.02467557489872
  - 4.071120262145996
  - 3.475913324952126
  - 3.361580812931061
  - 3.5683187305927277
  - 4.131921249628067
  - 3.308884307742119
  - 3.078460836410523
  - 3.772938346862793
  - 3.3970052838325504
  - 2.9164108633995056
  - 3.0024857074022293
  validation_losses:
  - 0.4692002832889557
  - 0.39640456438064575
  - 0.3989337980747223
  - 0.3960438370704651
  - 0.4214623272418976
  - 0.5045065879821777
  - 0.422980397939682
  - 0.420891135931015
  - 0.4276382327079773
  - 0.40425658226013184
  - 0.3962339162826538
  - 0.3968826234340668
loss_records_fold2:
  train_losses:
  - 3.0230211555957798
  - 3.329428970813751
  - 4.610740423202515
  - 3.745798057317734
  - 4.233184409141541
  - 3.221777188777924
  - 3.598426651954651
  - 3.609192189574242
  - 3.090470105409622
  - 3.474872499704361
  - 3.2218508422374725
  - 3.736179637908936
  - 3.3102217555046085
  - 3.1057318776845935
  - 3.103160148859024
  - 3.1408012717962266
  - 2.9716829836368563
  - 3.07657567858696
  - 3.08522829413414
  - 3.040941244363785
  - 2.9653315007686616
  - 3.21450012922287
  - 2.9003026604652407
  - 3.164136618375778
  - 2.835968601703644
  - 2.895291352272034
  - 3.0102303087711335
  - 3.0120889842510223
  - 2.9157345384359363
  - 3.049836638569832
  - 3.3389511108398438
  - 3.4484718292951584
  - 2.984714472293854
  - 2.9411513566970826
  - 2.9392318725585938
  - 2.9251025676727296
  - 3.111221152544022
  - 2.9383926510810854
  - 3.0901072412729267
  - 3.2955445051193237
  - 3.005505806207657
  - 2.89139915406704
  - 3.111803162097931
  - 3.12883677482605
  - 2.978875941038132
  - 2.9624486714601517
  - 2.9869426131248478
  - 2.9497856080532077
  - 3.0306363582611087
  - 3.027872160077095
  - 3.2407712370157244
  - 3.0506148219108584
  - 2.916510665416718
  - 2.9660919666290284
  - 2.8792782455682757
  - 2.960839438438416
  - 3.073243582248688
  - 3.0451897978782654
  - 2.9214902967214584
  - 2.9446440905332567
  - 2.8959835648536685
  - 3.103510463237763
  - 2.9489163100719455
  - 2.9497273683547975
  validation_losses:
  - 0.3748588263988495
  - 0.3888177275657654
  - 0.45110154151916504
  - 0.3956698179244995
  - 0.3756621181964874
  - 0.389859676361084
  - 0.3786912262439728
  - 0.37669637799263
  - 0.4354381561279297
  - 0.4335293173789978
  - 0.39409154653549194
  - 0.37891557812690735
  - 0.3901849389076233
  - 0.4554508328437805
  - 0.44043540954589844
  - 0.3850705921649933
  - 0.3897782564163208
  - 0.4153998792171478
  - 0.4636129140853882
  - 0.375911146402359
  - 0.4435891807079315
  - 0.40232181549072266
  - 0.4820560812950134
  - 0.37649232149124146
  - 0.37867122888565063
  - 0.38785862922668457
  - 0.4133385121822357
  - 0.4066205620765686
  - 0.3806406259536743
  - 0.37439075112342834
  - 0.43453726172447205
  - 0.3855021297931671
  - 0.3858049213886261
  - 0.38204360008239746
  - 0.3815947473049164
  - 0.40666699409484863
  - 0.37972182035446167
  - 0.3749725818634033
  - 0.4865361750125885
  - 0.3799864649772644
  - 0.3762144446372986
  - 0.39076918363571167
  - 0.41973841190338135
  - 0.3776465654373169
  - 0.38799190521240234
  - 0.41351836919784546
  - 0.3821515440940857
  - 0.37654757499694824
  - 0.3748932182788849
  - 0.49200326204299927
  - 0.37825366854667664
  - 0.43422484397888184
  - 0.3826572299003601
  - 0.4361568093299866
  - 0.4033697843551636
  - 0.38761383295059204
  - 0.37824776768684387
  - 0.45502716302871704
  - 0.4142920970916748
  - 0.4063371419906616
  - 0.3775140643119812
  - 0.3750883936882019
  - 0.37864038348197937
  - 0.37727224826812744
loss_records_fold3:
  train_losses:
  - 2.950078853964806
  - 2.9207516402006153
  - 2.9914722800254823
  - 2.9717980653047564
  - 2.8611280947923663
  - 2.9295914709568027
  - 2.8935208797454837
  - 2.899654480814934
  - 2.913841313123703
  - 2.9650389671325685
  - 2.8170101791620255
  - 2.8743966221809387
  - 3.034162274003029
  - 2.927985352277756
  - 2.973078063130379
  - 2.985380506515503
  - 2.979493457078934
  - 2.974753969907761
  - 2.9959671258926392
  - 2.9661600619554522
  - 2.963504356145859
  - 2.8884195744991303
  - 2.9047062516212465
  - 2.993420505523682
  - 2.895659953355789
  - 2.929001832008362
  - 3.035335382819176
  validation_losses:
  - 0.3827322721481323
  - 0.38777852058410645
  - 0.3979972004890442
  - 0.4360348880290985
  - 0.3824480175971985
  - 0.4330160617828369
  - 0.40846067667007446
  - 0.38539472222328186
  - 0.401792973279953
  - 0.40292808413505554
  - 0.38601434230804443
  - 0.3841243088245392
  - 0.3974781632423401
  - 0.41009819507598877
  - 0.4098266363143921
  - 0.42040690779685974
  - 0.4074751138687134
  - 0.39766210317611694
  - 0.5575345158576965
  - 0.4132143557071686
  - 0.4357598125934601
  - 0.3991730213165283
  - 0.3873025178909302
  - 0.3916994035243988
  - 0.39053234457969666
  - 0.38726142048835754
  - 0.38924461603164673
loss_records_fold4:
  train_losses:
  - 3.0064770936965943
  - 2.953653705120087
  - 2.939546558260918
  - 2.993135169148445
  - 2.9674526095390323
  - 3.000653558969498
  - 2.945579445362091
  - 3.0101609170436863
  - 2.942155158519745
  - 3.148405534029007
  - 2.942208462953568
  - 2.885224634408951
  - 2.9056033879518512
  - 2.997178465127945
  - 2.8769876152276996
  - 3.0296003341674806
  - 2.9381773948669436
  - 3.0702762424945833
  - 3.039309000968933
  - 2.89115828871727
  - 2.8904172092676164
  - 2.8612438559532167
  - 2.842625266313553
  - 2.909863713383675
  - 2.928820395469666
  - 3.0003296434879303
  - 2.9099013805389404
  - 3.0716110438108446
  - 3.0230170488357544
  - 2.898888921737671
  - 3.011139413714409
  - 3.0750069230794908
  - 2.981662756204605
  - 2.9767423033714295
  - 2.9301467776298527
  - 2.866257879137993
  - 2.8686692118644714
  - 2.878374987840653
  - 2.8863871514797212
  - 2.852711203694344
  - 2.9461420208215716
  - 3.0264788389205934
  - 2.935988107323647
  - 3.1551712572574617
  - 2.9515702009201052
  - 2.9471573531627655
  validation_losses:
  - 0.38059085607528687
  - 0.3777633607387543
  - 0.3987962305545807
  - 0.384395956993103
  - 0.40014389157295227
  - 0.4303189814090729
  - 0.4240680932998657
  - 0.3849075138568878
  - 0.39836499094963074
  - 0.3827172815799713
  - 0.3892906904220581
  - 0.37462517619132996
  - 0.504650354385376
  - 0.4003809690475464
  - 0.3769054114818573
  - 0.3992520570755005
  - 0.3979373276233673
  - 0.3798598051071167
  - 0.3766170144081116
  - 0.3750649094581604
  - 0.39450013637542725
  - 0.3741014003753662
  - 0.38158494234085083
  - 0.4065115451812744
  - 0.40487581491470337
  - 0.385692834854126
  - 0.37790414690971375
  - 0.38325291872024536
  - 0.407214492559433
  - 0.3856414258480072
  - 0.4467204511165619
  - 0.42683443427085876
  - 0.3817184269428253
  - 0.39194273948669434
  - 0.392026424407959
  - 0.3733053207397461
  - 0.37702393531799316
  - 0.3761674463748932
  - 0.40338513255119324
  - 0.416861891746521
  - 0.3984781503677368
  - 0.3977832496166229
  - 0.3979280889034271
  - 0.3854367136955261
  - 0.39536136388778687
  - 0.38546398282051086
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 32 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 64 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 46 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:16:10.990006'
