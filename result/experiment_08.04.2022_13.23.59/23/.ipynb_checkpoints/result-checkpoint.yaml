config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 15:54:43.900227'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/23/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/23/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/23/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/23/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/23/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/23/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/23/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/23/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/23/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/23/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 3.8880767822265625
  - 3.9633825719356537
  - 4.23381495475769
  - 4.4089303612709045
  - 4.072049468755722
  - 3.777136340737343
  - 4.083856374025345
  - 4.818889021873474
  - 4.281309902667999
  - 4.167885571718216
  - 3.9318760335445404
  - 3.9540807008743286
  - 3.8888482451438904
  - 3.9209954142570496
  - 6.799499124288559
  - 4.234799414873123
  - 3.938329815864563
  - 4.7008441388607025
  - 4.679504483938217
  - 3.8120451271533966
  - 4.08374559879303
  - 4.493377983570099
  - 4.515309438109398
  - 4.6194102466106415
  - 5.179385483264923
  - 5.377073764801025
  - 6.055404245853424
  - 5.635784089565277
  - 10.248167157173157
  - 5.887379288673401
  - 4.057259917259216
  - 8.083300918340683
  - 4.1282651126384735
  - 4.179213851690292
  - 3.9193867594003677
  - 3.8362115025520325
  - 3.87015138566494
  - 4.160869121551514
  - 4.744577407836914
  - 4.873742550611496
  - 4.089998185634613
  - 4.229099273681641
  - 4.755240947008133
  - 5.670230209827423
  - 5.065275818109512
  - 4.639723092317581
  - 4.7253095507621765
  - 4.322027802467346
  - 3.9858486354351044
  - 3.8286069333553314
  - 3.905963122844696
  - 4.040725260972977
  - 3.9212675392627716
  - 3.9386453926563263
  - 3.848028779029846
  - 4.599746406078339
  - 5.688171327114105
  - 4.318176060914993
  - 4.359057664871216
  - 4.386520236730576
  - 7.467366248369217
  - 7.083851367235184
  - 4.762718886137009
  - 4.234251022338867
  - 4.18424266576767
  - 4.211591392755508
  - 4.270084232091904
  - 4.414125025272369
  - 4.525204598903656
  - 7.969638526439667
  - 6.96185228228569
  - 5.340098291635513
  - 4.342797040939331
  - 4.090096801519394
  - 3.9572263062000275
  - 4.2554726004600525
  - 4.24922102689743
  - 4.0823052525520325
  - 4.277908146381378
  - 4.6014798283576965
  - 6.301098793745041
  - 5.013878136873245
  - 4.426345527172089
  - 4.172659158706665
  - 4.190464407205582
  - 4.203327924013138
  - 3.8991308212280273
  - 4.215218722820282
  - 5.111362904310226
  - 4.556052893400192
  - 5.015462517738342
  - 5.107314497232437
  - 7.297597765922546
  - 5.937250435352325
  - 5.689906507730484
  - 5.7915990352630615
  - 4.120634347200394
  - 4.176354825496674
  - 3.849317654967308
  - 4.458507299423218
  validation_losses:
  - 0.40621787309646606
  - 0.4230276048183441
  - 0.47646117210388184
  - 0.5238013863563538
  - 0.40770429372787476
  - 0.43851080536842346
  - 0.4622921049594879
  - 0.5187525153160095
  - 0.40861350297927856
  - 0.451779842376709
  - 0.4021497368812561
  - 0.4160386621952057
  - 0.40312525629997253
  - 0.4035263657569885
  - 0.4366505444049835
  - 0.40365561842918396
  - 0.5269727110862732
  - 0.421988844871521
  - 0.404558002948761
  - 0.46081113815307617
  - 0.44769832491874695
  - 0.554462194442749
  - 0.6414166688919067
  - 0.4463992118835449
  - 0.7878379821777344
  - 0.7695022821426392
  - 0.7316490411758423
  - 0.9581412076950073
  - 0.4232044219970703
  - 0.46578681468963623
  - 0.41262558102607727
  - 0.432325154542923
  - 0.4140781760215759
  - 0.4094651937484741
  - 0.47068145871162415
  - 0.4302091896533966
  - 0.4501970112323761
  - 0.4797147810459137
  - 0.4169383943080902
  - 0.45982465147972107
  - 0.47287651896476746
  - 0.4199729263782501
  - 0.4157145023345947
  - 0.5603936314582825
  - 0.46295639872550964
  - 0.5790051817893982
  - 0.43937110900878906
  - 0.406486451625824
  - 0.4520304501056671
  - 0.44308263063430786
  - 0.42280539870262146
  - 0.4177454113960266
  - 0.42520418763160706
  - 0.43623846769332886
  - 0.5207275152206421
  - 0.713637113571167
  - 0.4399775266647339
  - 0.5069359540939331
  - 0.4170173704624176
  - 0.8699369430541992
  - 0.9134823083877563
  - 0.6428330540657043
  - 0.4309733211994171
  - 6.855556964874268
  - 9747.615234375
  - 205.32821655273438
  - 1458.7398681640625
  - 5655.35400390625
  - 1787.017578125
  - 0.5916615724563599
  - 0.6702575087547302
  - 0.6294382214546204
  - 0.4564273953437805
  - 0.40415236353874207
  - 0.4089414179325104
  - 0.4689904749393463
  - 0.4600192606449127
  - 0.45892637968063354
  - 0.4607691168785095
  - 0.41094204783439636
  - 0.4220697581768036
  - 0.4574393630027771
  - 0.49226173758506775
  - 0.4259167015552521
  - 0.4772246778011322
  - 0.4298873543739319
  - 0.4455712139606476
  - 0.5747449398040771
  - 0.5529200434684753
  - 0.5611313581466675
  - 0.570928156375885
  - 0.9257610440254211
  - 0.5489581823348999
  - 0.6733924150466919
  - 0.44733715057373047
  - 0.5355061292648315
  - 0.44186440110206604
  - 0.45448604226112366
  - 0.47029799222946167
  - 0.40957197546958923
loss_records_fold2:
  train_losses:
  - 4.145177334547043
  - 3.9329161643981934
  - 4.220652014017105
  - 4.181688636541367
  - 3.9895243048667908
  - 4.232231944799423
  - 4.3659723699092865
  - 4.254141122102737
  - 4.004578500986099
  - 4.189599856734276
  - 4.101266533136368
  - 4.1821281015872955
  - 4.5037713050842285
  - 4.337549597024918
  - 4.805583208799362
  - 8.580262184143066
  - 8.555083692073822
  - 10.111398220062256
  - 6.434305518865585
  - 6.830543547868729
  - 5.204663246870041
  - 4.322674512863159
  - 4.303301960229874
  - 4.196298375725746
  - 5.4897103905677795
  - 5.304546415805817
  - 4.617044359445572
  - 4.486775815486908
  - 5.422915399074554
  - 5.257140129804611
  - 5.5079792737960815
  - 4.680568605661392
  - 4.657851308584213
  - 4.464580535888672
  - 4.814149081707001
  - 3.948761209845543
  - 4.091066777706146
  - 4.673630774021149
  - 4.686368525028229
  - 5.125004440546036
  - 4.751711130142212
  - 4.132105261087418
  - 4.031515121459961
  - 4.078867107629776
  - 4.226081401109695
  - 3.9714932441711426
  - 4.2205986976623535
  - 4.457717567682266
  - 4.436577498912811
  - 4.24290207028389
  - 3.997605413198471
  - 4.243406742811203
  - 4.496115460991859
  - 4.034380406141281
  - 4.0138856172561646
  - 3.9466071724891663
  - 4.330446541309357
  - 4.26840814948082
  - 3.9732205867767334
  - 4.049939721822739
  - 4.305201083421707
  - 5.072835385799408
  - 5.948563784360886
  - 4.319879353046417
  - 4.387172996997833
  - 4.674204468727112
  - 4.552954018115997
  - 4.283423602581024
  - 4.689407199621201
  - 3.9884878993034363
  - 4.030680701136589
  - 5.007164835929871
  - 6.665823221206665
  - 6.904665231704712
  - 7.400050550699234
  - 7.935243904590607
  - 6.638572573661804
  - 4.561912089586258
  - 4.355470031499863
  - 4.744067966938019
  - 4.851987808942795
  - 4.64165934920311
  - 5.580089330673218
  - 6.057128548622131
  - 5.4691842794418335
  - 5.722118973731995
  - 4.595552533864975
  - 4.291716068983078
  - 4.303352266550064
  - 4.068162202835083
  - 4.250195682048798
  - 4.1544970870018005
  - 4.0586318373680115
  - 3.999960631132126
  - 4.384829759597778
  - 4.030176818370819
  - 4.091502517461777
  - 4.343381196260452
  - 4.961801052093506
  - 5.0510328114032745
  validation_losses:
  - 0.38327595591545105
  - 0.3850153684616089
  - 0.42139849066734314
  - 0.4000905156135559
  - 0.4171648621559143
  - 0.4273580312728882
  - 0.5471277832984924
  - 0.4038691222667694
  - 0.4009144604206085
  - 0.3851792812347412
  - 0.39568638801574707
  - 0.5337718725204468
  - 0.4800705313682556
  - 0.4401610493659973
  - 0.4536553919315338
  - 1.0411192178726196
  - 0.7426896691322327
  - 0.673426628112793
  - 0.7976418137550354
  - 0.5139623880386353
  - 0.47410309314727783
  - 0.4208916425704956
  - 0.393625944852829
  - 0.5254674553871155
  - 0.559020459651947
  - 0.4374338984489441
  - 0.42554157972335815
  - 0.6594207882881165
  - 0.5019851326942444
  - 0.6273064017295837
  - 0.5341192483901978
  - 0.38437601923942566
  - 0.39521557092666626
  - 0.48689934611320496
  - 0.41225287318229675
  - 0.43292075395584106
  - 0.4174656569957733
  - 0.4501818120479584
  - 0.4377007484436035
  - 0.49537861347198486
  - 0.4038311541080475
  - 0.4246200919151306
  - 0.38945022225379944
  - 0.38469088077545166
  - 0.41647273302078247
  - 0.400124192237854
  - 0.43268781900405884
  - 0.42559313774108887
  - 0.4850112199783325
  - 0.40540811419487
  - 0.39793285727500916
  - 0.42299884557724
  - 0.38782599568367004
  - 0.4151618182659149
  - 0.3922354280948639
  - 0.39371562004089355
  - 0.5951768755912781
  - 0.38343992829322815
  - 0.3864492177963257
  - 0.39175185561180115
  - 0.4849470853805542
  - 0.4392639100551605
  - 0.48327428102493286
  - 0.399076372385025
  - 0.4280170202255249
  - 0.5195155143737793
  - 0.38578861951828003
  - 0.44497811794281006
  - 0.3833133280277252
  - 0.39408034086227417
  - 0.5251333713531494
  - 0.7259604930877686
  - 0.7947124242782593
  - 0.5056829452514648
  - 0.40688765048980713
  - 0.42801403999328613
  - 0.47469890117645264
  - 0.41382575035095215
  - 0.5115566849708557
  - 0.477008193731308
  - 0.38378602266311646
  - 0.9697755575180054
  - 0.5774006247520447
  - 0.38678988814353943
  - 0.5443859696388245
  - 0.6396629810333252
  - 25842.990234375
  - 211.98843383789062
  - 0.3847410976886749
  - 0.4025591015815735
  - 0.4439694285392761
  - 0.4182884395122528
  - 0.389865517616272
  - 0.4055141806602478
  - 0.38398149609565735
  - 0.3847881853580475
  - 0.3964543044567108
  - 0.6154082417488098
  - 0.6254618763923645
  - 0.441053569316864
loss_records_fold3:
  train_losses:
  - 4.178224742412567
  - 3.9205389320850372
  - 3.895011305809021
  - 4.052846610546112
  - 3.8063733875751495
  - 4.286628633737564
  - 4.048679500818253
  - 4.038141876459122
  - 4.587381452322006
  - 4.600824147462845
  - 4.158885031938553
  - 4.126641869544983
  - 3.844014421105385
  - 4.1473861038684845
  - 7.574372947216034
  - 6.953199326992035
  - 6.869784086942673
  - 4.122452110052109
  - 5.512731969356537
  - 4.406712144613266
  - 4.895574539899826
  - 6.080060124397278
  - 7.030124366283417
  - 5.745060831308365
  - 4.261582285165787
  - 4.281972497701645
  - 4.193843722343445
  - 5.873983338475227
  - 7.03076359629631
  - 6.230302959680557
  - 4.766947418451309
  - 4.633526504039764
  - 7.222211733460426
  - 5.689809024333954
  - 4.657238066196442
  - 3.9817181527614594
  - 4.646018385887146
  - 4.19489985704422
  - 3.8831440210342407
  - 3.911099851131439
  - 4.332594126462936
  - 4.067516386508942
  - 4.162454605102539
  - 4.263119846582413
  - 4.1161757707595825
  - 3.8802555799484253
  - 4.281971454620361
  - 6.186140924692154
  - 5.154180616140366
  - 5.407373428344727
  - 5.332043558359146
  - 4.720431953668594
  - 4.052622064948082
  - 4.4812358021736145
  - 4.844872683286667
  - 6.346552163362503
  - 6.058900833129883
  - 6.533106669783592
  - 5.00375908613205
  - 4.456862717866898
  - 4.007106676697731
  - 4.170713543891907
  - 3.8933458626270294
  - 4.139673233032227
  - 4.1034999787807465
  - 4.052769660949707
  - 4.0392806231975555
  - 4.053130567073822
  - 4.415346026420593
  - 4.379034221172333
  - 5.546706646680832
  - 5.274778425693512
  - 9.859943836927414
  - 8.254686921834946
  - 7.254349559545517
  - 7.4393629133701324
  - 6.55403932929039
  - 4.988363444805145
  - 4.483616143465042
  - 4.129542022943497
  - 3.9377303421497345
  - 4.024362921714783
  - 3.9086427986621857
  - 3.993354946374893
  - 4.027427971363068
  - 4.001363188028336
  - 4.660445034503937
  - 4.294564947485924
  - 3.9694197475910187
  - 4.323350042104721
  - 9.371292650699615
  - 7.764995038509369
  - 5.533695071935654
  - 4.456957757472992
  - 3.948469489812851
  - 3.9336269795894623
  - 4.0023492872715
  - 3.9327329993247986
  - 3.93200346827507
  - 4.068841218948364
  validation_losses:
  - 0.41591569781303406
  - 0.3991047143936157
  - 0.4171789288520813
  - 0.40526100993156433
  - 0.4569501280784607
  - 0.41462042927742004
  - 0.4111831486225128
  - 0.47074753046035767
  - 0.40553736686706543
  - 0.44804999232292175
  - 0.4300420880317688
  - 0.40638187527656555
  - 0.4198252856731415
  - 0.7314755916595459
  - 0.7473191618919373
  - 1.0642008781433105
  - 0.4716377854347229
  - 0.5129418969154358
  - 0.4057215750217438
  - 0.5844213962554932
  - 0.7136410474777222
  - 0.7770286202430725
  - 0.6590542197227478
  - 0.4398486018180847
  - 0.4688147008419037
  - 0.4009367525577545
  - 0.6959176063537598
  - 0.8716747164726257
  - 0.8149569630622864
  - 0.4115138351917267
  - 0.5126433372497559
  - 1.2695748805999756
  - 0.6409212946891785
  - 0.5894381999969482
  - 0.44919726252555847
  - 0.5399108529090881
  - 0.43609824776649475
  - 0.3995300829410553
  - 0.407442569732666
  - 0.4768633544445038
  - 0.39795833826065063
  - 0.4065704047679901
  - 0.5004423260688782
  - 0.42323416471481323
  - 0.4053211510181427
  - 0.44046321511268616
  - 0.5737379193305969
  - 0.6222840547561646
  - 0.7196367383003235
  - 0.6681972742080688
  - 0.5494822859764099
  - 0.4045630991458893
  - 0.502212643623352
  - 0.39947086572647095
  - 0.4528294801712036
  - 0.506939172744751
  - 0.760032057762146
  - 0.6709584593772888
  - 0.46282437443733215
  - 0.40807685256004333
  - 0.46292686462402344
  - 0.3985866606235504
  - 0.4055210053920746
  - 0.44365984201431274
  - 0.41655462980270386
  - 0.39890623092651367
  - 0.4022766649723053
  - 0.4882720112800598
  - 0.4143686294555664
  - 0.9481683969497681
  - 0.5284838676452637
  - 2.4766509532928467
  - 0.9951453804969788
  - 0.5304069519042969
  - 0.8365105390548706
  - 0.8821230530738831
  - 0.6155790090560913
  - 0.4458963871002197
  - 0.39741796255111694
  - 0.41505321860313416
  - 0.4000304043292999
  - 0.3988497257232666
  - 0.40527036786079407
  - 0.40309247374534607
  - 0.4181332290172577
  - 5490174.5
  - 85373552.0
  - 792565568.0
  - 104733504.0
  - 2417215.75
  - 15527248.0
  - 3308617.75
  - 18273102.0
  - 99776304.0
  - 20402844.0
  - 99047144.0
  - 428.1918640136719
  - 31.8055419921875
  - 0.4187314808368683
  - 0.40241771936416626
loss_records_fold4:
  train_losses:
  - 3.8624759763479233
  - 4.091184765100479
  - 3.853965997695923
  - 4.592259585857391
  - 12.781781166791916
  - 6.157686948776245
  - 7.729426711797714
  - 4.201487086713314
  - 4.449309170246124
  - 4.081130564212799
  - 4.1546216905117035
  - 4.239890366792679
  - 4.784369200468063
  - 19.42572256922722
  - 8.15637144446373
  - 8.065489828586578
  - 8.167417705059052
  - 6.08244788646698
  - 5.038121432065964
  - 5.928651422262192
  - 5.023998260498047
  - 4.225015044212341
  - 4.060135632753372
  - 3.8525105714797974
  - 4.163839012384415
  - 4.084045112133026
  - 4.068054795265198
  - 4.12653523683548
  - 17.385623455047607
  - 14.986525237560272
  - 10.819946587085724
  - 10.881496518850327
  - 5.772918403148651
  - 154.36616411805153
  - 8.080970376729965
  - 4.351915746927261
  - 20.346937745809555
  - 20.240209192037582
  - 21.07621693611145
  - 13.083987802267075
  - 5.809892117977142
  - 4.386174380779266
  - 4.739445477724075
  - 4.865776717662811
  - 3.896891623735428
  - 4.32688045501709
  - 3.8717979043722153
  - 3.9147912859916687
  - 7.145473390817642
  - 17.459951013326645
  - 9.631231188774109
  - 5.493734896183014
  - 4.049745261669159
  - 4.100963234901428
  - 4.042852878570557
  - 4.600170016288757
  - 4.068561434745789
  - 4.306360214948654
  - 4.169581443071365
  - 4.115945875644684
  - 3.933782309293747
  - 4.502640008926392
  - 4.096771866083145
  - 4.260991841554642
  - 4.138882547616959
  - 4.163193166255951
  - 4.22526228427887
  - 4.4257925152778625
  - 4.125086084008217
  - 4.660950094461441
  - 4.230595201253891
  - 4.264495223760605
  - 4.998412519693375
  - 4.318992018699646
  - 4.000716030597687
  - 4.361929178237915
  - 3.954969733953476
  - 4.277503222227097
  - 5.0211906135082245
  - 3.988663285970688
  - 4.695312172174454
  - 6.17108690738678
  - 4.679983019828796
  - 4.35441580414772
  - 4.718186765909195
  - 4.2261922955513
  - 4.206190258264542
  - 4.115636825561523
  - 3.9049932062625885
  - 4.278662294149399
  - 5.425701767206192
  - 6.844783306121826
  - 6.966991513967514
  - 11.168596804141998
  - 9.453476160764694
  - 6.806183069944382
  - 5.217095494270325
  - 4.367707788944244
  - 4.087960451841354
  - 4.162698209285736
  validation_losses:
  - 0.41205742955207825
  - 0.39234721660614014
  - 0.4535493552684784
  - 205.51458740234375
  - 8.281966209411621
  - 0.7549049854278564
  - 1.6740792989730835
  - 0.6963979005813599
  - 0.4991706907749176
  - 0.4571765065193176
  - 0.45704373717308044
  - 0.4533461928367615
  - 0.6300034523010254
  - 22.4081974029541
  - 1.9737045764923096
  - 0.7906908392906189
  - 0.8407273292541504
  - 0.42103955149650574
  - 0.49156445264816284
  - 0.465025931596756
  - 1.113552212715149
  - 24.952430725097656
  - 18.32002830505371
  - 2.521144151687622
  - 0.4028376340866089
  - 0.4143717288970947
  - 0.5421075820922852
  - 0.44162264466285706
  - 6.945751190185547
  - 4.179447174072266
  - 1066.7613525390625
  - 2.2043919563293457
  - 0.6110172867774963
  - 0.490963876247406
  - 5.071146488189697
  - 0.7238405346870422
  - 1.2336864471435547
  - 3.742098569869995
  - 3.80831241607666
  - 0.9026687741279602
  - 0.46184295415878296
  - 0.663213849067688
  - 0.8608297109603882
  - 0.9428542256355286
  - 0.9532678723335266
  - 1.154579997062683
  - 0.6215620636940002
  - 0.8097358345985413
  - 0.39730411767959595
  - 0.47982558608055115
  - 2.628814935684204
  - 0.41902583837509155
  - 0.39795562624931335
  - 0.39447253942489624
  - 0.4251823425292969
  - 0.4716590940952301
  - 0.39223572611808777
  - 0.39438533782958984
  - 0.40401792526245117
  - 0.4785531759262085
  - 0.4256903827190399
  - 0.4128080904483795
  - 0.4048033654689789
  - 0.4462759494781494
  - 0.47806304693222046
  - 3158240.75
  - 65153532.0
  - 591833088.0
  - 2349281024.0
  - 3.729357039835546e+16
  - 4.097009160950907e+17
  - 5.272000543986811e+17
  - 6.39463868086616e+17
  - 7.805852234389914e+17
  - 6.967060239765996e+17
  - 4.299376337627382e+17
  - 6.071302547707003e+17
  - 7.219396783951053e+17
  - 3.624759983289139e+17
  - 5.0549610718324326e+17
  - 5.967446115321119e+17
  - 5.040224523643781e+17
  - 3.669667474141348e+17
  - 3.151121641405153e+17
  - 2.189457160792965e+17
  - 4.327253424157491e+17
  - 6.216778931178045e+17
  - 3.057768637440983e+17
  - 5.673181413197742e+17
  - 3.3860175259919974e+17
  - 4.154842785377157e+17
  - 5.425339122791219e+17
  - 3.413125985174815e+17
  - 4.3054119692691046e+17
  - 4.8913338129711104e+17
  - 5.32002102713516e+17
  - 5.512754076770632e+17
  - 3.924516745993257e+17
  - 8.005662359337042e+17
  - 4.836351359634637e+17
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.140893470790378]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.24698795180722893]'
  mean_eval_accuracy: 0.7149711469882643
  mean_f1_accuracy: 0.04939759036144579
  total_train_time: '0:03:22.945237'
