config:
  aggregation: mean
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:57:33.196155'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_67fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_67fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_67fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_67fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_67fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_67fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_67fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_67fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_67fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_67fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 0.9335712969303132
  - 0.8539028108119965
  - 0.8080668270587922
  - 0.7474768996238709
  - 0.802073472738266
  - 0.7768232226371765
  - 0.7887612879276276
  - 0.7571991592645646
  - 0.7887532591819764
  - 0.7719874143600465
  - 0.8175672352313996
  - 0.7718911826610566
  - 0.7442632198333741
  - 0.7553775072097779
  - 0.7761143267154694
  - 0.8018334388732911
  - 0.8427945077419281
  - 0.7721801221370698
  - 0.7509201407432556
  - 0.762973815202713
  - 0.7777781367301941
  - 0.7786013245582581
  - 0.7940845847129823
  validation_losses:
  - 0.4177800416946411
  - 0.43037471175193787
  - 0.4366776645183563
  - 0.428920179605484
  - 0.39945653080940247
  - 0.4161302149295807
  - 0.4269272983074188
  - 0.4002538025379181
  - 0.3905496597290039
  - 0.3825916647911072
  - 0.39268484711647034
  - 0.40540093183517456
  - 0.4265521466732025
  - 0.38430356979370117
  - 0.3896004557609558
  - 0.39389273524284363
  - 0.40604084730148315
  - 0.3861841559410095
  - 0.3836354613304138
  - 0.38346540927886963
  - 0.38298264145851135
  - 0.3893086314201355
  - 0.38382264971733093
loss_records_fold1:
  train_losses:
  - 0.7407501459121705
  - 0.7569815158843994
  - 0.745215255022049
  - 0.7415416717529297
  - 0.8033994853496552
  - 0.7665320217609406
  - 0.7596295833587647
  - 0.7599375069141389
  - 0.721754738688469
  - 0.7803773701190949
  - 0.8051675140857697
  validation_losses:
  - 0.38660556077957153
  - 0.38753175735473633
  - 0.38923320174217224
  - 0.38612595200538635
  - 0.3987720012664795
  - 0.3883049786090851
  - 0.3861115872859955
  - 0.3878568708896637
  - 0.3853665590286255
  - 0.39215272665023804
  - 0.38862043619155884
loss_records_fold2:
  train_losses:
  - 0.7553199768066406
  - 0.8087803900241852
  - 0.7730420470237732
  - 0.7740092873573303
  - 0.8096400260925294
  - 0.7722330272197724
  - 0.7695864379405976
  - 0.7729759216308594
  - 0.7467740058898926
  - 0.7506150722503663
  - 0.7356014132499695
  - 0.7605562388896943
  - 0.7336788833141328
  - 0.7887302577495575
  - 0.7429369866847992
  - 0.7637396037578583
  - 0.7660540521144867
  - 0.7613668382167816
  - 0.746963268518448
  - 0.7607625603675843
  - 0.7987622916698456
  - 0.7711053252220155
  - 0.753850781917572
  - 0.8151654362678529
  - 0.7363989770412446
  - 0.7882340729236603
  validation_losses:
  - 0.3814351558685303
  - 0.3830483853816986
  - 0.3829045295715332
  - 0.38248810172080994
  - 0.3837507963180542
  - 0.3856481909751892
  - 0.38967183232307434
  - 0.3884695768356323
  - 0.38316166400909424
  - 0.3836360275745392
  - 0.3977380394935608
  - 0.40241408348083496
  - 0.3875386416912079
  - 0.4687465727329254
  - 0.398025244474411
  - 0.38430458307266235
  - 0.38709983229637146
  - 0.387666791677475
  - 0.3996536433696747
  - 0.4121490716934204
  - 0.38243210315704346
  - 0.386959433555603
  - 0.38869237899780273
  - 0.38497599959373474
  - 0.39042580127716064
  - 0.38798898458480835
loss_records_fold3:
  train_losses:
  - 0.7790605306625367
  - 0.7429028868675233
  - 0.7559416592121124
  - 0.7517548680305481
  - 0.7473868310451508
  - 0.7583324611186981
  - 0.7841217160224915
  - 0.8455475568771362
  - 0.7604130983352662
  - 0.8088659763336182
  - 0.7835640549659729
  - 0.7893625557422639
  - 0.7929880201816559
  validation_losses:
  - 0.36785945296287537
  - 0.4168385863304138
  - 0.3714557886123657
  - 0.3684614598751068
  - 0.3670189678668976
  - 0.3673408329486847
  - 0.37875041365623474
  - 0.3789457082748413
  - 0.37177205085754395
  - 0.3746768832206726
  - 0.3686704933643341
  - 0.37619298696517944
  - 0.3767779767513275
loss_records_fold4:
  train_losses:
  - 0.7374241113662721
  - 0.7478414714336395
  - 0.7801559925079347
  - 0.7347088962793351
  - 0.7464403808116913
  - 0.7490936279296876
  - 0.7451049029827118
  - 0.8039772391319275
  - 0.7764525413513184
  - 0.8116247594356537
  - 0.724324107170105
  - 0.7384765625
  - 0.7489894330501556
  - 0.7714527249336243
  - 0.7406856000423432
  - 0.7428054451942444
  validation_losses:
  - 0.3771214485168457
  - 0.3820643424987793
  - 0.3820899426937103
  - 0.3812975585460663
  - 0.37926170229911804
  - 0.3833284080028534
  - 0.3888377249240875
  - 0.41661781072616577
  - 0.3912366032600403
  - 0.41787463426589966
  - 0.38250818848609924
  - 0.3868187963962555
  - 0.3858875334262848
  - 0.3851202130317688
  - 0.3848644196987152
  - 0.38957083225250244
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:07:29.608985'
