config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 19:34:12.403331'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/58/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/58/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/58/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/58/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/58/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/58/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/58/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/58/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/58/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/58/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 237.42245107889175
  - 63.05632929503918
  - 72.15400895476341
  - 38.452576860785484
  - 32.7386876642704
  - 27.065191388130188
  - 35.13620239496231
  - 64.78875136375427
  - 37.81009143590927
  - 26.826466590166092
  - 44.44567510485649
  - 29.97856730222702
  - 26.11672931909561
  - 30.681435883045197
  - 27.261665374040604
  - 23.489921987056732
  - 38.89422193169594
  - 38.190992176532745
  - 36.781714379787445
  - 58.99788200855255
  - 30.785566419363022
  - 26.78415447473526
  - 18.140179723501205
  - 26.897698134183884
  - 47.15483286976814
  - 40.90342527627945
  - 42.63704487681389
  - 31.091818690299988
  - 21.155796974897385
  - 16.344965904951096
  - 16.75289359688759
  - 32.22696337103844
  - 26.823335230350494
  - 25.171829774975777
  - 21.218224495649338
  - 19.016228824853897
  - 26.566764801740646
  - 26.716083258390427
  - 23.14766737818718
  - 21.516650438308716
  - 17.049560368061066
  - 17.03215628862381
  - 21.289920389652252
  - 24.33000421524048
  - 20.156756550073624
  - 16.479916363954544
  - 18.6385857462883
  - 15.892111539840698
  - 16.20819680392742
  - 19.613430738449097
  - 17.826695263385773
  - 17.403958410024643
  - 16.261858075857162
  - 16.356421679258347
  - 17.09561875462532
  - 17.507053524255753
  - 17.774841904640198
  - 19.75287914276123
  - 17.278199583292007
  - 19.78108249604702
  - 17.5814011991024
  - 16.384710788726807
  - 16.09162750840187
  - 17.11985021829605
  - 16.822147130966187
  - 17.20520506799221
  - 16.857817709445953
  - 16.132274597883224
  - 16.408017873764038
  - 16.82073226571083
  - 17.711484611034393
  - 17.835507556796074
  - 16.37666392326355
  - 16.431494504213333
  - 16.36268761754036
  - 15.304147109389305
  - 15.482229769229889
  - 16.026446372270584
  - 15.76088497042656
  - 16.313694834709167
  - 15.505527377128601
  - 16.83862403035164
  - 16.159613609313965
  - 16.31833729147911
  - 16.029409289360046
  - 17.26918065547943
  - 15.71596759557724
  - 16.138204872608185
  - 15.993201971054077
  - 15.677973732352257
  - 15.683405354619026
  - 16.056673735380173
  - 15.516850262880325
  - 16.168484047055244
  - 16.110151261091232
  - 15.95100012421608
  - 16.084765881299973
  - 16.123159781098366
  - 16.464785546064377
  - 16.345466032624245
  validation_losses:
  - 4.781285285949707
  - 0.5643475651741028
  - 0.4255356192588806
  - 0.6144754886627197
  - 0.5450375080108643
  - 0.46525195240974426
  - 0.8181671500205994
  - 0.5278987884521484
  - 0.4964025914669037
  - 0.44746214151382446
  - 0.5139982104301453
  - 0.42990270256996155
  - 0.4740775525569916
  - 0.40967223048210144
  - 0.468835711479187
  - 0.4394614100456238
  - 0.6957771182060242
  - 0.42252811789512634
  - 0.4706820845603943
  - 0.3959012031555176
  - 0.41434529423713684
  - 0.41016262769699097
  - 0.45163917541503906
  - 0.3977031111717224
  - 0.41847726702690125
  - 0.4716850519180298
  - 0.8019104599952698
  - 0.4160388708114624
  - 0.43310388922691345
  - 0.47393471002578735
  - 0.41464653611183167
  - 0.42431050539016724
  - 0.45388004183769226
  - 0.5001857280731201
  - 0.40168365836143494
  - 0.5426546931266785
  - 0.40835413336753845
  - 0.47538676857948303
  - 0.4150032699108124
  - 0.4200817048549652
  - 0.39575427770614624
  - 0.4277932345867157
  - 0.3936599791049957
  - 0.39988183975219727
  - 0.4010161757469177
  - 0.3957412838935852
  - 0.41281604766845703
  - 0.3849538266658783
  - 0.3824155330657959
  - 0.5093173384666443
  - 0.4003165662288666
  - 0.39095714688301086
  - 0.40914711356163025
  - 0.38845095038414
  - 0.39843159914016724
  - 0.4865030348300934
  - 0.3939446806907654
  - 0.47002702951431274
  - 0.41651609539985657
  - 0.44452670216560364
  - 0.3921721279621124
  - 0.41060671210289
  - 0.40602198243141174
  - 0.39926791191101074
  - 0.3958708643913269
  - 0.4005085229873657
  - 0.43288418650627136
  - 0.42279237508773804
  - 0.41584455966949463
  - 0.3958037197589874
  - 0.718594491481781
  - 0.40918171405792236
  - 0.4119665324687958
  - 0.3950340449810028
  - 0.4108414053916931
  - 0.4088385999202728
  - 0.41472190618515015
  - 0.40561050176620483
  - 0.38520607352256775
  - 0.4040142297744751
  - 0.397176057100296
  - 0.3967595100402832
  - 0.41544023156166077
  - 0.40382373332977295
  - 0.46795451641082764
  - 0.4110182821750641
  - 0.4211735129356384
  - 0.4239536225795746
  - 0.4656538665294647
  - 0.4195711612701416
  - 0.4303801655769348
  - 0.41977459192276
  - 0.4493259787559509
  - 0.4076179265975952
  - 0.4753803312778473
  - 0.4401859641075134
  - 2.6513781547546387
  - 0.43867483735084534
  - 0.5055945515632629
  - 0.3884662091732025
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 59 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 36 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 80 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:07:03.702450'
