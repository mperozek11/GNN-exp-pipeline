config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:03:40.313875'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_70fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_70fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_70fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_70fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_70fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_70fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_70fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_70fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_70fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_70fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.843984192609787
  - 1.6772219181060792
  - 1.642768681049347
  - 1.6043562829494478
  - 1.7184838950634003
  - 1.581113749742508
  - 1.586473023891449
  - 1.5205671876668931
  - 1.6065570294857026
  - 1.5822715222835542
  - 1.6255432784557344
  - 1.6356218099594118
  - 1.6709630489349365
  - 1.6199839234352114
  - 1.5972524523735048
  - 1.7986191868782044
  - 1.807093244791031
  - 1.6090386390686036
  - 1.546541541814804
  - 1.568526601791382
  - 1.6233097791671753
  - 1.5894653260707856
  - 1.6061584115028382
  validation_losses:
  - 0.4897756278514862
  - 0.4088239371776581
  - 0.40149015188217163
  - 0.4005863666534424
  - 0.40631967782974243
  - 0.4155736267566681
  - 0.394229918718338
  - 0.3820202648639679
  - 0.4087027907371521
  - 0.4242924451828003
  - 0.4180408716201782
  - 0.3935791552066803
  - 0.3982451260089874
  - 0.38659167289733887
  - 0.4426623582839966
  - 0.38551780581474304
  - 0.41480353474617004
  - 0.3954969346523285
  - 0.3823447525501251
  - 0.3870139420032501
  - 0.3926560878753662
  - 0.3933100998401642
  - 0.3924984633922577
loss_records_fold1:
  train_losses:
  - 1.5518501043319703
  - 1.5585832178592682
  - 1.555784744024277
  - 1.564788889884949
  - 1.6442183554172516
  - 1.6032665193080904
  - 1.5835978627204896
  - 1.5900503516197206
  - 1.5107966274023057
  - 1.6428686559200287
  - 1.619891345500946
  validation_losses:
  - 0.3937916159629822
  - 0.404703289270401
  - 0.3981291949748993
  - 0.3978903293609619
  - 0.4136437475681305
  - 0.4010818302631378
  - 0.3928202986717224
  - 0.39037954807281494
  - 0.3919810354709625
  - 0.39941877126693726
  - 0.3958974778652191
loss_records_fold2:
  train_losses:
  - 1.575792795419693
  - 1.6652281105518343
  - 1.6289615690708161
  - 1.6335295617580414
  - 1.5855125904083254
  - 1.534195876121521
  - 1.6017768025398256
  - 1.5514964520931245
  - 1.5655797541141512
  - 1.5364041209220887
  - 1.5493536829948427
  - 1.585596537590027
  - 1.5530381679534913
  - 1.6657451689243317
  - 1.559213948249817
  - 1.6184042513370516
  - 1.6236041128635408
  - 1.5693853855133058
  - 1.5549757778644562
  - 1.5599477648735047
  validation_losses:
  - 0.40380340814590454
  - 0.4189920723438263
  - 0.4227728843688965
  - 0.3878152072429657
  - 0.3943713903427124
  - 0.3935852646827698
  - 0.39336562156677246
  - 0.3875288665294647
  - 0.39613181352615356
  - 0.38817712664604187
  - 0.40556636452674866
  - 0.39537858963012695
  - 0.3947293758392334
  - 0.4266951084136963
  - 0.3956710398197174
  - 0.3960723280906677
  - 0.39971625804901123
  - 0.3964028060436249
  - 0.3945273756980896
  - 0.4015010893344879
loss_records_fold3:
  train_losses:
  - 1.5689346909523012
  - 1.536765742301941
  - 1.5753504931926727
  - 1.5907671213150025
  - 1.5601902425289156
  - 1.5665952563285828
  - 1.5851706743240357
  - 1.5554954200983049
  - 1.5747145950794221
  - 1.5441108465194704
  - 1.5363372027873994
  validation_losses:
  - 0.3835200369358063
  - 0.37888002395629883
  - 0.38190919160842896
  - 0.38971373438835144
  - 0.38020265102386475
  - 0.38619205355644226
  - 0.38183173537254333
  - 0.3902066648006439
  - 0.3931320905685425
  - 0.3835899829864502
  - 0.3901265263557434
loss_records_fold4:
  train_losses:
  - 1.560834687948227
  - 1.5537154495716097
  - 1.5532842695713045
  - 1.525242692232132
  - 1.545616376399994
  - 1.5599429965019227
  - 1.6229392290115356
  - 1.5141432106494905
  - 1.554335206747055
  - 1.5296212553977968
  - 1.5580073595046997
  - 1.550665944814682
  - 1.5633461356163025
  - 1.5949075043201448
  - 1.5416842162609101
  - 1.7232218801975252
  - 1.6529730916023255
  validation_losses:
  - 0.3963419497013092
  - 0.3913253843784332
  - 0.3914007246494293
  - 0.3886473476886749
  - 0.38165396451950073
  - 0.3815241754055023
  - 0.4173143208026886
  - 0.3818051517009735
  - 0.3815237581729889
  - 0.37534549832344055
  - 0.40413734316825867
  - 0.3875710070133209
  - 0.3863894045352936
  - 0.38946571946144104
  - 0.39055562019348145
  - 0.3920346796512604
  - 0.39186394214630127
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:07:11.602212'
