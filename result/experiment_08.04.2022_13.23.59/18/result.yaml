config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 15:23:01.797418'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/18/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/18/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/18/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/18/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/18/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/18/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/18/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/18/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/18/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/18/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 185.66697311401367
  - 40.88323199748993
  - 38.242633655667305
  - 23.961159855127335
  - 23.722427919507027
  - 20.50356024503708
  - 31.908910125494003
  - 31.68466892838478
  - 32.265586137771606
  - 27.901236906647682
  - 40.90623186528683
  - 23.040636360645294
  - 18.196775257587433
  - 19.37371078133583
  - 26.28880488872528
  - 19.492790937423706
  - 30.353657707571983
  - 28.64577655494213
  - 27.441851884126663
  - 26.19659522175789
  - 20.17720851302147
  - 23.716330513358116
  - 18.76910150051117
  - 20.949362456798553
  - 30.69915321469307
  - 22.39403074979782
  - 28.23313617706299
  - 23.960082322359085
  - 16.561466455459595
  - 15.658171832561493
  - 14.872139304876328
  - 19.02198225259781
  - 14.791872411966324
  - 18.808628037571907
  - 18.132867753505707
  - 15.195735529065132
  - 22.50456738471985
  - 18.45912715792656
  - 26.961041688919067
  - 15.101365387439728
  - 15.626503109931946
  - 16.07522565126419
  - 15.676274597644806
  - 17.85142481327057
  - 16.326808363199234
  - 15.547235891222954
  - 19.486922174692154
  - 15.519770473241806
  - 16.386288791894913
  - 16.77804557979107
  - 16.590440541505814
  - 19.674119114875793
  - 16.012269839644432
  - 15.651445627212524
  - 16.719698533415794
  - 14.5264260917902
  - 15.72504948079586
  - 15.178818091750145
  - 15.995931625366211
  - 18.268350437283516
  - 16.012044578790665
  - 14.844788908958435
  - 14.703472465276718
  - 15.963272631168365
  - 14.804729789495468
  - 16.16346099972725
  - 15.815523147583008
  - 15.398567721247673
  - 14.418550878763199
  - 15.177253544330597
  - 15.654146894812584
  - 15.857744380831718
  - 14.788028180599213
  - 15.970142155885696
  - 15.390364482998848
  - 14.506872117519379
  - 14.293456077575684
  - 14.89192807674408
  - 16.707478642463684
  - 15.240215092897415
  - 14.986801385879517
  - 15.404881924390793
  - 14.9587961435318
  - 15.497611194849014
  - 14.369354784488678
  - 14.737549245357513
  - 14.686272829771042
  - 14.591962963342667
  - 14.67959463596344
  - 14.392780542373657
  - 14.210303917527199
  - 14.651952415704727
  - 14.194596081972122
  - 14.605456501245499
  - 14.910277605056763
  - 14.390439838171005
  - 14.120382696390152
  - 14.273235574364662
  - 14.581815600395203
  - 15.82521541416645
  validation_losses:
  - 2.7508060932159424
  - 0.5882228016853333
  - 0.4160626530647278
  - 0.45139580965042114
  - 0.9681515097618103
  - 0.5440771579742432
  - 1.3170981407165527
  - 0.3973448574542999
  - 0.8256309628486633
  - 0.3854978382587433
  - 0.8419811725616455
  - 0.400302529335022
  - 0.43016281723976135
  - 0.5006733536720276
  - 0.3790743052959442
  - 0.37937942147254944
  - 0.5382109880447388
  - 0.47335419058799744
  - 0.38020816445350647
  - 0.37518107891082764
  - 0.41965752840042114
  - 0.37173888087272644
  - 0.5002346038818359
  - 0.3764724135398865
  - 0.4008888602256775
  - 0.44882360100746155
  - 0.4383150339126587
  - 0.4280204176902771
  - 0.45311716198921204
  - 0.38554051518440247
  - 0.38769590854644775
  - 0.37704455852508545
  - 0.5012291073799133
  - 0.3742506802082062
  - 0.38088858127593994
  - 0.5178171396255493
  - 0.43347448110580444
  - 0.3856554329395294
  - 0.39511826634407043
  - 0.395033061504364
  - 0.40703070163726807
  - 0.9354730844497681
  - 0.41676291823387146
  - 0.3835408091545105
  - 0.377530038356781
  - 0.38861408829689026
  - 0.38324907422065735
  - 0.38618147373199463
  - 0.3989676833152771
  - 0.4451007843017578
  - 0.38539043068885803
  - 0.4057367742061615
  - 0.38377657532691956
  - 0.3802140951156616
  - 0.5507786870002747
  - 0.3824395537376404
  - 0.44967594742774963
  - 0.4094694256782532
  - 0.3987414240837097
  - 0.37876954674720764
  - 0.4004415273666382
  - 0.3805025517940521
  - 0.4067876935005188
  - 0.39923974871635437
  - 0.6688960194587708
  - 0.4618908762931824
  - 0.40422067046165466
  - 0.38172006607055664
  - 0.4182993769645691
  - 0.39164045453071594
  - 0.4269729256629944
  - 0.3918347656726837
  - 0.39739248156547546
  - 0.38396987318992615
  - 0.4595700800418854
  - 0.3940895199775696
  - 0.38940832018852234
  - 0.40173324942588806
  - 0.5177628397941589
  - 0.4093681573867798
  - 0.38499942421913147
  - 0.4137786626815796
  - 0.41787946224212646
  - 0.3935604691505432
  - 0.38097408413887024
  - 0.40737712383270264
  - 0.38018277287483215
  - 0.389474093914032
  - 0.42575350403785706
  - 0.3845916986465454
  - 0.4390983283519745
  - 0.3811113238334656
  - 0.4161592125892639
  - 0.37779921293258667
  - 0.6641207337379456
  - 0.3886986970901489
  - 0.3975946307182312
  - 0.3775489330291748
  - 0.3812479078769684
  - 0.3841419517993927
loss_records_fold1:
  train_losses:
  - 14.175344094634056
  - 14.152992963790894
  - 14.505291908979416
  - 14.699139192700386
  - 14.604005292057991
  - 14.358499258756638
  - 14.308226704597473
  - 14.089016199111938
  - 14.62277339398861
  - 15.299306079745293
  - 15.125190794467926
  - 14.356515720486641
  - 14.677862256765366
  - 14.351970717310905
  - 14.190633356571198
  - 13.906896322965622
  - 14.330092519521713
  - 14.199909895658493
  - 14.221677780151367
  - 14.159904062747955
  - 14.632856994867325
  - 14.116427183151245
  - 14.314744025468826
  - 14.881726816296577
  - 14.39474692940712
  - 13.999102026224136
  - 14.369789376854897
  - 14.916572034358978
  - 14.743160039186478
  - 14.141473561525345
  - 13.942945137619972
  - 14.018571451306343
  - 14.194488495588303
  - 13.705952405929565
  - 14.169619023799896
  - 13.987470179796219
  - 14.902571827173233
  - 13.941922470927238
  - 13.898220747709274
  - 14.196395561099052
  - 13.899100229144096
  - 13.676208734512329
  - 13.775714248418808
  - 14.02189752459526
  - 13.806475013494492
  - 13.671399667859077
  - 13.797786295413971
  - 14.135766595602036
  - 14.197030901908875
  - 13.542149886488914
  - 14.491496533155441
  - 13.803859874606133
  - 14.042216360569
  - 13.903924822807312
  - 13.83343981206417
  - 14.089038774371147
  - 13.886023297905922
  - 14.249888449907303
  - 14.053454756736755
  - 13.683745935559273
  - 13.58761416375637
  - 13.90201486647129
  - 13.731065019965172
  - 13.853367328643799
  - 14.251736596226692
  - 13.857366651296616
  - 14.379614770412445
  - 13.903720438480377
  - 13.73105888068676
  - 14.080651372671127
  - 13.901442527770996
  - 14.2206349670887
  - 14.33000998198986
  - 13.797363221645355
  - 14.813757494091988
  - 14.190377175807953
  - 13.82838687300682
  - 14.071851804852486
  - 14.200515568256378
  - 13.90468493103981
  - 14.214417934417725
  - 14.282653644680977
  - 13.972800448536873
  - 13.876281887292862
  - 13.849230036139488
  - 13.91563755273819
  - 13.771539077162743
  - 13.522414669394493
  - 13.880942404270172
  - 13.821903988718987
  - 13.843113884329796
  - 13.794152319431305
  - 13.969356685876846
  - 13.766920566558838
  - 13.369299352169037
  - 14.025315970182419
  - 13.593824371695518
  - 13.60673639178276
  - 13.799577131867409
  - 14.310280501842499
  validation_losses:
  - 0.392768532037735
  - 0.3895288407802582
  - 0.4090760052204132
  - 0.40091419219970703
  - 0.43233349919319153
  - 0.4269413948059082
  - 0.39488181471824646
  - 0.42515262961387634
  - 0.4023813307285309
  - 0.4169691503047943
  - 0.3929423391819
  - 0.4560491442680359
  - 0.3885529637336731
  - 0.4070935547351837
  - 0.3887200355529785
  - 0.40028616786003113
  - 0.4080196022987366
  - 0.47258004546165466
  - 0.4076922535896301
  - 0.394232839345932
  - 0.3932219445705414
  - 0.3981795608997345
  - 3.7917895317077637
  - 0.4258904755115509
  - 0.40050408244132996
  - 0.39631348848342896
  - 0.4639606773853302
  - 0.39393293857574463
  - 0.39601150155067444
  - 0.3975120782852173
  - 0.39293888211250305
  - 0.41216224431991577
  - 0.39442962408065796
  - 0.4007723033428192
  - 0.422320157289505
  - 0.4183531701564789
  - 0.5037726759910583
  - 0.3999534845352173
  - 0.3879624903202057
  - 0.40682727098464966
  - 0.4173899292945862
  - 0.45490366220474243
  - 0.4486898183822632
  - 0.42101702094078064
  - 0.41741520166397095
  - 0.8635932803153992
  - 0.7351446151733398
  - 0.4124577045440674
  - 1.6240906715393066
  - 3.73732328414917
  - 0.4353048801422119
  - 0.41989216208457947
  - 0.413784384727478
  - 0.3996473550796509
  - 0.7065956592559814
  - 0.5576186776161194
  - 0.4644695818424225
  - 0.42630735039711
  - 0.4030244052410126
  - 0.6359677314758301
  - 0.5003474950790405
  - 0.38840875029563904
  - 0.38889095187187195
  - 0.4552038311958313
  - 0.4013512432575226
  - 0.4558202028274536
  - 0.4578402638435364
  - 0.39293769001960754
  - 0.39877477288246155
  - 0.3907502591609955
  - 0.4294688105583191
  - 0.41269758343696594
  - 0.4733031392097473
  - 2.2090916633605957
  - 0.5849095582962036
  - 0.3950107991695404
  - 0.46590450406074524
  - 0.7340705990791321
  - 0.4618217349052429
  - 4.410881519317627
  - 0.48911961913108826
  - 0.7530743479728699
  - 0.4098144471645355
  - 0.4137691259384155
  - 0.4241582453250885
  - 0.4081000089645386
  - 0.5838027596473694
  - 0.6327117681503296
  - 0.38280433416366577
  - 2.9743900299072266
  - 6.046788692474365
  - 16.575239181518555
  - 2.0561881065368652
  - 2.2077560424804688
  - 5.877396106719971
  - 3.151230573654175
  - 0.8546839356422424
  - 0.41231709718704224
  - 1.0722010135650635
  - 0.4880806803703308
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 48 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8061749571183533, 0.8576329331046312, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.2206896551724138, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8479791103016157
  mean_f1_accuracy: 0.044137931034482755
  total_train_time: '0:07:33.981379'
