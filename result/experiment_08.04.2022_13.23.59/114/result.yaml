config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 02:55:58.367065'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/114/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/114/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/114/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/114/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/114/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/114/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/114/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/114/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/114/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/114/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 165.85308393836021
  - 42.45512914657593
  - 34.88467589020729
  - 53.22224181890488
  - 39.826472252607346
  - 27.75886008143425
  - 41.44702097773552
  - 18.29048565030098
  - 19.846278816461563
  - 19.6023907661438
  - 25.17877221107483
  - 32.21249429881573
  - 46.95864196121693
  - 38.550871312618256
  - 33.01054133474827
  - 31.418112441897392
  - 57.63598936796188
  - 42.28241570293903
  - 35.68894538283348
  - 25.79985037446022
  - 24.198066234588623
  - 33.73728993535042
  - 21.693804189562798
  - 54.3653062582016
  - 19.680441066622734
  - 24.43311856687069
  - 19.60284784436226
  - 23.913703814148903
  - 48.00592786073685
  - 24.463686391711235
  - 17.33798250555992
  - 18.188542932271957
  - 25.44943416118622
  - 31.365889012813568
  - 25.989107996225357
  - 19.30813829600811
  - 18.776968762278557
  - 21.430378079414368
  - 16.691677257418633
  - 16.11487205326557
  - 20.200255274772644
  - 20.41838328540325
  - 17.1370290517807
  - 20.74590466916561
  - 15.977310046553612
  - 22.815760925412178
  - 19.825884014368057
  - 20.271693512797356
  - 18.084293335676193
  - 17.236939802765846
  - 17.18387220799923
  - 17.541361689567566
  - 16.312090516090393
  - 17.445536375045776
  - 14.541607812047005
  - 16.522630751132965
  - 17.839173555374146
  - 14.668731048703194
  - 15.846887201070786
  - 15.383089676499367
  - 15.32141861319542
  - 16.176022917032242
  - 16.04271686077118
  - 17.26250746846199
  - 18.61919505894184
  - 15.547222658991814
  - 14.476633250713348
  - 14.71030506491661
  - 14.461679875850677
  - 14.527864173054695
  - 16.09436894953251
  - 16.175021290779114
  - 14.92722798883915
  - 15.590122640132904
  - 14.65379685163498
  - 15.709343686699867
  - 15.029727518558502
  - 14.669753730297089
  - 14.723809897899628
  - 14.560631170868874
  - 14.54915802180767
  - 14.260176062583923
  - 13.973894089460373
  - 14.538817882537842
  - 15.796727254986763
  - 14.64596876502037
  - 15.57104167342186
  - 14.793750181794167
  - 16.671714022755623
  - 15.107172071933746
  - 14.692583203315735
  - 16.935357987880707
  - 15.03609111905098
  - 14.127724334597588
  - 14.499609723687172
  - 16.033827126026154
  - 14.90510630607605
  - 15.108934670686722
  - 14.949330776929855
  - 14.122032046318054
  validation_losses:
  - 2.320858955383301
  - 0.808576226234436
  - 3.260821580886841
  - 0.580660879611969
  - 2.162332534790039
  - 0.4334926903247833
  - 0.44531524181365967
  - 0.3886282444000244
  - 0.5900145173072815
  - 0.42375075817108154
  - 0.6331126689910889
  - 0.8536760807037354
  - 0.89111328125
  - 0.41952085494995117
  - 0.4340628981590271
  - 0.4362851083278656
  - 0.49930861592292786
  - 0.37809494137763977
  - 0.3943069577217102
  - 0.41667863726615906
  - 0.6871638894081116
  - 0.48585429787635803
  - 0.747844934463501
  - 0.9459649920463562
  - 0.4264255464076996
  - 0.39612898230552673
  - 0.4233660101890564
  - 1.4768095016479492
  - 0.5653344988822937
  - 0.41220909357070923
  - 0.5783158540725708
  - 0.6045079827308655
  - 0.41697582602500916
  - 0.7179333567619324
  - 0.39147087931632996
  - 0.41339054703712463
  - 0.4231622517108917
  - 0.4118671715259552
  - 0.39516547322273254
  - 0.4006410539150238
  - 1.6508064270019531
  - 0.4779044985771179
  - 0.43008115887641907
  - 0.3718031644821167
  - 0.4920651316642761
  - 0.4121921956539154
  - 0.4029088020324707
  - 0.8521676659584045
  - 0.3979049324989319
  - 0.4021030366420746
  - 0.45452257990837097
  - 0.40479743480682373
  - 0.46760043501853943
  - 0.3885882794857025
  - 0.3795056641101837
  - 0.48472315073013306
  - 0.3923242688179016
  - 0.4083244204521179
  - 0.48484161496162415
  - 0.38359031081199646
  - 0.37536153197288513
  - 0.3746841251850128
  - 0.38139232993125916
  - 0.38275930285453796
  - 0.4475801885128021
  - 0.3778151273727417
  - 0.39858344197273254
  - 0.37449443340301514
  - 0.39158663153648376
  - 0.3799440264701843
  - 0.3788083791732788
  - 0.43133243918418884
  - 0.38428547978401184
  - 0.3790236711502075
  - 0.3847481906414032
  - 0.41829362511634827
  - 0.378828227519989
  - 0.3895471692085266
  - 0.38267257809638977
  - 0.38296404480934143
  - 0.38019147515296936
  - 0.38172072172164917
  - 0.4038721024990082
  - 0.3728083372116089
  - 0.3966456949710846
  - 0.41127362847328186
  - 0.43130210041999817
  - 0.3785085082054138
  - 0.44616392254829407
  - 0.38817068934440613
  - 0.3795017898082733
  - 0.38118842244148254
  - 0.4862666726112366
  - 0.3824593722820282
  - 0.4569888710975647
  - 0.4410831928253174
  - 0.38709238171577454
  - 0.4027199447154999
  - 0.3819831311702728
  - 0.3787272870540619
loss_records_fold1:
  train_losses:
  - 14.14703318476677
  - 14.5379076898098
  - 14.275299459695816
  - 13.874651730060577
  - 14.189336210489273
  - 14.04477646946907
  - 13.978799432516098
  - 14.443027526140213
  - 13.925904795527458
  - 14.553261056542397
  - 14.567279115319252
  - 15.408273696899414
  - 14.701163172721863
  - 14.745493292808533
  - 14.447115182876587
  - 14.087708592414856
  - 14.553425997495651
  - 14.172226369380951
  - 13.928100660443306
  - 14.037081748247147
  - 14.400026768445969
  - 14.101848736405373
  - 14.747285410761833
  - 14.261931642889977
  - 13.81822156906128
  - 13.899809181690216
  - 13.96469634771347
  - 14.7499278485775
  - 13.902044743299484
  - 14.077642157673836
  - 13.772864997386932
  - 14.058537051081657
  - 13.711074829101562
  - 13.929450988769531
  - 13.855727881193161
  - 14.623034507036209
  - 14.412243217229843
  - 15.548840835690498
  - 15.137685343623161
  - 14.970363810658455
  - 14.415048882365227
  - 13.947546347975731
  - 14.03843142092228
  - 15.54440687596798
  - 14.512470930814743
  - 14.016381710767746
  - 14.035051703453064
  - 13.727969735860825
  - 14.355622559785843
  - 14.318399518728256
  - 14.137720987200737
  - 14.070329412817955
  - 13.709211349487305
  - 13.691387623548508
  - 13.99503667652607
  - 14.750664561986923
  - 13.89522436261177
  - 13.915756195783615
  - 14.12887316942215
  - 14.27179591357708
  - 13.915660440921783
  - 14.284888163208961
  - 13.908483028411865
  - 13.88040229678154
  - 14.181856736540794
  - 14.177516385912895
  - 13.980452343821526
  - 14.56646341085434
  - 13.919831305742264
  - 14.816393077373505
  - 14.052002906799316
  - 14.12525226175785
  - 13.65559384226799
  - 14.230476334691048
  - 13.858216002583504
  - 13.841804906725883
  - 14.209245890378952
  - 14.347229570150375
  - 14.257506534457207
  - 13.982087343931198
  - 13.56500980257988
  - 14.467513918876648
  - 14.624092102050781
  - 14.008187249302864
  - 13.901534751057625
  - 13.720526993274689
  - 13.835850715637207
  - 14.127922490239143
  - 13.847979545593262
  - 13.7756986618042
  - 13.714962646365166
  - 15.008250564336777
  - 14.328892946243286
  - 14.08818532526493
  - 13.840987503528595
  - 13.99304085969925
  - 13.73123550415039
  - 14.258736729621887
  - 13.975895553827286
  - 14.114261507987976
  validation_losses:
  - 0.41903001070022583
  - 0.393214613199234
  - 0.40302300453186035
  - 0.39633315801620483
  - 0.3934602737426758
  - 0.4140695333480835
  - 0.3983885645866394
  - 0.39539799094200134
  - 0.4287411570549011
  - 0.39681726694107056
  - 0.38438132405281067
  - 0.4226510226726532
  - 0.4052993953227997
  - 0.441363662481308
  - 0.47209081053733826
  - 0.4012848138809204
  - 0.4407798945903778
  - 0.40717020630836487
  - 0.39942994713783264
  - 0.4040932059288025
  - 0.41371849179267883
  - 0.8609958291053772
  - 0.405298113822937
  - 0.4835163354873657
  - 0.39711877703666687
  - 20.72547721862793
  - 2.058671236038208
  - 0.3892115652561188
  - 10.555778503417969
  - 0.8241075873374939
  - 0.46838244795799255
  - 0.42427393794059753
  - 0.4621831178665161
  - 0.40752556920051575
  - 0.39824819564819336
  - 0.7804868221282959
  - 0.41789624094963074
  - 0.45750629901885986
  - 0.43888843059539795
  - 0.5502607226371765
  - 0.4164518415927887
  - 0.41553375124931335
  - 0.41181546449661255
  - 0.6181356310844421
  - 0.4028826951980591
  - 0.4406927824020386
  - 0.39876264333724976
  - 0.44341957569122314
  - 0.40014931559562683
  - 0.41638487577438354
  - 0.3965552747249603
  - 0.5230880975723267
  - 0.39998355507850647
  - 0.4063960611820221
  - 0.40135571360588074
  - 0.42013055086135864
  - 0.40109384059906006
  - 0.4599764943122864
  - 0.3988708555698395
  - 0.3962773084640503
  - 0.39848339557647705
  - 0.40994992852211
  - 0.39491620659828186
  - 0.41150566935539246
  - 0.39780393242836
  - 0.4297248125076294
  - 0.4065004885196686
  - 0.39994820952415466
  - 0.4094085097312927
  - 0.4143214523792267
  - 0.5195412039756775
  - 0.40773695707321167
  - 0.42860090732574463
  - 0.39333751797676086
  - 0.39870572090148926
  - 0.39834514260292053
  - 0.40867742896080017
  - 0.41506490111351013
  - 0.4855140149593353
  - 1.1882665157318115
  - 0.4109819531440735
  - 0.4277730882167816
  - 0.4158423840999603
  - 0.40402162075042725
  - 0.40157297253608704
  - 0.397765576839447
  - 0.9759883880615234
  - 0.43327444791793823
  - 0.4074934124946594
  - 0.39869338274002075
  - 0.41255810856819153
  - 0.41237184405326843
  - 0.39366966485977173
  - 0.4283149838447571
  - 0.40344181656837463
  - 0.40078428387641907
  - 0.39875033497810364
  - 0.43388062715530396
  - 0.40712425112724304
  - 0.534444272518158
loss_records_fold2:
  train_losses:
  - 14.197821110486984
  - 14.376365095376968
  - 14.81355133652687
  - 15.275383085012436
  - 14.511167496442795
  - 14.496213167905807
  - 14.811457961797714
  - 14.137430503964424
  - 14.163790822029114
  - 14.608215674757957
  - 14.236338794231415
  - 14.278875306248665
  - 14.153707146644592
  - 14.230976462364197
  - 15.188059270381927
  - 14.261668488383293
  - 13.786080166697502
  - 14.66465374827385
  - 14.892919138073921
  - 15.033921003341675
  - 14.451972723007202
  - 13.857798457145691
  - 14.57422387599945
  - 14.760398298501968
  - 14.116799116134644
  - 14.652128204703331
  - 15.584749028086662
  - 14.98818290233612
  - 14.731451272964478
  - 15.38327819108963
  - 14.880119949579239
  - 14.48451679944992
  - 14.617427974939346
  - 14.69340093433857
  - 14.681799501180649
  - 14.68596488237381
  - 14.577086299657822
  - 14.768363356590271
  - 15.172586679458618
  - 15.090522170066833
  - 14.676746726036072
  - 15.112209096550941
  - 14.807540357112885
  - 14.833827584981918
  - 15.326174318790436
  - 15.095709443092346
  - 14.49420940876007
  - 14.507242262363434
  - 15.165022060275078
  - 15.023482769727707
  - 14.930033877491951
  - 14.640682995319366
  - 15.735986694693565
  - 14.940241545438766
  - 15.094200134277344
  - 15.020207077264786
  - 15.019372060894966
  - 15.214092671871185
  - 15.08551450073719
  - 15.049909308552742
  - 14.912971675395966
  - 14.835852324962616
  - 14.955597430467606
  - 14.965321004390717
  - 14.763232111930847
  - 15.740421935915947
  - 15.004122942686081
  - 15.139496445655823
  - 15.00682669878006
  - 15.717917740345001
  - 15.569204717874527
  - 15.706137627363205
  - 14.902518093585968
  - 14.938209354877472
  - 14.979790478944778
  - 15.39910340309143
  - 15.010638445615768
  - 14.898176103830338
  - 14.60300524532795
  - 14.97716772556305
  - 15.102763280272484
  - 15.404154971241951
  - 14.72153526544571
  - 14.915549501776695
  - 14.829395532608032
  - 22.031723111867905
  - 19.60454660654068
  - 19.704013839364052
  - 19.42311531305313
  - 15.576643377542496
  - 15.489754289388657
  - 14.794674441218376
  - 15.550005555152893
  - 17.966089695692062
  - 15.56541158258915
  - 17.988076478242874
  - 15.773137673735619
  - 19.37724481523037
  - 17.101278215646744
  - 15.920889973640442
  validation_losses:
  - 0.42729783058166504
  - 0.44849780201911926
  - 0.3880600035190582
  - 0.3944058418273926
  - 0.590329110622406
  - 0.38703015446662903
  - 0.4032990336418152
  - 0.649449348449707
  - 0.3723454475402832
  - 0.3703539967536926
  - 0.4040563702583313
  - 0.3723728358745575
  - 0.39919939637184143
  - 0.38037699460983276
  - 0.38192853331565857
  - 0.6191943883895874
  - 0.3786587715148926
  - 0.4029097259044647
  - 0.3870089650154114
  - 0.36920344829559326
  - 0.3861638307571411
  - 0.3682214021682739
  - 1.3868308067321777
  - 0.3683459162712097
  - 0.5147867202758789
  - 0.6022329330444336
  - 9.847537994384766
  - 0.6737842559814453
  - 0.44540536403656006
  - 0.47675272822380066
  - 0.3815835118293762
  - 0.39216148853302
  - 0.3846580982208252
  - 0.39798104763031006
  - 0.3882295787334442
  - 0.37307506799697876
  - 0.3741990625858307
  - 0.46866923570632935
  - 0.42681440711021423
  - 0.40428680181503296
  - 0.4035492539405823
  - 0.38468578457832336
  - 0.4020209312438965
  - 0.3951559066772461
  - 0.5232983231544495
  - 0.4645368158817291
  - 1.0261297225952148
  - 0.6986343264579773
  - 0.39649951457977295
  - 0.3821425437927246
  - 0.4102593660354614
  - 0.3841048777103424
  - 0.43969765305519104
  - 0.38219979405403137
  - 0.38517460227012634
  - 0.3823471963405609
  - 0.3835688531398773
  - 0.3822961747646332
  - 0.40342023968696594
  - 0.42251041531562805
  - 0.5057857036590576
  - 0.8397470116615295
  - 0.5036229491233826
  - 0.4936573803424835
  - 0.587013840675354
  - 1.3157856464385986
  - 0.5487584471702576
  - 0.4520263671875
  - 0.4352369010448456
  - 0.9123053550720215
  - 0.5500212907791138
  - 0.7784880995750427
  - 0.6637706160545349
  - 0.8183074593544006
  - 25.74867820739746
  - 14228.6689453125
  - 2329597.0
  - 16146800.0
  - 11652802.0
  - 11764429.0
  - 7001867.5
  - 7852173.5
  - 16803000.0
  - 27357736.0
  - 23.142318725585938
  - 0.4009692966938019
  - 0.45191389322280884
  - 0.38087090849876404
  - 0.39088982343673706
  - 0.38099658489227295
  - 0.3823239803314209
  - 0.3837668299674988
  - 0.40512844920158386
  - 0.409487247467041
  - 0.38526394963264465
  - 0.38293707370758057
  - 16.839805603027344
  - 0.39268091320991516
  - 0.38387537002563477
  - 0.38542434573173523
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 10 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.7924528301886793, 0.8593481989708405, 0.8576329331046312,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.062015503875969005, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8452346849156808
  mean_f1_accuracy: 0.012403100775193802
  total_train_time: '0:08:36.454656'
