config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 13:55:03.457128'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/2/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/2/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/2/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/2/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/2/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/2/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/2/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/2/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/2/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/2/fold_4_state_dict.pt
loss_records_fold2:
  train_losses:
  - 14.344955146312714
  - 14.113623440265656
  - 14.276088744401932
  - 14.088794514536858
  - 14.159185618162155
  - 14.209699809551239
  - 14.086112827062607
  - 14.364959806203842
  - 14.398376822471619
  - 14.200564950704575
  - 14.160440057516098
  - 14.146556943655014
  - 14.093135252594948
  - 14.103114187717438
  - 14.164865270256996
  - 14.114092394709587
  - 14.235117241740227
  - 13.885365098714828
  - 14.098364502191544
  - 14.101079851388931
  - 14.094195932149887
  - 14.0292449593544
  - 13.81211832165718
  - 14.047549858689308
  - 14.064936995506287
  - 13.85227383673191
  - 14.164318472146988
  - 14.211059957742691
  - 14.159232050180435
  - 14.061272233724594
  - 14.14910826086998
  - 14.061357975006104
  - 14.045967787504196
  - 13.916089504957199
  - 13.849097400903702
  - 13.851199954748154
  - 13.95729261636734
  - 13.770652189850807
  - 13.758631646633148
  - 13.872809797525406
  - 13.912233769893646
  - 13.906569719314575
  - 13.966545701026917
  - 13.63506831228733
  - 13.657279253005981
  - 13.6503387093544
  - 13.810937136411667
  - 13.843681633472443
  - 13.869559228420258
  - 13.794771656394005
  - 13.850535243749619
  - 13.838043436408043
  - 13.767021119594574
  - 13.768173202872276
  - 13.552521869540215
  - 13.76658409833908
  - 13.857283756136894
  - 13.711689203977585
  - 14.065524950623512
  - 13.769382178783417
  - 13.555780053138733
  - 13.653560489416122
  - 13.720378547906876
  - 13.821043908596039
  - 13.732212036848068
  - 13.797896027565002
  - 13.541990101337433
  - 13.597002819180489
  - 13.542287528514862
  - 13.575323298573494
  - 13.62689833343029
  - 13.664510145783424
  - 13.660139098763466
  - 13.891358152031898
  - 13.815923482179642
  - 13.370426744222641
  - 13.60738691687584
  - 13.560490101575851
  - 13.706611052155495
  - 13.696562737226486
  - 13.767356127500534
  - 13.71254950761795
  - 13.445055410265923
  - 13.8192697763443
  - 13.404985934495926
  - 13.614133775234222
  - 13.544239342212677
  - 13.658109426498413
  - 13.415351301431656
  - 13.392505317926407
  - 13.500487864017487
  - 13.393211260437965
  - 13.35695005953312
  - 13.652712479233742
  - 13.405644565820694
  - 13.241631865501404
  - 13.548196747899055
  - 13.557843923568726
  - 13.446714356541634
  - 13.731218501925468
  validation_losses:
  - 0.3929229974746704
  - 0.38803693652153015
  - 0.4068690836429596
  - 0.4216844439506531
  - 0.4056936502456665
  - 0.38710564374923706
  - 0.4015596807003021
  - 0.3953891694545746
  - 0.39051634073257446
  - 0.46670904755592346
  - 0.3856630027294159
  - 0.3857306241989136
  - 0.3887251317501068
  - 0.43452954292297363
  - 0.38970646262168884
  - 0.39244216680526733
  - 0.4482177495956421
  - 0.38505351543426514
  - 0.40011006593704224
  - 0.404313862323761
  - 0.38388559222221375
  - 0.44034871459007263
  - 0.44718047976493835
  - 0.4011833667755127
  - 0.38587304949760437
  - 0.3840648829936981
  - 0.40396836400032043
  - 0.5352257490158081
  - 0.38858935236930847
  - 0.39575257897377014
  - 0.40412455797195435
  - 0.444876104593277
  - 0.455990731716156
  - 0.39417195320129395
  - 0.3919234573841095
  - 0.610928475856781
  - 0.43761706352233887
  - 0.48752906918525696
  - 0.6345728039741516
  - 0.4464959502220154
  - 0.3974819481372833
  - 0.4018419086933136
  - 0.44261661171913147
  - 0.6421645283699036
  - 0.5196609497070312
  - 0.6041266322135925
  - 0.8521819114685059
  - 0.3855729401111603
  - 0.5533431768417358
  - 0.4307790994644165
  - 0.46206170320510864
  - 0.48351916670799255
  - 0.5454086065292358
  - 0.4533458650112152
  - 0.46495527029037476
  - 0.4740622639656067
  - 0.42064934968948364
  - 0.5439127087593079
  - 0.5022442936897278
  - 0.5247943997383118
  - 0.43553727865219116
  - 0.5171489715576172
  - 0.6287374496459961
  - 0.4679926633834839
  - 0.3932760953903198
  - 0.3891887366771698
  - 0.4405156373977661
  - 0.4992924630641937
  - 0.5085445046424866
  - 10.350619316101074
  - 5.231827735900879
  - 0.3810809552669525
  - 0.3829684257507324
  - 0.44571352005004883
  - 0.4089509844779968
  - 0.4895975589752197
  - 0.4779273271560669
  - 6.546790599822998
  - 0.6599537134170532
  - 1.6403782367706299
  - 2.3946456909179688
  - 1.198486328125
  - 2.3533966541290283
  - 1.3238093852996826
  - 0.6812295317649841
  - 0.8056332468986511
  - 0.7109524011611938
  - 0.43093475699424744
  - 0.46728071570396423
  - 0.6314756870269775
  - 0.3939473330974579
  - 1.9545555114746094
  - 1.2211353778839111
  - 1.6967328786849976
  - 0.5715845227241516
  - 0.6938819885253906
  - 2.1144251823425293
  - 0.7310391068458557
  - 0.4337990880012512
  - 0.4275115728378296
loss_records_fold4:
  train_losses:
  - 13.290724352002144
  - 13.333468332886696
  - 13.368192106485367
  - 13.30315263569355
  - 13.292255520820618
  - 13.501306340098381
  - 13.36581563949585
  - 13.064574718475342
  - 13.20275229215622
  - 13.469899848103523
  - 13.38025039434433
  - 13.06165337562561
  - 13.067601352930069
  - 13.292063727974892
  - 13.17889954149723
  - 13.022640496492386
  - 13.036184564232826
  - 13.248056903481483
  - 13.451113060116768
  - 13.14300936460495
  - 13.515219911932945
  - 13.21002671122551
  - 13.043716549873352
  - 13.139403656125069
  - 13.245107397437096
  - 13.612194910645485
  - 14.159785330295563
  - 13.540674924850464
  - 13.109241157770157
  - 13.075582772493362
  - 13.123131409287453
  - 13.290559023618698
  - 13.256460517644882
  - 13.14516857266426
  - 13.135391876101494
  - 12.94090624153614
  - 12.963774338364601
  - 13.046381384134293
  - 12.894607424736023
  - 13.137090846896172
  - 13.032579228281975
  - 12.92984226346016
  - 12.964456617832184
  - 12.819510772824287
  - 12.972796559333801
  - 12.889760315418243
  - 13.216523334383965
  - 13.27090710401535
  - 12.83748297393322
  - 13.017740562558174
  - 13.066339552402496
  - 13.311444371938705
  - 13.060952961444855
  - 13.185906320810318
  - 12.765146359801292
  - 13.003462210297585
  - 12.785740241408348
  - 12.840005844831467
  - 12.968777775764465
  - 12.828095734119415
  - 12.85740402340889
  - 12.884220600128174
  - 12.916776582598686
  - 12.945735394954681
  - 12.930673614144325
  - 12.947149381041527
  - 12.561886861920357
  - 12.743344515562057
  - 12.819065243005753
  - 12.95303812623024
  - 12.967189371585846
  - 14.316759869456291
  - 13.15082286298275
  - 13.223228231072426
  - 13.11680118739605
  - 13.126324534416199
  - 13.104432716965675
  - 12.827050879597664
  - 12.785713255405426
  - 12.948136687278748
  - 13.066390290856361
  - 13.224235281348228
  - 13.034723043441772
  - 12.95651949942112
  - 12.693259850144386
  - 12.622534722089767
  - 12.666188970208168
  - 12.908246845006943
  - 12.727572456002235
  - 12.836820930242538
  - 12.823245391249657
  - 12.801431685686111
  - 12.636422276496887
  - 12.796102344989777
  - 12.69466020166874
  - 12.683507546782494
  - 13.070558428764343
  - 12.647808969020844
  - 12.63273486495018
  - 12.87366333603859
  validation_losses:
  - 0.6680331230163574
  - 0.47330084443092346
  - 0.7331446409225464
  - 0.8249983787536621
  - 0.6306580901145935
  - 0.6024897694587708
  - 0.5651026964187622
  - 0.6452046632766724
  - 0.634540319442749
  - 0.6594337224960327
  - 0.5809263586997986
  - 0.6218351721763611
  - 0.774642288684845
  - 0.7470752000808716
  - 0.9568946957588196
  - 1.1983909606933594
  - 0.6935588717460632
  - 0.6621371507644653
  - 0.6406736969947815
  - 0.741317868232727
  - 0.5497809052467346
  - 0.6828306913375854
  - 0.6282219886779785
  - 1.0650689601898193
  - 0.9592818021774292
  - 2.009165048599243
  - 0.4482883810997009
  - 0.5196693539619446
  - 0.5563281178474426
  - 0.6792364716529846
  - 0.8637616634368896
  - 0.9149934649467468
  - 1.1725165843963623
  - 0.9681016206741333
  - 1.2378623485565186
  - 1.5388414859771729
  - 1.0052382946014404
  - 0.8509867787361145
  - 1.0545870065689087
  - 0.8511734008789062
  - 1.048089623451233
  - 1.2201941013336182
  - 1.204070806503296
  - 1.1875303983688354
  - 1.7155201435089111
  - 1.0369564294815063
  - 1.4662379026412964
  - 1.021659016609192
  - 1.1852977275848389
  - 0.9988088607788086
  - 0.8689303994178772
  - 0.563687264919281
  - 0.6272934675216675
  - 0.6166243553161621
  - 0.6634586453437805
  - 0.6583254337310791
  - 0.6234036684036255
  - 0.9491843581199646
  - 0.6573491096496582
  - 0.6647272706031799
  - 0.777607798576355
  - 0.8035290837287903
  - 0.5481051206588745
  - 0.6182548999786377
  - 0.6891327500343323
  - 0.8317368626594543
  - 0.773074209690094
  - 0.9652202725410461
  - 0.8791881203651428
  - 0.8576287031173706
  - 1.3497658967971802
  - 0.6143600344657898
  - 0.7629266977310181
  - 0.8087942004203796
  - 0.731522798538208
  - 0.6332494020462036
  - 0.7487046718597412
  - 0.8117617964744568
  - 0.8209621906280518
  - 0.7680374979972839
  - 0.8195574283599854
  - 0.7294949293136597
  - 0.7299282550811768
  - 0.7349976301193237
  - 0.7024406790733337
  - 0.910089373588562
  - 1.4070360660552979
  - 0.822349488735199
  - 0.7996242046356201
  - 0.7473087906837463
  - 0.7822664380073547
  - 0.7531801462173462
  - 0.8851931095123291
  - 0.8030527830123901
  - 0.8368790745735168
  - 0.7347744703292847
  - 0.5900571942329407
  - 0.9937525987625122
  - 0.8708516359329224
  - 0.7517803907394409
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 67 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8490566037735849, 0.7958833619210978,
    0.781786941580756]'
  fold_eval_f1: '[0.0, 0.0, 0.08333333333333334, 0.25157232704402516, 0.22085889570552147]'
  mean_eval_accuracy: 0.8283985546969402
  mean_f1_accuracy: 0.11115291121657597
  total_train_time: '0:07:21.898746'
