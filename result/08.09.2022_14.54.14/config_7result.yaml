config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cpu
  dropout: 0.0
  epochs: 30
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.05
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 5
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-09 15:00:05.747214'
fold_0_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_7fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_7fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_7fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_7fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_7fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_7fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_7fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_7fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_7fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_7fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.082540106773377
  - 5.889569801092148
  - 5.982910403609276
  - 6.045663914084435
  - 5.943573185801506
  - 5.969674617052078
  - 5.820861965417862
  - 5.727273392677308
  - 5.806452485918999
  - 5.656747680902481
  - 5.589491346478463
  - 5.701833319664002
  - 5.681285209953785
  - 5.828487786650658
  validation_losses:
  - 0.4407312572002411
  - 0.5103181004524231
  - 0.48446744680404663
  - 0.4435192346572876
  - 0.41525253653526306
  - 0.7025021314620972
  - 1.1828995943069458
  - 0.7802075743675232
  - 0.3942178189754486
  - 3.116732597351074
  - 0.3928413391113281
  - 0.40228673815727234
  - 0.39836469292640686
  - 0.38399967551231384
loss_records_fold1:
  train_losses:
  - 5.717750641703606
  - 5.659699779748917
  - 5.604823006689549
  - 5.588692316412926
  - 5.7501460313797
  - 5.589783683419228
  - 5.651742690801621
  - 5.68465670645237
  - 5.8382581084966665
  - 5.659557998180389
  - 5.612365180253983
  - 5.670491290092468
  validation_losses:
  - 0.4025481343269348
  - 0.3894863724708557
  - 0.40660998225212097
  - 0.3946450352668762
  - 0.39786744117736816
  - 0.38895565271377563
  - 0.41056981682777405
  - 0.5480520725250244
  - 0.4199129641056061
  - 0.385664701461792
  - 0.3899620473384857
  - 0.40163278579711914
loss_records_fold2:
  train_losses:
  - 5.6091996997594835
  - 5.55135672390461
  - 5.65425627231598
  - 5.61174884736538
  - 5.612007638812066
  - 5.571558913588524
  - 5.530739182233811
  - 5.56957729458809
  - 5.651336815953255
  - 5.604377648234368
  - 5.615417885780335
  validation_losses:
  - 0.3896392285823822
  - 1.1787481307983398
  - 0.38497841358184814
  - 0.3879966735839844
  - 0.4122951626777649
  - 0.39507392048835754
  - 0.38579630851745605
  - 0.4010511636734009
  - 0.4168696403503418
  - 0.4574225842952728
  - 0.38901668787002563
loss_records_fold3:
  train_losses:
  - 5.567497971653939
  - 5.680728381872178
  - 5.539500993490219
  - 5.5986227869987495
  - 5.544489246606827
  - 5.551132649183273
  - 5.518258887529374
  - 5.570735958218575
  - 5.5538026124238975
  - 5.483320933580399
  - 5.618610927462578
  - 5.563462340831757
  - 5.5927721679210665
  - 5.5374216675758365
  - 5.552278806269169
  - 5.635488000512123
  - 5.558102405071259
  - 5.580702674388886
  - 5.586569851636887
  - 5.529552721977234
  - 5.479468512535096
  - 5.486865985393525
  - 5.56500163525343
  - 5.57726466357708
  - 5.560024276375771
  - 5.552805519104004
  - 5.5403826445341116
  validation_losses:
  - 0.507716953754425
  - 0.5357567667961121
  - 0.43358129262924194
  - 0.5920423865318298
  - 0.5030776858329773
  - 3.228041887283325
  - 0.5580377578735352
  - 0.5788968205451965
  - 0.990837812423706
  - 0.3928898274898529
  - 0.4251011908054352
  - 0.5614184141159058
  - 0.485492467880249
  - 0.6945837140083313
  - 0.8211514949798584
  - 0.563190221786499
  - 0.6386970281600952
  - 0.37964290380477905
  - 0.39866238832473755
  - 0.9432828426361084
  - 0.8744185566902161
  - 0.4686565101146698
  - 7.1396484375
  - 0.7773048281669617
  - 0.64493727684021
  - 0.6574370861053467
  - 0.5696043968200684
loss_records_fold4:
  train_losses:
  - 5.604947596788406
  - 5.5512515097856525
  - 5.542459478974343
  - 5.498790061473847
  - 5.491778096556664
  - 5.513042244315148
  - 5.562607535719872
  - 5.615693119168282
  - 5.535527831315995
  - 5.516709440946579
  - 5.5328966110944755
  - 5.565370339155198
  - 5.488956335186959
  - 5.501461076736451
  - 5.43712295293808
  validation_losses:
  - 0.3936738073825836
  - 0.3888626992702484
  - 0.4343050420284271
  - 0.44819656014442444
  - 0.44819992780685425
  - 0.37421903014183044
  - 0.41196224093437195
  - 0.5695165395736694
  - 0.43058979511260986
  - 0.36865171790122986
  - 0.4208452105522156
  - 0.3900216221809387
  - 0.3728993535041809
  - 0.4177822172641754
  - 0.4469190239906311
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8593481989708405, 0.8404802744425386,
    0.8281786941580757]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.13084112149532712, 0.16666666666666666]'
  mean_eval_accuracy: 0.8483115535829014
  mean_f1_accuracy: 0.05950155763239875
  total_train_time: '0:02:54.937698'
