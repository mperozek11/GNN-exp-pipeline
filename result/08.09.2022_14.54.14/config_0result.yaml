config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cpu
  dropout: 0.0
  epochs: 30
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.05
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 5
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-09 14:54:34.027336'
fold_0_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_0fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_0fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_0fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_0fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_0fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_0fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_0fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_0fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_0fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_0fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 119.05012914985419
  - 27.911740118265154
  - 13.386091616749765
  - 8.383221861720086
  - 8.698886570334436
  - 7.93792278021574
  - 8.332421547174453
  - 15.147327736020088
  - 8.287505015730858
  - 94.66230646967888
  - 58.8732142418623
  - 24.957451826334
  - 10.509267243742944
  - 7.512338736653328
  - 8.96939354389906
  - 12.98700737655163
  - 41.20381194055081
  - 152.7178850054741
  - 85.19177837967874
  - 20.771437525749207
  - 7.007210242003203
  - 7.642025008797646
  - 7.427713024616242
  - 10.59922399148345
  - 8.285009035468102
  - 8.954492431879045
  - 7.478710153698922
  - 7.515600886940956
  - 7.446856720745564
  - 7.25974646806717
  validation_losses:
  - 0.5565506815910339
  - 0.6232661008834839
  - 0.5812280774116516
  - 122.59408569335938
  - 5.728975772857666
  - 5312194560.0
  - 63.791690826416016
  - 43702.21484375
  - 190.5406494140625
  - 0.766874372959137
  - 0.8590176105499268
  - 0.6739028692245483
  - 1732.7562255859375
  - 1656326.125
  - 115083640.0
  - 0.4122050404548645
  - 62.10225296020508
  - 0.5497449040412903
  - 0.8060905933380127
  - 899.0877685546875
  - 1807.0308837890625
  - 6762978607104.0
  - 4585908862976.0
  - 0.7192105054855347
  - 471727.3125
  - 3494103296.0
  - 182030729216.0
  - 1798440484864.0
  - 1200326574080.0
  - 1481004548096.0
loss_records_fold1:
  train_losses:
  - 6.393730574846268
  - 6.918162330985069
  - 7.915868738293648
  - 7.198234203457833
  - 7.815924713015557
  - 7.4066276252269745
  - 6.736389136314393
  - 7.268021588027477
  - 7.229106588661671
  - 8.472362512350083
  - 8.570987713336946
  - 6.1353887736797335
  - 8.064009273052216
  - 6.693170765042305
  - 11.176716110110284
  - 7.701583963632584
  - 6.450725463032723
  - 9.18426963761449
  - 8.485647347569467
  - 9.839884305000306
  - 7.966359123587608
  - 7.837268914282323
  - 6.907011288404465
  - 10.254123580455781
  - 7.6457425415515905
  - 7.194799605011941
  - 8.202202215790749
  - 7.522876071929932
  - 8.229698485136032
  - 7.35190520733595
  validation_losses:
  - 1222996918272.0
  - 242544787456.0
  - 306296553472.0
  - 1143524687872.0
  - 641964113920.0
  - 630682550272.0
  - 732058157056.0
  - 581752979456.0
  - 299559419904.0
  - 659932643328.0
  - 840563032064.0
  - 26480523264.0
  - 298355621888.0
  - 297765240832.0
  - 1045083717632.0
  - 227440279552.0
  - 173125156864.0
  - 281966772224.0
  - 2378721656832.0
  - 445758668800.0
  - 213518499840.0
  - 100021248000.0
  - 1018288275456.0
  - 1556937310208.0
  - 1181475667968.0
  - 543299502080.0
  - 787897516032.0
  - 611448651776.0
  - 1870726430720.0
  - 107967234048.0
loss_records_fold2:
  train_losses:
  - 8.471380239725113
  - 8.512753957509995
  - 8.304549303650857
  - 7.81147463619709
  - 6.903264757990837
  - 6.692431411147118
  - 7.300357800722122
  - 8.297488221526146
  - 8.626488694548607
  - 7.0509404897689825
  - 8.498590219020844
  - 6.9687126904726036
  - 8.0395153850317
  - 8.050170037150384
  - 9.80009901225567
  - 7.01636163443327
  - 6.764753422141076
  - 6.55468524992466
  - 8.444911521673204
  - 6.444846519827843
  - 7.703188310563565
  - 6.964941549301148
  - 7.912894187867642
  - 9.398765737935902
  - 8.976687127351761
  - 10.879675115644933
  - 7.386755064129829
  - 6.267009693384171
  - 7.047512418031693
  - 29.550983376801014
  validation_losses:
  - 1995166580736.0
  - 479281020928.0
  - 125657.6640625
  - 180979904.0
  - 35067260.0
  - 19042702.0
  - 11095145.0
  - 71618.9609375
  - 267480.21875
  - 23861990.0
  - 8234817.5
  - 0.5346637964248657
  - 36173308.0
  - 4061246.0
  - 19450362.0
  - 13347.1220703125
  - 0.39584383368492126
  - 0.4010092616081238
  - 18914904.0
  - 13189390.0
  - 29512154.0
  - 16948260.0
  - 0.4016927182674408
  - 26506726.0
  - 2915197.0
  - 44960776.0
  - 34772764.0
  - 13956114.0
  - 0.6399145722389221
  - 855498.75
loss_records_fold3:
  train_losses:
  - 6.690966695547104
  - 7.485591911524534
  - 9.015492975711823
  - 7.892363753914833
  - 9.527707192301751
  - 8.037979190051557
  - 8.488437046110631
  - 7.09526885598898
  - 6.151495796442032
  - 8.395776757597924
  - 8.14303720742464
  - 8.30469500720501
  - 6.666174522042275
  - 7.488540433347225
  - 7.034001296758652
  - 7.405187743902207
  - 10.201143735647202
  - 6.95982393026352
  - 6.98613717854023
  - 9.70774450004101
  - 11.04757165312767
  - 8.83622305393219
  - 8.313452062010766
  - 6.389688014984131
  - 6.529943072795868
  - 7.088534029573203
  - 6.7222680985927585
  - 7.21407824754715
  - 8.414391472935677
  - 10.225910571217538
  validation_losses:
  - 517107875840.0
  - 153271394107392.0
  - 1501414534676480.0
  - 4621954430533632.0
  - 0.5076016783714294
  - 0.5368757247924805
  - 118063802351616.0
  - 0.4119841754436493
  - 0.3978279232978821
  - 1106357402992640.0
  - 40841175367680.0
  - 0.6644148230552673
  - 0.5325783491134644
  - 1228459430903808.0
  - 0.44682085514068604
  - 27889978310656.0
  - 7228151037952.0
  - 1270407571177472.0
  - 143564835127296.0
  - 668940887916544.0
  - 105015146446848.0
  - 0.6857796311378479
  - 0.8183404803276062
  - 0.40075770020484924
  - 166627685433344.0
  - 0.4164696931838989
  - 145818988314624.0
  - 0.6457892060279846
  - 11803411087360.0
  - 180462093860864.0
loss_records_fold4:
  train_losses:
  - 7.998703745007515
  - 9.122947750985622
  - 6.618186037242413
  - 7.8907974690198905
  - 6.918350428342819
  - 7.289481315016747
  - 13.939231282472612
  - 7.577332013845444
  - 8.224136418104171
  - 6.210003382712603
  - 6.691273541748524
  - 9.173161432147026
  - 7.7157027810812
  - 8.929941045492887
  - 8.44505629837513
  - 11.075519086420536
  - 8.296769320964813
  - 9.053517746925355
  - 7.204806397110224
  - 8.095266481116415
  - 7.673493106663227
  - 7.524228498339653
  - 7.344737531244755
  - 9.769743096828462
  - 8.356049901247024
  - 6.606825482845307
  - 8.127473857998849
  - 8.271395561099053
  - 7.7601407602429395
  - 8.353002670407296
  validation_losses:
  - 0.4925002157688141
  - 81462242050048.0
  - 592498053349376.0
  - 76919198449664.0
  - 0.4540325701236725
  - 195495133708288.0
  - 9577339813888.0
  - 0.39629507064819336
  - 39841777582080.0
  - 429571488874496.0
  - 9707484872704.0
  - 1353592471552.0
  - 0.3910380005836487
  - 1.1941497325897217
  - 736283290763264.0
  - 1448512751403008.0
  - 55657722019840.0
  - 10204373581824.0
  - 1469564399386624.0
  - 1348560138272768.0
  - 693568431718400.0
  - 0.8292195200920105
  - 12099454500864.0
  - 834114190049280.0
  - 4686605189120.0
  - 40685101121536.0
  - 585916217294848.0
  - 0.468607097864151
  - 3900024553472.0
  - 62795794087936.0
training fold messages:
  fold 0 training message: completed 30 epochs without stopping early
  fold 1 training message: completed 30 epochs without stopping early
  fold 2 training message: completed 30 epochs without stopping early
  fold 3 training message: completed 30 epochs without stopping early
  fold 4 training message: completed 30 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8507718696397941, 0.855917667238422, 0.14065180102915953, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.24661654135338348, 0.0, 0.0]'
  mean_eval_accuracy: 0.7131592132175676
  mean_f1_accuracy: 0.0493233082706767
  total_train_time: '0:05:18.772305'
