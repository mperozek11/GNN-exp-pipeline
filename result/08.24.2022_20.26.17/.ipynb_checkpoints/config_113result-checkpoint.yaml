config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:01:37.731607'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_113fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_113fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_113fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_113fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_113fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_113fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_113fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_113fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_113fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_113fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.27466596364975
  - 3.0873973667621613
  - 2.989528253674507
  - 3.017823338508606
  - 3.0190921783447267
  - 2.949214643239975
  - 2.953581923246384
  - 2.9446834802627566
  - 3.0748924881219866
  - 3.1208430230617523
  - 2.901202362775803
  - 2.846008813381195
  - 3.161876779794693
  - 2.9633317798376084
  - 2.997565889358521
  - 2.8190102219581608
  - 2.8389966368675235
  - 2.8756231456995014
  - 2.9327135264873507
  - 2.8850816935300827
  - 2.9515717029571533
  - 2.9135842204093936
  validation_losses:
  - 0.4074040651321411
  - 0.41907283663749695
  - 0.40432238578796387
  - 0.39438971877098083
  - 0.39645901322364807
  - 0.4173581898212433
  - 0.3989062011241913
  - 0.419515460729599
  - 0.3935149312019348
  - 0.3822304308414459
  - 0.6803082823753357
  - 0.38771989941596985
  - 0.39666587114334106
  - 0.3961561620235443
  - 0.38809987902641296
  - 0.4140014350414276
  - 0.39496588706970215
  - 0.4029749929904938
  - 0.3842400312423706
  - 0.3838762044906616
  - 0.3875507116317749
  - 0.3817507326602936
loss_records_fold1:
  train_losses:
  - 2.8977758049964906
  - 2.954100674390793
  - 2.892629858851433
  - 2.88831080198288
  - 2.941476821899414
  - 2.8293386936187748
  - 2.8340547263622287
  - 2.8047574877738954
  - 2.836603876948357
  - 2.777778932452202
  - 2.824186784029007
  - 2.858116066455841
  - 2.8033223986625675
  - 2.8231316030025484
  - 2.938896667957306
  - 2.786093303561211
  - 2.8189620763063434
  - 2.900316905975342
  - 2.790695545077324
  - 2.793754306435585
  - 2.8187100976705555
  - 2.8388437032699585
  - 2.81927319765091
  - 2.814781066775322
  - 2.7795789688825607
  - 2.802145227789879
  - 2.7928629219532013
  - 2.8003875792026522
  - 2.806377446651459
  - 2.8207201182842256
  - 2.7944479405879976
  - 2.7728217244148254
  - 2.819560492038727
  - 2.790283310413361
  - 2.7847085475921634
  - 2.787750667333603
  - 2.773968878388405
  - 2.8172619819641116
  - 2.8015900731086734
  - 2.8022803336381914
  - 2.7560675978660587
  - 2.802676111459732
  - 2.763194015622139
  - 2.8067786872386935
  - 2.771839028596878
  - 2.7328134864568714
  - 2.717151546478272
  - 2.8198696553707125
  - 2.7566016763448715
  - 2.823862200975418
  - 2.738745149970055
  - 2.7601693242788317
  - 2.8164603859186172
  - 2.794024407863617
  - 2.7705828934907917
  - 2.793462198972702
  - 2.7432256937026978
  - 2.8162360787391663
  - 2.7269665181636813
  - 2.7920841455459597
  - 2.739240810275078
  - 2.77698290348053
  - 2.7764047503471376
  - 2.8040663778781894
  - 2.755431926250458
  - 2.734877067804337
  - 2.7911489933729174
  - 2.7679205536842346
  - 2.814585191011429
  - 2.7619726568460465
  - 2.7295154333114624
  - 2.8337877571582797
  - 2.9194104164838794
  - 2.7920145362615587
  - 2.776700818538666
  - 2.736625531315804
  - 2.7575483649969104
  - 2.7218678891658783
  - 2.742581880092621
  - 2.7563321739435196
  - 2.860218930244446
  - 2.8011077433824543
  - 2.815780609846115
  - 2.7727757632732395
  - 2.7599297612905502
  - 2.7466644793748856
  - 2.7191664576530457
  - 2.7443215608596803
  - 2.76100606918335
  - 2.792548814415932
  - 2.817116379737854
  - 2.7728158771991733
  - 2.787199020385742
  - 2.751113897562027
  - 2.729670134186745
  - 2.6886380791664126
  - 2.792567050457001
  - 2.7307802528142933
  - 2.745830881595612
  - 2.7530403554439546
  validation_losses:
  - 0.3931969106197357
  - 0.39338141679763794
  - 0.3985529839992523
  - 0.41222697496414185
  - 0.4292263984680176
  - 0.40056127309799194
  - 0.4297463893890381
  - 0.4280603528022766
  - 0.40770256519317627
  - 0.4241105020046234
  - 0.4069911241531372
  - 0.40731996297836304
  - 0.4797326624393463
  - 0.42238566279411316
  - 0.41324740648269653
  - 0.44312727451324463
  - 0.5013240575790405
  - 0.3976077735424042
  - 0.5055537223815918
  - 0.6337544322013855
  - 0.5747506022453308
  - 0.40100231766700745
  - 0.4854878783226013
  - 0.7562522888183594
  - 1.0694646835327148
  - 0.45411697030067444
  - 0.6706759333610535
  - 0.5528272986412048
  - 0.5284271240234375
  - 0.5969915986061096
  - 1.8452526330947876
  - 0.5484768152236938
  - 0.4342080354690552
  - 0.9725388288497925
  - 1.382851004600525
  - 2.438591480255127
  - 0.6252501010894775
  - 0.48320499062538147
  - 0.7568598389625549
  - 0.4366459250450134
  - 0.38301771879196167
  - 0.39595720171928406
  - 0.39677363634109497
  - 0.5698232650756836
  - 0.6417758464813232
  - 0.5185850262641907
  - 0.8876568078994751
  - 1.1993649005889893
  - 3.0062785148620605
  - 0.40350037813186646
  - 0.5809407234191895
  - 0.3888995051383972
  - 0.39626380801200867
  - 0.38952717185020447
  - 0.39898383617401123
  - 0.4377858340740204
  - 0.4197065234184265
  - 0.4689773917198181
  - 1.2798473834991455
  - 0.6063684225082397
  - 1.316529393196106
  - 0.9397527575492859
  - 1.223594307899475
  - 2.6755526065826416
  - 1.1160930395126343
  - 0.6411604881286621
  - 1.029052495956421
  - 0.38769233226776123
  - 0.4541352391242981
  - 0.675251305103302
  - 0.4034186899662018
  - 0.6672881841659546
  - 1.168388843536377
  - 0.8922308683395386
  - 0.7895387411117554
  - 0.9808281064033508
  - 1.202633261680603
  - 1.0428516864776611
  - 0.7220911979675293
  - 0.9717763662338257
  - 0.5544754266738892
  - 0.7919964790344238
  - 0.972081184387207
  - 0.6541129946708679
  - 0.6584429144859314
  - 0.8574109673500061
  - 0.3841491639614105
  - 0.6931619644165039
  - 1.0785409212112427
  - 0.5551486015319824
  - 0.588580846786499
  - 0.516374409198761
  - 0.9140165448188782
  - 0.6611175537109375
  - 1.001768708229065
  - 0.6381210088729858
  - 0.5507888197898865
  - 0.43048620223999023
  - 0.7494553923606873
  - 0.7556760311126709
loss_records_fold2:
  train_losses:
  - 2.8085412323474888
  - 2.753985565900803
  - 2.725595673918724
  - 2.7613286256790164
  - 2.7864156842231753
  - 2.73398157954216
  - 2.6872359782457353
  - 2.741638877987862
  - 2.7435854375362396
  - 2.716296434402466
  - 2.751275545358658
  - 2.74467833340168
  - 2.7265159964561465
  - 2.6917180329561234
  - 2.6941617697477342
  - 2.775893759727478
  - 2.7526821434497837
  - 2.761465507745743
  - 2.7357766032218933
  - 2.68183189034462
  - 2.694793272018433
  - 2.7215256273746493
  - 2.711594596505165
  - 2.6860094130039216
  - 2.7122113674879076
  - 2.720005935430527
  - 2.6708447605371477
  - 2.6681654632091525
  - 2.6499359965324403
  - 2.7217516362667085
  - 2.6482003003358843
  - 2.678710749745369
  - 2.6866142421960832
  - 2.6713680773973465
  - 2.6838440358638764
  - 2.642963367700577
  - 2.6616782635450367
  - 2.7437032580375673
  - 2.684533154964447
  - 2.665160572528839
  - 2.6761745095252993
  - 2.7076536118984222
  - 2.741696292161942
  - 2.7042570412158966
  - 2.654118758440018
  - 2.6735230624675754
  - 2.6379468560218813
  - 2.6242816686630253
  - 2.6678196370601657
  - 2.6401040345430378
  - 2.6322952002286915
  - 2.6448370188474657
  - 2.707914271950722
  - 2.6850428104400637
  - 2.6312744170427322
  - 2.659307298064232
  - 2.6001292169094086
  - 2.6183556735515596
  - 2.647757035493851
  - 2.630802217125893
  - 2.6471752494573595
  - 2.7192597776651386
  - 2.6103869020938877
  - 2.597393608093262
  - 2.638156831264496
  - 2.645359694957733
  - 2.647760331630707
  - 2.6886708796024323
  - 2.65195731818676
  - 2.6170596271753315
  - 2.674071425199509
  - 2.6390191316604614
  - 2.615754118561745
  - 2.749297574162483
  - 2.6594162553548815
  - 2.6570006340742114
  - 2.621268090605736
  - 2.646234115958214
  - 2.586624735593796
  - 2.632543310523033
  - 2.607708397507668
  - 2.584642890095711
  - 2.6239516705274584
  - 2.6201929599046707
  - 2.5743304014205934
  - 2.589865776896477
  - 2.688759207725525
  - 2.605496895313263
  - 2.5734313488006593
  - 2.6000961363315582
  - 2.5562825798988342
  - 2.6139754325151445
  - 2.615666872262955
  - 2.7112437665462497
  - 2.5972696185112003
  - 2.573128390312195
  - 2.599613976478577
  - 2.5635422080755235
  - 2.5910759508609775
  - 2.6442593306303026
  validation_losses:
  - 0.4839511215686798
  - 0.44248008728027344
  - 0.5323783755302429
  - 0.4227030277252197
  - 0.976230263710022
  - 0.6019186973571777
  - 0.6563302874565125
  - 0.6016089916229248
  - 0.582811176776886
  - 0.662759006023407
  - 0.38107410073280334
  - 0.394862562417984
  - 0.3991840183734894
  - 0.9652087092399597
  - 0.9306215643882751
  - 0.5103419423103333
  - 0.5576379299163818
  - 0.9215281009674072
  - 0.6413229703903198
  - 0.5247130990028381
  - 0.40027815103530884
  - 0.4236406981945038
  - 0.47402530908584595
  - 0.42355963587760925
  - 0.5115726590156555
  - 0.4639635682106018
  - 0.9706347584724426
  - 1.8185746669769287
  - 0.400434672832489
  - 0.46743476390838623
  - 0.8061424493789673
  - 0.6451656222343445
  - 0.44617536664009094
  - 0.5404176115989685
  - 0.5420763492584229
  - 0.5291262269020081
  - 0.609798789024353
  - 0.3743167519569397
  - 0.3946477174758911
  - 0.4512515664100647
  - 0.4692043364048004
  - 0.4173900783061981
  - 0.42270466685295105
  - 0.6169049143791199
  - 0.5429500341415405
  - 0.4881686270236969
  - 0.4330242872238159
  - 0.6519578695297241
  - 0.4431028366088867
  - 0.5610339045524597
  - 0.6149380803108215
  - 0.9353762269020081
  - 0.559282660484314
  - 0.5238195061683655
  - 0.5183668732643127
  - 0.6717175841331482
  - 0.6274541616439819
  - 0.6849266290664673
  - 0.8951645493507385
  - 0.9621814489364624
  - 0.7812919616699219
  - 0.9506276249885559
  - 0.6183619499206543
  - 0.5886968970298767
  - 0.5309253931045532
  - 0.6545435190200806
  - 0.648472249507904
  - 0.4648578464984894
  - 0.447366863489151
  - 0.4869866371154785
  - 0.5577155351638794
  - 0.5100257992744446
  - 0.5037016868591309
  - 0.44235748052597046
  - 0.46002712845802307
  - 0.6614383459091187
  - 0.7609935998916626
  - 1.6229451894760132
  - 1.3761773109436035
  - 2.263453960418701
  - 1.6762306690216064
  - 3.0642037391662598
  - 0.45239248871803284
  - 2.440789222717285
  - 3.9143435955047607
  - 2.075918197631836
  - 1.317483901977539
  - 1.99004328250885
  - 2.6621909141540527
  - 0.4718526601791382
  - 0.6090174913406372
  - 0.7613406181335449
  - 0.6677799224853516
  - 0.69996178150177
  - 0.7437525391578674
  - 0.7248157262802124
  - 0.593082845211029
  - 0.99781334400177
  - 1.8847931623458862
  - 1.8128376007080078
loss_records_fold3:
  train_losses:
  - 2.6300900489091874
  - 2.623030111193657
  - 2.68487241268158
  - 2.62172532081604
  - 2.6349542856216432
  - 2.623609331250191
  - 2.62190218269825
  - 2.586234760284424
  - 2.61030013859272
  - 2.5479686975479128
  - 2.589680087566376
  - 2.600303623080254
  - 2.5824854224920273
  - 2.801744067668915
  - 2.7140377819538117
  - 2.6542746245861055
  - 2.62267895936966
  - 2.638239488005638
  - 2.6261536836624146
  - 2.629565411806107
  - 2.638811755180359
  - 2.660938930511475
  - 2.600679334998131
  - 2.608049961924553
  - 2.553656417131424
  - 2.5849581867456437
  - 2.62041871547699
  - 2.5907450407743458
  - 2.6085864901542664
  - 2.570002630352974
  - 2.62315126657486
  - 2.6697310626506807
  - 2.5614105463027954
  - 2.614597809314728
  - 2.6117638260126115
  - 2.605449202656746
  - 2.598493105173111
  - 2.5861714839935304
  - 2.5596936255693437
  - 2.564300239086151
  - 2.558004707098007
  - 2.5313417404890064
  - 2.6194051980972293
  - 2.5579705983400345
  - 2.5660150408744813
  - 2.581800937652588
  - 2.5127095937728883
  - 2.531470334529877
  - 2.598094600439072
  - 2.680318570137024
  - 2.569225010275841
  - 2.546828138828278
  - 2.6758038192987446
  - 2.6493339955806734
  - 2.5611255109310154
  - 2.551316049695015
  - 2.539955803751946
  - 2.521144464612007
  - 2.5659825980663302
  - 2.5664414882659914
  - 2.564601531624794
  - 2.522126588225365
  - 2.580336052179337
  - 2.495528143644333
  - 2.6021852403879167
  - 2.5584112614393235
  - 2.5672886937856676
  - 2.5175112664699557
  - 2.5257745742797852
  - 2.5801731914281847
  - 2.5001622945070268
  - 2.5044156998395923
  - 2.5468713700771333
  - 2.496058461070061
  - 2.521912369132042
  - 2.5425849944353107
  - 2.502684637904167
  - 2.479181480407715
  - 2.511804074048996
  - 2.499745646119118
  - 2.5473528057336807
  - 2.517641153931618
  - 2.4905985534191135
  - 2.478776687383652
  - 2.533482709527016
  - 2.48677521944046
  - 2.4864232510328295
  - 2.4941027611494064
  - 2.4784807831048967
  - 2.504585260152817
  - 2.4733519881963733
  - 2.4789379686117172
  - 2.4912823230028156
  - 2.508737802505493
  - 2.5422130554914477
  - 2.5747038394212725
  - 2.5184109926223757
  - 2.476771003007889
  - 2.628277623653412
  - 2.5153629720211033
  validation_losses:
  - 4.479638576507568
  - 2.281278133392334
  - 0.6007384657859802
  - 0.7371352314949036
  - 1.3408238887786865
  - 1.6727174520492554
  - 1.97334623336792
  - 0.7626262307167053
  - 3.202162265777588
  - 1.0901024341583252
  - 0.8329319357872009
  - 0.6634290814399719
  - 0.7488057017326355
  - 1.417101502418518
  - 0.5595663785934448
  - 0.4878179132938385
  - 0.5561851859092712
  - 0.8044887185096741
  - 0.8687829971313477
  - 0.8702597618103027
  - 0.9849163889884949
  - 0.902263343334198
  - 0.7286256551742554
  - 1.0504313707351685
  - 0.8040092587471008
  - 1.1046819686889648
  - 1.0162431001663208
  - 0.6735370755195618
  - 0.988034725189209
  - 2.048766851425171
  - 0.6046158671379089
  - 0.5575445294380188
  - 1.0244699716567993
  - 0.8452706933021545
  - 1.072274088859558
  - 0.7584748268127441
  - 1.1631710529327393
  - 0.9505999684333801
  - 1.5155338048934937
  - 1.4049586057662964
  - 1.2482411861419678
  - 1.6439563035964966
  - 2.238649606704712
  - 1.0238850116729736
  - 1.5325901508331299
  - 1.4372771978378296
  - 1.1633217334747314
  - 3.253427267074585
  - 4.861882209777832
  - 3.404970645904541
  - 4.154594898223877
  - 3.0077505111694336
  - 0.6665410995483398
  - 4.236197471618652
  - 2.0365068912506104
  - 5.649438381195068
  - 4.983921527862549
  - 8.251993179321289
  - 7.249903202056885
  - 1.5039886236190796
  - 2.1028878688812256
  - 1.4764597415924072
  - 0.9460225701332092
  - 1.0581930875778198
  - 1.3622902631759644
  - 2.107154369354248
  - 8.565122604370117
  - 3.875884532928467
  - 5.361115455627441
  - 2.6120312213897705
  - 1.5781058073043823
  - 2.376763105392456
  - 2.398589611053467
  - 1.280946135520935
  - 12.251252174377441
  - 37.3920783996582
  - 8.202879905700684
  - 4.179438591003418
  - 9.339141845703125
  - 2.655169725418091
  - 5.725214958190918
  - 16.972145080566406
  - 5.289961338043213
  - 19.230398178100586
  - 15.092842102050781
  - 20.122642517089844
  - 15.201483726501465
  - 18.823759078979492
  - 6.349308967590332
  - 14.856830596923828
  - 0.5817950367927551
  - 0.9907413125038147
  - 3.141775369644165
  - 4.669184684753418
  - 14.39770793914795
  - 3.2213213443756104
  - 3.573331117630005
  - 1.1908396482467651
  - 2.3011772632598877
  - 1.242931842803955
loss_records_fold4:
  train_losses:
  - 2.690141734480858
  - 2.6590727627277375
  - 2.6533169358968736
  - 2.617042261362076
  - 2.6081149101257326
  - 2.577394872903824
  - 2.5753890186548234
  - 2.6255624651908875
  - 2.5984856307506563
  - 2.5334217458963395
  - 2.5476382076740265
  - 2.5175204157829287
  - 2.539797687530518
  - 2.575588455796242
  - 2.5771760791540146
  - 2.5712030798196794
  - 2.536702474951744
  - 2.5242451608181002
  - 2.5066302806138996
  - 2.6170794457197193
  - 2.5090829193592072
  - 2.494714763760567
  - 2.552931118011475
  - 2.516270336508751
  - 2.5655776083469393
  - 2.543220964074135
  - 2.6067489683628082
  - 2.4909120351076126
  - 2.4872065395116807
  - 2.493886262178421
  - 2.554838734865189
  - 2.487727215886116
  - 2.5851265221834185
  - 2.4899907499551777
  - 2.488582906126976
  - 2.4723280280828477
  - 2.5243090987205505
  - 2.4841199576854707
  - 2.5441353499889376
  - 2.5669475704431535
  - 2.479779079556465
  - 2.464765149354935
  - 2.469726595282555
  - 2.5294569730758667
  - 2.5004281401634216
  - 2.5422855198383334
  - 2.5597115367650987
  - 2.567281883955002
  - 2.4818248450756073
  - 2.469659385085106
  - 2.575923475623131
  - 2.5670133560895922
  - 2.6698753505945207
  - 2.5388128340244296
  - 2.5285249561071397
  - 2.543298172950745
  - 2.495565795898438
  - 2.5732005268335345
  - 2.5473792225122454
  - 2.803763294219971
  - 2.8770457178354265
  - 2.909972846508026
  - 2.6804360866546633
  - 2.6759297490119938
  - 2.665254458785057
  - 2.673821830749512
  - 2.5845489501953125
  - 2.6339295983314517
  - 2.634332823753357
  - 2.5377248734235764
  - 2.6116591513156893
  - 2.51761092543602
  - 2.528006273508072
  - 2.4753389924764635
  - 2.507385152578354
  - 2.495048034191132
  - 2.588432064652443
  - 2.5113790422677997
  - 2.5405196219682695
  - 2.5235377430915835
  - 2.4857986718416214
  - 2.466626700758934
  - 2.4775091409683228
  - 2.5219263911247256
  - 2.457937178015709
  - 2.517750841379166
  - 2.5289574503898624
  - 2.5415602266788486
  - 2.4867765069007874
  - 2.470397570729256
  - 2.439841791987419
  - 2.621191689372063
  - 2.489301007986069
  - 2.4711173057556155
  - 2.434682521224022
  - 2.4782934337854385
  - 2.4986866891384127
  - 2.4696807265281677
  - 2.470585197210312
  - 2.430985552072525
  validation_losses:
  - 0.5299886465072632
  - 0.6061655282974243
  - 0.7292578816413879
  - 0.6519618034362793
  - 0.6180092096328735
  - 0.5102810859680176
  - 0.5086356401443481
  - 0.789067268371582
  - 0.7453057765960693
  - 0.5683290958404541
  - 0.8235267400741577
  - 0.8487025499343872
  - 1.5017054080963135
  - 0.7728032469749451
  - 0.8174622654914856
  - 0.8588792085647583
  - 1.1426563262939453
  - 0.9060207605361938
  - 1.8963199853897095
  - 1.0082913637161255
  - 1.0189260244369507
  - 0.6754782795906067
  - 1.1000617742538452
  - 1.7373504638671875
  - 1.2326964139938354
  - 0.8413428664207458
  - 0.5520139336585999
  - 1.7783197164535522
  - 1.7559093236923218
  - 1.5464732646942139
  - 1.1605690717697144
  - 0.8366014361381531
  - 1.0636181831359863
  - 7.486171245574951
  - 0.9955916404724121
  - 0.466200590133667
  - 0.570196807384491
  - 1.0287466049194336
  - 0.5403661131858826
  - 0.6652185320854187
  - 1.1260102987289429
  - 0.6887301802635193
  - 0.6833606362342834
  - 0.9637455344200134
  - 1.1601823568344116
  - 0.6634747982025146
  - 1.7539732456207275
  - 0.5943261981010437
  - 0.7245551943778992
  - 0.8145840764045715
  - 1.1338423490524292
  - 0.5791534781455994
  - 0.523474395275116
  - 0.892009973526001
  - 2.440990447998047
  - 1.4350441694259644
  - 0.8394421339035034
  - 1.7201652526855469
  - 2.0809013843536377
  - 0.8001536130905151
  - 0.6392428874969482
  - 0.39951449632644653
  - 0.5763733386993408
  - 0.7390801906585693
  - 0.5423586964607239
  - 0.5752219557762146
  - 0.4989348351955414
  - 1.7729414701461792
  - 0.5505404472351074
  - 0.6429763436317444
  - 0.6720907092094421
  - 0.5824292302131653
  - 0.6544235944747925
  - 0.6615312695503235
  - 11.537772178649902
  - 5.736111164093018
  - 1.2051657438278198
  - 1.5370852947235107
  - 11.329751014709473
  - 4.483940601348877
  - 2.396042585372925
  - 2.994300603866577
  - 2.3728439807891846
  - 2.5111541748046875
  - 6.186682224273682
  - 1.660138726234436
  - 3.0984959602355957
  - 2.2306530475616455
  - 1.35469388961792
  - 2.002096176147461
  - 5.337980270385742
  - 3.7628774642944336
  - 0.49054500460624695
  - 1.9398767948150635
  - 0.9144184589385986
  - 2.6471974849700928
  - 0.8861136436462402
  - 0.7131105065345764
  - 1.4328445196151733
  - 1.2338165044784546
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8267581475128645, 0.8370497427101201, 0.7993138936535163,
    0.8436426116838488]'
  fold_eval_f1: '[0.0, 0.15126050420168066, 0.25196850393700787, 0.25477707006369427,
    0.1651376146788991]'
  mean_eval_accuracy: 0.8328794657329961
  mean_f1_accuracy: 0.1646287385762564
  total_train_time: '0:35:56.205024'
