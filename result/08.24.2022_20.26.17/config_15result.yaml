config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.761993'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_15fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_15fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_15fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_15fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_15fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_15fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_15fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_15fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_15fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_15fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 23.760041332244874
  - 9.77986490726471
  - 3.756566023826599
  - 4.323620843887329
  - 3.021695160865784
  - 2.0247665643692017
  - 1.6057097256183626
  - 1.7734537839889528
  - 1.5815017879009248
  - 1.2466355443000794
  - 1.3106640219688417
  - 1.006347441673279
  - 0.9514512777328492
  - 2.9592716038227085
  - 2.1529636442661286
  - 1.1390272736549378
  - 1.4416034877300263
  - 1.184954458475113
  - 1.0383821189403535
  - 1.0008532762527467
  - 1.0003687918186188
  - 1.0176812887191773
  - 0.8999771118164063
  - 0.9892428159713745
  - 3.8382730126380924
  - 2.253218799829483
  - 6.841184884309769
  - 2.871447110176087
  - 1.8690666437149048
  - 2.8419362127780916
  - 1.587398624420166
  - 1.788978672027588
  - 2.0316186368465425
  - 1.5761557161808015
  - 1.2718812584877015
  - 1.2667388439178469
  - 2.5917528510093693
  - 1.5501949667930603
  - 1.3641937673091888
  - 5.466930627822876
  - 3.574812060594559
  - 1.093668156862259
  - 2.868364775180817
  - 5.328844898939133
  - 5.014799284934998
  - 1.3095355033874512
  - 1.1537551999092102
  - 1.0593157172203065
  - 1.034076637029648
  - 0.8853213489055634
  - 1.0210018455982208
  - 1.4599904656410219
  - 0.8263499632477761
  - 0.8836345672607422
  - 1.1832070052623749
  - 0.8839778065681458
  - 0.8799237430095673
  - 0.8275859266519547
  - 1.1124763250350953
  - 0.9191287606954575
  - 1.050343531370163
  - 0.944927191734314
  - 1.0417693555355072
  - 0.9377504467964173
  - 0.9476758658885956
  - 0.917612111568451
  - 0.9569620609283448
  - 0.9725481867790222
  - 2.547620117664337
  - 1.1550685822963715
  - 1.0567975163459777
  - 0.886791428923607
  - 0.9628251492977142
  - 1.487678611278534
  - 0.8420491039752961
  - 0.8487404823303223
  - 0.8458285957574845
  - 0.8978343486785889
  - 0.8371165990829468
  - 0.8975512027740479
  - 0.8375830352306366
  - 0.8138638198375703
  - 0.9249497950077057
  - 0.9109507322311402
  - 0.8517607092857361
  - 1.8689285635948183
  - 1.6268910467624664
  - 1.203038251399994
  - 0.9895538151264192
  - 0.9496357262134553
  - 0.9426828622817993
  - 1.0637143433094025
  - 0.9347762227058412
  - 1.0059868752956391
  - 0.9430130243301392
  - 1.106800478696823
  - 0.8917630195617676
  - 0.8317238867282868
  - 1.1067362308502198
  validation_losses:
  - 5.660656452178955
  - 1.6604055166244507
  - 1.9288830757141113
  - 2.068629503250122
  - 0.8330855965614319
  - 0.6085683703422546
  - 0.46446749567985535
  - 0.5854172110557556
  - 0.6862340569496155
  - 0.47391119599342346
  - 0.6009325981140137
  - 0.420834481716156
  - 0.42540451884269714
  - 0.48097851872444153
  - 0.5624977350234985
  - 0.4519354999065399
  - 0.42567312717437744
  - 0.49929314851760864
  - 0.459917813539505
  - 0.46258652210235596
  - 0.4025159776210785
  - 0.47609469294548035
  - 0.4356476068496704
  - 0.4511200189590454
  - 0.4473084509372711
  - 0.7534902095794678
  - 0.6233591437339783
  - 1.0485607385635376
  - 0.7884641289710999
  - 0.6558958292007446
  - 1.0641885995864868
  - 0.5226578116416931
  - 0.6573053002357483
  - 0.631281316280365
  - 0.477738618850708
  - 0.6192314624786377
  - 0.39704614877700806
  - 0.7909319996833801
  - 0.44465169310569763
  - 0.4685240387916565
  - 0.43625935912132263
  - 0.42998006939888
  - 0.40364202857017517
  - 0.4514622986316681
  - 0.4145160913467407
  - 0.4930608570575714
  - 0.5540638566017151
  - 0.4553983807563782
  - 0.39587581157684326
  - 0.5328116416931152
  - 0.41873103380203247
  - 0.40115290880203247
  - 0.3930380344390869
  - 0.44119226932525635
  - 0.39147335290908813
  - 0.39939671754837036
  - 0.42732155323028564
  - 0.4002237021923065
  - 0.3971705138683319
  - 0.42630162835121155
  - 0.40049809217453003
  - 0.42151162028312683
  - 0.4807581305503845
  - 0.4470491409301758
  - 0.41698673367500305
  - 0.40033265948295593
  - 0.4738331139087677
  - 0.4813477396965027
  - 0.4249327480792999
  - 0.4656272232532501
  - 0.3990571200847626
  - 0.40178000926971436
  - 0.4068076014518738
  - 0.4245327413082123
  - 0.400489866733551
  - 0.3983384966850281
  - 0.4018939137458801
  - 0.39069825410842896
  - 0.3908194303512573
  - 0.40332379937171936
  - 0.39176714420318604
  - 0.39258551597595215
  - 0.40263083577156067
  - 0.4087863564491272
  - 0.41241246461868286
  - 0.39599883556365967
  - 0.39750832319259644
  - 0.4171602725982666
  - 0.3967541456222534
  - 0.40769124031066895
  - 0.40797653794288635
  - 0.3909372091293335
  - 0.42837202548980713
  - 0.4005127549171448
  - 0.40236541628837585
  - 0.39728420972824097
  - 0.39992260932922363
  - 0.39319396018981934
  - 0.39652034640312195
loss_records_fold1:
  train_losses:
  - 1.048384428024292
  - 0.8454608619213104
  - 0.9536343514919281
  - 0.9093356609344483
  - 0.875641667842865
  - 0.9469699561595917
  - 0.9168558716773987
  - 0.8997514069080353
  - 0.8599252581596375
  - 0.835156911611557
  - 0.8328839659690858
  validation_losses:
  - 0.4368118345737457
  - 0.4210834801197052
  - 0.4077659249305725
  - 0.40881186723709106
  - 0.4849138557910919
  - 0.41063863039016724
  - 0.41754037141799927
  - 0.4214366674423218
  - 0.41613519191741943
  - 0.406742125749588
  - 0.39719754457473755
loss_records_fold2:
  train_losses:
  - 0.8819492161273956
  - 1.1424277186393739
  - 0.9567195296287537
  - 0.9235716760158539
  - 0.8820548892021179
  - 0.873397845029831
  - 0.8746148526668549
  - 1.0667592704296112
  - 1.0332144737243654
  - 0.8590851962566376
  - 0.9002112627029419
  - 0.8858973801136018
  - 1.0023919999599458
  - 1.296915763616562
  - 1.1954101204872132
  - 1.0767639398574829
  - 0.8521316051483154
  - 0.8799845457077027
  - 0.9802828252315522
  - 0.8826078712940216
  - 0.8562053918838501
  - 0.8311429709196091
  - 0.8990958571434021
  - 0.910664826631546
  - 0.8477566778659821
  validation_losses:
  - 0.3926350474357605
  - 0.4533497989177704
  - 0.4249642491340637
  - 0.40615108609199524
  - 0.3992195129394531
  - 0.40981826186180115
  - 0.38901612162590027
  - 0.4639841318130493
  - 0.3922264575958252
  - 0.3976818919181824
  - 0.39620423316955566
  - 0.3903126120567322
  - 0.43261170387268066
  - 0.5529991388320923
  - 0.4601449966430664
  - 0.41252779960632324
  - 0.40987104177474976
  - 0.3941424489021301
  - 0.41366860270500183
  - 0.40526527166366577
  - 0.3990984857082367
  - 0.40555790066719055
  - 0.4046920835971832
  - 0.39756664633750916
  - 0.39248305559158325
loss_records_fold3:
  train_losses:
  - 0.8489804148674012
  - 0.8462904572486878
  - 0.8280327439308167
  - 0.8477793753147126
  - 0.8308684706687928
  - 0.880219852924347
  - 0.8527190208435059
  - 0.8708459496498109
  - 0.8454206466674805
  - 1.0767252683639528
  - 1.281168168783188
  - 2.3526707351207734
  - 2.2627346754074096
  - 1.2275347292423249
  - 1.145996081829071
  - 1.630636614561081
  - 3.2696622192859652
  - 1.6018744945526124
  - 1.288494771718979
  - 1.190153032541275
  - 1.764570724964142
  - 0.9840272068977356
  - 0.9139230251312256
  - 0.9193520367145539
  - 1.0550441861152648
  - 0.880751758813858
  - 1.4672008812427522
  validation_losses:
  - 0.40619704127311707
  - 0.40660879015922546
  - 0.41682034730911255
  - 0.4015349745750427
  - 0.3973468244075775
  - 0.409193217754364
  - 0.39954739809036255
  - 0.40020906925201416
  - 0.432350754737854
  - 0.4098002016544342
  - 0.4199676811695099
  - 1.7917178869247437
  - 0.6251055002212524
  - 0.5814908742904663
  - 0.4619563817977905
  - 0.5447795391082764
  - 0.448769211769104
  - 0.43215686082839966
  - 0.4107322096824646
  - 0.4827927052974701
  - 0.5432747006416321
  - 0.40628308057785034
  - 0.4092475473880768
  - 0.41368210315704346
  - 0.39870959520339966
  - 0.4034174680709839
  - 0.4061488211154938
loss_records_fold4:
  train_losses:
  - 0.806738030910492
  - 0.8159752607345582
  - 0.8574913918972016
  - 0.8878258407115937
  - 1.2281345784664155
  - 5.7566940844059
  - 1.3980461835861206
  - 1.5859653651714325
  - 1.119949394464493
  - 0.9141474723815919
  - 0.8726186156272888
  - 2.068602555990219
  - 0.8369450807571411
  - 0.8481154203414918
  - 0.9463444471359254
  - 0.911799818277359
  - 0.9526323258876801
  - 0.8716444313526154
  - 0.8481085032224656
  - 1.4005900740623476
  - 0.89184929728508
  - 0.8504888474941255
  - 0.8559427261352539
  - 0.8475745677947999
  - 0.8799788534641266
  - 0.8392438471317292
  - 0.8577465534210206
  - 0.8442879021167755
  - 0.8163702726364136
  - 0.8277435719966889
  - 1.9852114021778107
  - 1.083041387796402
  - 0.9798194646835328
  - 0.9885846555233002
  - 1.029244774580002
  - 0.9022081851959229
  - 0.9147486150264741
  - 0.9980429947376251
  - 0.8721557497978211
  - 0.85087508559227
  - 0.9006342887878418
  - 2.215810549259186
  - 1.0124621570110321
  - 0.8754304349422455
  - 0.8920260012149811
  - 0.86346834897995
  - 0.8609845221042634
  - 1.5214876949787142
  - 0.9703110575675965
  - 0.8557266652584077
  - 0.8443471133708954
  - 0.8533613622188568
  - 0.8873994588851929
  - 0.886977243423462
  - 0.8741161882877351
  - 0.8256299316883088
  - 1.4911834716796877
  - 0.9966816902160645
  - 1.1452644705772401
  - 0.8720423877239227
  - 0.9919126868247986
  - 0.8661061823368073
  - 0.8608247339725494
  - 0.825018310546875
  - 0.8082585364580155
  - 0.8639855444431306
  - 0.8490195572376251
  - 0.8929211497306824
  - 0.9077230870723725
  - 1.0291657447814941
  validation_losses:
  - 0.409695029258728
  - 0.4254733622074127
  - 0.43683767318725586
  - 0.4241899847984314
  - 0.4012335240840912
  - 0.4453829824924469
  - 0.42101994156837463
  - 0.4214347004890442
  - 0.4387166202068329
  - 0.5326798558235168
  - 0.4474354684352875
  - 0.4665305018424988
  - 0.4966384470462799
  - 0.4211137890815735
  - 0.41537198424339294
  - 0.47402188181877136
  - 0.4798429310321808
  - 0.4813790023326874
  - 0.4958297908306122
  - 0.43894219398498535
  - 0.4543543756008148
  - 0.4555151164531708
  - 0.3940019905567169
  - 0.4147401750087738
  - 0.4455626904964447
  - 0.394061803817749
  - 0.42598146200180054
  - 0.43985211849212646
  - 0.43302685022354126
  - 0.4657570719718933
  - 0.42183414101600647
  - 0.4341576397418976
  - 0.46503713726997375
  - 0.5298352241516113
  - 0.5636552572250366
  - 0.5846948027610779
  - 0.42448583245277405
  - 0.38956260681152344
  - 0.4547690749168396
  - 0.4001258909702301
  - 0.46552395820617676
  - 0.6008722186088562
  - 0.44563913345336914
  - 0.4509364366531372
  - 0.4112718403339386
  - 0.4342402219772339
  - 0.44199368357658386
  - 0.47878363728523254
  - 0.47105875611305237
  - 0.4177547097206116
  - 0.4077388644218445
  - 0.39878636598587036
  - 0.49102547764778137
  - 0.42567718029022217
  - 0.44586601853370667
  - 0.4098394811153412
  - 0.4229714870452881
  - 0.474134624004364
  - 0.43356913328170776
  - 0.39790239930152893
  - 0.4383666515350342
  - 0.4364425241947174
  - 0.4441320300102234
  - 0.461143434047699
  - 0.4226089417934418
  - 0.43257132172584534
  - 0.4242318570613861
  - 0.42623674869537354
  - 0.4026213586330414
  - 0.39231571555137634
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 99 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 70 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:18:34.618513'
