config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:57:21.826905'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_118fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_118fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_118fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_118fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_118fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_118fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_118fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_118fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_118fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_118fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.9762547016143799
  - 1.7100227713584901
  - 1.5968387961387636
  - 1.5859389305114746
  - 1.6731579244136812
  - 1.7150696635246279
  - 1.6167572259902956
  - 1.531123983860016
  - 1.5527052283287048
  - 1.5700323522090913
  - 1.54987490773201
  - 1.5597764253616333
  - 1.5684798955917358
  - 1.693612849712372
  - 1.6363425195217134
  - 1.5847395539283753
  - 1.5868447363376619
  validation_losses:
  - 0.5540001392364502
  - 0.3945765793323517
  - 0.39475640654563904
  - 0.3930865228176117
  - 0.4055582880973816
  - 0.4213743805885315
  - 0.39684903621673584
  - 0.3884592056274414
  - 0.39398640394210815
  - 0.40268823504447937
  - 0.42554014921188354
  - 0.41091054677963257
  - 0.4005775451660156
  - 0.4045834541320801
  - 0.3872750401496887
  - 0.3925185799598694
  - 0.38789114356040955
loss_records_fold1:
  train_losses:
  - 1.5804006159305573
  - 1.593344783782959
  - 1.7623369455337525
  - 1.6702560424804689
  - 1.6445361196994783
  - 1.605006992816925
  - 1.7026516139507295
  - 1.5502211868762972
  - 1.5458393752574922
  - 1.5849417626857758
  - 1.5507244050502778
  validation_losses:
  - 0.4001217782497406
  - 0.40189501643180847
  - 0.40280017256736755
  - 0.3933204412460327
  - 0.4086354076862335
  - 0.39403000473976135
  - 0.40108808875083923
  - 0.39802131056785583
  - 0.4026086926460266
  - 0.3953697979450226
  - 0.40320274233818054
loss_records_fold2:
  train_losses:
  - 1.5552582979202272
  - 1.6912557423114778
  - 1.5791825711727143
  - 1.603280133008957
  - 1.5716836124658586
  - 1.58618381023407
  - 1.5720027685165405
  - 1.5773717045783997
  - 1.520455425977707
  - 1.5549879908561708
  - 1.5966957032680513
  - 1.4965276658535005
  - 1.5183823943138124
  - 1.5430057764053347
  - 1.5268420398235323
  - 1.6087290346622467
  - 1.5552618682384491
  - 1.5193369328975679
  - 1.5018632113933563
  - 1.5240364134311677
  - 1.6188438892364503
  - 1.5427184522151949
  - 1.5297574698925018
  - 1.5862727105617525
  - 1.5308199763298036
  - 1.5026386737823487
  - 1.5368516683578493
  - 1.5519081056118011
  - 1.5023395478725434
  - 1.5254208981990816
  - 1.5422519624233246
  - 1.5326074838638306
  - 1.573655503988266
  - 1.5352502942085267
  - 1.5863493740558625
  - 1.5271244049072266
  - 1.6259484708309175
  - 1.552083134651184
  - 1.5087099879980088
  - 1.5665529787540438
  - 1.5262493133544923
  - 1.526258248090744
  - 1.5757782936096192
  - 1.520946526527405
  - 1.5812491297721865
  - 1.508249568939209
  - 1.493981271982193
  - 1.5901478946208956
  validation_losses:
  - 0.3908638656139374
  - 0.4030602276325226
  - 0.3915553390979767
  - 0.40968823432922363
  - 0.3963770866394043
  - 0.39131441712379456
  - 0.3928598165512085
  - 0.39431968331336975
  - 0.3912310004234314
  - 0.38915565609931946
  - 0.4016558825969696
  - 0.38760906457901
  - 0.38981443643569946
  - 0.39177393913269043
  - 0.4004049599170685
  - 0.3912522792816162
  - 0.4070490598678589
  - 0.39523252844810486
  - 0.40353813767433167
  - 0.3981095254421234
  - 0.3975294232368469
  - 0.38652873039245605
  - 0.39891716837882996
  - 0.39186620712280273
  - 0.39780572056770325
  - 0.3982032239437103
  - 0.4197388291358948
  - 0.39091405272483826
  - 0.3890171945095062
  - 0.3961789608001709
  - 0.3914451003074646
  - 0.42690223455429077
  - 0.4011904299259186
  - 0.3962858319282532
  - 0.395346462726593
  - 0.40758028626441956
  - 0.3920326828956604
  - 0.38575422763824463
  - 0.3883914053440094
  - 0.4274865686893463
  - 0.4005882441997528
  - 0.4143374562263489
  - 0.39023393392562866
  - 0.3903978765010834
  - 0.39924299716949463
  - 0.38550716638565063
  - 0.3906313478946686
  - 0.39906933903694153
loss_records_fold3:
  train_losses:
  - 1.5797312796115877
  - 1.4953256905078889
  - 1.5563599884510042
  - 1.543044674396515
  - 1.5757863819599152
  - 1.5957917153835297
  - 1.5424368262290955
  - 1.5302933394908906
  - 1.5510660588741303
  - 1.5787878632545471
  - 1.5639228463172914
  - 1.5118260383605957
  - 1.7267474055290224
  - 1.6256770312786104
  - 1.5831026613712311
  - 1.574601197242737
  - 1.616079354286194
  - 1.525422590970993
  - 1.534035462141037
  - 1.591417056322098
  - 1.5159976184368134
  - 1.5094405829906465
  - 1.528578168153763
  - 1.5499645948410035
  - 1.5364509642124178
  - 1.5033329725265503
  - 1.5132058203220369
  - 1.5410899579524995
  - 1.5304154217243195
  - 1.487844777107239
  - 1.532194119691849
  - 1.5235212087631227
  - 1.5004049360752107
  - 1.5853501677513124
  - 1.4965874493122102
  - 1.4990617096424104
  - 1.5768084526062012
  - 1.4969379901885986
  - 1.5442926526069642
  - 1.6258973360061646
  - 1.4822265803813934
  - 1.5586298167705537
  - 1.5219392299652101
  - 1.4986672937870027
  - 1.4964752137660982
  - 1.5460403442382813
  - 1.509416300058365
  - 1.4928672254085542
  - 1.4733686476945878
  - 1.526217842102051
  - 1.4966986119747163
  - 1.5325522482395173
  - 1.580322140455246
  - 1.4815146744251253
  - 1.4714405536651611
  - 1.4645999670028687
  - 1.5283768832683564
  - 1.5509290218353273
  - 1.508373284339905
  - 1.5106712102890016
  - 1.476240110397339
  - 1.5013408362865448
  - 1.498460590839386
  - 1.526835584640503
  - 1.5469735264778137
  - 1.4679491102695466
  - 1.467500025033951
  - 1.4929619610309601
  - 1.509788066148758
  - 1.5647855281829834
  - 1.520754048228264
  - 1.4773324310779572
  - 1.5149724245071412
  - 1.4957126021385194
  - 1.4888431131839752
  - 1.4925727725028992
  - 1.511482435464859
  - 1.5378963470458986
  - 1.4946974694728852
  - 1.4905179798603059
  - 1.5518289268016816
  - 1.4420611619949342
  - 1.522103625535965
  - 1.519542557001114
  - 1.5234584629535677
  - 1.4846179127693178
  - 1.5152924537658692
  - 1.4945811152458193
  - 1.5211395084857942
  - 1.532311886548996
  - 1.4896145164966583
  - 1.512311726808548
  - 1.5152548491954805
  - 1.4608126670122148
  - 1.5032714128494264
  - 1.5732760965824129
  - 1.5314551681280137
  - 1.5903234660625458
  - 1.5459347784519197
  - 1.5374049127101899
  validation_losses:
  - 0.37921085953712463
  - 0.39572426676750183
  - 0.4061427712440491
  - 0.39556294679641724
  - 0.39380353689193726
  - 0.41041815280914307
  - 0.38803786039352417
  - 0.39422285556793213
  - 0.39399486780166626
  - 0.40912291407585144
  - 0.3947390913963318
  - 0.4202098250389099
  - 0.39799684286117554
  - 0.37703147530555725
  - 0.39921584725379944
  - 0.3808857500553131
  - 0.3835085928440094
  - 0.38426777720451355
  - 0.41728875041007996
  - 0.39617496728897095
  - 0.41248631477355957
  - 0.4938621520996094
  - 0.3830934464931488
  - 0.4309397339820862
  - 0.4071604013442993
  - 0.43788301944732666
  - 0.5504531860351562
  - 0.3688655197620392
  - 0.39570820331573486
  - 0.505480945110321
  - 0.39006075263023376
  - 0.3778096139431
  - 0.37713611125946045
  - 0.4089219868183136
  - 0.4161991477012634
  - 0.38855278491973877
  - 0.4072212874889374
  - 0.40511563420295715
  - 0.38954418897628784
  - 0.4206731915473938
  - 0.37855780124664307
  - 0.43296360969543457
  - 0.38354483246803284
  - 0.4360896050930023
  - 3.440072774887085
  - 0.6202086210250854
  - 0.6265835762023926
  - 0.7800517678260803
  - 0.4056977927684784
  - 0.4244646728038788
  - 0.5217460989952087
  - 0.8842597007751465
  - 0.7695674896240234
  - 0.407010555267334
  - 0.6996983885765076
  - 2.316802501678467
  - 0.3773159384727478
  - 0.497374027967453
  - 6.324855327606201
  - 0.6440882086753845
  - 0.5506370067596436
  - 0.8135696053504944
  - 0.3772137761116028
  - 0.4316403269767761
  - 1.5293488502502441
  - 0.3816220164299011
  - 0.4494233727455139
  - 1.5470613241195679
  - 0.8506219983100891
  - 1.429280161857605
  - 0.45104196667671204
  - 0.4498746991157532
  - 1.0160117149353027
  - 0.6829128265380859
  - 1.343988060951233
  - 0.41782936453819275
  - 0.7119144797325134
  - 2.994394063949585
  - 13.840800285339355
  - 1.3653552532196045
  - 0.4768277406692505
  - 0.4567123055458069
  - 2.5520737171173096
  - 1.6932193040847778
  - 4.585971832275391
  - 0.400799959897995
  - 0.6089932322502136
  - 0.41452401876449585
  - 0.8983371257781982
  - 0.5071242451667786
  - 1.2007652521133423
  - 0.7300970554351807
  - 0.5413765907287598
  - 0.47476014494895935
  - 0.773361086845398
  - 0.428067684173584
  - 0.39448657631874084
  - 0.3943943381309509
  - 0.38262808322906494
  - 0.3868754208087921
loss_records_fold4:
  train_losses:
  - 1.465082359313965
  - 1.572402709722519
  - 1.5639128029346467
  - 1.5270362854003907
  - 1.463251656293869
  - 1.5460818767547608
  - 1.4736775934696198
  - 1.496406126022339
  - 1.4768359392881394
  - 1.5166814506053925
  - 1.4756938934326174
  - 1.4888233602046967
  - 1.5248580992221834
  - 1.4897236108779908
  - 1.459833300113678
  - 1.4625469148159027
  - 1.4765510380268099
  - 1.5115537524223328
  - 1.4942213714122774
  - 1.532212769985199
  - 1.4723496943712235
  - 1.4635341405868532
  - 1.5167213380336761
  - 1.579297661781311
  - 1.4569724380970002
  - 1.4551719278097153
  - 1.4630055725574493
  - 1.4957421243190767
  - 1.4668969571590424
  - 1.4929573357105257
  - 1.480939781665802
  - 1.5204570472240448
  - 1.4437230795621874
  - 1.4825059056282044
  - 1.4870357155799867
  - 1.5280678272247314
  - 1.5134491682052613
  - 1.485083031654358
  - 1.4681245923042299
  - 1.545996004343033
  - 1.4670558691024782
  - 1.5087158262729645
  - 1.477899181842804
  - 1.466530591249466
  - 1.4978310763835907
  - 1.453498435020447
  - 1.5212869465351107
  - 1.5149154603481294
  - 1.4910775005817414
  - 1.4809045135974885
  - 1.5065291702747345
  - 1.5397506535053254
  - 1.537159264087677
  - 1.4980470657348635
  - 1.4824914038181305
  - 1.491521406173706
  - 1.526843172311783
  - 1.4541475534439088
  - 1.5050396740436556
  - 1.4679334789514542
  - 1.46515309214592
  - 1.4328823864459992
  - 1.494057458639145
  - 1.4488440394401552
  - 1.4830926835536957
  - 1.466263610124588
  - 1.4787157595157625
  - 1.4790362179279328
  - 1.4151032030582429
  - 1.4667330026626588
  - 1.486420488357544
  - 1.4869175255298615
  - 1.4797038197517396
  - 1.4840128600597382
  - 1.5176258504390718
  - 1.458054429292679
  - 1.4534185826778412
  - 1.440434056520462
  - 1.489158856868744
  - 1.5010054767131806
  - 1.5055043280124665
  - 1.470406699180603
  - 1.5140814125537874
  - 1.4823852121829988
  - 1.4419734120368959
  - 1.4879766225814821
  - 1.452342611551285
  - 1.4680987894535065
  - 1.4911159455776215
  - 1.4735263526439668
  - 1.4730055272579194
  - 1.412913328409195
  - 1.4974137723445893
  - 1.499340033531189
  - 1.4058706402778627
  - 1.469366079568863
  - 1.4693707764148713
  - 1.447586414217949
  - 1.452475756406784
  - 1.4503005504608155
  validation_losses:
  - 0.3952828645706177
  - 0.3898896276950836
  - 0.36492621898651123
  - 0.3683735728263855
  - 0.4021337628364563
  - 0.42013439536094666
  - 0.3912387788295746
  - 0.36815616488456726
  - 0.43210136890411377
  - 0.3758313059806824
  - 0.37457790970802307
  - 0.3761865496635437
  - 0.6069777607917786
  - 0.38968029618263245
  - 0.5854393243789673
  - 0.37251168489456177
  - 0.3750578463077545
  - 0.3793173134326935
  - 0.3973507881164551
  - 0.43329405784606934
  - 0.41042473912239075
  - 0.42174428701400757
  - 0.3643377721309662
  - 0.43652084469795227
  - 0.39010339975357056
  - 0.40424439311027527
  - 0.36325153708457947
  - 0.38539835810661316
  - 0.4106486737728119
  - 0.406542032957077
  - 0.44386422634124756
  - 0.4492139220237732
  - 0.3873025178909302
  - 0.36760637164115906
  - 0.36775270104408264
  - 0.4631366729736328
  - 0.4404481053352356
  - 0.36387908458709717
  - 0.39531180262565613
  - 0.41117075085639954
  - 0.38448241353034973
  - 0.42668142914772034
  - 0.3994559645652771
  - 0.404629647731781
  - 0.36864468455314636
  - 0.35842230916023254
  - 0.4807431101799011
  - 0.4135865271091461
  - 0.4429314434528351
  - 0.45607712864875793
  - 0.40685492753982544
  - 0.3706771731376648
  - 0.44700518250465393
  - 0.37023112177848816
  - 0.3978327512741089
  - 0.40874725580215454
  - 0.4103054106235504
  - 0.37228548526763916
  - 0.37824851274490356
  - 0.38715487718582153
  - 0.4507223069667816
  - 0.3998420238494873
  - 0.45376843214035034
  - 0.39705419540405273
  - 0.4936647117137909
  - 0.4534868597984314
  - 0.38840487599372864
  - 0.39966854453086853
  - 0.3687022030353546
  - 0.8040039539337158
  - 0.6209947466850281
  - 0.3745456635951996
  - 0.38334670662879944
  - 0.38189736008644104
  - 0.40160059928894043
  - 0.37725022435188293
  - 0.38291582465171814
  - 0.39005982875823975
  - 0.40179240703582764
  - 0.3811447322368622
  - 0.40064164996147156
  - 0.3839488625526428
  - 0.43173947930336
  - 0.4244149923324585
  - 0.37588444352149963
  - 0.4205237925052643
  - 0.4211774468421936
  - 0.4021247327327728
  - 0.39687591791152954
  - 0.39319923520088196
  - 0.4365258514881134
  - 0.453126460313797
  - 0.3575493395328522
  - 0.40161651372909546
  - 0.43004706501960754
  - 0.41635584831237793
  - 0.3958762586116791
  - 0.3702342212200165
  - 0.35918986797332764
  - 0.34832194447517395
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 48 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8576329331046312,
    0.8625429553264605]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0697674418604651]'
  mean_eval_accuracy: 0.8589579907222389
  mean_f1_accuracy: 0.01395348837209302
  total_train_time: '0:24:19.540815'
