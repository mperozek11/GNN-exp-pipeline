config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:35:29.190618'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_52fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_52fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_52fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_52fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_52fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_52fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_52fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_52fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_52fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_52fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.604109454154969
  - 6.536825546622277
  - 6.503822684288025
  - 6.288872817158699
  - 6.267757162451744
  - 6.4147660642862325
  - 6.515638479590416
  - 6.234143283963204
  - 6.466859894990922
  - 6.139357277750969
  - 6.0332121640443805
  - 6.14327843785286
  - 6.145160833001137
  - 6.099582633376122
  - 6.008065456151963
  validation_losses:
  - 0.40842723846435547
  - 0.41623231768608093
  - 0.4232029616832733
  - 0.40701228380203247
  - 0.3938298225402832
  - 0.4033564031124115
  - 0.4004775583744049
  - 0.3919733464717865
  - 0.43669337034225464
  - 0.39086487889289856
  - 0.3981988728046417
  - 0.39782261848449707
  - 0.4043615758419037
  - 0.406697541475296
  - 0.3942314386367798
loss_records_fold1:
  train_losses:
  - 6.001003524661065
  - 5.895930810272694
  - 6.0566135287284855
  - 6.231525424122811
  - 5.998082834482194
  - 6.090656036138535
  - 5.956314238905907
  - 6.011845675110817
  - 6.040062057971955
  - 5.912287029623986
  - 6.101755449175835
  validation_losses:
  - 0.39384719729423523
  - 0.40067481994628906
  - 0.3991851806640625
  - 0.3972565233707428
  - 0.3913109004497528
  - 0.3957226872444153
  - 0.39505741000175476
  - 0.3979904353618622
  - 0.3915909230709076
  - 0.39295870065689087
  - 0.39301472902297974
loss_records_fold2:
  train_losses:
  - 6.081070083379746
  - 5.9044295966625215
  - 5.792193514108658
  - 6.00751596391201
  - 5.933550453186036
  - 5.977484557032586
  - 5.915824505686761
  - 5.935968062281609
  - 6.07787289917469
  - 5.961799255013466
  - 5.998235648870469
  validation_losses:
  - 0.3871205151081085
  - 0.3975203037261963
  - 0.3917110562324524
  - 0.391154408454895
  - 0.40359246730804443
  - 0.3916814923286438
  - 0.38891252875328064
  - 0.3919563591480255
  - 0.38752293586730957
  - 0.39435017108917236
  - 0.38223761320114136
loss_records_fold3:
  train_losses:
  - 5.8499428778886795
  - 6.202332401275635
  - 5.962534302473069
  - 5.955559554696084
  - 5.86233523786068
  - 5.953892001509667
  - 5.826708030700684
  - 5.85945480465889
  - 6.020395094156266
  - 5.937986108660699
  - 5.822625993192196
  - 6.083787888288498
  - 6.09846408367157
  - 5.975404962897301
  - 6.09709013402462
  - 5.915499490499497
  - 5.955162000656128
  - 6.067950633168221
  - 6.1032326281070715
  - 5.9792655438184745
  - 6.061260882019997
  - 6.050964456796646
  - 6.078477504849435
  - 5.992717453837395
  - 5.848666563630104
  - 5.955306133627892
  - 6.028303867578507
  - 5.978542983531952
  - 5.925539439916611
  - 5.8200583338737495
  validation_losses:
  - 0.38430777192115784
  - 0.37526586651802063
  - 0.37963682413101196
  - 0.3839143216609955
  - 0.3792661428451538
  - 0.3769177496433258
  - 0.40834495425224304
  - 0.372464120388031
  - 0.37587618827819824
  - 0.3739359378814697
  - 0.39149630069732666
  - 0.38507944345474243
  - 0.37391403317451477
  - 0.3868790566921234
  - 0.39471468329429626
  - 0.38453876972198486
  - 0.38791146874427795
  - 0.395646870136261
  - 0.37506183981895447
  - 0.3859226107597351
  - 0.37845897674560547
  - 0.3821174204349518
  - 0.37746354937553406
  - 0.391854465007782
  - 0.3835582435131073
  - 0.3841005861759186
  - 0.38901183009147644
  - 0.38230234384536743
  - 0.3808579742908478
  - 0.37969905138015747
loss_records_fold4:
  train_losses:
  - 5.889766103029252
  - 5.845572221279145
  - 5.830097213387489
  - 6.001472690701485
  - 5.999631768465043
  - 5.87603472173214
  - 5.897556248307229
  - 5.999327847361565
  - 6.076412308216096
  - 5.926461723446846
  - 6.071027755737305
  - 5.823988363146782
  - 6.0082733094692236
  - 5.9879588931798935
  - 5.983050081133843
  - 5.787914541363716
  - 5.933756822347641
  - 5.8873136222362525
  - 5.939115372300148
  - 6.043336069583893
  - 5.8825902312994005
  - 6.072896707057954
  - 5.92703478038311
  - 5.942679488658905
  - 6.075436782836914
  - 5.96714226603508
  - 5.929288589954377
  - 6.023784285783768
  - 5.867669448256493
  - 5.88937121629715
  - 5.961152076721191
  - 5.977679887413979
  - 5.89606195986271
  - 5.905227938294411
  validation_losses:
  - 0.3862931728363037
  - 0.4007185697555542
  - 0.37986835837364197
  - 0.3920852839946747
  - 0.3797624409198761
  - 0.38456061482429504
  - 0.3809422254562378
  - 0.4056943655014038
  - 0.38595062494277954
  - 0.3914813995361328
  - 0.3848069906234741
  - 0.4337872266769409
  - 0.38820451498031616
  - 0.39685559272766113
  - 0.3881627321243286
  - 0.38446280360221863
  - 0.38730770349502563
  - 0.40760040283203125
  - 0.39384523034095764
  - 0.383093923330307
  - 0.3895333409309387
  - 0.3745194673538208
  - 0.39277952909469604
  - 0.39343756437301636
  - 0.3920902609825134
  - 0.37977421283721924
  - 0.3827669024467468
  - 0.39629098773002625
  - 0.38635507225990295
  - 0.38020578026771545
  - 0.38588249683380127
  - 0.3833392262458801
  - 0.3885536789894104
  - 0.37567469477653503
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:11:05.555477'
