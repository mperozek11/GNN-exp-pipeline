config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:15:11.704192'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_93fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_93fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_93fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_93fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_93fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_93fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_93fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_93fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_93fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_93fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 36.32213528156281
  - 11.00390250682831
  - 9.631427317857742
  - 20.471733415126803
  - 18.92841304540634
  - 8.148606735467911
  - 8.714116156101227
  - 6.9842267662286766
  - 10.025232815742493
  - 9.01206923723221
  - 10.162152171134949
  - 7.252750942111016
  - 7.440613129734993
  - 6.468208891153336
  - 8.366162544488907
  - 16.425060939788818
  - 8.598012191057206
  - 7.364920914173126
  - 5.224898317456246
  - 4.327648097276688
  - 6.275452107191086
  - 8.284082293510437
  - 6.492058044672013
  - 3.7014364063739777
  - 5.6295261323452
  - 4.2556885302066805
  - 4.909030842781068
  - 4.59895949959755
  - 4.227868914604187
  - 3.278336071968079
  - 3.610837370157242
  - 3.3537363409996033
  - 4.1411953389644625
  - 4.11843364238739
  - 3.9394147098064423
  - 3.2434814751148227
  - 4.2471684217453
  - 3.3005233705043793
  - 3.238675221800804
  - 3.976428443193436
  - 3.962400567531586
  - 3.285913395881653
  - 3.251075714826584
  - 3.6997059762477877
  - 4.0715784490108495
  - 4.022368147969246
  - 3.4967803716659547
  - 5.193798649311066
  - 3.7416273772716524
  - 3.8951571583747864
  - 3.4979558765888217
  - 4.092808783054352
  - 3.43047793507576
  - 3.1639183044433596
  - 3.2303573191165924
  - 3.2111171185970306
  - 3.1986182868480686
  - 3.7208001375198365
  - 3.362134808301926
  - 3.345054173469544
  - 3.2395176947116853
  - 3.521426218748093
  - 4.0576511323452
  - 3.4608210921287537
  - 3.54170737862587
  - 3.2554337978363037
  - 3.1713067948818208
  - 3.285328352451325
  - 3.194612753391266
  - 3.349552845954895
  - 3.383055928349495
  - 3.281379109621048
  - 3.123875629901886
  - 3.23892257809639
  - 3.39267156124115
  - 3.340066742897034
  - 3.4682664930820466
  - 3.135863327980042
  - 3.1402212500572206
  - 3.433248454332352
  - 3.288501286506653
  - 3.1287398099899293
  - 3.2862974524497988
  - 3.2276783287525177
  - 3.2323052018880847
  - 3.3335986316204074
  - 3.2905921459198
  - 3.4209755599498752
  - 3.2282626271247867
  - 3.2120329916477206
  - 3.20575413107872
  - 3.279392248392105
  - 3.2068237483501436
  - 3.1565480113029483
  - 3.200990700721741
  - 3.227558267116547
  - 3.2607244551181793
  - 3.196183902025223
  - 3.301251554489136
  - 3.1890592873096466
  validation_losses:
  - 1.1832194328308105
  - 1.1097996234893799
  - 0.6528549790382385
  - 0.8616835474967957
  - 0.6051415205001831
  - 0.671268105506897
  - 0.7164193987846375
  - 0.44001853466033936
  - 0.45694321393966675
  - 0.6146634221076965
  - 0.49937009811401367
  - 0.7217633128166199
  - 0.426360160112381
  - 0.38459643721580505
  - 1.5859335660934448
  - 0.6576809287071228
  - 0.4096028804779053
  - 0.6142178773880005
  - 0.42208561301231384
  - 0.46673479676246643
  - 0.535875678062439
  - 0.4030151069164276
  - 0.5895689725875854
  - 0.3898181915283203
  - 0.40230581164360046
  - 0.43314436078071594
  - 0.4439160227775574
  - 0.415820837020874
  - 0.40003275871276855
  - 0.38342776894569397
  - 0.38667741417884827
  - 0.38770532608032227
  - 0.3998768627643585
  - 0.4196193218231201
  - 0.4242263436317444
  - 0.40630221366882324
  - 0.4396020174026489
  - 0.4040563106536865
  - 0.41776975989341736
  - 0.45382383465766907
  - 0.40445125102996826
  - 0.4024721682071686
  - 0.40609756112098694
  - 0.4209933876991272
  - 0.4111365079879761
  - 0.39858388900756836
  - 0.4035666584968567
  - 0.4289018511772156
  - 0.41110169887542725
  - 0.4541487991809845
  - 0.4002556800842285
  - 0.4093426465988159
  - 0.4462786018848419
  - 0.4187743663787842
  - 0.4252063035964966
  - 0.4035797119140625
  - 0.39453694224357605
  - 0.4140472710132599
  - 0.4248007833957672
  - 0.40472710132598877
  - 0.40970364212989807
  - 0.41495281457901
  - 0.541218638420105
  - 0.42257586121559143
  - 0.4186364710330963
  - 0.3996385931968689
  - 0.4075632691383362
  - 0.45086944103240967
  - 0.4173526167869568
  - 0.404989093542099
  - 0.4024893641471863
  - 0.39735448360443115
  - 0.43029895424842834
  - 0.41045013070106506
  - 0.39361876249313354
  - 0.4563226103782654
  - 0.42855918407440186
  - 0.40979066491127014
  - 0.3977046608924866
  - 0.42251938581466675
  - 0.41506096720695496
  - 0.40215110778808594
  - 0.40832728147506714
  - 0.4187273383140564
  - 0.4840685725212097
  - 0.42489731311798096
  - 0.4523591995239258
  - 0.41652432084083557
  - 0.41552817821502686
  - 0.40778660774230957
  - 0.4108799397945404
  - 0.397726446390152
  - 0.4101443290710449
  - 0.4124537706375122
  - 0.410211980342865
  - 0.4068235754966736
  - 0.4129622280597687
  - 0.409512996673584
  - 0.45024538040161133
  - 0.43432337045669556
loss_records_fold1:
  train_losses:
  - 3.289197874069214
  - 3.161493903398514
  - 3.2158961653709413
  - 3.3250825643539432
  - 3.1605240970849993
  - 3.1955671906471252
  - 3.2039531588554384
  - 3.22184219956398
  - 3.3013435244560245
  - 3.2209294855594637
  - 3.2010610520839693
  - 3.1568889856338505
  - 3.1709481418132786
  - 3.142063617706299
  - 3.3063635706901553
  - 3.3239664793014527
  - 3.1576774567365646
  - 3.35928989648819
  - 3.1948022067546846
  - 3.1109791100025177
  - 3.1928949236869815
  - 3.2110254764556885
  - 3.126764237880707
  - 3.2888410449028016
  - 3.1802304625511173
  - 3.25880942940712
  - 3.152525097131729
  - 3.2054859519004824
  - 3.2756632685661318
  - 3.208605316281319
  - 3.1888614475727084
  - 3.2417241454124452
  - 3.236280769109726
  - 3.149714076519013
  - 3.210316187143326
  - 3.200478595495224
  - 3.110869431495667
  - 3.94233535528183
  - 3.817763859033585
  - 3.3447583615779877
  - 4.075890839099884
  - 3.775852656364441
  - 3.2648203670978546
  - 3.1756592988967896
  - 3.2537650704383854
  - 3.2831523358821872
  - 3.3243041157722475
  - 3.0992422699928284
  - 3.2511684089899067
  - 3.1996778011322022
  - 3.2223916798830032
  - 3.2321198999881746
  - 3.187704783678055
  - 3.197245806455612
  - 3.2023186147212983
  - 3.1244905591011047
  - 3.143459898233414
  - 3.339001089334488
  - 3.1825437247753143
  - 3.3599254071712497
  - 3.15372753739357
  - 3.134628760814667
  - 3.2214465320110324
  - 3.256657385826111
  - 3.1223078012466434
  - 3.2850789189338685
  - 3.394651943445206
  - 3.2050498843193056
  - 3.1225057542324066
  - 3.2453741371631626
  - 3.403455775976181
  - 3.1867056787014008
  - 3.1953750580549243
  - 3.195232927799225
  - 3.14094250202179
  - 3.638696950674057
  - 3.2783155620098117
  - 3.3986228585243228
  - 3.4277070939540866
  - 3.138868618011475
  - 3.252844506502152
  - 3.2383132517337803
  - 6.1263202965259556
  - 3.4304289489984514
  - 3.2219887733459474
  - 3.94106787443161
  - 3.830946719646454
  - 3.4906559884548187
  - 3.4219802856445316
  - 3.175271785259247
  - 3.199876827001572
  - 3.606499892473221
  - 3.6070902407169343
  - 3.4343080997467044
  - 3.338054805994034
  - 3.3743823766708374
  - 3.4380332589149476
  - 3.309586274623871
  - 3.141635400056839
  - 3.191157427430153
  validation_losses:
  - 0.4216064214706421
  - 0.43876415491104126
  - 0.4327853322029114
  - 0.42911601066589355
  - 0.4209855794906616
  - 0.4160565435886383
  - 0.42094656825065613
  - 0.4249199330806732
  - 0.42317476868629456
  - 0.4093934893608093
  - 0.42863577604293823
  - 0.4194694757461548
  - 0.43073195219039917
  - 0.4948175847530365
  - 1.772910714149475
  - 0.5938979387283325
  - 3.2654221057891846
  - 11.780217170715332
  - 0.419442355632782
  - 0.4112703800201416
  - 0.42627671360969543
  - 0.42194536328315735
  - 0.42839521169662476
  - 0.4994584321975708
  - 0.43618905544281006
  - 0.40919917821884155
  - 0.40984538197517395
  - 0.4201165735721588
  - 0.5068063735961914
  - 68816.6328125
  - 7.348312854766846
  - 0.4329499900341034
  - 20690638.0
  - 11489784.0
  - 0.41361016035079956
  - 342.4710693359375
  - 1552.6226806640625
  - 0.4108664393424988
  - 0.46037328243255615
  - 0.41690298914909363
  - 0.41960054636001587
  - 0.46211498975753784
  - 0.41644594073295593
  - 0.46523672342300415
  - 0.43998977541923523
  - 0.428451806306839
  - 0.4476819336414337
  - 0.42578014731407166
  - 0.44704651832580566
  - 0.4234037697315216
  - 0.40960022807121277
  - 0.4158380925655365
  - 0.44945254921913147
  - 0.41822823882102966
  - 0.4154929518699646
  - 0.43577203154563904
  - 0.41753271222114563
  - 0.42000308632850647
  - 0.5257806181907654
  - 0.42060765624046326
  - 0.43846598267555237
  - 0.45229867100715637
  - 0.4169788956642151
  - 0.41588902473449707
  - 0.41263264417648315
  - 0.4608319401741028
  - 0.47303158044815063
  - 0.42653313279151917
  - 0.4274396300315857
  - 0.4643484055995941
  - 0.437135249376297
  - 0.42772015929222107
  - 0.43906649947166443
  - 0.4387664496898651
  - 0.4163142144680023
  - 0.414014607667923
  - 0.4682396352291107
  - 0.41350939869880676
  - 0.463680624961853
  - 0.45642003417015076
  - 0.4340214133262634
  - 0.4339120388031006
  - 0.4298307001590729
  - 0.5089897513389587
  - 0.4752463400363922
  - 0.41768404841423035
  - 0.42629295587539673
  - 0.4667615592479706
  - 0.44994449615478516
  - 0.4186300039291382
  - 0.4182775616645813
  - 0.42942580580711365
  - 0.45612385869026184
  - 0.4488009512424469
  - 0.4169861674308777
  - 0.43363553285598755
  - 0.4445071220397949
  - 0.4183245003223419
  - 0.42209184169769287
  - 0.44659534096717834
loss_records_fold2:
  train_losses:
  - 3.482544815540314
  - 3.669943392276764
  - 3.28845471739769
  - 3.251009774208069
  - 3.233290231227875
  - 3.1576345741748812
  - 3.3102080821990967
  - 3.323175013065338
  - 3.4041358292102815
  - 3.1775207161903385
  - 3.2923069298267365
  - 3.3835220813751223
  - 3.3595500171184542
  - 3.238311541080475
  - 3.622627460956574
  - 4.140345177054406
  - 3.886074334383011
  - 6.083321839570999
  - 3.8129445850849155
  - 6.253383058309556
  - 3.6293119549751283
  - 3.3880547881126404
  - 3.558131936192513
  - 3.3208597719669344
  - 3.2349891483783724
  - 3.3945144712924957
  - 3.3320817887783054
  - 3.5172664880752564
  - 3.2346517503261567
  - 3.177773118019104
  - 3.336527019739151
  - 3.3316999077796936
  - 3.9773704648017887
  - 3.4695497512817384
  - 5.081448435783386
  - 5.76275053024292
  - 3.5771237432956697
  - 3.216188752651215
  - 3.860985463857651
  - 3.262264382839203
  - 3.929500007629395
  - 3.4638038337230683
  - 3.1717490732669833
  - 3.8801784396171572
  - 3.374368017911911
  - 3.253543895483017
  - 3.399784600734711
  - 3.5944369912147525
  validation_losses:
  - 0.4337977468967438
  - 0.4066261351108551
  - 0.42253586649894714
  - 0.4031914174556732
  - 0.4054698944091797
  - 0.4272229075431824
  - 0.39990150928497314
  - 0.41778698563575745
  - 0.4292701184749603
  - 0.41103944182395935
  - 0.4026809632778168
  - 0.397841215133667
  - 0.40204742550849915
  - 0.39712727069854736
  - 6.757080554962158
  - 2.6890392303466797
  - 58.704776763916016
  - 0.42792797088623047
  - 0.4139520823955536
  - 0.44566071033477783
  - 0.448029100894928
  - 0.39414578676223755
  - 0.39936596155166626
  - 0.45194000005722046
  - 0.39926213026046753
  - 0.4076763689517975
  - 0.40063080191612244
  - 0.43250298500061035
  - 0.41189083456993103
  - 0.41463589668273926
  - 11.2733154296875
  - 8.385334014892578
  - 0.39873021841049194
  - 1.4256834983825684
  - 0.4231966733932495
  - 0.41416722536087036
  - 0.3977850079536438
  - 0.4013533592224121
  - 0.3954164683818817
  - 0.4069388806819916
  - 0.4048934578895569
  - 0.4290696084499359
  - 0.39811280369758606
  - 0.4027423858642578
  - 0.4074714481830597
  - 0.39227357506752014
  - 0.39894652366638184
  - 0.4025392234325409
loss_records_fold3:
  train_losses:
  - 3.65461942255497
  - 3.3623586893081665
  - 3.299052220582962
  - 3.214844048023224
  - 3.5013746321201324
  - 3.3298597216606143
  - 3.3630314230918885
  - 3.382965695858002
  - 3.228588151931763
  - 3.1966095864772797
  - 3.1900635063648224
  validation_losses:
  - 0.41232234239578247
  - 0.4141088128089905
  - 0.41974836587905884
  - 0.4119015336036682
  - 0.41901060938835144
  - 0.4143752455711365
  - 0.4117283523082733
  - 0.4206729829311371
  - 0.4085444211959839
  - 0.40926167368888855
  - 0.40274661779403687
loss_records_fold4:
  train_losses:
  - 3.1327470362186434
  - 3.2439254105091098
  - 3.315149182081223
  - 3.302581185102463
  - 3.2645914256572723
  - 3.238096731901169
  - 3.68093678355217
  - 3.267229622602463
  - 3.573728907108307
  - 3.8650975555181506
  - 3.4923812031745913
  - 3.2766779363155365
  - 3.272313913702965
  - 3.6343867003917696
  - 3.2688556939363482
  - 3.637887766957283
  - 3.4635268807411195
  - 3.420709037780762
  - 3.275102347135544
  - 3.497265559434891
  - 3.676071831583977
  - 3.5868786811828617
  - 3.1685224771499634
  - 3.138181203603745
  - 3.250496059656143
  - 3.218779218196869
  - 3.235670417547226
  - 3.2918924868106845
  - 3.4527854442596437
  - 3.4173887968063354
  - 3.1964878648519517
  - 3.2503007769584658
  - 3.307448768615723
  - 3.381061574816704
  - 3.163487392663956
  - 3.3004072248935703
  - 3.148325112462044
  - 3.18152745962143
  - 3.194523710012436
  - 3.203693294525147
  - 3.2113952934741974
  - 3.299394774436951
  - 3.2972851365804674
  - 3.2169415414333344
  - 3.3515421867370607
  - 3.16272434592247
  - 3.1590946912765503
  - 3.2261640936136247
  - 3.367903220653534
  - 3.355359905958176
  - 3.3428250491619114
  - 3.326277881860733
  - 3.295750451087952
  - 3.3109458684921265
  - 3.343491542339325
  - 3.234123930335045
  validation_losses:
  - 0.4114864766597748
  - 0.4060524106025696
  - 0.4194908142089844
  - 0.40523457527160645
  - 0.4208768308162689
  - 0.40231549739837646
  - 0.4079485535621643
  - 0.44924911856651306
  - 0.4136495888233185
  - 0.42167624831199646
  - 0.4067467749118805
  - 0.39573293924331665
  - 0.4061160683631897
  - 0.4065999388694763
  - 0.43323004245758057
  - 0.516126811504364
  - 0.4241546094417572
  - 0.4301722049713135
  - 0.40757298469543457
  - 0.4008747637271881
  - 0.44621774554252625
  - 0.40629833936691284
  - 0.41521981358528137
  - 0.4070563316345215
  - 0.40895792841911316
  - 0.3996441066265106
  - 0.4276339113712311
  - 0.3929167687892914
  - 0.41832101345062256
  - 0.41639333963394165
  - 0.4176824688911438
  - 0.4075685441493988
  - 0.4084206819534302
  - 0.43141093850135803
  - 0.41187599301338196
  - 0.4135388433933258
  - 0.40517571568489075
  - 0.4022591710090637
  - 0.4048674404621124
  - 0.41610953211784363
  - 0.42126914858818054
  - 0.41916990280151367
  - 0.4088241159915924
  - 0.41062548756599426
  - 0.43800321221351624
  - 0.4085223972797394
  - 0.41640356183052063
  - 0.4406445622444153
  - 0.41343623399734497
  - 0.4649967551231384
  - 0.41103610396385193
  - 0.4087611734867096
  - 0.4127206802368164
  - 0.4188213646411896
  - 0.4107384979724884
  - 0.40044471621513367
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 48 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 56 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:29:18.394293'
