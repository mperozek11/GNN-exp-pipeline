config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 19:12:37.725589'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_127fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_127fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_127fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_127fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_127fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_127fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_127fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_127fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_127fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_127fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 16.256120681762695
  - 10.780837655067444
  - 17.356168603897096
  - 3.8800966858863832
  - 3.941532349586487
  - 3.86194885969162
  - 1.6617845416069033
  - 1.4682738110423088
  - 1.465942567586899
  - 2.002880942821503
  - 2.3645429611206055
  - 1.9656333565711976
  - 1.4101057648658752
  - 7.24908641576767
  - 5.737291467189789
  - 2.134473717212677
  - 4.283290946483612
  - 3.756973898410797
  - 2.4746997117996217
  - 2.691445076465607
  - 1.2313082933425905
  - 8.227769601345063
  - 2.543175995349884
  - 2.745234853029251
  - 1.6675867259502413
  - 1.1951262295246126
  - 3.0541605293750766
  - 8.827815520763398
  - 3.092271000146866
  - 1.2558215022087098
  - 1.0731454968452454
  - 1.2683422386646273
  - 0.9452225804328919
  - 1.1279488980770112
  - 1.6165000200271606
  - 12.261859071254731
  - 2.8099971532821657
  - 1.1679891705513001
  - 1.1189520716667176
  - 1.1425552845001221
  - 2.9071161270141603
  - 12.031857734918596
  - 3.9792332828044894
  - 2.1394778072834018
  - 1.5165029704570772
  - 1.6649229526519775
  - 1.6109804689884186
  - 2.216270434856415
  - 1.1522121965885164
  - 1.3240135073661805
  - 4.275852137804032
  - 2.9434759676456452
  - 14.039115941524507
  - 4.184275335073472
  - 1.1735366702079773
  - 2.175898551940918
  - 0.9700725972652435
  - 2.8513705968856815
  - 1.1624481737613679
  - 2.0233262300491335
  - 1.4921034455299378
  - 1.3216969132423402
  - 1.6384424924850465
  - 1.073813194036484
  - 0.9987968862056733
  - 0.8460017502307893
  - 0.8978438198566437
  - 0.8757881343364716
  - 0.8723393499851227
  - 0.948838859796524
  - 0.9373465180397034
  - 1.1869190037250519
  - 0.954984450340271
  - 0.9768636345863343
  - 1.018370944261551
  - 1.1681404888629914
  - 1.403557449579239
  - 0.8863688349723816
  - 0.8097829878330232
  - 0.8508499503135681
  - 1.0895196974277497
  - 0.8437075734138489
  - 0.8822512149810792
  - 0.8403755605220795
  - 0.8756698906421662
  - 0.8769171237945557
  - 0.8584535956382752
  - 0.9226902723312378
  - 0.9374332189559937
  - 0.8710520148277283
  - 0.8632331788539886
  - 0.8236328721046449
  - 0.8406153976917268
  - 0.9945981860160829
  - 1.8659480094909668
  - 0.8504992008209229
  - 0.9615653157234192
  - 0.8723789930343628
  - 1.2983555734157564
  - 0.834382975101471
  validation_losses:
  - 10.540228843688965
  - 13.266199111938477
  - 3.0362234115600586
  - 0.8273658752441406
  - 0.5648624897003174
  - 0.7522981762886047
  - 0.5635126829147339
  - 0.6921629905700684
  - 0.7827682495117188
  - 1.1049038171768188
  - 0.7274411916732788
  - 0.5661645531654358
  - 0.47824764251708984
  - 1.106738805770874
  - 0.6392706036567688
  - 1.2866523265838623
  - 1.363376498222351
  - 1.487338662147522
  - 0.5794880986213684
  - 0.5712292194366455
  - 0.4524403214454651
  - 0.5027575492858887
  - 0.5600352883338928
  - 0.5401824116706848
  - 0.5834085941314697
  - 0.43286028504371643
  - 0.4012046754360199
  - 0.4454028010368347
  - 0.4149034917354584
  - 0.6260321736335754
  - 0.7985223531723022
  - 0.4152548909187317
  - 0.44317716360092163
  - 0.48029059171676636
  - 0.4952310025691986
  - 0.4283127784729004
  - 0.421097993850708
  - 0.44536277651786804
  - 0.44483616948127747
  - 0.5978400707244873
  - 1.612352728843689
  - 0.8234127163887024
  - 0.7166641354560852
  - 0.6015323400497437
  - 0.47340235114097595
  - 0.6158355474472046
  - 0.5132671594619751
  - 0.48175981640815735
  - 0.418760746717453
  - 0.5483651161193848
  - 0.46710917353630066
  - 0.56330806016922
  - 0.45364412665367126
  - 0.5459656715393066
  - 0.4543226957321167
  - 0.43832555413246155
  - 0.4498714506626129
  - 0.39316225051879883
  - 0.5899741649627686
  - 1.0310248136520386
  - 0.579346239566803
  - 0.6392215490341187
  - 0.42115285992622375
  - 0.46458518505096436
  - 0.4278898239135742
  - 0.4173920452594757
  - 0.42900630831718445
  - 0.3999471068382263
  - 0.41864827275276184
  - 0.4179977476596832
  - 0.42232316732406616
  - 0.43284234404563904
  - 0.4265001714229584
  - 0.445762574672699
  - 0.42601722478866577
  - 0.4319444000720978
  - 0.41908979415893555
  - 0.40074262022972107
  - 0.4030378758907318
  - 0.41526076197624207
  - 0.41284382343292236
  - 0.42343440651893616
  - 0.4084610342979431
  - 0.40412193536758423
  - 0.40187868475914
  - 0.4140344560146332
  - 0.47081902623176575
  - 0.40510794520378113
  - 0.4102070927619934
  - 0.4034219980239868
  - 0.38421228528022766
  - 0.4008994400501251
  - 0.4044981002807617
  - 0.40813639760017395
  - 0.41430479288101196
  - 0.4307039678096771
  - 0.41021987795829773
  - 0.41082698106765747
  - 0.40661555528640747
  - 0.4185284376144409
loss_records_fold1:
  train_losses:
  - 0.8359731018543244
  - 0.9264630019664765
  - 0.8820257246494294
  - 0.8454449951648713
  - 0.7954115629196168
  - 1.020115077495575
  - 0.9551269054412842
  - 0.9219356179237366
  - 0.8775637090206146
  - 0.9368322253227235
  - 0.7959969878196717
  - 0.8756625890731812
  - 0.9939109206199647
  - 0.9007191836833954
  - 1.4810791909694672
  - 1.3923810958862306
  - 0.9199147045612336
  - 0.9171155631542206
  - 0.915202158689499
  - 0.8699892103672028
  - 0.9076165497303009
  validation_losses:
  - 0.40370893478393555
  - 0.4110349416732788
  - 0.4067136347293854
  - 0.4117198884487152
  - 0.4133676588535309
  - 0.4192882776260376
  - 0.4453352689743042
  - 0.4104897677898407
  - 0.40713396668434143
  - 0.40179482102394104
  - 0.4231686294078827
  - 0.40364763140678406
  - 0.40413346886634827
  - 0.41442984342575073
  - 0.4247739315032959
  - 0.40981006622314453
  - 0.4169255495071411
  - 0.42119547724723816
  - 0.4060904085636139
  - 0.4005589783191681
  - 0.3979911208152771
loss_records_fold2:
  train_losses:
  - 1.0332190036773683
  - 1.1184874653816224
  - 3.1125943779945375
  - 1.4299418210983277
  - 6.54559298157692
  - 10.650298994779588
  - 2.4631618499755863
  - 2.5745578825473787
  - 1.5496867060661317
  - 1.182233053445816
  - 0.9551488935947419
  - 11.431523728370667
  - 2.804358196258545
  - 1.0077640891075135
  - 1.0776966631412506
  - 0.8545507371425629
  - 0.8912874639034272
  - 0.9988339185714722
  - 1.0819246411323549
  - 0.8994147002696992
  - 1.8470838367938995
  - 1.1697435677051544
  - 0.9969080448150636
  - 1.1121420919895173
  - 1.0595034658908844
  - 1.000381863117218
  - 1.101113647222519
  - 7.317750197649002
  - 1.4646960973739624
  - 1.2643178164958955
  - 1.5600829899311066
  - 1.269045525789261
  - 1.3467701792716982
  - 0.9629620015621185
  - 1.0009826600551606
  - 0.8807544231414796
  - 0.9318451404571534
  validation_losses:
  - 0.38831955194473267
  - 0.42308494448661804
  - 0.415294885635376
  - 0.8510099649429321
  - 0.40830525755882263
  - 0.44857358932495117
  - 0.39580342173576355
  - 0.38328102231025696
  - 0.40455490350723267
  - 0.4005764126777649
  - 0.401242733001709
  - 0.5872799754142761
  - 0.848983645439148
  - 0.3986089527606964
  - 0.4046162962913513
  - 0.4074271023273468
  - 0.4119546413421631
  - 0.38999393582344055
  - 0.4173643887042999
  - 0.40286627411842346
  - 0.44964277744293213
  - 0.4027842581272125
  - 0.4146214425563812
  - 0.40337786078453064
  - 0.4129731059074402
  - 0.40947914123535156
  - 0.41165605187416077
  - 0.42405134439468384
  - 0.47183850407600403
  - 0.4018162786960602
  - 0.6339126825332642
  - 0.6043321490287781
  - 0.4850482940673828
  - 0.42506301403045654
  - 0.40143680572509766
  - 0.40773117542266846
  - 0.3989734947681427
loss_records_fold3:
  train_losses:
  - 0.8900895476341248
  - 0.8526855409145355
  - 0.8465315997600555
  - 0.846577537059784
  - 0.8573054730892182
  - 0.8748230040073395
  - 1.0129304468631746
  - 0.9775401711463929
  - 1.8223765194416046
  - 1.1841201543807984
  - 0.9557817459106446
  - 0.8587352991104127
  - 1.159234815835953
  - 1.3296466112136842
  - 0.8504074126482011
  - 0.8826948523521424
  - 1.2391701519489289
  - 1.2027780711650848
  - 1.8400339245796205
  - 0.9646981358528137
  - 1.2103875935077668
  - 1.1951148986816407
  - 1.1129970669746398
  - 1.2117127299308779
  - 1.1441193580627442
  - 1.1808283448219299
  - 1.1893247604370119
  - 1.183184278011322
  - 1.1674820363521576
  - 1.0628804981708526
  - 0.9224089801311494
  - 0.8202311754226685
  - 0.7935386151075363
  - 0.9133806288242341
  - 0.8331104099750519
  - 0.8382480144500732
  - 0.948802250623703
  - 0.9578008592128754
  - 0.8859005153179169
  - 1.552150571346283
  - 1.0300603926181793
  - 0.9469936192035675
  - 0.9696677505970002
  - 0.8896568715572357
  - 0.8712374895811081
  - 1.33191739320755
  - 1.4225802659988405
  - 0.8723527669906617
  - 0.940350091457367
  - 0.9609097957611085
  - 1.4677268862724304
  - 0.8888100743293763
  - 0.8636321783065797
  - 0.9382374703884125
  - 0.7952208578586579
  - 0.9059915244579315
  - 0.8537322223186493
  - 0.8722257673740388
  - 0.9011080980300904
  validation_losses:
  - 0.4153517782688141
  - 1.0857248306274414
  - 0.7842880487442017
  - 0.43152880668640137
  - 0.40402284264564514
  - 0.45611250400543213
  - 0.4656219482421875
  - 0.39815789461135864
  - 0.6615129113197327
  - 0.4159835875034332
  - 0.47305524349212646
  - 0.526854932308197
  - 1.1228842735290527
  - 0.6509093046188354
  - 0.4237898290157318
  - 0.4369623363018036
  - 0.4198734164237976
  - 0.4403146505355835
  - 0.4306785762310028
  - 1.0353773832321167
  - 1.7983049154281616
  - 0.8876888751983643
  - 1.1415647268295288
  - 0.5952807068824768
  - 0.5519974827766418
  - 0.4733257591724396
  - 0.580213189125061
  - 0.5491722822189331
  - 0.5344586968421936
  - 0.5014103055000305
  - 0.4240323305130005
  - 0.40967535972595215
  - 0.4743078351020813
  - 0.45020046830177307
  - 0.42481759190559387
  - 0.4182853400707245
  - 0.4527074098587036
  - 1.219821810722351
  - 0.4335877597332001
  - 0.47657591104507446
  - 0.5087599754333496
  - 0.4339374303817749
  - 0.40714478492736816
  - 0.4555986523628235
  - 0.4129573404788971
  - 0.49669793248176575
  - 0.4863838851451874
  - 0.410661518573761
  - 0.3956741690635681
  - 0.40804654359817505
  - 0.4182983338832855
  - 0.39293172955513
  - 0.4056791663169861
  - 0.4043988287448883
  - 0.40216535329818726
  - 0.4042030870914459
  - 0.41010725498199463
  - 0.4131374657154083
  - 0.39666345715522766
loss_records_fold4:
  train_losses:
  - 0.8674290478229523
  - 0.8195239424705506
  - 0.8174377858638764
  - 0.8405551850795746
  - 0.8239277303218842
  - 0.85331289768219
  - 0.8314345598220826
  - 0.8148402273654938
  - 0.8279859244823456
  - 0.8582780957221985
  - 0.8550481259822846
  - 0.8407071352005006
  - 0.8388610124588013
  - 0.8231644809246064
  - 0.8474760532379151
  - 0.8063109934329987
  - 0.8015111476182938
  - 0.8241650044918061
  - 0.8626759767532349
  - 0.8462766349315644
  - 0.816116887331009
  - 0.8077802181243897
  - 0.8791643679141998
  - 0.8879247188568116
  - 1.4324266016483307
  - 1.0054021954536438
  - 0.8271926164627076
  - 0.8324515283107758
  - 0.833969807624817
  - 0.834586638212204
  - 0.8292233347892761
  - 0.832377588748932
  - 0.8699065387248993
  - 0.8820979952812196
  - 0.8719041049480438
  - 0.8588320851325989
  - 0.8049100756645203
  - 2.0767443895339968
  - 0.8931035697460175
  - 0.918881392478943
  - 0.8571107566356659
  - 1.0003992676734925
  - 0.8139159262180329
  - 0.8837080359458924
  validation_losses:
  - 0.41432687640190125
  - 0.4182695150375366
  - 0.4240477979183197
  - 0.4295268654823303
  - 0.4170664846897125
  - 0.424837589263916
  - 0.41821640729904175
  - 0.45902255177497864
  - 0.4386771321296692
  - 0.4472624957561493
  - 0.4022884666919708
  - 0.4110613465309143
  - 0.4289785623550415
  - 0.42568182945251465
  - 0.43376877903938293
  - 0.4069663882255554
  - 0.4164113998413086
  - 0.4153985381126404
  - 0.4583682119846344
  - 0.4412011206150055
  - 0.4234408140182495
  - 0.4226231575012207
  - 0.44295090436935425
  - 0.4281589090824127
  - 0.4516734778881073
  - 0.39752262830734253
  - 0.39883092045783997
  - 0.4073341488838196
  - 0.4556271731853485
  - 0.4101043939590454
  - 0.4407902956008911
  - 0.43609556555747986
  - 0.42902350425720215
  - 0.437090128660202
  - 0.47705981135368347
  - 0.4032135605812073
  - 0.42103466391563416
  - 0.44067147374153137
  - 0.4233943819999695
  - 0.4098699390888214
  - 0.4191041886806488
  - 0.4283544719219208
  - 0.3985203504562378
  - 0.4063839316368103
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 37 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 59 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 44 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:14:46.261638'
