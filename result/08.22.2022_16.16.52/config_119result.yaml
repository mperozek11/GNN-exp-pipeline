config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:58:49.713988'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_119fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_119fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_119fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_119fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_119fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_119fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_119fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_119fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_119fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_119fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.188866651058197
  - 0.9089937746524811
  - 0.9277352929115296
  - 0.9034270465373994
  - 0.9722866415977478
  - 0.8813599050045013
  - 0.8519221842288971
  - 0.8047051787376405
  - 0.8085304439067841
  - 0.8122517168521881
  - 0.8085398197174073
  - 0.8181106209754945
  validation_losses:
  - 0.4501352310180664
  - 0.5193918347358704
  - 0.3998873233795166
  - 0.4170494079589844
  - 0.39047110080718994
  - 0.42031341791152954
  - 0.3975920081138611
  - 0.3963727653026581
  - 0.4057537019252777
  - 0.4094149172306061
  - 0.41754040122032166
  - 0.39703768491744995
loss_records_fold1:
  train_losses:
  - 0.8191590249538422
  - 0.8858473658561707
  - 0.8183959364891052
  - 0.7786023139953614
  - 0.790599411725998
  - 0.8843037247657777
  - 0.7823825836181642
  - 0.853285300731659
  - 0.8949318766593933
  - 0.8777952253818513
  - 0.8100658535957337
  - 0.8750418543815613
  - 0.7733952403068542
  - 0.8728949368000031
  - 0.8412812471389771
  - 0.8265163898468018
  - 0.8786378741264343
  - 0.8351208209991455
  - 0.8372437119483949
  - 0.7831255674362183
  - 0.8224239528179169
  - 1.0477210819721223
  - 0.8515431225299835
  - 0.7967248678207398
  - 0.7974996030330659
  - 0.8119025886058808
  - 0.7858010530471802
  - 0.8483160138130188
  - 0.8093564748764038
  - 0.796664559841156
  - 0.8448548495769501
  - 0.7935774624347687
  - 0.8429172813892365
  - 0.7877797484397888
  - 0.7976221680641175
  - 0.8030179798603059
  - 0.8476998984813691
  - 0.8609367430210114
  - 0.8233149886131287
  - 0.7883049964904786
  - 0.7922018587589265
  - 0.7795597791671753
  - 0.8233497262001038
  - 0.8598092317581177
  - 0.909183019399643
  - 0.8175458252429962
  - 0.8673966228961945
  validation_losses:
  - 0.3940921723842621
  - 0.39262187480926514
  - 0.3958194851875305
  - 0.3902748227119446
  - 0.3955666124820709
  - 0.3977948725223541
  - 0.42706993222236633
  - 0.4712485671043396
  - 0.39636147022247314
  - 0.40297481417655945
  - 0.393473356962204
  - 0.3899959623813629
  - 0.610127866268158
  - 0.4398191273212433
  - 0.3934366703033447
  - 0.4145587384700775
  - 0.3945165276527405
  - 0.39799582958221436
  - 0.41014349460601807
  - 0.4066639542579651
  - 0.41317495703697205
  - 0.41205960512161255
  - 0.40279969573020935
  - 0.4163370430469513
  - 0.4107268452644348
  - 0.3918229341506958
  - 0.38670074939727783
  - 0.41058775782585144
  - 0.42718830704689026
  - 0.3927081525325775
  - 0.435890257358551
  - 0.5163102149963379
  - 0.7709943652153015
  - 0.39515843987464905
  - 0.40770086646080017
  - 0.3873235881328583
  - 0.41043147444725037
  - 0.393050879240036
  - 0.39368686079978943
  - 0.39143359661102295
  - 0.408239483833313
  - 0.3985309302806854
  - 0.40587761998176575
  - 0.3863825798034668
  - 0.39016208052635193
  - 0.3978707194328308
  - 0.38666820526123047
loss_records_fold2:
  train_losses:
  - 0.9041377246379853
  - 0.8781223177909852
  - 0.827040708065033
  - 0.8309546113014221
  - 0.8138416767120362
  - 0.9450206875801087
  - 0.8243725657463075
  - 0.807270485162735
  - 0.8684821963310242
  - 0.8128519833087922
  - 0.9412124633789063
  - 0.8794373691082001
  - 0.9149078667163849
  - 0.8609997689723969
  - 0.8022958517074585
  - 0.7958149015903473
  - 0.8732257068157196
  - 0.797701907157898
  - 0.8340687572956086
  - 0.7894130527973175
  - 0.8051931142807007
  - 0.7682648569345475
  - 0.8154251575469971
  validation_losses:
  - 0.4155033230781555
  - 0.40091565251350403
  - 0.39681652188301086
  - 0.392331600189209
  - 0.3895643949508667
  - 0.390658974647522
  - 0.3926945626735687
  - 0.38980668783187866
  - 0.40900471806526184
  - 0.388963907957077
  - 0.42575088143348694
  - 0.39057812094688416
  - 0.3874894082546234
  - 0.400253564119339
  - 0.3897028863430023
  - 0.3876262605190277
  - 0.40455150604248047
  - 0.3993954658508301
  - 0.40290042757987976
  - 0.39837008714675903
  - 0.3977164924144745
  - 0.39073729515075684
  - 0.3953350782394409
loss_records_fold3:
  train_losses:
  - 0.81153564453125
  - 0.8063540041446686
  - 0.77362300157547
  - 0.8719303488731385
  - 0.796699744462967
  - 0.8937847018241882
  - 0.9349457561969757
  - 0.9917326807975769
  - 0.8443317711353302
  - 0.8292645514011383
  - 1.1634859681129457
  - 0.8136834383010865
  - 0.7852438062429429
  - 0.8238286972045898
  - 0.805013120174408
  - 0.7819001317024231
  - 0.809061074256897
  - 0.8793002247810364
  - 0.8324410736560822
  - 0.8074189305305481
  - 0.8013211309909821
  - 0.8290794968605042
  validation_losses:
  - 0.41865721344947815
  - 0.38774868845939636
  - 0.3744760751724243
  - 0.3887476325035095
  - 0.40273603796958923
  - 0.38360756635665894
  - 0.3754279315471649
  - 0.37895822525024414
  - 0.3932572305202484
  - 0.4268622398376465
  - 0.40340015292167664
  - 0.3721427023410797
  - 0.36981743574142456
  - 0.3809526264667511
  - 0.5696601867675781
  - 1.380427360534668
  - 1.1498167514801025
  - 0.7050596475601196
  - 0.3721359372138977
  - 0.3819808065891266
  - 0.3771238923072815
  - 0.381377249956131
loss_records_fold4:
  train_losses:
  - 0.8341077983379365
  - 0.7862493336200714
  - 0.8183706760406495
  - 0.8783760428428651
  - 0.8234938800334931
  - 0.7917452096939087
  - 0.7929373145103455
  - 0.8031999468803406
  - 0.830106794834137
  - 0.7751156568527222
  - 0.8587354481220246
  - 0.7809091210365295
  - 0.7689670324325562
  - 0.7853910028934479
  - 0.7945591151714325
  - 0.8102887094020844
  - 0.8488975405693054
  - 0.8090135037899018
  - 0.8066781520843507
  - 0.7581754952669144
  - 0.8419776856899261
  - 0.781221228837967
  - 0.7840373933315278
  - 0.7958693027496339
  - 0.8223814904689789
  - 0.7752370893955232
  - 0.8069472312927246
  - 0.7779859960079194
  - 0.7839725017547607
  - 0.7937346875667572
  - 0.7833018004894257
  - 0.7503062069416047
  - 0.8191668391227722
  - 0.8473368704319001
  - 0.7805009841918946
  - 0.8350225687026978
  - 0.8165661990642548
  - 0.7796217322349549
  - 0.8028379917144776
  - 0.7931277930736542
  - 0.812145483493805
  - 0.7834791064262391
  - 0.8080377459526062
  - 0.790861040353775
  - 0.8171258032321931
  - 0.7406092464923859
  - 0.8760425508022309
  - 0.8718488395214081
  - 0.7820680499076844
  - 0.7895539581775666
  validation_losses:
  - 0.388117253780365
  - 0.3849959075450897
  - 0.3996403217315674
  - 0.433062344789505
  - 0.4271181523799896
  - 0.402782678604126
  - 0.3971913456916809
  - 0.3807927966117859
  - 0.41267868876457214
  - 0.3932678699493408
  - 0.422819048166275
  - 0.4525446593761444
  - 0.3912236988544464
  - 0.3713175058364868
  - 0.38690921664237976
  - 0.3893304467201233
  - 0.38915008306503296
  - 0.4669892191886902
  - 0.3815663158893585
  - 0.39442378282546997
  - 0.39361342787742615
  - 0.3901878595352173
  - 0.39347025752067566
  - 0.3808461129665375
  - 0.4006613790988922
  - 0.37354591488838196
  - 0.39777082204818726
  - 0.3754076659679413
  - 0.4164789021015167
  - 0.40076395869255066
  - 0.4649474620819092
  - 0.4236055016517639
  - 0.3923027515411377
  - 0.5046882033348083
  - 0.47342243790626526
  - 0.3933684825897217
  - 0.39324095845222473
  - 0.371963232755661
  - 0.4023939371109009
  - 0.378479927778244
  - 0.37770944833755493
  - 0.3778790831565857
  - 0.37848541140556335
  - 0.3900880813598633
  - 0.3847814202308655
  - 0.39157089591026306
  - 0.3903471529483795
  - 0.3771055042743683
  - 0.3735429346561432
  - 0.37674733996391296
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 47 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 50 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:13:38.447645'
