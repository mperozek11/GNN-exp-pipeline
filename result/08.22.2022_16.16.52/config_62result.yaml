config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:28:10.597881'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_62fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_62fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_62fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_62fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_62fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_62fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_62fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_62fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_62fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_62fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 38.10056929588318
  - 11.324859714508058
  - 18.19251592159271
  - 4.824929797649384
  - 3.537289702892304
  - 2.880531418323517
  - 5.311401295661927
  - 3.2465066969394685
  - 2.3900970816612244
  - 4.1590718030929565
  - 3.0254875063896183
  - 2.1656290531158446
  - 1.922325396537781
  - 1.8160239160060883
  - 1.8596548318862915
  - 1.739129650592804
  - 6.684627425670624
  - 2.8019471108913425
  - 3.537348204851151
  - 3.514246237277985
  - 3.1940190970897677
  - 5.350776720046998
  - 4.030248326063156
  - 7.259180712699891
  - 7.7699386358261116
  - 7.039790527522564
  - 6.032478713989258
  - 4.70946307182312
  - 2.9695336520671844
  - 2.442644762992859
  - 3.1898267924785615
  - 7.701057094335557
  - 5.382125771045685
  - 1.8635967195034029
  - 2.0031538248062133
  - 1.822290688753128
  - 1.976322376728058
  - 3.061208486557007
  - 1.8032885819673539
  - 1.9268731594085695
  - 3.9102130830287933
  - 1.8409162521362306
  - 4.946263641119003
  - 2.2361212015151977
  - 1.7744880378246308
  - 1.8952315211296082
  - 1.7757922768592835
  - 1.6218253493309023
  - 2.0920634269714355
  - 2.1337319672107697
  - 1.6860024988651277
  - 1.6805026710033417
  - 1.7260980904102325
  - 1.660378384590149
  - 1.7236498951911927
  - 1.886225199699402
  - 1.6420897901058198
  - 1.6826143085956575
  - 1.9621194660663606
  - 1.566642063856125
  - 1.5782532930374147
  - 2.4628355860710145
  - 2.739497423171997
  - 3.93990797996521
  - 3.410694789886475
  - 2.8762977480888368
  - 2.2783790051937105
  - 1.7725720286369324
  - 2.1558973371982577
  - 2.2687964141368866
  - 3.6796391248703006
  - 3.3590466737747193
  - 4.1275216281414036
  - 7.469727557897568
  - 2.2889534533023834
  - 3.1219184756278993
  - 2.0716024577617644
  - 1.9359465718269349
  - 1.8542873799800874
  - 1.7382887959480287
  - 2.023191899061203
  - 2.6917323529720307
  - 1.8008736431598664
  validation_losses:
  - 1.6901746988296509
  - 2.3745598793029785
  - 1.4631692171096802
  - 0.730503499507904
  - 0.6537469625473022
  - 0.8734291791915894
  - 0.7284372448921204
  - 0.6919201016426086
  - 0.7107153534889221
  - 0.7200810313224792
  - 0.5690895915031433
  - 0.40832793712615967
  - 0.4294719099998474
  - 0.40318626165390015
  - 0.48509299755096436
  - 0.4195863902568817
  - 0.5233596563339233
  - 0.5822349190711975
  - 0.4144092798233032
  - 0.6399300694465637
  - 0.9317922592163086
  - 0.9502530097961426
  - 0.656090259552002
  - 0.4371020197868347
  - 0.535528838634491
  - 0.527370274066925
  - 0.442884624004364
  - 0.7031648755073547
  - 0.4337504506111145
  - 0.40624722838401794
  - 0.4680124521255493
  - 0.43219295144081116
  - 0.4483458399772644
  - 0.4137468636035919
  - 0.3975147008895874
  - 0.4078778922557831
  - 0.49416282773017883
  - 0.43216729164123535
  - 0.4480164349079132
  - 0.4025045931339264
  - 0.4061868190765381
  - 0.3975681960582733
  - 0.4992476999759674
  - 0.45573440194129944
  - 0.4097980558872223
  - 0.40023332834243774
  - 0.3964325189590454
  - 0.4404636323451996
  - 0.4016037881374359
  - 0.4490981996059418
  - 0.423366516828537
  - 0.4090753197669983
  - 0.40546849370002747
  - 0.421992689371109
  - 0.41330814361572266
  - 0.411783903837204
  - 0.4050563871860504
  - 0.4124728739261627
  - 0.39208436012268066
  - 0.40808233618736267
  - 0.41875872015953064
  - 0.41816526651382446
  - 0.3949684500694275
  - 0.4412555694580078
  - 0.4199700355529785
  - 0.43365219235420227
  - 0.40830302238464355
  - 0.44898393750190735
  - 0.3960438370704651
  - 0.4293270707130432
  - 0.3991878926753998
  - 0.40941864252090454
  - 1.344223976135254
  - 0.5054686665534973
  - 0.39107203483581543
  - 0.4010232388973236
  - 0.42525866627693176
  - 0.39673349261283875
  - 0.4066721796989441
  - 0.4030872881412506
  - 0.3995535373687744
  - 0.4016376733779907
  - 0.402098149061203
loss_records_fold1:
  train_losses:
  - 2.307256335020065
  - 2.106268060207367
  - 2.728276187181473
  - 1.7462197184562684
  - 1.9534080982208253
  - 1.6288295686244965
  - 1.6167786061763765
  - 1.7210767507553102
  - 1.6145680963993074
  - 1.628334242105484
  - 1.6665561378002167
  - 1.8180940687656404
  - 1.6983571648597717
  - 2.0328599750995635
  - 1.6500006735324861
  - 3.0359309911727905
  - 2.4214010536670685
  - 3.5352854490280152
  - 1.7681852996349336
  - 1.7160974800586701
  - 1.632416868209839
  - 1.5944848477840425
  - 1.642220413684845
  - 1.6565054416656495
  - 2.255305361747742
  - 1.7421121954917909
  - 1.7032636523246767
  - 1.689344757795334
  - 3.6989276468753816
  - 2.0960745811462402
  - 3.6727667033672335
  - 2.498805516958237
  - 1.9820549070835114
  - 1.7264685690402986
  - 2.048248881101608
  - 1.944137305021286
  - 2.498248314857483
  - 1.5358131855726243
  - 1.6743873178958895
  - 1.5944312393665314
  - 2.5664924025535587
  - 1.673626893758774
  - 1.6024271339178087
  - 1.669487562775612
  - 2.127735102176666
  - 2.41586109995842
  - 1.9287151575088501
  - 2.3992556631565094
  - 1.877242159843445
  - 1.5583522439002992
  - 1.5231791079044343
  - 1.6816721320152284
  - 1.787219399213791
  - 1.7588355720043183
  - 1.753871437907219
  - 2.1155077874660493
  - 2.379708927869797
  - 2.287643450498581
  - 1.8279024422168733
  - 2.0032267570495605
  - 2.243616354465485
  - 1.71819766163826
  - 1.5684389412403108
  - 1.5663350343704225
  - 1.6537761211395265
  - 1.590030461549759
  - 1.867204922437668
  - 1.7535369157791139
  - 1.7518071711063385
  - 1.564000028371811
  - 1.5774696350097657
  - 1.7631005823612214
  - 2.036714828014374
  - 1.7018903374671936
  - 1.7890391856431962
  - 1.7527465760707857
  - 2.163329952955246
  - 1.589527213573456
  - 1.53969506919384
  - 1.7486724197864534
  - 1.875724858045578
  - 1.7116157472133637
  - 1.8327136158943178
  - 1.8128946125507355
  - 1.7714721083641054
  - 1.6490832328796388
  - 1.5809686720371248
  - 1.62186718583107
  - 1.641915285587311
  - 1.641413849592209
  - 1.6493263006210328
  - 1.7773435175418855
  - 1.7110592842102053
  - 1.6682359457015992
  - 1.558842593431473
  - 1.5772673308849336
  - 1.6445634067058563
  - 1.7166280627250672
  - 1.7194211602211
  - 1.630416876077652
  validation_losses:
  - 0.4681473970413208
  - 0.41464197635650635
  - 0.40633639693260193
  - 0.40964916348457336
  - 0.41170820593833923
  - 0.4077630639076233
  - 0.42972567677497864
  - 0.42641350626945496
  - 0.4660981297492981
  - 0.4155406951904297
  - 0.4113427698612213
  - 0.41227051615715027
  - 0.4241392910480499
  - 0.42433375120162964
  - 0.4215557873249054
  - 0.42493870854377747
  - 0.40696197748184204
  - 0.4125750660896301
  - 0.4375004768371582
  - 0.40627923607826233
  - 0.4144400358200073
  - 0.4301516115665436
  - 0.44306492805480957
  - 0.4135584831237793
  - 0.4348469078540802
  - 0.40611696243286133
  - 0.40100806951522827
  - 0.4398399293422699
  - 0.4657124876976013
  - 0.461933970451355
  - 0.4052782356739044
  - 0.5188292860984802
  - 0.4106750786304474
  - 0.40720656514167786
  - 0.4513738453388214
  - 0.40718093514442444
  - 0.4104311764240265
  - 0.44663527607917786
  - 0.4324665367603302
  - 0.41506659984588623
  - 0.44067832827568054
  - 0.4100804030895233
  - 0.40856772661209106
  - 0.40762418508529663
  - 0.41039973497390747
  - 0.42435726523399353
  - 0.4028637707233429
  - 0.444962739944458
  - 0.41908392310142517
  - 0.4148285388946533
  - 0.41383039951324463
  - 0.40038764476776123
  - 0.4118574559688568
  - 0.4340920150279999
  - 0.480182021856308
  - 0.41506367921829224
  - 0.4874328374862671
  - 0.40692394971847534
  - 0.4131491184234619
  - 0.418182373046875
  - 0.4124942719936371
  - 0.4093640148639679
  - 0.4412112832069397
  - 0.4204906225204468
  - 0.40730583667755127
  - 0.4304497241973877
  - 0.4025847017765045
  - 0.4070517122745514
  - 0.4021719694137573
  - 0.40864500403404236
  - 0.4555874168872833
  - 0.4179338812828064
  - 0.4230555593967438
  - 0.4136030673980713
  - 0.4294847846031189
  - 0.41499805450439453
  - 0.4237021207809448
  - 0.4090374708175659
  - 0.4064735174179077
  - 0.43675535917282104
  - 0.41289281845092773
  - 0.43979787826538086
  - 0.41348889470100403
  - 0.41418707370758057
  - 0.4082026779651642
  - 0.4085443913936615
  - 0.44828301668167114
  - 0.43939268589019775
  - 0.43079641461372375
  - 0.4052272439002991
  - 0.45552024245262146
  - 0.5350223183631897
  - 0.40889620780944824
  - 0.42322298884391785
  - 0.4111163318157196
  - 0.4142841696739197
  - 0.41656601428985596
  - 0.4192837178707123
  - 0.40946415066719055
  - 0.3955727219581604
loss_records_fold2:
  train_losses:
  - 1.6371133863925935
  - 2.494333803653717
  - 2.0516025841236116
  - 1.70790610909462
  - 1.8225713908672334
  - 2.4980185091495515
  - 1.6373864710330963
  - 1.6831295728683473
  - 1.9827891111373903
  - 1.7135952472686768
  - 2.3708326637744905
  - 1.850841212272644
  - 1.7117250978946688
  - 1.6558542788028718
  - 1.6119288504123688
  - 1.6019518315792085
  - 1.6725719690322878
  - 1.7498108744621277
  - 1.6554671823978424
  - 1.6871903359889986
  - 1.6188723921775818
  - 1.5673085153102875
  - 1.958018308877945
  - 1.5882731199264528
  - 1.6438809543848039
  - 1.6516943335533143
  - 1.564276522397995
  - 1.587764358520508
  validation_losses:
  - 0.3770734965801239
  - 0.5479254722595215
  - 0.405629962682724
  - 0.38995057344436646
  - 0.6168807744979858
  - 0.38984569907188416
  - 0.3981454074382782
  - 0.38428935408592224
  - 0.3850080668926239
  - 0.3873070180416107
  - 0.41204634308815
  - 0.3922628164291382
  - 0.4054870307445526
  - 0.3953365981578827
  - 0.40104803442955017
  - 0.3834269940853119
  - 0.4443552792072296
  - 0.4123900532722473
  - 0.42751750349998474
  - 0.3821173310279846
  - 0.3884720206260681
  - 0.40939566493034363
  - 0.38017094135284424
  - 0.38729408383369446
  - 0.3860097825527191
  - 0.3936566710472107
  - 0.3813875913619995
  - 0.39006903767585754
loss_records_fold3:
  train_losses:
  - 1.6299873352050782
  - 1.9017083108425141
  - 1.658748060464859
  - 1.5944238722324373
  - 1.66656414270401
  - 1.6270081758499146
  - 1.7829095661640169
  - 1.7615929543972015
  - 1.6243075698614122
  - 1.5959129571914674
  - 1.6015486299991608
  - 1.8182855010032655
  validation_losses:
  - 0.442094087600708
  - 0.40786734223365784
  - 0.3946285843849182
  - 0.38376325368881226
  - 0.39312484860420227
  - 0.41176044940948486
  - 0.3963533937931061
  - 0.3873273432254791
  - 0.39351919293403625
  - 0.39515793323516846
  - 0.3900468945503235
  - 0.39165282249450684
loss_records_fold4:
  train_losses:
  - 1.584127449989319
  - 1.6678234815597535
  - 1.652029550075531
  - 1.5623319387435914
  - 1.5315604746341707
  - 1.7900461852550507
  - 1.5780029237270357
  - 1.546404141187668
  - 1.623834925889969
  - 1.5843546867370606
  - 1.5846210181713105
  - 1.575058913230896
  - 1.5888980865478517
  - 1.6941876649856569
  - 1.6765629529953003
  - 1.6462797164916994
  - 1.5797673344612122
  - 1.539761370420456
  - 1.6265575766563416
  - 1.764314663410187
  - 1.6367048978805543
  - 1.5718043982982637
  - 1.6090777158737184
  - 1.6222013235092163
  - 1.6086937904357912
  validation_losses:
  - 0.4102058708667755
  - 0.4108603596687317
  - 0.39230579137802124
  - 0.3977440893650055
  - 0.3819419741630554
  - 0.3938729166984558
  - 0.388805627822876
  - 0.38296106457710266
  - 0.4306892454624176
  - 0.4084571599960327
  - 0.3852861821651459
  - 0.40884625911712646
  - 0.42965003848075867
  - 0.4195505976676941
  - 0.39494895935058594
  - 0.41609272360801697
  - 0.39116397500038147
  - 0.3929676413536072
  - 0.4104672372341156
  - 0.4028586745262146
  - 0.407433420419693
  - 0.4049570858478546
  - 0.40000516176223755
  - 0.40141627192497253
  - 0.40073561668395996
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 83 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 100 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:21:50.278805'
