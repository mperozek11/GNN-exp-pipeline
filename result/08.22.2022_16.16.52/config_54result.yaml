config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:18:28.687898'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_54fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_54fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_54fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_54fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_54fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_54fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_54fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_54fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_54fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_54fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 2.013443225622177
  - 1.606190198659897
  - 1.7313039302825928
  - 1.644734388589859
  - 1.5860370337963106
  - 1.5808979809284212
  - 1.6293466925621034
  - 1.5624903202056886
  - 1.521785482764244
  - 1.6112007975578309
  - 1.5896512508392335
  - 1.5970598995685579
  - 1.543403846025467
  - 1.5991428673267365
  - 1.539685618877411
  - 1.5479691684246064
  - 1.639487612247467
  - 1.5767522156238556
  - 1.5786793649196627
  - 1.5608714759349824
  - 1.609384459257126
  - 1.5932157516479493
  - 1.5749511182308198
  - 1.7362637579441071
  - 1.6171561062335968
  - 1.5976984173059465
  - 1.585633596777916
  - 1.628556489944458
  - 1.6121015131473542
  - 1.56306791305542
  validation_losses:
  - 0.4667862057685852
  - 0.41644203662872314
  - 0.40471717715263367
  - 0.41415050625801086
  - 0.3952814042568207
  - 0.3984033465385437
  - 0.4038504958152771
  - 0.3912339210510254
  - 0.4049651324748993
  - 0.4974832832813263
  - 0.4257142245769501
  - 0.3904542922973633
  - 0.4058539569377899
  - 0.3991738259792328
  - 0.3906438946723938
  - 0.43308117985725403
  - 0.39338603615760803
  - 0.3905218839645386
  - 0.3886358141899109
  - 0.45575571060180664
  - 0.39858004450798035
  - 0.3870442807674408
  - 0.7056141495704651
  - 1.050563097000122
  - 0.3978559374809265
  - 0.3996017873287201
  - 0.39201459288597107
  - 0.3860737383365631
  - 0.39449071884155273
  - 0.3854999244213104
loss_records_fold1:
  train_losses:
  - 1.5704796373844148
  - 1.5926435410976412
  - 1.5514535248279573
  - 1.5367485940456391
  - 1.5106259644031526
  - 1.5704714417457581
  - 1.5108448326587678
  - 1.4950196146965027
  - 1.5393711268901826
  - 1.545537966489792
  - 1.4909153103828432
  - 1.5804049015045167
  - 1.6186762988567354
  - 1.5817986726760864
  - 1.6943598210811617
  - 1.5564951777458191
  - 1.654643315076828
  - 1.6314096450805664
  - 1.540443015098572
  - 1.7104775726795198
  - 1.5853214383125307
  - 1.5376229226589204
  - 1.582046812772751
  - 1.5005500674247743
  - 1.510148310661316
  validation_losses:
  - 0.4131087064743042
  - 0.3891585171222687
  - 0.3941636383533478
  - 0.391367644071579
  - 0.4096052050590515
  - 0.4213993549346924
  - 0.40830206871032715
  - 0.39019763469696045
  - 0.41929489374160767
  - 0.40379127860069275
  - 0.6609843969345093
  - 0.4002794325351715
  - 0.39728888869285583
  - 0.3984126150608063
  - 0.40179911255836487
  - 0.5183401703834534
  - 0.4664258062839508
  - 0.3969448208808899
  - 0.4356018900871277
  - 0.41938963532447815
  - 0.41380637884140015
  - 0.39917895197868347
  - 0.39813509583473206
  - 0.4042668342590332
  - 0.38937705755233765
loss_records_fold2:
  train_losses:
  - 1.5597951412200928
  - 1.5600311219692231
  - 1.5400623440742494
  - 1.480838656425476
  - 1.4645572841167451
  - 1.5120713472366334
  - 1.536153668165207
  - 1.5594713747501374
  - 1.5198023140430452
  - 1.499549001455307
  - 1.524432134628296
  - 1.5094919502735138
  - 1.571457886695862
  - 1.4989301145076752
  - 1.5791379570961
  - 1.4871599018573762
  - 1.521913605928421
  - 1.5061166405677797
  - 1.5214871108531953
  - 1.5602902412414552
  - 1.4905170261859895
  - 1.5113803148269653
  - 1.5072977840900421
  - 1.5586555898189545
  - 1.4986088275909424
  - 1.582635271549225
  - 1.5801663517951967
  - 1.4819559276103975
  - 1.4481633007526398
  - 1.5260342597961427
  - 1.5372175693511965
  - 1.4770190060138704
  - 1.4799157917499544
  - 1.519369834661484
  - 1.5170788407325746
  - 1.5119116425514223
  - 1.4985653519630433
  - 1.5475221872329712
  - 1.4983935952186584
  - 1.517635887861252
  - 1.5067040205001831
  - 1.4894985914230348
  - 1.476643317937851
  - 1.567695063352585
  - 1.51574125289917
  - 1.5109031021595003
  - 1.475076460838318
  - 1.5133786261081696
  - 1.435646575689316
  - 1.5254893064498902
  - 1.516152447462082
  - 1.4910828053951264
  - 1.53803368806839
  - 1.4922707736492158
  - 1.5179948270320893
  - 1.5424381315708162
  - 1.4850106835365295
  - 1.5173324942588806
  - 1.517340821027756
  - 1.5268115758895875
  - 1.4975882947444916
  - 1.5178583681583406
  - 1.44563005566597
  - 1.489972472190857
  - 1.4966080129146577
  - 1.4568764686584474
  - 1.500627225637436
  - 1.4759097635746004
  - 1.510885512828827
  - 1.4545330226421358
  - 1.4702508538961412
  - 1.477889144420624
  - 1.455700969696045
  - 1.4959244012832642
  - 1.4648316442966463
  - 1.4963362097740174
  - 1.4492564022541048
  - 1.5514078080654146
  - 1.448918777704239
  - 1.5022262275218965
  - 1.5214405775070192
  - 1.534362691640854
  - 1.5011612057685852
  - 1.4475514084100725
  - 1.4982912957668306
  - 1.5408816993236543
  - 1.5025297641754152
  - 1.5106425642967225
  - 1.5269766449928284
  - 1.4719292402267456
  - 1.5743224263191224
  - 1.485394722223282
  - 1.5482049643993379
  - 1.4700708806514742
  - 1.424791294336319
  - 1.4738849103450775
  - 1.448201906681061
  - 1.4666202992200852
  - 1.4547794699668886
  - 1.489694130420685
  validation_losses:
  - 0.45546114444732666
  - 0.3916279971599579
  - 0.39168044924736023
  - 0.39755603671073914
  - 0.41482338309288025
  - 0.39725059270858765
  - 0.42589646577835083
  - 0.44814881682395935
  - 0.39923715591430664
  - 0.4061533808708191
  - 0.4728529155254364
  - 0.4209570288658142
  - 0.3879473805427551
  - 0.39414137601852417
  - 0.40328729152679443
  - 0.39661654829978943
  - 0.42744991183280945
  - 0.4079030156135559
  - 0.384691447019577
  - 0.5487573742866516
  - 0.6382982134819031
  - 0.4460597038269043
  - 0.6387783885002136
  - 0.3983819782733917
  - 0.4105820953845978
  - 0.491379976272583
  - 0.8374013900756836
  - 0.5521551370620728
  - 0.7020642161369324
  - 1.1801921129226685
  - 0.38127222657203674
  - 0.6078482270240784
  - 0.476382851600647
  - 0.7609907984733582
  - 0.41521286964416504
  - 0.7089195847511292
  - 0.5326233506202698
  - 0.7091454863548279
  - 0.42238521575927734
  - 0.39859360456466675
  - 0.4033973217010498
  - 0.389945924282074
  - 0.4660834074020386
  - 0.39152613282203674
  - 0.3835355043411255
  - 0.4001794457435608
  - 0.4316524863243103
  - 0.43731990456581116
  - 0.6874539256095886
  - 1.1655455827713013
  - 0.48630982637405396
  - 0.5781015753746033
  - 0.41063517332077026
  - 0.4646163582801819
  - 0.6553183794021606
  - 0.6112992763519287
  - 1.4913578033447266
  - 0.3923541009426117
  - 0.3799865245819092
  - 0.44631415605545044
  - 0.40044522285461426
  - 0.4376797676086426
  - 1.3691928386688232
  - 0.397024929523468
  - 0.5127575397491455
  - 0.49318575859069824
  - 0.5493317246437073
  - 0.4821321964263916
  - 0.4938598573207855
  - 0.615318238735199
  - 0.4634617269039154
  - 0.7047919034957886
  - 1.143080711364746
  - 0.8576791286468506
  - 0.38208070397377014
  - 0.7650616765022278
  - 0.633956789970398
  - 1.1653485298156738
  - 1.2486538887023926
  - 1.4513213634490967
  - 0.4043824076652527
  - 0.3896852433681488
  - 0.38931745290756226
  - 0.3995141386985779
  - 0.4107224643230438
  - 0.4292302131652832
  - 0.3887478709220886
  - 0.3901446759700775
  - 0.4047371447086334
  - 0.42082735896110535
  - 0.5007092952728271
  - 0.49985837936401367
  - 0.44575953483581543
  - 0.40064913034439087
  - 0.3849417269229889
  - 0.6187245845794678
  - 0.7155605554580688
  - 0.39306336641311646
  - 0.38699790835380554
  - 2.161820411682129
loss_records_fold3:
  train_losses:
  - 1.5990302443504334
  - 1.5244087517261506
  - 1.5953125715255738
  - 1.5245626747608185
  - 1.5217717051506043
  - 1.5062313735485078
  - 1.5094028532505037
  - 1.536561405658722
  - 1.5215040147304535
  - 1.5312447130680085
  - 1.4629032522439958
  - 1.4990210831165314
  - 1.5246027350425722
  - 1.5039059340953829
  - 1.472647339105606
  - 1.4866030335426332
  - 1.5041484236717224
  - 1.4584943234920502
  - 1.4715442061424255
  - 1.5326753854751587
  - 1.4687644481658937
  - 1.464249312877655
  - 1.510736656188965
  - 1.4754875779151917
  - 1.5056285977363588
  - 1.4942127108573915
  - 1.524810391664505
  - 1.4452445209026337
  - 1.5088363945484162
  - 1.478458207845688
  - 1.5375810027122498
  - 1.5024909496307375
  - 1.485756754875183
  - 1.5137631058692933
  - 1.4534051299095154
  - 1.49051251411438
  - 1.485147625207901
  - 1.443206536769867
  - 1.4701019704341889
  - 1.5627290964126588
  - 1.4841757357120515
  - 1.5757259249687197
  - 1.4823758363723756
  - 1.5083678781986238
  - 1.532453888654709
  - 1.5036563098430635
  - 1.4715539693832398
  - 1.493971860408783
  - 1.485027724504471
  - 1.475528919696808
  - 1.5274461627006533
  - 1.516959309577942
  - 1.4591450929641725
  - 1.4867279171943666
  - 1.4759391069412233
  - 1.4821639746427537
  - 1.5046529233455659
  - 1.5245549321174623
  - 1.4534860193729402
  - 1.4998367071151735
  - 1.5090731978416443
  - 1.495432209968567
  - 1.5161780536174776
  - 1.4639990866184236
  - 1.4312969207763673
  - 1.472480297088623
  - 1.490988487005234
  - 1.4844553470611572
  - 1.4913150072097778
  - 1.5232713341712953
  - 1.4667194008827211
  - 1.4966633737087252
  - 1.4499690115451813
  - 1.5084103643894196
  - 1.4679133832454683
  - 1.4891117036342623
  - 1.484236091375351
  - 1.479984247684479
  - 1.5062147557735444
  - 1.4453051149845124
  - 1.4547711610794067
  - 1.4552688211202622
  - 1.4889402449131013
  - 1.4970812618732454
  - 1.446193689107895
  - 1.478731620311737
  - 1.5124938011169435
  - 1.487123280763626
  - 1.4942240118980408
  - 1.4423666238784791
  - 1.4123067289590836
  - 1.52465518116951
  - 1.487506765127182
  - 1.451281523704529
  - 1.430298912525177
  - 1.4614101588726045
  - 1.462745749950409
  - 1.4278725743293763
  - 1.4403302788734438
  - 1.4551744401454927
  validation_losses:
  - 8.706724166870117
  - 0.4137202203273773
  - 0.400457501411438
  - 0.38356003165245056
  - 0.6848608255386353
  - 0.5899466276168823
  - 0.7978407144546509
  - 2.44783878326416
  - 0.45625361800193787
  - 0.5502125024795532
  - 0.5108996033668518
  - 0.4831678569316864
  - 0.7530142068862915
  - 0.957000732421875
  - 0.38971132040023804
  - 0.6289369463920593
  - 1.6788268089294434
  - 1.0800026655197144
  - 1.3185715675354004
  - 0.38059109449386597
  - 0.5193174481391907
  - 0.7259151339530945
  - 0.6846310496330261
  - 0.4836139976978302
  - 0.4998258054256439
  - 0.7003762125968933
  - 0.6895321607589722
  - 0.7398074865341187
  - 0.8913760781288147
  - 1.190625786781311
  - 0.7697383761405945
  - 0.9492080211639404
  - 1.1783777475357056
  - 0.6476337909698486
  - 0.6681651473045349
  - 0.7936644554138184
  - 16.039220809936523
  - 1.412109375
  - 0.49960994720458984
  - 0.470632404088974
  - 0.9627354741096497
  - 4.586150169372559
  - 1.6778278350830078
  - 1.4316611289978027
  - 0.6579174399375916
  - 1.0218790769577026
  - 1.236290454864502
  - 0.5688962340354919
  - 1.8440810441970825
  - 0.4680364429950714
  - 0.5515892505645752
  - 0.6012357473373413
  - 0.5648503303527832
  - 0.5563623905181885
  - 0.746790885925293
  - 0.4829791486263275
  - 0.5058172345161438
  - 0.5192411541938782
  - 0.6698517799377441
  - 0.6620094180107117
  - 0.5148177146911621
  - 0.5553966164588928
  - 0.6254945993423462
  - 1.034026861190796
  - 0.5181029438972473
  - 0.4854393005371094
  - 0.8050201535224915
  - 0.44921860098838806
  - 1.2406648397445679
  - 3.837406635284424
  - 6.663477897644043
  - 0.4873567223548889
  - 1.0381460189819336
  - 1.813785433769226
  - 1.8605685234069824
  - 0.3849581778049469
  - 0.430686891078949
  - 0.4817351698875427
  - 0.593442440032959
  - 0.3843615651130676
  - 19.152286529541016
  - 1.0182691812515259
  - 0.43547335267066956
  - 0.5597550868988037
  - 0.885887861251831
  - 0.49190548062324524
  - 0.9905931949615479
  - 1.0633344650268555
  - 1.5924301147460938
  - 0.5370573401451111
  - 0.6904934644699097
  - 0.5613616108894348
  - 0.5064197778701782
  - 0.48957985639572144
  - 0.5448992848396301
  - 0.5413478016853333
  - 0.4041026532649994
  - 0.46707192063331604
  - 0.49071136116981506
  - 0.7367338538169861
loss_records_fold4:
  train_losses:
  - 1.547456067800522
  - 1.5209254503250123
  - 1.5295364320278169
  - 1.47638538479805
  - 1.4631993412971498
  - 1.4595951080322267
  - 1.4750482499599458
  - 1.4481563568115234
  - 1.483907961845398
  - 1.4804752945899964
  - 1.475336915254593
  - 1.5106050431728364
  - 1.580478549003601
  - 1.492342007160187
  - 1.474719363451004
  - 1.4406846404075624
  - 1.4543455600738526
  - 1.429033049941063
  - 1.4913088381290436
  - 1.525066787004471
  - 1.4848527431488039
  - 1.479881912469864
  - 1.4660467565059663
  - 1.4475465208292009
  - 1.5174706280231476
  - 1.480943387746811
  - 1.4844517707824707
  - 1.4666470527648927
  - 1.5533627510070802
  - 1.4375693440437318
  - 1.479994571208954
  - 1.4571996986865998
  - 1.4794800102710726
  - 1.4401820540428163
  - 1.4358809471130372
  - 1.4360677540302278
  - 1.4388844192028047
  - 1.4444330334663391
  - 1.5037343382835389
  - 1.4986148297786714
  - 1.489286470413208
  - 1.5047649204730988
  - 1.5026345789432527
  - 1.4995726943016052
  - 1.5352569818496704
  - 1.4572965919971468
  - 1.4986766040325166
  - 1.4643664538860321
  - 1.4661396205425263
  - 1.4741010904312135
  - 1.434995412826538
  - 1.4489175200462343
  - 1.4919769167900085
  - 1.4464352905750275
  - 1.519045424461365
  - 1.4634750068187714
  - 1.4748575747013093
  - 1.4362720727920533
  - 1.4535327136516571
  - 1.4315934896469118
  - 1.420938628911972
  - 1.4363007366657259
  - 1.4847876489162446
  - 1.4302131175994874
  - 1.4208250641822815
  - 1.448671442270279
  - 1.4516054689884186
  - 1.41772121489048
  - 1.4458169043064117
  - 1.4549515008926392
  - 1.430894207954407
  - 1.4289433658123016
  - 1.444796895980835
  - 1.4777648389339448
  - 1.4370725095272066
  - 1.417349174618721
  - 1.417925500869751
  - 1.4777646601200105
  - 1.4394526541233064
  - 1.4340983629226685
  - 1.4389630258083344
  - 1.4611165583133698
  - 1.4648191690444947
  - 1.4446657299995422
  - 1.468673747777939
  - 1.4316078901290894
  - 1.3857852131128312
  - 1.442200416326523
  - 1.4460193872451783
  - 1.4456806659698487
  - 1.479994660615921
  - 1.5073975622653961
  - 1.4861863136291504
  - 1.4514741599559784
  - 1.4041856348514559
  - 1.4459982693195343
  - 1.3960040748119356
  - 1.4360064387321474
  - 1.4887215554714204
  - 1.4571881651878358
  validation_losses:
  - 0.37803760170936584
  - 0.4130651354789734
  - 0.44242051243782043
  - 0.37710756063461304
  - 0.398377388715744
  - 0.5096160173416138
  - 0.4172826409339905
  - 0.884230375289917
  - 0.4110623598098755
  - 0.46579745411872864
  - 0.504214346408844
  - 1.486314296722412
  - 0.374159038066864
  - 0.37533873319625854
  - 0.4363515079021454
  - 0.45372727513313293
  - 0.415823757648468
  - 0.6654012203216553
  - 0.5353990197181702
  - 0.4346732497215271
  - 0.39888980984687805
  - 0.48847049474716187
  - 0.49654901027679443
  - 0.7916808128356934
  - 0.610505223274231
  - 1.0609406232833862
  - 0.3689756691455841
  - 0.391887366771698
  - 0.8530580997467041
  - 0.47311633825302124
  - 0.4747856855392456
  - 0.4781751036643982
  - 0.5365228056907654
  - 0.39027243852615356
  - 0.44434481859207153
  - 0.4432678818702698
  - 0.4382336735725403
  - 0.35579347610473633
  - 0.39428749680519104
  - 0.42326802015304565
  - 0.4357964098453522
  - 0.4054255187511444
  - 0.4201674163341522
  - 0.41920915246009827
  - 0.6049821972846985
  - 0.46355515718460083
  - 0.43239909410476685
  - 0.4551853537559509
  - 0.4981241822242737
  - 0.4258821904659271
  - 0.44363877177238464
  - 0.4910017251968384
  - 0.8941681981086731
  - 0.5329626798629761
  - 0.62416672706604
  - 0.6333389282226562
  - 0.4910023510456085
  - 0.36269062757492065
  - 0.3605555593967438
  - 0.37395644187927246
  - 0.38370469212532043
  - 0.4271642565727234
  - 0.5177499651908875
  - 0.5271926522254944
  - 0.4715501666069031
  - 0.4438355565071106
  - 0.6781619191169739
  - 0.6480646133422852
  - 0.6788919568061829
  - 0.9654927253723145
  - 0.5240551233291626
  - 0.5553142428398132
  - 0.8195828795433044
  - 0.42223286628723145
  - 0.5622485280036926
  - 0.9257972240447998
  - 0.5118825435638428
  - 0.5477953553199768
  - 0.6269643306732178
  - 0.6094961166381836
  - 0.562012255191803
  - 0.7951223850250244
  - 0.5326137542724609
  - 0.45387348532676697
  - 0.6035668849945068
  - 0.5833547115325928
  - 0.7464659214019775
  - 0.4896562695503235
  - 0.6327263116836548
  - 0.624641478061676
  - 0.6547710299491882
  - 0.6089581251144409
  - 0.4496997892856598
  - 0.4996468126773834
  - 0.6107049584388733
  - 0.5554095506668091
  - 0.47960540652275085
  - 0.6151515245437622
  - 0.3640613555908203
  - 0.37218180298805237
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8250428816466552, 0.7975986277873071,
    0.8539518900343642]'
  fold_eval_f1: '[0.0, 0.0, 0.19047619047619044, 0.25316455696202533, 0.02298850574712644]'
  mean_eval_accuracy: 0.8383718531355177
  mean_f1_accuracy: 0.09332585063706846
  total_train_time: '0:31:13.739511'
