config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:47:49.786655'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_61fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_61fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_61fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_61fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_61fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_61fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_61fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_61fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_61fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_61fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 59.10547834634781
  - 17.847884118556976
  - 17.85294668674469
  - 12.361720433831216
  - 7.979035514593125
  - 8.347920846939088
  - 8.360574847459793
  - 8.45253445506096
  - 6.194490453600884
  - 5.541614866256714
  - 6.624243503808976
  - 8.2129565179348
  - 10.47062619328499
  - 7.42271198630333
  - 7.837268814444542
  - 9.05062392950058
  - 14.717935368418694
  - 11.613265585899354
  - 8.247647339105606
  - 6.043668746948242
  - 6.119914346933365
  - 8.304305464029312
  - 6.372160017490387
  - 7.347523194551468
  - 6.974167954921723
  - 5.38351301252842
  - 5.094307562708855
  - 8.027584433555603
  - 9.215609604120255
  - 6.072230929136277
  - 4.4240221053361894
  - 4.9065123260021215
  - 5.531094947457314
  - 5.518442648649216
  - 4.751957583427429
  - 4.916727900505066
  - 4.795568585395813
  - 7.675009495019913
  - 5.0475606471300125
  - 6.145033621788025
  - 5.587678688764573
  - 4.445861411094666
  - 5.453683012723923
  - 4.41204900443554
  - 3.2971088469028476
  - 4.810172921419144
  validation_losses:
  - 3.4946186542510986
  - 8.033690452575684
  - 0.9215191006660461
  - 0.4882761240005493
  - 0.5842962861061096
  - 0.42590510845184326
  - 0.44209009408950806
  - 0.7770993113517761
  - 0.4516507387161255
  - 0.5602884292602539
  - 0.5221993327140808
  - 0.5563300251960754
  - 0.4507693350315094
  - 0.48742029070854187
  - 0.48705610632896423
  - 0.43899771571159363
  - 0.7583200931549072
  - 0.4411884546279907
  - 0.6185413599014282
  - 0.9275867938995361
  - 0.5738573670387268
  - 0.4052976965904236
  - 0.4182758033275604
  - 0.4220679700374603
  - 0.4210517108440399
  - 0.5602270364761353
  - 0.4248494803905487
  - 0.6124441623687744
  - 0.4169328510761261
  - 0.39458346366882324
  - 0.5549627542495728
  - 0.40388405323028564
  - 0.3866182863712311
  - 0.5294360518455505
  - 0.5639751553535461
  - 0.5309432744979858
  - 0.42162710428237915
  - 1.1062302589416504
  - 0.5359535813331604
  - 0.6160146594047546
  - 0.40072372555732727
  - 0.40264061093330383
  - 0.40897080302238464
  - 0.4148089587688446
  - 0.40947026014328003
  - 0.3994426727294922
loss_records_fold1:
  train_losses:
  - 3.815715026855469
  - 4.397838920354843
  - 3.3387543797492985
  - 4.118814617395401
  - 5.576054835319519
  - 4.366186553239823
  - 5.352998524904251
  - 3.61011255979538
  - 4.679706621170044
  - 4.150431931018829
  - 3.623369711637497
  - 4.5556332767009735
  - 3.2925252437591555
  - 4.032344233989716
  - 3.4872199296951294
  - 3.6907171428203585
  - 3.6250613868236545
  - 3.2669648915529255
  - 3.2054256916046144
  - 3.396139979362488
  - 3.3640622138977054
  - 3.298702120780945
  - 3.651807647943497
  - 3.28298117518425
  - 3.555280953645706
  - 3.467085021734238
  - 3.630519422888756
  - 5.020295357704163
  - 3.407307761907578
  - 3.5006730616092683
  - 3.176703590154648
  - 3.4143132865428925
  - 3.551816347241402
  - 3.711378276348114
  - 3.6320783495903015
  - 3.8207668304443363
  - 3.2288343608379364
  - 3.271595484018326
  - 3.118535727262497
  - 3.348592621088028
  - 3.415774488449097
  - 3.176218658685684
  - 3.1939995467662814
  - 3.352573218941689
  - 3.266480961441994
  - 3.312458556890488
  - 3.3258693456649784
  - 3.6404920369386673
  - 3.247824788093567
  - 3.144768476486206
  - 3.123833966255188
  - 3.224305808544159
  - 3.2966764688491823
  - 3.50122729241848
  - 3.357840657234192
  - 3.1761118412017826
  - 3.284735226631165
  - 3.1489499747753147
  - 3.245710510015488
  - 3.2518861413002016
  - 3.2821002662181855
  - 3.2930528163909916
  - 3.1751939892768863
  - 3.243549877405167
  - 3.2827625513076786
  - 3.121072265505791
  - 3.1856957107782367
  - 3.3369525343179705
  - 3.3097892224788668
  - 3.135877561569214
  - 3.169082796573639
  - 3.1184542000293733
  - 3.109152483940125
  - 3.172086253762245
  - 3.124808603525162
  - 3.33128761947155
  - 3.1349380493164065
  - 3.2264542996883394
  - 3.1944400131702424
  - 3.170051693916321
  - 3.2062158524990085
  - 3.3720087647438053
  - 3.2699347138404846
  - 3.181976312398911
  - 3.310774248838425
  - 3.3035348415374757
  - 3.255078285932541
  - 3.2708233654499055
  - 3.192868322134018
  - 3.3623465180397036
  - 3.4771096944808964
  - 4.366909262537956
  - 3.264338666200638
  - 3.5299552559852603
  - 3.3319902598857882
  - 3.7357060581445696
  - 3.506183195114136
  - 4.674394279718399
  - 7.209917378425598
  - 3.800778439640999
  validation_losses:
  - 0.4802212715148926
  - 0.4218444228172302
  - 0.41318827867507935
  - 0.48619821667671204
  - 0.4200970530509949
  - 0.43861937522888184
  - 0.40309539437294006
  - 0.42204004526138306
  - 0.4450971484184265
  - 0.4093113839626312
  - 0.4377107620239258
  - 0.42585670948028564
  - 0.4174421429634094
  - 0.41922760009765625
  - 0.421383261680603
  - 0.4037218689918518
  - 0.4178408682346344
  - 0.4168141782283783
  - 0.628777801990509
  - 0.4754520058631897
  - 0.4123075604438782
  - 0.49171116948127747
  - 0.39833900332450867
  - 0.4400208294391632
  - 0.4267317056655884
  - 0.4163385331630707
  - 0.43383827805519104
  - 0.4259701669216156
  - 0.4064521789550781
  - 0.41915151476860046
  - 0.41034674644470215
  - 0.45436859130859375
  - 0.43427708745002747
  - 0.44552016258239746
  - 0.4215208888053894
  - 0.42022234201431274
  - 0.4627126157283783
  - 0.4083728790283203
  - 0.41776609420776367
  - 0.4778691530227661
  - 0.4052577614784241
  - 0.4354283809661865
  - 0.4425292909145355
  - 0.40369516611099243
  - 0.4300289750099182
  - 0.5050440430641174
  - 0.4565962553024292
  - 0.4118315875530243
  - 0.44773736596107483
  - 0.43591082096099854
  - 0.41645461320877075
  - 0.4011078476905823
  - 0.42266613245010376
  - 0.4342867136001587
  - 0.40360313653945923
  - 0.4250302016735077
  - 0.4121006429195404
  - 0.4093179404735565
  - 0.4355529546737671
  - 0.47230398654937744
  - 0.4430742561817169
  - 0.4211609661579132
  - 0.4202345609664917
  - 0.43137046694755554
  - 0.4394744336605072
  - 0.40629836916923523
  - 0.42235487699508667
  - 0.544241189956665
  - 0.43488943576812744
  - 0.40871137380599976
  - 0.42772459983825684
  - 0.4071059823036194
  - 0.4032721221446991
  - 0.4135240316390991
  - 0.42496243119239807
  - 0.421653687953949
  - 0.41573041677474976
  - 0.4142947793006897
  - 0.41412749886512756
  - 0.45181554555892944
  - 0.4226362109184265
  - 0.4532451927661896
  - 0.49045878648757935
  - 0.41443726420402527
  - 0.41436177492141724
  - 0.40874868631362915
  - 0.4433748424053192
  - 0.43294620513916016
  - 0.41842538118362427
  - 0.4246346652507782
  - 0.44124189019203186
  - 0.4128846824169159
  - 0.43886813521385193
  - 0.44781091809272766
  - 0.40423497557640076
  - 0.41013580560684204
  - 0.45261046290397644
  - 0.7658929228782654
  - 0.45474353432655334
  - 0.4229305684566498
loss_records_fold2:
  train_losses:
  - 3.3852749049663546
  - 3.3788877427577972
  - 3.764214509725571
  - 3.286737447977066
  - 3.18571894466877
  - 3.2180729269981385
  - 3.25389860868454
  - 3.207447862625122
  - 3.303347110748291
  - 3.3300454139709474
  - 3.2546864449977875
  - 3.5446310937404633
  - 3.7384953558444978
  - 3.2612834155559542
  - 3.268968176841736
  - 3.213780254125595
  - 3.249293911457062
  - 3.2627853691577915
  - 3.303106790781021
  - 3.3007489442825317
  - 3.25531359910965
  - 3.356321477890015
  - 3.252371656894684
  - 3.131217706203461
  - 3.2112333327531815
  - 3.8834055721759797
  - 4.9473712682724
  - 4.220394623279572
  - 3.7698938935995105
  - 5.099199575185776
  - 3.83031302690506
  - 3.9512097597122193
  - 3.7633404910564425
  - 3.3167403995990754
  - 3.3948371499776844
  - 3.408875972032547
  - 3.2708226978778843
  - 3.4104829490184785
  - 3.4633114814758303
  - 3.5300257802009583
  - 3.237451195716858
  - 3.2186644434928895
  - 3.2981435239315036
  - 3.150917273759842
  validation_losses:
  - 0.45120134949684143
  - 0.4010332226753235
  - 0.418186753988266
  - 0.6398614048957825
  - 1.2633222341537476
  - 1.510746717453003
  - 2.8385891914367676
  - 3.976898431777954
  - 0.4777504503726959
  - 2.337343692779541
  - 3.61419415473938
  - 0.4391842484474182
  - 617.05615234375
  - 5986203.0
  - 77728288.0
  - 0.4235720634460449
  - 0.4021368622779846
  - 0.4003012776374817
  - 9185518.0
  - 139746.5625
  - 121004232.0
  - 49312276.0
  - 686442.9375
  - 1020539904.0
  - 17970247680.0
  - 0.4188055992126465
  - 0.5583245754241943
  - 0.4458668529987335
  - 0.39582177996635437
  - 0.44940292835235596
  - 0.40473082661628723
  - 0.46481430530548096
  - 0.3876189887523651
  - 0.7376037836074829
  - 0.41481098532676697
  - 1.0193966627120972
  - 32.12998580932617
  - 87.62157440185547
  - 0.49236464500427246
  - 0.40168115496635437
  - 0.39318057894706726
  - 0.3973703384399414
  - 0.39035817980766296
  - 0.39417335391044617
loss_records_fold3:
  train_losses:
  - 3.229096293449402
  - 3.1818791508674624
  - 3.222397339344025
  - 3.2335657954216006
  - 3.34733344912529
  - 3.173439079523087
  - 3.311559581756592
  - 3.23664608001709
  - 3.195151644945145
  - 3.169592297077179
  - 3.152724671363831
  - 3.158802753686905
  - 3.249206167459488
  - 3.31963409781456
  - 3.0524131000041965
  - 3.4412637889385227
  - 3.2601399183273316
  - 3.1733910858631136
  - 3.231881755590439
  - 3.215710586309433
  - 3.0734698414802555
  - 3.214420384168625
  - 3.1950700879096985
  - 3.2362764477729797
  - 3.1486833631992344
  - 3.1296245336532595
  - 3.1616107642650606
  - 3.2360040068626406
  - 3.175761961936951
  - 3.120717698335648
  - 3.158351588249207
  - 3.118417495489121
  - 3.1507871806621552
  - 3.1593793988227845
  - 3.2045877993106844
  - 3.2747732520103456
  - 3.1238788008689884
  - 3.106559103727341
  - 3.3402673661708833
  - 3.1920553982257847
  - 3.1972023844718933
  - 3.0434839725494385
  - 3.3264311850070953
  - 3.213070380687714
  - 3.344360458850861
  - 3.1807264268398288
  - 3.2229865282773975
  - 3.2829941660165787
  - 3.2314335107803345
  - 3.2070643961429597
  - 3.211598360538483
  - 3.288305526971817
  - 3.190704494714737
  - 3.2186144649982453
  - 3.194569516181946
  - 3.080773562192917
  - 3.1821520030498505
  validation_losses:
  - 0.41740599274635315
  - 0.4201788902282715
  - 0.404077410697937
  - 0.40424180030822754
  - 0.4137052893638611
  - 0.40312591195106506
  - 0.39845263957977295
  - 0.4242289364337921
  - 0.40634146332740784
  - 0.40866294503211975
  - 0.40307676792144775
  - 0.41557976603507996
  - 0.40383195877075195
  - 0.410861074924469
  - 0.43345797061920166
  - 0.45525047183036804
  - 0.42223039269447327
  - 0.40934720635414124
  - 0.45156845450401306
  - 0.40123307704925537
  - 10.477246284484863
  - 0.4085646867752075
  - 0.421176940202713
  - 0.4371466040611267
  - 0.4096277356147766
  - 0.4224735200405121
  - 0.4375379681587219
  - 0.4168701171875
  - 0.41304710507392883
  - 0.4069327712059021
  - 0.4313242733478546
  - 0.40040087699890137
  - 0.4093901813030243
  - 0.4061598777770996
  - 0.46318602561950684
  - 0.46037396788597107
  - 0.41194644570350647
  - 0.39817893505096436
  - 0.401407927274704
  - 0.4216156303882599
  - 0.41684290766716003
  - 0.41903793811798096
  - 0.42121389508247375
  - 0.3998960554599762
  - 0.4230973720550537
  - 0.4229426980018616
  - 0.3969786763191223
  - 0.4158477187156677
  - 0.4831603169441223
  - 0.39910057187080383
  - 0.42009222507476807
  - 0.4112606644630432
  - 0.41464686393737793
  - 0.4170067608356476
  - 0.42607489228248596
  - 0.4082433879375458
  - 0.4001200497150421
loss_records_fold4:
  train_losses:
  - 3.189802879095078
  - 3.315332227945328
  - 3.1040389239788055
  - 3.1344005137681963
  - 3.1336520969867707
  - 3.1262895464897156
  - 3.170098435878754
  - 3.245860588550568
  - 3.2215437173843386
  - 3.1605870902538302
  - 3.1539839029312136
  - 3.133420443534851
  - 3.2355225801467897
  - 3.1380996227264406
  - 3.1748297154903415
  - 3.0735416620969773
  - 3.1155012726783755
  - 3.144353097677231
  - 3.1720077574253085
  - 3.139776492118836
  - 3.279387313127518
  - 3.1380019724369053
  - 3.2124646186828616
  - 3.127552551031113
  - 3.0918511271476747
  - 3.0592414796352387
  - 3.129487991333008
  - 3.2526501655578617
  - 3.1485752284526827
  - 3.179444134235382
  - 3.1140617340803147
  - 3.129427728056908
  - 3.2909210443496706
  - 3.1296562492847446
  - 3.1612277925014496
  - 3.1962682574987413
  - 3.166046464443207
  - 3.1609139472246173
  - 3.1563308238983154
  - 3.322658544778824
  - 3.099666601419449
  - 3.230048790574074
  - 3.246369278430939
  - 3.303008806705475
  - 3.2417609035968784
  - 3.230151778459549
  - 3.1768863618373873
  - 3.157195580005646
  - 3.2992038011550906
  - 3.4797070264816288
  - 3.26017427444458
  - 3.1533371746540073
  - 3.1604477703571323
  - 3.213654762506485
  - 3.3842314302921297
  - 3.222026520967484
  - 3.173087388277054
  - 3.3173088431358337
  - 3.2954922676086427
  - 3.204848730564118
  - 3.2522023856639866
  - 3.1169051826000214
  - 3.213046795129776
  - 3.1483984231948856
  - 3.1830402076244355
  - 3.160444933176041
  - 3.2140400886535647
  - 3.1200180053710938
  - 3.0619955569505692
  - 3.19393789768219
  - 3.134111535549164
  - 3.2010114014148714
  - 4.0212235212326055
  - 3.32149929702282
  - 3.3242671489715576
  - 3.221573826670647
  - 3.2519025385379794
  - 3.201439166069031
  - 3.2789074778556824
  - 3.278226548433304
  - 4.357374161481857
  - 3.6483052611351017
  - 3.443919721245766
  - 3.2558947265148164
  - 3.129126226902008
  - 3.0864208698272706
  - 3.2726160079240803
  - 3.5009222745895388
  - 3.488452184200287
  - 3.4102389931678774
  - 3.5971591353416446
  - 3.5366615056991577
  - 3.232004511356354
  - 3.2093633472919465
  - 3.2579202950000763
  - 3.278303772211075
  - 3.215082907676697
  - 3.164377743005753
  - 3.6000456511974335
  - 3.480711537599564
  validation_losses:
  - 1.0079492330551147
  - 97.1060791015625
  - 186.69081115722656
  - 301.05450439453125
  - 1529.14990234375
  - 6.746029376983643
  - 602.64013671875
  - 383.42645263671875
  - 16.91752815246582
  - 34.953208923339844
  - 70.68956756591797
  - 1283.2113037109375
  - 517.6754760742188
  - 62.9483757019043
  - 642.1138305664062
  - 81.00422668457031
  - 12.534579277038574
  - 25.085525512695312
  - 15.843219757080078
  - 342.45086669921875
  - 255.48147583007812
  - 95.79405212402344
  - 1179.4007568359375
  - 1216.3204345703125
  - 6612.58056640625
  - 33.96888732910156
  - 29.240970611572266
  - 17.102367401123047
  - 536.7841796875
  - 1986.683837890625
  - 44.767539978027344
  - 0.4106912612915039
  - 127.1227035522461
  - 6.901425838470459
  - 5.550260543823242
  - 24.19623565673828
  - 266.8119812011719
  - 481.96929931640625
  - 3.174834966659546
  - 1180.16650390625
  - 15.922194480895996
  - 51.170101165771484
  - 53.694087982177734
  - 53.36226272583008
  - 212.519775390625
  - 278.0610046386719
  - 21.18072509765625
  - 57.42572021484375
  - 80.59082794189453
  - 531.4575805664062
  - 30.997718811035156
  - 81.0964126586914
  - 507.5205078125
  - 7.892062187194824
  - 6.059554576873779
  - 7.631824493408203
  - 5.097356796264648
  - 17.52726936340332
  - 38.11212921142578
  - 44.07827377319336
  - 8.690497398376465
  - 5.172616004943848
  - 29.913780212402344
  - 201.68280029296875
  - 8.566274642944336
  - 68.04705810546875
  - 509.4635925292969
  - 4.145589351654053
  - 5.897273540496826
  - 117.8612289428711
  - 101.87576293945312
  - 568.0751342773438
  - 0.44535738229751587
  - 0.5747514367103577
  - 23.063148498535156
  - 0.4388093650341034
  - 333.259521484375
  - 948.11279296875
  - 883.1854248046875
  - 557.3827514648438
  - 0.5241340398788452
  - 0.42025694251060486
  - 0.43946728110313416
  - 155469.328125
  - 41646800.0
  - 22696334.0
  - 0.41317760944366455
  - 0.45635655522346497
  - 0.43409618735313416
  - 0.44215813279151917
  - 0.4288291931152344
  - 0.4108172655105591
  - 0.40509334206581116
  - 0.4239957928657532
  - 0.4149896204471588
  - 0.4347984194755554
  - 0.4107935130596161
  - 0.4520117938518524
  - 0.4221234917640686
  - 0.4234030246734619
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 46 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 44 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 57 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:32:56.675161'
