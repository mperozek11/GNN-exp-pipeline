config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 17:09:51.848283'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/35/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/35/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/35/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/35/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/35/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/35/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/35/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/35/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/35/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/35/fold_4_state_dict.pt
loss_records_fold2:
  train_losses:
  - 15.175171300768852
  - 15.329377263784409
  - 15.8044553399086
  - 15.18299001455307
  - 15.151232451200485
  - 15.179900169372559
  - 15.26016503572464
  - 15.21759045124054
  - 15.273976743221283
  - 15.476889699697495
  - 15.71284395456314
  - 15.057175368070602
  - 15.434272274374962
  - 15.33851608633995
  - 15.10961127281189
  - 15.11063927412033
  - 15.137183114886284
  - 15.138890862464905
  - 15.210461765527725
  - 15.062739953398705
  - 15.317348629236221
  - 15.40151883661747
  - 15.362027287483215
  - 15.191350817680359
  - 15.288353353738785
  - 15.219036385416985
  - 15.27120903134346
  - 15.104794651269913
  - 14.950590133666992
  - 15.017318606376648
  - 15.13504147529602
  - 15.255814522504807
  - 15.055448323488235
  - 15.500496655702591
  - 15.349054783582687
  - 15.166299372911453
  - 15.365011990070343
  - 15.091065913438797
  - 15.02469153702259
  - 15.031109601259232
  - 15.025724649429321
  - 14.986251682043076
  - 15.13960412144661
  - 15.167385414242744
  - 15.20562668144703
  - 15.166404038667679
  - 15.288625627756119
  - 15.375844612717628
  - 15.011697441339493
  - 15.690669864416122
  - 15.705616146326065
  - 15.197040110826492
  - 15.24902531504631
  - 15.12501648068428
  - 15.305600956082344
  - 15.241088271141052
  - 15.25932514667511
  - 15.17638909816742
  - 15.058298796415329
  - 15.278292059898376
  - 15.064869850873947
  - 15.204408198595047
  - 15.16275654733181
  - 15.18911099433899
  - 15.144212454557419
  - 15.168333262205124
  - 15.069359242916107
  - 15.13689386844635
  - 15.352839514613152
  - 15.100608676671982
  - 15.109817624092102
  - 15.248615860939026
  - 15.171965718269348
  - 15.333517163991928
  - 15.575575292110443
  - 15.17609691619873
  - 14.99755935370922
  - 15.217289954423904
  - 15.409634485840797
  - 15.114905714988708
  - 15.263327956199646
  - 15.284079924225807
  - 15.305641949176788
  - 15.299401819705963
  - 15.079350993037224
  - 15.224297106266022
  - 15.02933344244957
  - 15.645377963781357
  - 15.214197933673859
  - 15.034598633646965
  - 15.345904067158699
  - 15.268810838460922
  - 15.285185679793358
  - 15.334845036268234
  - 15.11277623474598
  - 15.209812372922897
  - 15.080904334783554
  - 15.350830405950546
  - 17.80190435051918
  - 17.870484203100204
  validation_losses:
  - 0.4180348217487335
  - 0.4073563516139984
  - 0.410942018032074
  - 54.90291213989258
  - 0.40818074345588684
  - 0.40290048718452454
  - 0.4106341600418091
  - 0.4022104740142822
  - 0.43169888854026794
  - 0.40709438920021057
  - 186.61952209472656
  - 0.41859057545661926
  - 8096.66748046875
  - 365.90863037109375
  - 0.40429723262786865
  - 307.5329284667969
  - 0.4030608534812927
  - 327.6789855957031
  - 277.92474365234375
  - 0.4282865822315216
  - 0.4048250913619995
  - 287.7291259765625
  - 0.4050387144088745
  - 0.40249380469322205
  - 0.41747671365737915
  - 0.4121718406677246
  - 0.4036314785480499
  - 0.40484896302223206
  - 182.39976501464844
  - 0.41047027707099915
  - 14993.916015625
  - 0.4074319899082184
  - 0.40648725628852844
  - 0.41475528478622437
  - 0.4038444757461548
  - 0.4022577106952667
  - 20817.111328125
  - 0.40170273184776306
  - 0.41102373600006104
  - 1084.819091796875
  - 0.4042354226112366
  - 0.4030628800392151
  - 0.41551509499549866
  - 3116.668701171875
  - 0.4097311794757843
  - 1977.8865966796875
  - 0.40099120140075684
  - 0.40137341618537903
  - 0.4005570113658905
  - 0.4035954475402832
  - 0.43190285563468933
  - 0.404483437538147
  - 0.406441330909729
  - 0.40437906980514526
  - 0.4099774658679962
  - 0.4283468425273895
  - 0.40336543321609497
  - 0.4062637388706207
  - 0.4068593978881836
  - 0.40774595737457275
  - 0.4214963912963867
  - 0.4086211621761322
  - 0.4032098650932312
  - 25594.755859375
  - 0.4053380489349365
  - 0.4000370502471924
  - 0.4039543569087982
  - 0.4175052344799042
  - 0.40449872612953186
  - 0.40119293332099915
  - 0.41168898344039917
  - 0.4025430679321289
  - 0.4083232283592224
  - 0.42931100726127625
  - 57067.37890625
  - 89413.328125
  - 90083.1171875
  - 90957.6328125
  - 95063.9453125
  - 106110.359375
  - 107115.0
  - 120486.25
  - 117133.2890625
  - 114205.1328125
  - 115739.953125
  - 114183.6171875
  - 114183.609375
  - 115250.2265625
  - 114748.1640625
  - 113890.53125
  - 113643.0625
  - 172086.484375
  - 116155.2265625
  - 117379.625
  - 115250.25
  - 117400.46875
  - 115683.375
  - 3.9306390285491943
  - 0.4087992310523987
  - 0.404873251914978
loss_records_fold4:
  train_losses:
  - 15.07489001750946
  - 15.06382605433464
  - 15.256759077310562
  - 15.226892799139023
  - 15.05051851272583
  - 15.74065537750721
  - 15.18818861246109
  - 15.634925931692123
  - 15.330926060676575
  - 15.53953555226326
  - 15.431386530399323
  - 15.153393894433975
  - 14.853538662195206
  - 14.932451456785202
  - 15.057558864355087
  - 15.190811157226562
  - 15.22509391605854
  - 15.717534214258194
  - 15.109029367566109
  - 15.108208119869232
  - 15.28473225235939
  - 15.117115706205368
  - 15.552970260381699
  - 15.22020599246025
  - 15.059314578771591
  - 15.090953886508942
  - 14.936117500066757
  - 15.154594779014587
  - 15.256293386220932
  - 15.084776759147644
  - 15.101475939154625
  - 15.378652215003967
  - 15.07866558432579
  - 15.940684258937836
  - 15.369865119457245
  - 15.10137827694416
  - 15.029322758316994
  - 15.341132134199142
  - 15.365235060453415
  - 14.966141015291214
  - 15.261829733848572
  - 15.402936846017838
  - 14.951002985239029
  - 15.084796160459518
  - 15.08682245016098
  - 15.0465607047081
  - 15.066908299922943
  - 15.092632293701172
  - 15.180685222148895
  - 15.208908438682556
  - 15.123673468828201
  - 15.249647825956345
  - 15.802551180124283
  - 15.343765676021576
  - 15.095628559589386
  - 15.032150626182556
  - 15.04105031490326
  - 15.267127647995949
  - 15.411095082759857
  - 15.319477707147598
  - 15.282899290323257
  - 15.181732177734375
  - 15.102894306182861
  - 15.266875609755516
  - 15.099936872720718
  - 15.099111586809158
  - 15.225580632686615
  - 15.178363159298897
  - 15.256994098424911
  - 15.220222264528275
  - 15.094772756099701
  - 15.437632277607918
  - 15.27008144557476
  - 15.803964450955391
  - 15.232270121574402
  - 14.966810688376427
  - 15.009536623954773
  - 15.068246304988861
  - 15.233859270811081
  - 15.212262690067291
  - 15.718922644853592
  - 14.9789067953825
  - 15.40254408121109
  - 15.486953973770142
  - 15.27450492978096
  - 15.264617800712585
  - 15.16472139954567
  - 15.072147086262703
  - 15.222768634557724
  - 14.94013200700283
  - 14.94593133032322
  - 15.047400191426277
  - 15.324532717466354
  - 15.282157748937607
  - 15.34816375374794
  - 14.981104552745819
  - 15.309692651033401
  - 15.576406329870224
  - 15.14331516623497
  - 15.302079290151596
  validation_losses:
  - 124.97189331054688
  - 8694.943359375
  - 8423.7578125
  - 15938.41796875
  - 5215.96484375
  - 2513.78515625
  - 15670.9365234375
  - 21272.724609375
  - 5014.17041015625
  - 5365.7607421875
  - 6287.02734375
  - 7405.6171875
  - 16056.5673828125
  - 13892.603515625
  - 296.4968566894531
  - 8480.869140625
  - 4371.14501953125
  - 16579.515625
  - 21731.896484375
  - 11196.4814453125
  - 8617.841796875
  - 4103.91748046875
  - 88002.765625
  - 1065.0198974609375
  - 549.4457397460938
  - 40016.76953125
  - 10443.7392578125
  - 50747.2734375
  - 77761.8671875
  - 95.30536651611328
  - 58222.765625
  - 374817.96875
  - 23396848.0
  - 814226176.0
  - 1039206528.0
  - 1031195328.0
  - 3001710080.0
  - 3279210496.0
  - 2324315136.0
  - 1046372544.0
  - 1774547072.0
  - 30392547328.0
  - 42225754112.0
  - 2487270912.0
  - 8541968896.0
  - 1424855296.0
  - 1464934912.0
  - 9383046144.0
  - 1395274880.0
  - 6233929216.0
  - 25234434048.0
  - 3474639616.0
  - 54725238784.0
  - 970095168.0
  - 6523426816.0
  - 2019280640.0
  - 976464448.0
  - 36295360512.0
  - 1270449664.0
  - 7827437056.0
  - 1323429504.0
  - 52460285952.0
  - 1582220416.0
  - 2000376320.0
  - 1684785152.0
  - 192751534080.0
  - 2849956096.0
  - 49229176832.0
  - 1790050432.0
  - 1476123264.0
  - 7690639360.0
  - 2054567552.0
  - 1365394304.0
  - 2059973120.0
  - 7256343040.0
  - 68958650368.0
  - 1228634496.0
  - 1741526528.0
  - 1500911360.0
  - 52268675072.0
  - 20643713024.0
  - 1317775104.0
  - 36588847104.0
  - 1961008768.0
  - 3371631872.0
  - 6167578624.0
  - 85317992448.0
  - 49819500544.0
  - 3394378496.0
  - 15688322048.0
  - 1228517120.0
  - 6498717696.0
  - 41441378304.0
  - 61051080704.0
  - 2521681408.0
  - 1227381120.0
  - 2992790272.0
  - 1722137856.0
  - 41870884864.0
  - 2524899328.0
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.140893470790378]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.24698795180722893]'
  mean_eval_accuracy: 0.7149711469882643
  mean_f1_accuracy: 0.04939759036144579
  total_train_time: '0:05:50.975862'
