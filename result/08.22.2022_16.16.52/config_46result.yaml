config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:07:12.596022'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_46fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_46fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_46fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_46fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_46fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_46fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_46fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_46fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_46fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_46fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 38.205324506759645
  - 12.855733960866928
  - 13.081255984306337
  - 3.3818520545959476
  - 3.242139345407486
  - 2.9895382046699526
  - 6.420283967256546
  - 3.082280170917511
  - 2.963720738887787
  - 3.1717060983181002
  - 2.5386337757110597
  - 2.2781305432319643
  - 2.2607020139694214
  - 2.20715965628624
  - 1.9413183420896531
  - 1.6666153967380524
  - 7.2592575609684
  - 3.432083371281624
  - 5.816459977626801
  - 2.8587525665760043
  - 2.3445347726345065
  - 3.6718837797641757
  - 6.391096544265747
  - 5.6563483655452735
  - 8.060764801502229
  - 7.616247247159482
  - 6.833651271462441
  - 5.033073979616166
  - 3.3126397609710696
  - 2.0525914311409
  - 2.376226651668549
  validation_losses:
  - 2.15242600440979
  - 1.8427081108093262
  - 1.2849221229553223
  - 0.597116231918335
  - 0.5595598816871643
  - 0.5548603534698486
  - 0.5217805504798889
  - 0.5601415038108826
  - 0.5308476090431213
  - 0.5884368419647217
  - 0.8162459135055542
  - 0.7219452857971191
  - 0.4896363914012909
  - 0.48368263244628906
  - 0.4016094505786896
  - 0.4340003430843353
  - 0.5898114442825317
  - 0.4539520740509033
  - 0.4030194878578186
  - 0.44649961590766907
  - 0.44568583369255066
  - 0.5533958077430725
  - 0.5073190927505493
  - 0.4571111500263214
  - 0.6568599343299866
  - 0.5713794231414795
  - 0.4785732328891754
  - 0.48574313521385193
  - 0.4556289315223694
  - 0.4080677628517151
  - 0.3880085051059723
loss_records_fold1:
  train_losses:
  - 2.259271025657654
  - 2.056198197603226
  - 1.9001933693885804
  - 1.7194959640502931
  - 1.971916174888611
  - 2.4101942241191865
  - 4.000984382629395
  - 3.087984883785248
  - 1.9031607270240785
  - 1.6786092638969423
  - 1.8308196842670441
  - 1.9066797614097597
  - 1.9130137264728546
  - 4.598919105529785
  - 6.86554383635521
  - 4.940713000297547
  - 3.1548905193805696
  - 1.8585310459136963
  - 6.562527358531952
  - 3.8327998220920563
  - 1.7564300298690796
  - 2.6216511130332947
  - 1.6672597587108613
  - 1.610334786772728
  - 1.6806746840476992
  - 1.6217557907104494
  - 1.7161541521549226
  - 1.9081091284751892
  - 2.112030464410782
  - 3.6836499989032747
  - 1.5839104890823366
  - 3.0638471066951753
  - 2.550767308473587
  - 1.8090701043605806
  - 1.6367497324943543
  - 1.659156858921051
  - 2.5233344554901125
  - 3.3706392586231235
  - 2.9558740615844727
  - 2.9684412419795994
  - 2.2934187591075896
  - 1.7692871987819672
  - 4.399774378538132
  - 1.7335168838500978
  - 1.6475245237350464
  - 2.659534546732903
  - 2.000866764783859
  - 1.6960589587688446
  - 1.6934821009635925
  - 2.541303223371506
  - 2.7052522361278535
  - 1.992798388004303
  - 3.540477502346039
  - 1.7019246518611908
  - 1.9564753472805023
  - 1.6824342012405396
  - 1.7797405540943148
  - 1.8298952400684358
  - 1.6589429616928102
  - 2.0089796483516693
  - 1.6641834020614625
  - 1.6291178643703461
  - 1.6998547911643982
  - 2.125955694913864
  - 1.7716711997985841
  - 2.0302975058555606
  - 1.7324446201324464
  - 3.072320181131363
  - 3.6402346372604373
  - 3.243201208114624
  - 1.7705725491046906
  - 1.8282846689224244
  - 1.8186665177345276
  - 1.8182420253753664
  - 1.6575103521347048
  - 1.8465294003486634
  - 2.007024401426315
  - 1.8320522129535677
  - 1.829469406604767
  - 1.7669912040233613
  - 3.3202840447425843
  - 2.509671151638031
  - 4.264828217029572
  - 2.54726437330246
  - 1.9166579186916353
  - 1.7659602522850038
  - 2.542015486955643
  - 1.9638520658016205
  - 2.6995178282260897
  - 1.5555331826210024
  - 1.7093621492385864
  - 1.6127597838640213
  - 2.893742650747299
  - 1.6482595533132554
  - 1.6711092054843903
  - 1.6671950459480287
  - 2.2164142549037935
  - 1.6844655930995942
  - 1.7421767950057985
  - 2.7839019000530243
  validation_losses:
  - 0.44228222966194153
  - 0.5019842386245728
  - 0.4370199739933014
  - 0.44205084443092346
  - 0.5153276920318604
  - 0.5146970152854919
  - 0.5719479918479919
  - 0.5004907250404358
  - 0.4345790147781372
  - 0.4184945225715637
  - 0.4518370032310486
  - 0.4264986515045166
  - 0.44615575671195984
  - 0.7407042384147644
  - 0.4902401864528656
  - 0.5609925389289856
  - 0.42556658387184143
  - 0.4100797176361084
  - 0.4056025445461273
  - 0.4937390983104706
  - 0.42764046788215637
  - 0.43357083201408386
  - 0.4079705774784088
  - 0.4068622589111328
  - 0.40480008721351624
  - 0.4184316396713257
  - 0.4181654751300812
  - 0.45447009801864624
  - 0.42737600207328796
  - 0.4138587713241577
  - 0.4359029233455658
  - 0.4224303066730499
  - 0.4394057095050812
  - 0.40335220098495483
  - 0.4571485221385956
  - 0.4314980208873749
  - 0.771888792514801
  - 0.5693905353546143
  - 0.4561922550201416
  - 0.397263765335083
  - 0.41036921739578247
  - 0.4438454806804657
  - 0.4076511859893799
  - 0.41410738229751587
  - 0.41839736700057983
  - 0.4125252068042755
  - 0.4122619032859802
  - 0.4299395978450775
  - 0.40668073296546936
  - 0.4163857102394104
  - 0.523310661315918
  - 0.426538348197937
  - 0.4261917173862457
  - 0.4161600172519684
  - 0.40403038263320923
  - 0.4240969717502594
  - 0.5552791953086853
  - 0.4209522008895874
  - 0.4559438228607178
  - 0.45797547698020935
  - 0.4419974088668823
  - 0.4255571663379669
  - 0.40715229511260986
  - 0.43414968252182007
  - 0.43346676230430603
  - 0.410419225692749
  - 0.40053102374076843
  - 0.41093504428863525
  - 0.40817561745643616
  - 0.40365350246429443
  - 0.48311102390289307
  - 0.40733209252357483
  - 0.41972699761390686
  - 0.40458640456199646
  - 0.42209839820861816
  - 0.40914762020111084
  - 0.4759977459907532
  - 0.4124774932861328
  - 0.40230926871299744
  - 0.4313884973526001
  - 0.4939543902873993
  - 0.4070335328578949
  - 0.40368130803108215
  - 0.5345671772956848
  - 0.41843360662460327
  - 0.42240655422210693
  - 0.4169420003890991
  - 0.41846373677253723
  - 0.41221290826797485
  - 0.4402118921279907
  - 0.4158482253551483
  - 0.4189607799053192
  - 0.4352370798587799
  - 0.41161608695983887
  - 0.42445865273475647
  - 0.4187850058078766
  - 0.42916980385780334
  - 0.441765159368515
  - 0.43294841051101685
  - 0.4326883852481842
loss_records_fold2:
  train_losses:
  - 1.8642525553703309
  - 2.8149969398975374
  - 1.8164362788200379
  - 1.973850053548813
  - 1.62152498960495
  - 2.076887893676758
  - 1.6615777790546418
  - 1.7097234815359117
  - 1.6586037099361421
  - 2.100739169120789
  - 1.6763511776924134
  - 1.7102300822734833
  - 1.6015108466148378
  - 1.700696873664856
  - 1.6570850849151613
  - 1.6053019404411317
  - 1.6987735748291017
  - 1.638337427377701
  - 1.5876749515533448
  - 1.5731117993593218
  - 1.5751510739326477
  - 1.5675169646739961
  - 1.5690621614456177
  - 1.7398921251296997
  validation_losses:
  - 0.3915078043937683
  - 0.3946290910243988
  - 0.7462474703788757
  - 0.4003562331199646
  - 0.39540785551071167
  - 0.41362622380256653
  - 0.40073323249816895
  - 0.3940349817276001
  - 0.40500450134277344
  - 0.3947645425796509
  - 0.41738665103912354
  - 0.3881259858608246
  - 0.39737948775291443
  - 0.39271673560142517
  - 0.42103344202041626
  - 0.40099701285362244
  - 0.38976162672042847
  - 0.4070463478565216
  - 0.3838421106338501
  - 0.39269155263900757
  - 0.3945063352584839
  - 0.3911522924900055
  - 0.39205583930015564
  - 0.4006412625312805
loss_records_fold3:
  train_losses:
  - 1.6687259554862977
  - 1.610045623779297
  - 2.068388766050339
  - 1.80588213801384
  - 1.6259988963603975
  - 2.2261150538921357
  - 1.6120344221591951
  - 1.648172062635422
  - 1.550870317220688
  - 1.5999796628952028
  - 1.8468553811311723
  - 1.6226287245750428
  - 1.6617134869098664
  - 1.8641073167324067
  - 1.63448543548584
  - 1.6003536880016327
  - 1.5848299264907837
  - 1.6649933934211731
  - 1.773275411128998
  - 1.7192824542522431
  - 1.6216522693634035
  - 1.5650589883327486
  - 2.1079637944698333
  - 1.633458822965622
  - 1.8296979367733002
  - 1.9297453284263613
  - 1.6215140402317048
  - 1.7052865743637087
  - 1.6834423303604127
  - 1.6721287548542023
  - 1.8390099823474886
  - 1.7025272607803346
  - 1.7026299834251404
  - 1.6851360559463502
  - 1.6550349712371828
  - 1.669182074069977
  - 1.6471243679523468
  - 1.5763032138347626
  - 1.6132619380950928
  - 1.6330923557281496
  - 1.6100106477737428
  - 1.6371545195579529
  - 1.696315270662308
  - 1.579393881559372
  - 1.6087788283824922
  - 1.609729939699173
  - 1.6237277030944826
  - 1.6329690873622895
  validation_losses:
  - 0.4028811454772949
  - 0.4037408232688904
  - 0.4095819294452667
  - 0.4009104371070862
  - 0.4510895609855652
  - 0.4158996343612671
  - 0.4100046157836914
  - 0.441341757774353
  - 0.3953629434108734
  - 0.4057585299015045
  - 0.4207402169704437
  - 0.40141671895980835
  - 0.41371211409568787
  - 0.411493182182312
  - 0.40017902851104736
  - 0.40371057391166687
  - 0.3917359411716461
  - 0.4227353036403656
  - 0.4101908206939697
  - 0.45307454466819763
  - 0.4226037263870239
  - 0.39397501945495605
  - 0.4086480736732483
  - 0.48172685503959656
  - 0.42385807633399963
  - 0.3898015320301056
  - 0.4046119451522827
  - 0.41910555958747864
  - 0.4354493021965027
  - 0.45898324251174927
  - 0.45850875973701477
  - 0.4598623812198639
  - 0.4059618413448334
  - 0.4074679911136627
  - 0.4260938763618469
  - 0.40044331550598145
  - 0.41194626688957214
  - 0.4414193630218506
  - 0.40337496995925903
  - 0.41085177659988403
  - 0.39503780007362366
  - 0.44070881605148315
  - 0.41378629207611084
  - 0.4018348455429077
  - 0.41007062792778015
  - 0.41547802090644836
  - 0.41082268953323364
  - 0.41448545455932617
loss_records_fold4:
  train_losses:
  - 1.6406715989112854
  - 1.6459719598293305
  - 1.6205074429512025
  - 1.58100603222847
  - 1.5949638724327089
  - 1.5892691373825074
  - 1.6177726507186891
  - 1.62009614109993
  - 1.6380633831024172
  - 1.6697226941585541
  - 2.0473771154880525
  - 1.7236936450004579
  - 1.5964835941791535
  - 1.5732098400592804
  - 1.6709923028945923
  - 1.5905328512191774
  - 1.5854951679706575
  - 1.5570318311452866
  - 1.6512658774852753
  - 1.5695440232753755
  - 1.6222472488880157
  - 1.6787737786769867
  - 1.6569456338882447
  - 1.581243494153023
  validation_losses:
  - 0.4090760350227356
  - 0.3943977952003479
  - 1.049355387687683
  - 0.486123263835907
  - 0.40051594376564026
  - 0.4021332561969757
  - 0.3978983759880066
  - 0.43393179774284363
  - 0.395298033952713
  - 0.42736122012138367
  - 0.4019383490085602
  - 0.41084784269332886
  - 0.4085038900375366
  - 0.4154313802719116
  - 0.4121353328227997
  - 0.4258420467376709
  - 0.3937017321586609
  - 0.4184442162513733
  - 0.4036116898059845
  - 0.40784379839897156
  - 0.4095028042793274
  - 0.41385340690612793
  - 0.39959776401519775
  - 0.40643081068992615
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 31 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 48 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.855917667238422, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.023255813953488372, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.004651162790697674
  total_train_time: '0:19:42.401795'
