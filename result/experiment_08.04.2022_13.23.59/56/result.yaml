config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 19:03:47.885817'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/56/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/56/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/56/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/56/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/56/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/56/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/56/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/56/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/56/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/56/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 31.993797048926353
  - 31.518703192472458
  - 32.428085178136826
  - 32.08396254479885
  - 32.750773683190346
  - 32.34743110835552
  - 37.25824911892414
  - 33.463280364871025
  - 36.0127592086792
  - 33.0756798684597
  - 31.938351020216942
  - 32.88878020644188
  - 31.814673885703087
  - 32.62496192753315
  - 34.05444584786892
  - 41.48914980888367
  - 37.65119959414005
  - 35.05210426449776
  - 38.00390683114529
  - 35.970876947045326
  - 32.28361603617668
  - 32.087323263287544
  - 33.02050422132015
  - 32.5347471088171
  - 33.55480715632439
  - 32.69562657177448
  - 33.206200778484344
  - 32.6269184499979
  - 32.25331372022629
  - 31.717729657888412
  - 34.19171544909477
  - 32.914575949311256
  - 32.23684585094452
  - 33.32899045944214
  - 33.83896125853062
  - 32.337671756744385
  - 33.122378557920456
  - 31.45081403851509
  - 32.170137003064156
  - 33.52821961045265
  - 32.884725496172905
  - 32.61274071037769
  - 33.34339366853237
  - 32.2561766654253
  - 32.09862157702446
  - 32.3135836571455
  - 32.31980109214783
  - 32.499058067798615
  - 34.11711609363556
  - 31.94276513159275
  - 32.30490881204605
  - 31.117584764957428
  - 31.85681864619255
  - 31.83280785381794
  - 32.87784603238106
  - 32.45150063931942
  - 31.403253570199013
  - 32.25543563812971
  - 32.42761446535587
  - 33.66805523633957
  - 32.16164004802704
  - 30.987466245889664
  - 32.50230900943279
  - 31.65413101017475
  - 31.22529536485672
  - 31.79666194319725
  - 31.601254850625992
  - 31.88486698269844
  - 34.07097968459129
  - 33.92529012262821
  - 32.59178125113249
  - 32.851378470659256
  - 31.63633404672146
  - 32.25622448325157
  - 32.14104983210564
  - 31.975899025797844
  - 33.06496760249138
  - 32.032503962516785
  - 33.103697538375854
  - 32.33605632185936
  - 32.141052171587944
  - 31.55602329969406
  - 32.42103746533394
  - 32.639171704649925
  - 31.946911871433258
  - 32.41916259378195
  - 32.259066328406334
  - 32.39273253083229
  - 31.999687135219574
  - 32.236092150211334
  - 32.52494305372238
  - 31.410175994038582
  - 33.17784707248211
  - 31.886692568659782
  - 33.790088802576065
  - 31.384758055210114
  - 32.359427243471146
  - 31.255747191607952
  - 32.8084622323513
  - 31.415766775608063
  validation_losses:
  - 0.4044726490974426
  - 0.41010284423828125
  - 0.4700793921947479
  - 0.419888436794281
  - 0.43848443031311035
  - 1.624874472618103
  - 0.5569847822189331
  - 0.45506954193115234
  - 0.439165860414505
  - 0.45775526762008667
  - 3.5096755027770996
  - 0.4401719570159912
  - 0.42778104543685913
  - 1.6517435312271118
  - 0.47641295194625854
  - 0.47825050354003906
  - 0.47830793261528015
  - 0.4259675145149231
  - 0.41596662998199463
  - 0.4456346035003662
  - 0.40008002519607544
  - 0.4434468448162079
  - 0.4111390709877014
  - 0.4317089915275574
  - 0.41073065996170044
  - 0.4163300395011902
  - 0.4606376886367798
  - 0.443558931350708
  - 0.40671807527542114
  - 0.4109753668308258
  - 0.4509122967720032
  - 48.01727294921875
  - 0.4761604964733124
  - 0.4274984300136566
  - 0.44104668498039246
  - 0.5264325141906738
  - 0.45643776655197144
  - 0.4719852805137634
  - 0.42370858788490295
  - 0.4091663658618927
  - 0.42329084873199463
  - 0.4665600657463074
  - 0.431980162858963
  - 0.4423059821128845
  - 0.4249267280101776
  - 0.41438040137290955
  - 0.4124109447002411
  - 0.4261464774608612
  - 0.4615130126476288
  - 0.4233134686946869
  - 0.42125293612480164
  - 0.41447392106056213
  - 0.42896565794944763
  - 0.45859065651893616
  - 0.4311541020870209
  - 0.42069700360298157
  - 0.4148680567741394
  - 0.4566950500011444
  - 0.4276466369628906
  - 0.4358425438404083
  - 0.42310014367103577
  - 0.42454051971435547
  - 0.41743597388267517
  - 0.4305412769317627
  - 0.44967636466026306
  - 0.42454808950424194
  - 0.4125816524028778
  - 0.42398327589035034
  - 0.4241560995578766
  - 0.4078803062438965
  - 0.410135954618454
  - 0.46460622549057007
  - 0.4141833782196045
  - 0.4564206004142761
  - 0.45245856046676636
  - 0.5134040713310242
  - 0.41273945569992065
  - 2920.5927734375
  - 0.43000078201293945
  - 0.418963223695755
  - 0.4200740158557892
  - 0.43342697620391846
  - 0.49520471692085266
  - 0.4101894199848175
  - 0.5412245392799377
  - 0.4296363890171051
  - 0.4059920012950897
  - 0.4248628318309784
  - 0.4826987385749817
  - 0.4471043050289154
  - 0.4250609278678894
  - 0.4490760564804077
  - 0.4161396026611328
  - 0.5247702598571777
  - 0.4211275279521942
  - 0.4701654314994812
  - 0.45723792910575867
  - 0.41561564803123474
  - 0.4454762637615204
  - 0.4198503792285919
loss_records_fold2:
  train_losses:
  - 32.711055755615234
  - 33.10091128945351
  - 31.911039769649506
  - 31.395722195506096
  - 34.430057391524315
  - 33.8778338432312
  - 32.12105202674866
  - 33.13654124736786
  - 32.50234715640545
  - 32.32524873316288
  - 32.8459467291832
  - 34.898340955376625
  - 32.83966675400734
  - 34.98335610330105
  - 33.55604586005211
  - 34.49621346592903
  - 32.33076801896095
  - 32.89330996572971
  - 32.937002286314964
  - 33.385565757751465
  - 34.94364008307457
  - 34.21734867990017
  - 72.44459202885628
  - 39.88847202062607
  - 36.26979003846645
  - 36.240486577153206
  - 38.000298991799355
  - 40.117311626672745
  - 35.934440195560455
  - 36.19297255575657
  - 33.15750153362751
  - 34.470803171396255
  - 36.9750859439373
  - 34.01928128302097
  - 35.38845653831959
  - 34.414719983935356
  - 35.1793545037508
  - 34.287867948412895
  - 33.189033180475235
  - 34.77458380162716
  - 34.21716129779816
  - 61.188231736421585
  - 33.5957770049572
  - 35.86617922782898
  - 32.628079637885094
  - 33.88800171017647
  - 32.45624968409538
  - 33.74674113094807
  - 32.57826520502567
  - 33.541850447654724
  - 32.217718958854675
  - 35.18028096854687
  - 32.88395570218563
  - 32.297305300831795
  - 32.69302140176296
  - 31.657403647899628
  - 32.13273014128208
  - 34.126133769750595
  - 33.90653398633003
  - 33.20658913254738
  - 33.362529546022415
  - 32.697383388876915
  - 31.98054663836956
  - 34.499289736151695
  - 32.97109113633633
  - 32.39345049858093
  - 34.06914521753788
  - 32.40696634352207
  - 33.56943520903587
  - 34.710182681679726
  - 31.7010490745306
  - 35.76450155675411
  - 32.739661172032356
  - 33.45091433823109
  - 34.327383294701576
  - 33.12791518121958
  - 32.520125821232796
  - 32.63406194746494
  - 32.26849699020386
  - 34.132483780384064
  - 33.297806680202484
  - 32.11601439118385
  - 32.162844106554985
  - 33.05316250026226
  - 32.977069824934006
  - 34.04829551279545
  - 32.546617940068245
  - 32.61803628504276
  - 32.56532119214535
  - 31.618718534708023
  - 34.26882620155811
  - 39.28539399802685
  - 34.45314693450928
  - 33.42373660206795
  - 34.11359402537346
  - 34.07365469634533
  - 34.88991902768612
  - 32.459071680903435
  - 32.64684051275253
  - 32.97293147444725
  validation_losses:
  - 0.41576486825942993
  - 0.3960525095462799
  - 0.4302922487258911
  - 0.46106988191604614
  - 0.42487651109695435
  - 0.4627758860588074
  - 0.43612444400787354
  - 0.39261361956596375
  - 0.40314334630966187
  - 0.40234309434890747
  - 0.43683764338493347
  - 0.45440298318862915
  - 0.40389007329940796
  - 0.40279266238212585
  - 0.42062509059906006
  - 0.47536495327949524
  - 0.40955987572669983
  - 0.39094996452331543
  - 0.4136972427368164
  - 0.40438997745513916
  - 6927.2958984375
  - 2345611264.0
  - 0.4104422330856323
  - 0.4263956844806671
  - 0.41026613116264343
  - 0.4213973581790924
  - 0.4664550721645355
  - 0.39612865447998047
  - 0.44749072194099426
  - 0.4055407643318176
  - 0.42383143305778503
  - 0.44507402181625366
  - 0.46470367908477783
  - 0.4182339608669281
  - 0.4114271104335785
  - 0.4478677213191986
  - 0.43474650382995605
  - 0.41189828515052795
  - 0.47357556223869324
  - 0.39974719285964966
  - 0.48913124203681946
  - 0.46281367540359497
  - 0.44746312499046326
  - 0.40015503764152527
  - 0.45343017578125
  - 0.3959815502166748
  - 0.4293749928474426
  - 0.41148871183395386
  - 0.43794897198677063
  - 0.4184626042842865
  - 0.397765189409256
  - 0.5123123526573181
  - 0.4679325819015503
  - 0.43454834818840027
  - 0.3988821506500244
  - 0.4241022765636444
  - 0.4278927743434906
  - 0.41310253739356995
  - 0.42976057529449463
  - 0.4370630085468292
  - 0.4589252471923828
  - 0.38815805315971375
  - 0.3947303891181946
  - 0.3893391489982605
  - 0.43253207206726074
  - 0.41544008255004883
  - 0.4131585657596588
  - 0.4036203920841217
  - 0.40680745244026184
  - 0.4220789968967438
  - 0.40017199516296387
  - 0.4109536409378052
  - 0.40853601694107056
  - 0.44459182024002075
  - 0.3957138657569885
  - 0.41186821460723877
  - 0.4055759608745575
  - 0.4219432771205902
  - 0.40344303846359253
  - 180677.6875
  - 0.3923949897289276
  - 0.39794790744781494
  - 0.38673776388168335
  - 0.40683937072753906
  - 0.4313903748989105
  - 0.4984976351261139
  - 0.3992825746536255
  - 0.4300463795661926
  - 0.39468562602996826
  - 0.4015495479106903
  - 1194.7694091796875
  - 5527.31201171875
  - 23224.048828125
  - 3999.21875
  - 2545.841796875
  - 12097.310546875
  - 9448.732421875
  - 8752.81640625
  - 2263.4697265625
  - 1840.1353759765625
loss_records_fold3:
  train_losses:
  - 33.08705843985081
  - 32.77387185394764
  - 33.69740469753742
  - 33.49439363181591
  - 31.952235341072083
  - 31.82931362092495
  - 32.79087443649769
  - 33.05420380830765
  - 32.84031003713608
  - 33.36589567363262
  - 34.777102664113045
  - 33.29322698712349
  - 32.09257309138775
  - 31.544890493154526
  - 32.900151908397675
  - 32.350234150886536
  - 35.16433575749397
  - 31.647585213184357
  - 32.239175260066986
  - 33.78004947304726
  - 33.40998540818691
  - 33.06279909610748
  - 31.11192260682583
  - 31.660215988755226
  - 33.470437437295914
  - 32.379020631313324
  - 31.882895454764366
  - 31.71261541545391
  - 31.627385154366493
  - 33.63698472082615
  - 32.12108387053013
  - 31.85173860192299
  - 32.912018194794655
  - 32.6580666154623
  - 30.993745371699333
  - 32.88234516978264
  - 32.66360159218311
  - 32.59521420300007
  - 34.13184402883053
  - 32.415436029434204
  - 32.54726521670818
  - 31.947409734129906
  - 31.43473292887211
  - 33.83466349542141
  - 32.483297631144524
  - 31.7388666421175
  - 32.20573070645332
  - 32.990022376179695
  - 32.4197558760643
  - 32.853355303406715
  - 32.81092342734337
  - 32.816869005560875
  - 32.632951810956
  - 33.041219264268875
  - 31.794622778892517
  - 31.421530202031136
  - 31.671070754528046
  - 32.69421622157097
  - 32.05303314328194
  - 32.79790687561035
  - 32.97396817803383
  - 32.544720724225044
  - 31.772079050540924
  - 31.963320910930634
  - 32.157071232795715
  - 31.812305733561516
  - 32.505952671170235
  - 32.131331980228424
  - 34.13840697705746
  - 32.50147794187069
  - 32.197930589318275
  - 32.62960462272167
  - 33.429868057370186
  - 31.972254693508148
  - 31.54490628838539
  - 31.539530158042908
  - 32.81730303168297
  - 32.85462759435177
  - 34.07241187989712
  - 32.59893247485161
  - 31.463894188404083
  - 31.88128950446844
  - 32.91159063577652
  - 33.04859745502472
  - 33.08203245699406
  - 32.45662018656731
  - 32.24622145295143
  - 33.94222815334797
  - 32.75931470096111
  - 32.815982058644295
  - 32.45217929780483
  - 31.664119392633438
  - 32.78478533029556
  - 32.09184901416302
  - 32.26329120993614
  - 31.87368568778038
  - 32.02369222044945
  - 32.19862885773182
  - 31.664635479450226
  - 31.684442058205605
  validation_losses:
  - 3467.928466796875
  - 2683.586669921875
  - 26768.396484375
  - 1998.6248779296875
  - 2607.283203125
  - 2493.2822265625
  - 4459.00537109375
  - 4749.71630859375
  - 1039.966552734375
  - 2469.57763671875
  - 24958.05078125
  - 4835.1533203125
  - 15881.3623046875
  - 9256.927734375
  - 5438.2216796875
  - 33963.0859375
  - 4810.75244140625
  - 12627.5078125
  - 3515.197509765625
  - 12058.0712890625
  - 259.0878601074219
  - 19222.025390625
  - 455.79803466796875
  - 20052.263671875
  - 154325.65625
  - 115858.59375
  - 27322.70703125
  - 417251.1875
  - 8017437.0
  - 39155.3046875
  - 168.59780883789062
  - 71628.2109375
  - 22408574.0
  - 68.87047576904297
  - 0.49725547432899475
  - 1335.3726806640625
  - 184.5139617919922
  - 10194.287109375
  - 4.116013526916504
  - 10396.0126953125
  - 4863.5087890625
  - 2021.1524658203125
  - 2814.841064453125
  - 4168.6005859375
  - 4475.01806640625
  - 508.38079833984375
  - 3564.4658203125
  - 6741.94677734375
  - 4409.44775390625
  - 5980.82763671875
  - 4740.294921875
  - 474.1191711425781
  - 3908.749755859375
  - 110.41389465332031
  - 7988.23095703125
  - 9868.4931640625
  - 10376.357421875
  - 8082.56884765625
  - 1877.4310302734375
  - 534.5433349609375
  - 366.67510986328125
  - 4612.09423828125
  - 9462.5078125
  - 11658.291015625
  - 158.9931640625
  - 337.0514221191406
  - 3913.594482421875
  - 3829.373291015625
  - 7104.115234375
  - 6629.115234375
  - 4864.4091796875
  - 14.544219017028809
  - 38.317928314208984
  - 5831.64697265625
  - 4780.38427734375
  - 6233.26171875
  - 895.7344970703125
  - 658.4281616210938
  - 6827.8349609375
  - 8044.54931640625
  - 282.1573181152344
  - 52.42844009399414
  - 1018.50390625
  - 3463.58837890625
  - 788.8862915039062
  - 6270.50537109375
  - 4076.850830078125
  - 3915.925537109375
  - 6177.68896484375
  - 560.6611938476562
  - 3280.35205078125
  - 5919.8681640625
  - 53.478736877441406
  - 5893.18017578125
  - 310.72796630859375
  - 6656.1298828125
  - 991.0804443359375
  - 1904.4832763671875
  - 6634.859375
  - 4766.73876953125
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 9 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:10:36.884238'
