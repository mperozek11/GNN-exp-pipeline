config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:25:24.858279'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_57fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_57fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_57fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_57fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_57fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_57fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_57fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_57fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_57fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_57fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 32.629959881305695
  - 12.778975647687913
  - 21.803238725662233
  - 15.465303856134415
  - 6.409741318225861
  - 7.462209832668305
  - 10.090703278779984
  - 5.491038739681244
  - 3.581042352318764
  - 4.1013186335563665
  - 3.6397614181041718
  - 6.037435060739518
  - 5.753463482856751
  - 4.4106304228305815
  - 3.6463182449340823
  - 5.789522773027421
  - 13.121774792671204
  - 10.63151143193245
  - 4.728814238309861
  - 6.0063358545303345
  - 6.209209507703782
  - 5.215897285938263
  - 4.250330835580826
  - 11.350707703828812
  - 6.5912855744361885
  - 5.675123056769372
  - 3.9859952107071877
  - 3.987303909659386
  - 5.433344674110413
  - 3.6292131364345552
  - 3.8365066796541214
  - 3.736116218566895
  - 3.7222342371940615
  - 4.086153715848923
  - 3.9393613934516907
  - 3.28819540143013
  - 3.6791894614696505
  - 3.7043101817369464
  - 3.327362036705017
  - 3.162692627310753
  - 3.9040309607982637
  - 3.5365607261657717
  - 3.2097156405448914
  - 3.062602543830872
  - 3.111320504546166
  - 5.124231618642807
  - 5.561141270399094
  - 4.133219704031944
  - 3.4517782509326937
  - 4.027699995040893
  - 2.913068524003029
  - 3.24021235704422
  - 3.497467797994614
  - 3.255226689577103
  - 3.0971666455268863
  - 3.249121436476708
  - 3.391627287864685
  - 3.0177318394184116
  - 3.1583872348070146
  - 2.8497540742158893
  - 3.097006779909134
  - 2.921360391378403
  - 3.2362616896629337
  - 3.332667291164398
  - 3.582109355926514
  - 3.160022357106209
  - 2.9107256889343263
  - 2.941441237926483
  - 2.878240704536438
  - 2.8299712240695953
  - 3.0193152219057087
  - 3.1661484062671663
  - 2.9448534071445467
  - 3.1157987445592883
  - 2.9617023259401325
  - 3.2332408100366594
  - 2.8979861587285995
  - 2.810764250159264
  - 3.0448029309511186
  - 2.935281971096993
  validation_losses:
  - 2.229825258255005
  - 1.0521670579910278
  - 2.2257025241851807
  - 0.678551435470581
  - 0.7257706522941589
  - 0.40441635251045227
  - 0.4422309100627899
  - 0.40694940090179443
  - 0.4277162551879883
  - 0.5555678606033325
  - 0.6446635723114014
  - 0.6002857089042664
  - 0.6713465452194214
  - 0.4207811951637268
  - 0.49260884523391724
  - 0.4519125819206238
  - 0.4977455735206604
  - 0.388662725687027
  - 0.5780045390129089
  - 0.5333289504051208
  - 0.4541952610015869
  - 0.4721403419971466
  - 0.48941951990127563
  - 0.5410958528518677
  - 0.858415424823761
  - 0.3917232155799866
  - 0.3923434317111969
  - 0.5892745852470398
  - 0.400770366191864
  - 0.4169090688228607
  - 0.40060609579086304
  - 0.4168975055217743
  - 0.4061696529388428
  - 0.39256781339645386
  - 0.4473227560520172
  - 0.37698081135749817
  - 0.39173194766044617
  - 0.4060019552707672
  - 0.432521253824234
  - 0.38053709268569946
  - 0.43775123357772827
  - 0.4216362535953522
  - 0.3970881998538971
  - 0.3983016312122345
  - 0.43352144956588745
  - 0.384132981300354
  - 0.4385944604873657
  - 0.4881417751312256
  - 0.3913881480693817
  - 0.40438172221183777
  - 0.4973606467247009
  - 0.38037723302841187
  - 0.4443206787109375
  - 0.3940371572971344
  - 0.37727323174476624
  - 0.44084903597831726
  - 0.42066189646720886
  - 0.38312360644340515
  - 0.43649575114250183
  - 0.38340267539024353
  - 0.37330490350723267
  - 0.41079646348953247
  - 0.38250496983528137
  - 0.4033419191837311
  - 0.4227343797683716
  - 0.37843191623687744
  - 0.4232209026813507
  - 0.3837129473686218
  - 0.3960670530796051
  - 0.3829312324523926
  - 0.3804340958595276
  - 0.40086042881011963
  - 0.3811154067516327
  - 0.40656670928001404
  - 0.3800519108772278
  - 0.38630756735801697
  - 0.37711477279663086
  - 0.3824390172958374
  - 0.3799043595790863
  - 0.3897867798805237
loss_records_fold1:
  train_losses:
  - 2.890905970335007
  - 2.9008505254983903
  - 2.9130421191453935
  - 3.0453410625457766
  - 2.844144394993782
  - 2.955409413576126
  - 3.0726080328226093
  - 2.888816493749619
  - 2.838976258039475
  - 2.957228845357895
  - 2.9292745739221573
  - 2.8025515109300616
  - 2.7470827877521518
  - 3.285908028483391
  - 2.9194683074951175
  - 2.799357879161835
  - 2.8045033097267154
  - 3.0723381102085114
  - 3.224170976877213
  - 3.47803415954113
  - 2.86386231482029
  - 2.9468453943729402
  - 2.8508267879486087
  - 2.789173957705498
  - 2.8358090937137606
  - 2.877912411093712
  - 2.834825071692467
  - 2.8978536754846576
  - 2.781355455517769
  - 2.869077029824257
  - 3.116916662454605
  - 3.017171922326088
  - 2.8209109872579576
  - 2.879365947842598
  - 2.9511285781860352
  - 2.9950596630573276
  - 2.8583224654197696
  - 2.9062383472919464
  - 2.938822734355927
  - 2.918239670991898
  - 2.7814306348562243
  - 2.8849714249372482
  - 2.803790917992592
  - 2.8577890485525135
  - 2.7726803332567216
  - 2.787022814154625
  - 2.777905911207199
  - 2.89063361287117
  - 2.8151835024356844
  - 2.8028732478618625
  - 2.7775946199893955
  - 2.8329730331897736
  - 2.813357871770859
  - 3.0534821629524234
  - 3.0291167885065082
  - 2.948790103197098
  - 3.3703809410333636
  - 2.8571365207433703
  - 2.8512759000062946
  - 2.931437548995018
  - 2.8711197912693027
  - 2.8920113801956178
  - 2.8638296961784366
  - 3.038232770562172
  - 2.854462602734566
  - 2.918577173352242
  - 2.929604470729828
  - 2.757145196199417
  - 2.9168675482273105
  - 2.807052826881409
  - 2.8391188383102417
  - 2.843127200007439
  - 2.9004897087812425
  - 2.8944467991590503
  - 2.851587924361229
  - 2.804702651500702
  - 2.8606390714645387
  - 2.8692959606647492
  validation_losses:
  - 0.38937827944755554
  - 0.40883955359458923
  - 0.40225136280059814
  - 0.43564075231552124
  - 0.42964810132980347
  - 0.40377601981163025
  - 0.41439181566238403
  - 0.39222201704978943
  - 0.41733306646347046
  - 0.388849139213562
  - 0.4135969877243042
  - 0.40007463097572327
  - 0.4280281066894531
  - 0.40665268898010254
  - 0.41367822885513306
  - 0.3929055631160736
  - 0.41666778922080994
  - 0.402612566947937
  - 0.39401301741600037
  - 0.39310184121131897
  - 0.40455204248428345
  - 0.42181479930877686
  - 0.39165592193603516
  - 0.40240174531936646
  - 0.3941512703895569
  - 0.42233705520629883
  - 0.3983768820762634
  - 0.39207687973976135
  - 0.407505065202713
  - 0.41244110465049744
  - 0.40322670340538025
  - 0.45802101492881775
  - 0.4207044541835785
  - 0.40640830993652344
  - 0.40532246232032776
  - 0.41415348649024963
  - 0.4059351086616516
  - 0.4603278338909149
  - 0.4439910054206848
  - 0.43844011425971985
  - 0.741672933101654
  - 0.4104936420917511
  - 1.371018886566162
  - 4.978832721710205
  - 3.191896915435791
  - 10.68136215209961
  - 0.46540355682373047
  - 0.8225808143615723
  - 2.2334001064300537
  - 3.86139178276062
  - 0.4709833562374115
  - 1.0994075536727905
  - 0.6108525991439819
  - 0.3986070454120636
  - 0.46254315972328186
  - 0.4491213858127594
  - 0.4100070595741272
  - 0.3970569968223572
  - 0.43837836384773254
  - 0.4760833978652954
  - 0.3982090950012207
  - 0.39229872822761536
  - 0.41789934039115906
  - 0.4837702512741089
  - 0.4728791117668152
  - 0.5283395051956177
  - 0.3942492604255676
  - 0.401794970035553
  - 0.46736273169517517
  - 0.39550402760505676
  - 0.39234787225723267
  - 0.45328593254089355
  - 0.4377445578575134
  - 0.3926050662994385
  - 0.3999062180519104
  - 0.39550337195396423
  - 0.39624297618865967
  - 0.3918763995170593
loss_records_fold2:
  train_losses:
  - 3.0293606221675873
  - 3.03492144048214
  - 3.010263001918793
  - 2.896289736032486
  - 2.9707487285137177
  - 3.082137829065323
  - 3.0861863255500794
  - 2.857531809806824
  - 2.8434628546237946
  - 2.9040683031082155
  - 2.850080078840256
  - 2.9202738761901856
  - 2.9693884909152986
  - 2.9364215910434726
  - 2.9277963161468508
  - 2.9170886218547825
  - 2.8630463957786563
  - 2.8869723647832872
  - 2.8850127458572388
  - 2.9805680871009828
  - 2.8479037761688235
  - 2.8623042494058613
  - 2.870990762114525
  - 2.794405084848404
  - 2.922159492969513
  - 2.8892990797758102
  validation_losses:
  - 0.4969227612018585
  - 0.43042895197868347
  - 0.36988407373428345
  - 0.408860445022583
  - 0.3684237003326416
  - 0.37699946761131287
  - 0.36775609850883484
  - 0.3707568049430847
  - 0.4099530577659607
  - 0.37038561701774597
  - 0.3771606683731079
  - 0.37932640314102173
  - 0.3683435618877411
  - 0.4415360391139984
  - 0.37940332293510437
  - 0.37761446833610535
  - 0.3980564475059509
  - 0.3724195957183838
  - 0.37354856729507446
  - 0.38980337977409363
  - 0.37366026639938354
  - 0.36748120188713074
  - 0.3668777346611023
  - 0.36639463901519775
  - 0.37247323989868164
  - 0.3668334484100342
loss_records_fold3:
  train_losses:
  - 2.8169035196304324
  - 2.9431948572397233
  - 3.2224649488925934
  - 2.8902246057987213
  - 2.8858767211437226
  - 2.9074286043643953
  - 2.83580579161644
  - 2.8420194655656816
  - 2.842160314321518
  - 2.8085249692201617
  - 2.8902053862810138
  - 2.911141276359558
  - 2.9367509096860887
  - 2.990051692724228
  - 2.833440113067627
  - 2.94062756896019
  - 2.829518947005272
  - 2.8974672794342045
  - 2.9059084832668307
  - 2.8475796401500704
  - 2.814106971025467
  - 3.0380122631788256
  - 2.8785833895206454
  - 2.8351726263761523
  - 2.801238024234772
  - 2.913671073317528
  - 2.908725243806839
  - 2.9041071325540546
  validation_losses:
  - 0.39844053983688354
  - 0.4712405204772949
  - 0.38391825556755066
  - 0.3744984567165375
  - 0.41974499821662903
  - 0.3868273198604584
  - 0.3726217448711395
  - 0.3903716504573822
  - 0.37177085876464844
  - 0.37153807282447815
  - 0.5850725769996643
  - 0.3808111846446991
  - 0.46837055683135986
  - 0.42760491371154785
  - 0.3729342818260193
  - 0.37500816583633423
  - 0.37465593218803406
  - 0.3845389485359192
  - 0.399213969707489
  - 0.3764367699623108
  - 0.39236846566200256
  - 0.41529572010040283
  - 0.38441628217697144
  - 0.3825262188911438
  - 0.3845830261707306
  - 0.3767980933189392
  - 0.3738877773284912
  - 0.37438449263572693
loss_records_fold4:
  train_losses:
  - 2.890088760852814
  - 2.820862826704979
  - 2.885669076442719
  - 2.8666771352291107
  - 2.816775393486023
  - 2.8673580646514893
  - 2.925251412391663
  - 2.9071666538715366
  - 2.8136510044336323
  - 2.9186438620090485
  - 2.88761203289032
  - 2.9411096930503846
  - 3.283171290159226
  - 2.950852203369141
  - 2.9518939554691315
  - 2.91516532599926
  - 3.0392080485820774
  - 2.964702120423317
  - 2.8095932424068453
  - 2.859213960170746
  - 2.862333670258522
  - 2.816790610551834
  - 2.790650948882103
  - 2.8796206295490268
  validation_losses:
  - 0.3748062252998352
  - 0.38903144001960754
  - 0.3753402829170227
  - 0.37736108899116516
  - 0.8333635330200195
  - 0.39871945977211
  - 0.39643093943595886
  - 0.41079404950141907
  - 1.1060383319854736
  - 1.0022423267364502
  - 0.443442165851593
  - 0.37765082716941833
  - 0.4948534369468689
  - 0.4130416214466095
  - 0.39299336075782776
  - 0.3808309733867645
  - 0.3747536540031433
  - 0.4160284101963043
  - 0.39892372488975525
  - 0.39206477999687195
  - 0.3728356957435608
  - 0.3731369376182556
  - 0.3736765384674072
  - 0.37657225131988525
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 80 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 78 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:22:27.985757'
