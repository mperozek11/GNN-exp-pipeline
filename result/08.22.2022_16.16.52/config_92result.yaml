config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:14:18.108594'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_92fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_92fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_92fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_92fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_92fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_92fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_92fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_92fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_92fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_92fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 86.04034616947175
  - 45.21062522009015
  - 28.87429724186659
  - 33.74547879099846
  - 37.10920738428831
  - 31.50469411164522
  - 20.25100373029709
  - 22.15076420903206
  - 24.734664273262027
  - 28.00825691521168
  - 12.961919486522675
  - 16.61904650181532
  - 14.341694831848145
  - 11.171794718503953
  - 12.125756719708443
  - 9.976027357578278
  - 10.141757819056512
  - 8.29578592479229
  - 7.807338654994965
  - 7.302863705158234
  - 7.168449208140373
  - 6.94869673550129
  - 7.128944486379623
  - 7.241546306014062
  - 7.249387270212174
  - 7.159412622451782
  - 7.602518665790559
  - 7.660491412878037
  - 7.338380843400955
  - 6.999117442965508
  - 6.999673014879227
  - 6.564508572220802
  - 6.729432705044747
  - 6.366978964209557
  - 7.002434092760087
  - 7.0841972708702095
  - 6.938105180859566
  - 6.538411200046539
  - 6.692833364009857
  - 7.129408279061318
  - 6.529069775342942
  - 6.355508267879486
  - 6.4731915473937995
  - 6.728397765755654
  - 6.425793161988259
  - 6.92737030684948
  - 6.680929508805275
  - 6.545099893212319
  - 6.542708724737167
  - 6.255459341406823
  - 6.326187855005265
  - 6.412835815548897
  - 6.566016522049904
  - 6.451512649655342
  - 6.731607805192471
  - 6.368492671847344
  - 6.586464846134186
  - 8.305462640523912
  - 8.282650220394135
  - 8.205874446034432
  - 7.381807240843774
  - 7.962813210487366
  - 7.372833919525147
  - 6.904568678140641
  - 6.611624473333359
  - 6.384747648239136
  - 6.448381052911282
  - 6.2576468825340275
  - 6.483174586296082
  - 6.848336192965508
  - 6.375704970955849
  - 6.480672645568848
  - 6.401735490560532
  - 6.462679865956307
  - 6.649390870332718
  - 6.954605403542519
  - 6.507825636863709
  - 6.414231097698212
  - 6.581809231638909
  - 7.216394865512848
  - 7.863982617855072
  - 6.565243265032769
  - 6.270513904094696
  - 6.543537268042565
  - 6.675069132447243
  - 6.5450117826461796
  - 6.37688576579094
  - 6.6053941518068315
  - 6.361135765910149
  - 6.429293069243432
  - 6.2631820976734165
  - 6.402652400732041
  - 6.932554212212563
  - 6.728811019659043
  - 6.424231261014938
  - 6.875982919335366
  - 6.508292016386986
  - 6.699194890260697
  - 6.595567020773888
  - 6.416683766245843
  validation_losses:
  - 1.5663998126983643
  - 0.87851881980896
  - 0.5833830833435059
  - 0.6677740216255188
  - 3.7933080196380615
  - 0.45966240763664246
  - 0.507336437702179
  - 0.5387489199638367
  - 0.4707622826099396
  - 0.4787045121192932
  - 0.5805392265319824
  - 0.5475319623947144
  - 0.43109363317489624
  - 0.3962285816669464
  - 0.41875532269477844
  - 0.4425355792045593
  - 0.42936867475509644
  - 0.47915220260620117
  - 0.49116384983062744
  - 0.40953320264816284
  - 0.40590429306030273
  - 0.4258555471897125
  - 0.4977801442146301
  - 0.4272570312023163
  - 0.4463246762752533
  - 0.46496066451072693
  - 0.4115513861179352
  - 0.41303232312202454
  - 0.7922142744064331
  - 0.5076195597648621
  - 0.4547758996486664
  - 0.40350133180618286
  - 0.43214845657348633
  - 0.4332634508609772
  - 0.461148738861084
  - 0.4480716586112976
  - 0.4388570189476013
  - 0.4083375632762909
  - 0.44393813610076904
  - 0.428433895111084
  - 0.41224411129951477
  - 0.40659376978874207
  - 0.42412081360816956
  - 0.4146179258823395
  - 0.4161563515663147
  - 0.43934008479118347
  - 0.4523215591907501
  - 0.40931087732315063
  - 0.42286816239356995
  - 0.41384443640708923
  - 0.40923529863357544
  - 0.45764192938804626
  - 0.40865954756736755
  - 0.42431750893592834
  - 0.42387303709983826
  - 0.39882057905197144
  - 0.43753859400749207
  - 0.41723477840423584
  - 5.158566474914551
  - 0.41094064712524414
  - 0.43232253193855286
  - 0.41660076379776
  - 0.42723730206489563
  - 0.45115983486175537
  - 0.4360368549823761
  - 0.4300808310508728
  - 0.4112772047519684
  - 0.4014866054058075
  - 0.45135530829429626
  - 0.44097471237182617
  - 0.4514864981174469
  - 0.4164122939109802
  - 0.4330250322818756
  - 0.42350661754608154
  - 0.6091720461845398
  - 942.5409545898438
  - 4386.07470703125
  - 0.41652408242225647
  - 0.42089733481407166
  - 0.4338066279888153
  - 0.4027370810508728
  - 0.4129604399204254
  - 0.4195009768009186
  - 0.40750038623809814
  - 0.46407437324523926
  - 0.4476208984851837
  - 0.4122268557548523
  - 0.41301506757736206
  - 0.4178174138069153
  - 0.43964090943336487
  - 0.4608902335166931
  - 0.48429161310195923
  - 0.7010747194290161
  - 0.4064907729625702
  - 0.41425812244415283
  - 24.38265037536621
  - 118.3187484741211
  - 0.432738333940506
  - 0.41137826442718506
  - 0.40436527132987976
loss_records_fold1:
  train_losses:
  - 6.51671248972416
  - 6.426234313845635
  - 6.2477234482765205
  - 6.4230157226324085
  - 6.249211063981057
  - 6.3815933734178545
  - 6.441428098082543
  - 6.21196181178093
  - 6.241205811500549
  - 6.754994535446167
  - 6.409138929843903
  - 6.300056573748589
  - 6.36297370493412
  - 6.420433560013771
  - 6.6808542072772985
  - 6.401842856407166
  - 6.56574050784111
  - 6.913687065243721
  - 6.379463648796082
  - 6.4638888329267505
  - 6.410602098703385
  - 6.394397887587548
  - 6.420080012083054
  - 6.447993531823158
  - 6.4784483253955845
  - 6.277902698516846
  - 6.205825859308243
  - 6.55975793004036
  - 6.5991107523441315
  - 6.406169348955155
  - 6.384904560446739
  - 6.293809777498246
  - 6.310089468955994
  - 6.623644223809243
  - 6.444887632131577
  - 6.439491203427315
  - 6.664254096150398
  - 6.47215650677681
  - 6.4293090313673025
  - 6.460885545611382
  - 6.477338469028473
  validation_losses:
  - 0.4472507834434509
  - 0.42045095562934875
  - 0.45410680770874023
  - 0.4177058935165405
  - 0.4091169536113739
  - 0.4369736313819885
  - 0.4208868741989136
  - 0.4250189960002899
  - 0.43436723947525024
  - 0.4364595413208008
  - 0.42819681763648987
  - 0.4615316390991211
  - 0.4193022847175598
  - 0.4248445928096771
  - 0.4484374523162842
  - 10.104745864868164
  - 0.47576603293418884
  - 0.4325440526008606
  - 0.4473511576652527
  - 0.41376402974128723
  - 0.47627660632133484
  - 51233.32421875
  - 145841.21875
  - 0.501590371131897
  - 0.4375455379486084
  - 0.41745907068252563
  - 0.4058936536312103
  - 0.4766576886177063
  - 0.462868332862854
  - 0.4745522141456604
  - 0.41524600982666016
  - 0.44232285022735596
  - 0.41860392689704895
  - 0.4407634139060974
  - 0.49003422260284424
  - 0.4928727149963379
  - 0.42051634192466736
  - 0.4276108741760254
  - 0.43651461601257324
  - 0.43359115719795227
  - 0.4146503508090973
loss_records_fold2:
  train_losses:
  - 6.9482574462890625
  - 6.524457907676697
  - 6.6280215919017795
  - 6.596327167749405
  - 7.891184794902802
  - 8.288749089837074
  - 13.548357346653939
  - 8.502327099442482
  - 8.204989087581636
  - 7.093497577309609
  - 7.285498797893524
  - 6.572877216339112
  - 6.596517276763916
  - 6.416944438219071
  - 6.662190112471581
  - 6.408645310997963
  - 6.637322559952736
  - 6.613951954245568
  - 6.744381645321846
  - 6.454284939169884
  - 7.215850725769997
  - 6.601711145043374
  - 6.412592196464539
  - 6.589266732335091
  - 6.4717310756444935
  - 6.290801823139191
  - 6.527601230144501
  - 6.414456230401993
  - 6.64985354244709
  - 6.625491455197334
  - 6.87582175731659
  - 6.568506145477295
  - 6.656490477919579
  - 6.8881265997886665
  - 6.337740594148636
  - 6.488365855813027
  - 6.452758926153184
  - 6.3137647151947025
  - 6.393197613954545
  - 6.318697792291641
  - 6.565527588129044
  - 6.545730102062226
  - 6.9808013290166855
  - 6.986169821023942
  - 6.500692290067673
  - 6.765004470944405
  - 6.48397168815136
  - 6.387195992469788
  - 6.391880041360856
  - 6.416528016328812
  - 6.378005510568619
  - 6.294285818934441
  - 6.518924537301064
  - 6.685475000739098
  - 7.342000949382783
  - 6.819354525208474
  - 6.311547499895096
  - 6.459029322862626
  - 6.525262662768364
  - 6.6893484681844715
  - 6.630890488624573
  - 6.7572025001049045
  - 6.505756855010986
  - 6.378822752833367
  - 6.4759507268667225
  - 6.615203216671944
  - 6.5746071338653564
  - 6.88736935555935
  - 6.573548564314843
  - 6.579500979185105
  - 6.272040176391602
  - 6.548925676941872
  - 6.644259378314018
  - 6.621149563789368
  - 6.749498412013054
  - 6.654121321439743
  - 6.428463926911355
  - 6.580746749043465
  - 6.301831662654877
  - 6.829434135556221
  - 6.9908828511834145
  - 7.357323630154133
  - 6.444917145371438
  - 6.356240123510361
  - 6.614741492271424
  - 6.724612912535668
  - 6.572502839565278
  - 6.406141290068627
  - 6.465885782241822
  - 6.583736765384675
  - 6.343916106224061
  - 6.287999925017357
  - 6.620690798759461
  - 6.6027960062026985
  - 6.522643601894379
  - 6.516213557124138
  - 6.551868790388108
  - 6.797740668058395
  - 6.467553725838662
  - 6.269719770550728
  validation_losses:
  - 0.40759846568107605
  - 0.4328099489212036
  - 0.4359601140022278
  - 0.4478780925273895
  - 1.5627905130386353
  - 0.42474183440208435
  - 0.41325053572654724
  - 0.46191972494125366
  - 0.5774955153465271
  - 0.4017161428928375
  - 0.41894757747650146
  - 0.41358956694602966
  - 0.410405695438385
  - 0.47537854313850403
  - 0.4530141353607178
  - 0.39129915833473206
  - 0.45326340198516846
  - 0.464557945728302
  - 0.40237680077552795
  - 0.4036500155925751
  - 0.39724767208099365
  - 0.410365492105484
  - 0.43331676721572876
  - 0.3948728144168854
  - 0.4119807183742523
  - 0.4101005792617798
  - 0.393087238073349
  - 0.40783169865608215
  - 0.41026735305786133
  - 12980.0830078125
  - 12362312.0
  - 56715554816.0
  - 0.43517082929611206
  - 0.39906054735183716
  - 0.4338379502296448
  - 0.397573322057724
  - 0.40456724166870117
  - 0.40557238459587097
  - 0.3884655833244324
  - 0.4100804328918457
  - 0.4082169532775879
  - 0.40666595101356506
  - 0.553344190120697
  - 0.4063807725906372
  - 0.45100638270378113
  - 0.41984793543815613
  - 0.4096739590167999
  - 0.42775511741638184
  - 0.42393502593040466
  - 0.4206485450267792
  - 0.40143683552742004
  - 0.4264882206916809
  - 0.3917437493801117
  - 0.4247453510761261
  - 0.4318632185459137
  - 0.41491737961769104
  - 0.404159277677536
  - 0.4561309218406677
  - 0.40561825037002563
  - 0.40761101245880127
  - 0.41037946939468384
  - 0.45838215947151184
  - 0.4113733470439911
  - 0.4134688675403595
  - 0.4364897608757019
  - 0.424087256193161
  - 0.47821667790412903
  - 0.4679270386695862
  - 0.40228185057640076
  - 0.4095112979412079
  - 0.4049222767353058
  - 0.4391157031059265
  - 0.4059315025806427
  - 0.39321425557136536
  - 0.4808368682861328
  - 0.4006049335002899
  - 0.46720194816589355
  - 0.401982843875885
  - 0.41238778829574585
  - 0.40078145265579224
  - 0.49790334701538086
  - 0.43173491954803467
  - 0.398601233959198
  - 0.40569809079170227
  - 0.4305783808231354
  - 0.42835479974746704
  - 0.4067215025424957
  - 0.4244445562362671
  - 0.40792715549468994
  - 0.4024137556552887
  - 0.44141483306884766
  - 0.4003564417362213
  - 0.41127270460128784
  - 0.4260667860507965
  - 0.40093475580215454
  - 0.410957932472229
  - 0.404411643743515
  - 0.4119802415370941
  - 0.43046829104423523
  - 0.4050728976726532
loss_records_fold3:
  train_losses:
  - 6.428195437788964
  - 6.787678191065789
  - 6.334063276648521
  - 6.453698325157166
  - 6.488882839679718
  - 6.5843680769205095
  - 6.577967718243599
  - 6.612203821539879
  - 6.581902778148652
  - 6.38111564218998
  - 6.455650794506074
  - 6.545918563008309
  - 6.343379470705987
  - 6.546369680762291
  - 6.3545084446668625
  - 6.409069246053696
  - 6.346724817156792
  - 6.975185236334801
  - 6.4802443236112595
  - 6.339855518937111
  - 6.844905582070351
  - 6.520070606470108
  - 6.740165966749192
  - 6.67925289273262
  validation_losses:
  - 0.42107194662094116
  - 0.41535380482673645
  - 0.4119715690612793
  - 0.4356182813644409
  - 0.43355804681777954
  - 0.4147398769855499
  - 0.40413063764572144
  - 0.42504164576530457
  - 0.4172344505786896
  - 0.5194767713546753
  - 0.4048532247543335
  - 0.4118151366710663
  - 0.4101423919200897
  - 0.4301552176475525
  - 0.4125288724899292
  - 0.445073664188385
  - 0.409787654876709
  - 0.42502570152282715
  - 0.43403851985931396
  - 0.41624554991722107
  - 0.4259495437145233
  - 0.4124373495578766
  - 0.41589078307151794
  - 0.41668179631233215
loss_records_fold4:
  train_losses:
  - 6.536719730496407
  - 6.615967667102814
  - 6.837832283973694
  - 6.692915764451027
  - 6.704860427975655
  - 6.4765377908945085
  - 6.6462369680404665
  - 6.781715840101242
  - 6.554469120502472
  - 6.2942691296339035
  - 6.668666511774063
  - 6.838646858930588
  - 6.658206754922867
  - 6.606904602050782
  - 6.702117294073105
  - 6.489483681321144
  - 6.488585183024407
  - 6.46874977350235
  - 6.618580782413483
  - 6.308725944161416
  - 6.444133102893829
  - 6.650548562407494
  - 6.438864541053772
  - 6.551680514216423
  - 6.49469535946846
  - 6.362705877423287
  - 6.615897074341774
  - 6.543885380029678
  - 6.593202257156372
  - 6.381404682993889
  - 6.747772765159607
  - 6.688657328486443
  - 6.33202946484089
  - 6.664071851968766
  - 6.429266202449799
  - 6.4434651196002966
  - 6.484036234021187
  validation_losses:
  - 0.40542271733283997
  - 0.428917795419693
  - 0.4374730587005615
  - 0.41193315386772156
  - 0.4126223921775818
  - 0.4180007576942444
  - 0.41490548849105835
  - 0.4815026819705963
  - 0.40304267406463623
  - 0.4211113452911377
  - 0.4428577423095703
  - 0.4181988835334778
  - 0.44879570603370667
  - 0.399448037147522
  - 0.4409133195877075
  - 0.4077508747577667
  - 0.44992339611053467
  - 0.435528963804245
  - 0.4028269350528717
  - 0.4479115903377533
  - 0.4037543833255768
  - 0.44053834676742554
  - 0.4264848530292511
  - 0.40073204040527344
  - 0.4112301766872406
  - 0.42218437790870667
  - 0.48601287603378296
  - 0.4390139877796173
  - 0.4049687385559082
  - 0.41834989190101624
  - 0.45161518454551697
  - 0.4079817533493042
  - 0.4178818166255951
  - 0.4111465513706207
  - 0.41421306133270264
  - 0.41336748003959656
  - 0.4123799502849579
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 41 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 37 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:31:34.912960'
