config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.762404'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_1fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_1fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_1fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_1fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_1fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_1fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_1fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_1fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_1fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_1fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.2870470315217974
  - 2.938587319850922
  - 2.9304653257131577
  - 2.8786537498235703
  - 2.920025879144669
  - 2.9216419488191607
  - 2.8765354812145234
  - 2.9554649740457535
  - 2.9165494382381443
  - 2.86823970079422
  - 2.8860735267400743
  - 2.8054866254329682
  - 2.823375254869461
  - 2.833005940914154
  - 2.8593874365091327
  - 2.848650920391083
  - 2.856517964601517
  - 2.843710720539093
  - 2.8838799715042116
  validation_losses:
  - 0.4422808587551117
  - 0.41754093766212463
  - 0.3910405933856964
  - 0.4007176458835602
  - 0.401345431804657
  - 0.4194416105747223
  - 0.4128356873989105
  - 0.38768285512924194
  - 0.39318037033081055
  - 0.4070417881011963
  - 0.3859741985797882
  - 0.39312943816185
  - 0.406549334526062
  - 0.3915342390537262
  - 0.3902696967124939
  - 0.38444286584854126
  - 0.3843615651130676
  - 0.3877212107181549
  - 0.3861861526966095
loss_records_fold1:
  train_losses:
  - 2.8462369054555894
  - 2.853900927305222
  - 2.861502754688263
  - 2.861603429913521
  - 2.7971133530139927
  - 2.8715680301189423
  - 2.815709447860718
  - 2.796040201187134
  - 2.7768832951784135
  - 2.8031533598899845
  - 2.8500069230794907
  - 2.804240494966507
  validation_losses:
  - 0.3878246247768402
  - 0.3883131742477417
  - 0.38690343499183655
  - 0.4173949658870697
  - 0.38504841923713684
  - 0.41181713342666626
  - 0.3871455490589142
  - 0.38939931988716125
  - 0.39076897501945496
  - 0.3917609751224518
  - 0.3965202569961548
  - 0.38742318749427795
loss_records_fold2:
  train_losses:
  - 2.8500812858343125
  - 2.7981218278408053
  - 2.829779088497162
  - 2.814020127058029
  - 2.813679733872414
  - 2.82749020755291
  - 2.8094252169132234
  - 2.8507769405841827
  - 2.80836284160614
  - 2.7907870888710025
  - 2.8303573668003086
  - 2.824999111890793
  - 2.804862326383591
  - 2.815722942352295
  - 2.8175080835819246
  - 2.8007634699344637
  - 2.844231790304184
  - 2.7738597244024277
  - 2.8036542534828186
  - 2.7846852868795398
  - 2.7900459975004197
  - 2.772509515285492
  - 2.718445506691933
  - 2.7497071206569674
  - 2.7839550614357
  - 2.704309117794037
  - 2.86526997089386
  - 2.829229399561882
  - 2.8228438377380374
  - 2.817859780788422
  validation_losses:
  - 0.394520103931427
  - 0.38346418738365173
  - 0.4005814790725708
  - 0.4105471670627594
  - 0.3913459777832031
  - 0.3827773928642273
  - 0.4052128195762634
  - 0.3839685618877411
  - 0.3972596824169159
  - 0.536500871181488
  - 0.3894748389720917
  - 0.5161717534065247
  - 0.39305371046066284
  - 0.40032970905303955
  - 0.3894408941268921
  - 0.3886568248271942
  - 0.4131442606449127
  - 0.38808730244636536
  - 0.391972154378891
  - 0.40230414271354675
  - 0.3889544606208801
  - 0.42990443110466003
  - 0.4208495318889618
  - 0.46752670407295227
  - 0.47062623500823975
  - 0.38700732588768005
  - 0.3958873152732849
  - 0.4039657413959503
  - 0.3874758183956146
  - 0.38544005155563354
loss_records_fold3:
  train_losses:
  - 2.815924546122551
  - 2.8376974731683733
  - 2.8251207649707797
  - 2.8071349143981936
  - 2.8001449763774873
  - 2.8231616854667667
  - 2.8231551676988604
  - 2.8066105097532272
  - 2.7933899700641636
  - 2.8108200967311863
  - 2.7995957285165787
  validation_losses:
  - 0.3671073317527771
  - 0.36971333622932434
  - 0.366248220205307
  - 0.36780408024787903
  - 0.3766386806964874
  - 0.37023136019706726
  - 0.36959579586982727
  - 0.37438803911209106
  - 0.3762148320674896
  - 0.3699323236942291
  - 0.37567847967147827
loss_records_fold4:
  train_losses:
  - 2.80632917881012
  - 2.784402033686638
  - 2.7847104847431186
  - 2.7519180655479434
  - 2.806707274913788
  - 2.7954812288284305
  - 2.764670729637146
  - 2.7892176985740664
  - 2.7942383706569673
  - 2.7706439375877383
  - 2.7815235376358034
  - 2.779444879293442
  - 2.7923332393169407
  - 2.7369537502527237
  - 2.748626145720482
  - 2.7201579421758653
  - 2.7533192455768587
  - 2.779093298316002
  - 2.7340749502182007
  - 2.793577465415001
  - 2.788992139697075
  - 2.790228193998337
  - 2.772645401954651
  - 2.8106108844280246
  - 2.796271926164627
  validation_losses:
  - 0.3734561800956726
  - 0.36984583735466003
  - 0.3758057951927185
  - 0.3699966371059418
  - 0.37980517745018005
  - 0.37357816100120544
  - 0.367539644241333
  - 0.39963915944099426
  - 0.40472570061683655
  - 0.4114410877227783
  - 0.4260142147541046
  - 0.37201783061027527
  - 0.3663412034511566
  - 0.36954453587532043
  - 0.41048663854599
  - 0.3788611590862274
  - 0.5433217883110046
  - 0.3957546353340149
  - 0.42997968196868896
  - 0.37782546877861023
  - 0.37733492255210876
  - 0.37044665217399597
  - 0.37214598059654236
  - 0.3740096986293793
  - 0.37332284450531006
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:08:40.987469'
