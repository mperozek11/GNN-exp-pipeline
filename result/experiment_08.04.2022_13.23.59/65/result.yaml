config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 20:19:37.397367'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/65/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/65/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/65/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/65/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/65/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/65/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/65/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/65/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/65/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/65/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 39.31779454648495
  - 31.388213008642197
  - 31.9362054169178
  - 30.39975018799305
  - 30.089906066656113
  - 30.331406235694885
  - 30.341540843248367
  - 31.61796309053898
  - 30.83436881005764
  - 30.34217046201229
  - 30.09755313396454
  - 30.19695346057415
  - 30.147865653038025
  - 31.43659195303917
  - 30.496929228305817
  - 31.403128013014793
  - 30.073567301034927
  - 31.986694894731045
  - 29.87719714641571
  - 29.99814100563526
  - 30.473659083247185
  - 30.357037037611008
  - 30.031318604946136
  - 30.871786639094353
  - 30.345458671450615
  - 30.711092479526997
  - 31.856332197785378
  - 30.319924160838127
  - 30.80554123222828
  - 31.14620564877987
  - 30.171544536948204
  - 30.625721514225006
  - 30.348625406622887
  - 30.92524042725563
  - 30.567584186792374
  - 30.08164581656456
  - 30.306679040193558
  - 31.434349447488785
  - 30.03314933180809
  - 30.23329182714224
  - 31.254875048995018
  - 31.562446177005768
  - 30.071960151195526
  - 30.151219725608826
  - 32.06384778022766
  - 30.178808838129044
  - 30.802678763866425
  - 30.661476865410805
  - 31.42233246564865
  - 30.770174652338028
  - 31.86005687713623
  - 30.506215885281563
  - 30.888616420328617
  - 31.22558470070362
  - 30.189899370074272
  - 30.32121266424656
  - 30.627749547362328
  - 31.05306699872017
  - 30.098174959421158
  - 30.755223125219345
  - 30.610841393470764
  - 29.970573976635933
  - 30.461329832673073
  - 30.211048409342766
  - 30.57242527604103
  - 31.06904958933592
  - 30.190688267350197
  - 31.04210239648819
  - 30.57825493812561
  - 30.593276381492615
  - 30.05917379260063
  - 30.51173724234104
  - 31.05826561152935
  - 30.16726152598858
  - 31.25785121321678
  - 31.970597505569458
  - 30.87060394883156
  - 30.303472608327866
  - 31.5788411796093
  - 30.14099033176899
  - 30.335664331912994
  - 30.256259694695473
  - 30.470603555440903
  - 30.084065675735474
  - 30.70031701028347
  - 30.325318954885006
  - 30.3616391569376
  - 30.206799432635307
  - 30.455870300531387
  - 30.135141611099243
  - 30.78277985751629
  - 30.335178688168526
  - 30.008266240358353
  - 30.665453851222992
  - 30.738750964403152
  - 30.46641893684864
  - 30.322797015309334
  - 30.159054145216942
  - 30.585115745663643
  - 30.35142371058464
  validation_losses:
  - 0.4663774371147156
  - 0.44008007645606995
  - 0.4139915108680725
  - 0.4096788167953491
  - 0.4057508707046509
  - 0.43033111095428467
  - 0.4223562777042389
  - 0.4357718825340271
  - 0.4176360070705414
  - 0.40831610560417175
  - 5981.03515625
  - 0.4071353077888489
  - 8114.95068359375
  - 0.439634770154953
  - 0.443551242351532
  - 0.40827861428260803
  - 0.40531104803085327
  - 0.43454357981681824
  - 0.4185933470726013
  - 18720.048828125
  - 46158.42578125
  - 48293.359375
  - 59961.38671875
  - 67702.03125
  - 88030.8125
  - 75926.8046875
  - 63861.56640625
  - 51572.1484375
  - 46184.25
  - 50189.51171875
  - 56860.01171875
  - 50614.234375
  - 66158.8046875
  - 70281.703125
  - 71670.0078125
  - 57925.4140625
  - 71903.125
  - 57530.1328125
  - 53281.0390625
  - 64580.2421875
  - 60467.19921875
  - 58168.53515625
  - 65546.0625
  - 63087.4921875
  - 61779.6328125
  - 63363.01953125
  - 64455.1171875
  - 66621.1484375
  - 64332.34375
  - 78581.2265625
  - 46929.75
  - 50302.05078125
  - 65901.53125
  - 64957.49609375
  - 72029.171875
  - 57738.12109375
  - 67472.8984375
  - 80493.75
  - 54958.140625
  - 67480.8125
  - 76595.84375
  - 42704.8984375
  - 55043.359375
  - 73093.2734375
  - 88408.8828125
  - 68608.7578125
  - 84034.4609375
  - 87544.6328125
  - 72711.8671875
  - 53850.41796875
  - 77924.6015625
  - 60290.96484375
  - 60995.08203125
  - 66475.84375
  - 54872.9140625
  - 69332.890625
  - 59593.51953125
  - 50583.578125
  - 61343.6640625
  - 54632.73828125
  - 83104.9921875
  - 69575.7578125
  - 64490.84375
  - 72987.5390625
  - 67542.328125
  - 67967.7265625
  - 46201.53125
  - 78572.4375
  - 56197.23828125
  - 60962.16796875
  - 65176.8671875
  - 58952.46875
  - 51273.44140625
  - 60909.0234375
  - 64404.33984375
  - 61137.4375
  - 58562.30859375
  - 66428.765625
  - 67962.5
  - 61849.89453125
loss_records_fold1:
  train_losses:
  - 30.021709755063057
  - 30.348395973443985
  - 31.035753473639488
  - 30.3651692122221
  - 31.121144875884056
  - 29.96982727944851
  - 30.364890798926353
  - 31.034309178590775
  - 30.82847173511982
  - 30.728224851191044
  - 30.44968032091856
  - 30.759020686149597
  - 30.99110335111618
  - 30.498989939689636
  - 30.687645435333252
  - 30.3025753647089
  - 30.24283930659294
  - 30.211787819862366
  - 30.412775725126266
  - 31.36507122218609
  - 30.638244822621346
  - 29.90121929347515
  - 30.381035044789314
  - 31.204176679253578
  - 30.345302149653435
  - 31.53078120946884
  - 30.985380053520203
  - 30.363124676048756
  - 30.67538532614708
  - 30.92313252389431
  - 30.039068266749382
  - 30.1857271194458
  - 30.4873086810112
  - 31.256812646985054
  - 30.510284885764122
  - 30.607519641518593
  - 31.220377013087273
  - 30.715702801942825
  - 30.748158767819405
  - 30.824110239744186
  - 30.742774963378906
  - 30.36916010081768
  - 30.67285916209221
  - 30.040966764092445
  - 30.12873040139675
  - 30.50785581022501
  - 30.751601800322533
  - 30.64824602007866
  - 30.37656433880329
  - 30.46191442012787
  - 30.51144650578499
  - 30.30866101384163
  - 30.987395480275154
  - 30.974733412265778
  - 30.39649498462677
  - 29.95518320798874
  - 30.631050646305084
  - 30.312169909477234
  - 30.393371731042862
  - 30.272721141576767
  - 30.395890772342682
  - 30.362201184034348
  - 30.770945847034454
  - 30.2998918145895
  - 30.435428842902184
  - 30.380780085921288
  - 30.56664291024208
  - 30.294367015361786
  - 30.89770731329918
  - 30.73008506000042
  - 31.045551225543022
  - 30.80220438539982
  - 30.464734002947807
  - 30.351645424962044
  - 30.989962935447693
  - 31.60343910753727
  - 30.508764311671257
  - 30.927836701273918
  - 30.603475637733936
  - 31.64197586476803
  - 30.93707635998726
  - 31.13578000664711
  - 31.412567250430584
  - 30.855825424194336
  - 30.27240614593029
  - 30.57721909880638
  - 30.17574679851532
  - 30.235377863049507
  - 30.15498359501362
  - 30.60367377102375
  - 30.897401548922062
  - 31.189611449837685
  - 72.789460465312
  - 43.44552958011627
  - 46.24770721048117
  - 33.30233669281006
  - 40.10434928536415
  - 30.864233821630478
  - 30.767317354679108
  - 30.46739026904106
  validation_losses:
  - 73915.6953125
  - 65786.8984375
  - 73706.9921875
  - 68909.296875
  - 67214.5234375
  - 48810.9375
  - 73325.453125
  - 68901.859375
  - 78581.5859375
  - 76463.75
  - 57704.26953125
  - 81985.8828125
  - 79632.1484375
  - 60168.3984375
  - 75109.3046875
  - 80632.15625
  - 67235.375
  - 82236.1875
  - 69278.7109375
  - 72640.4140625
  - 84201.25
  - 41103.20703125
  - 61023.328125
  - 56819.81640625
  - 64853.2421875
  - 67344.03125
  - 69431.703125
  - 57495.625
  - 55776.41796875
  - 52971.6484375
  - 61229.51953125
  - 53418.59375
  - 68233.7890625
  - 71919.7578125
  - 53325.859375
  - 79089.796875
  - 60809.125
  - 60128.88671875
  - 70482.5
  - 69002.5234375
  - 53863.375
  - 85033.578125
  - 47930.08203125
  - 78320.828125
  - 70277.5546875
  - 75024.9140625
  - 69687.53125
  - 71019.140625
  - 54849.01953125
  - 82929.09375
  - 48192.43359375
  - 75478.3828125
  - 80170.515625
  - 75663.34375
  - 73945.40625
  - 71822.875
  - 62056.5859375
  - 50262.09765625
  - 71675.734375
  - 83721.609375
  - 59054.4296875
  - 69872.9453125
  - 76495.921875
  - 72728.953125
  - 68652.328125
  - 63670.9453125
  - 54399.5
  - 47240.6171875
  - 69857.5703125
  - 67400.78125
  - 61929.03125
  - 52753.96484375
  - 74203.1015625
  - 64630.6640625
  - 66418.890625
  - 53241.53125
  - 96687.46875
  - 57558.97265625
  - 81992.3203125
  - 42860.11328125
  - 70917.15625
  - 55965.40234375
  - 74779.59375
  - 75546.6953125
  - 52251.56640625
  - 63752.703125
  - 67861.2265625
  - 90893.25
  - 73703.703125
  - 69684.65625
  - 52805.8359375
  - 1.0751396417617798
  - 0.41609615087509155
  - 0.4389687180519104
  - 0.4211181402206421
  - 0.42170387506484985
  - 0.40991735458374023
  - 114296.109375
  - 1.7823281017506316e+27
  - 2.535771472600289e+30
loss_records_fold4:
  train_losses:
  - 30.67904680967331
  - 30.234816908836365
  - 30.083625614643097
  - 30.64382041990757
  - 31.0870448499918
  - 30.460279166698456
  - 31.5996425896883
  - 30.757734671235085
  - 30.475966438651085
  - 30.564739659428596
  - 30.02024033665657
  - 31.57342778146267
  - 30.04750882089138
  - 30.130907341837883
  - 30.927859723567963
  - 30.73322570323944
  - 31.194071128964424
  - 29.83865962922573
  - 30.766619980335236
  - 30.76894374191761
  - 30.5811685025692
  - 31.038789100944996
  - 30.283065170049667
  - 30.672391787171364
  - 30.6637674421072
  - 30.02937999367714
  - 30.243950739502907
  - 29.830362617969513
  - 31.28427119553089
  - 29.907854363322258
  - 31.599329486489296
  - 30.593004524707794
  - 30.906350664794445
  - 30.113244146108627
  - 30.301945462822914
  - 30.450341433286667
  - 30.953042820096016
  - 30.76755066215992
  - 30.248182728886604
  - 30.060115948319435
  - 30.358979299664497
  - 30.798691853880882
  - 30.347868382930756
  - 30.419100046157837
  - 30.20460243523121
  - 30.140498593449593
  - 31.267136991024017
  - 30.43538996577263
  - 29.90250277519226
  - 30.585420347750187
  - 30.04207716882229
  - 29.96970644593239
  - 30.423119768500328
  - 30.159022599458694
  - 30.049697071313858
  - 30.827488616108894
  - 30.803709149360657
  - 29.9905823469162
  - 31.354049295186996
  - 30.075810194015503
  - 30.725835487246513
  - 30.781436055898666
  - 31.17379330098629
  - 30.94815218448639
  - 32.3150539547205
  - 30.895362690091133
  - 30.068648770451546
  - 30.274414986371994
  - 31.243303269147873
  - 29.925724178552628
  - 30.417412906885147
  - 30.96014628559351
  - 30.362021431326866
  - 30.231065034866333
  - 30.065211236476898
  - 30.848878800868988
  - 29.929214596748352
  - 30.816942110657692
  - 30.674801036715508
  - 30.721241682767868
  - 30.170616641640663
  - 30.397130087018013
  - 30.628549084067345
  - 30.956263795495033
  - 31.158310815691948
  - 30.90893141925335
  - 30.653061404824257
  - 30.872667118906975
  - 30.5403401106596
  - 30.78832532465458
  - 30.878150179982185
  - 31.6244525462389
  - 31.771927274763584
  - 30.849324941635132
  - 30.362707659602165
  - 30.944250166416168
  - 30.649178817868233
  - 30.264550261199474
  - 30.364600658416748
  - 30.11590275168419
  validation_losses:
  - 0.4029562175273895
  - 0.4092598557472229
  - 0.407304972410202
  - 0.40512168407440186
  - 0.40428414940834045
  - 0.4030489921569824
  - 0.43979397416114807
  - 0.46378350257873535
  - 0.4072936475276947
  - 192925232.0
  - 727013632.0
  - 1001019200.0
  - 986089792.0
  - 683219200.0
  - 993798976.0
  - 736765376.0
  - 793974080.0
  - 795304000.0
  - 800143104.0
  - 656231360.0
  - 1153587712.0
  - 982513280.0
  - 676143296.0
  - 734090816.0
  - 783982144.0
  - 1063154624.0
  - 777079360.0
  - 695349440.0
  - 623718592.0
  - 735150400.0
  - 949187264.0
  - 1029681152.0
  - 0.4059072732925415
  - 0.4181461036205292
  - 0.41255393624305725
  - 0.40580928325653076
  - 11118597.0
  - 815738560.0
  - 734855296.0
  - 0.4180513322353363
  - 1890742.625
  - 1.1199947595596313
  - 0.5179527401924133
  - 0.40433040261268616
  - 0.6549370884895325
  - 0.8675923943519592
  - 1.2143388986587524
  - 1.665545105934143
  - 0.42336565256118774
  - 1.0779087543487549
  - 0.953165590763092
  - 220488096.0
  - 3.4590556621551514
  - 7.358623504638672
  - 1.366528034210205
  - 16386710.0
  - 73203872.0
  - 1.0025733709335327
  - 1.523998737335205
  - 1.9237505197525024
  - 0.42614856362342834
  - 86920624.0
  - 1.6589319705963135
  - 1.9332003593444824
  - 0.74971604347229
  - 4.768211841583252
  - 0.40556660294532776
  - 0.4071543514728546
  - 0.5096508264541626
  - 1.8694164752960205
  - 0.9799513220787048
  - 1.0893957614898682
  - 1.234474539756775
  - 3.5879523754119873
  - 1.53822922706604
  - 0.7133998870849609
  - 1.2734256982803345
  - 0.7478201389312744
  - 4.63564920425415
  - 236403086589952.0
  - 4.869724983253148e+23
  - 2.0398506153884033e+24
  - 2.2257283673560091e+24
  - 2.1111284021436528e+24
  - 2.488375271205307e+24
  - 2.1174046185843563e+24
  - 2.0979977791276854e+24
  - 1.7488722508304611e+24
  - 2.0460273923493345e+24
  - 1.5311378365386874e+24
  - 1.8756525435086127e+24
  - 1.930460414224989e+24
  - 1.5645591574597949e+24
  - 1.9453332457496055e+24
  - 2.5586138433393414e+24
  - 2.3467175353165205e+24
  - 2.3636357937054975e+24
  - 2.524734956696093e+24
  - 1.7509606240208684e+24
  - 2.3194624709476146e+24
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.855917667238422,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.0
  total_train_time: '0:16:51.891295'
