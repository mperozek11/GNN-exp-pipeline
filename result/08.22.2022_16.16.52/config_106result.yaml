config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:44:09.596596'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_106fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_106fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_106fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_106fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_106fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_106fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_106fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_106fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_106fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_106fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 18.18737268447876
  - 15.439604961872101
  - 7.864355856180191
  - 5.285068345069885
  - 5.625394290685654
  - 3.2182773053646088
  - 4.298184591531753
  - 2.9211975142359736
  - 2.675176364183426
  - 4.784111535549164
  - 5.460972023010254
  - 2.8183685898780824
  - 1.986710184812546
  - 3.915961199998856
  - 11.876815694570542
  - 5.917621994018555
  - 4.679499542713166
  - 2.4354090571403506
  - 5.152368593215943
  - 4.41222870349884
  - 2.3808684051036835
  - 3.7236894309520725
  - 2.1461587488651275
  - 1.8551116049289704
  - 2.794944077730179
  - 2.9144441306591036
  - 2.5983038723468783
  - 6.2383954465389255
  - 3.0059731900691986
  - 2.1197089314460755
  - 2.02373369038105
  - 2.756373065710068
  - 1.6640006959438325
  - 2.1263698160648348
  - 2.6026480436325077
  - 8.877472293376924
  - 3.505913901329041
  - 3.21943598985672
  - 2.0028772205114365
  - 1.6143072545528412
  - 1.5948044180870058
  - 5.5672813892364506
  - 5.879996633529664
  - 3.123466259241104
  - 2.8035448729991916
  - 2.7469992637634277
  - 4.4348756074905396
  - 3.4467377960681915
  - 1.6784373104572297
  - 1.6301881194114687
  - 3.347022378444672
  - 2.808847337961197
  - 3.3411418139934543
  - 2.626343303918839
  - 1.717122918367386
  - 2.089562302827835
  - 1.6337329626083374
  - 7.6999013304710395
  - 2.2477837055921555
  - 3.4500173419713978
  - 3.100362902879715
  - 1.9390435129404069
  - 1.858974426984787
  - 1.796141427755356
  - 2.0005568623542787
  - 2.011623162031174
  - 2.24994558095932
  - 1.7261314749717713
  - 2.40256307721138
  - 1.7792613327503206
  - 2.47229238152504
  - 1.8531269848346712
  - 1.6493505001068116
  - 1.6570677042007447
  - 1.5214155912399292
  - 1.5881182730197907
  - 2.294692295789719
  - 1.95371373295784
  - 1.491366094350815
  - 1.5865417420864105
  - 1.5232930123806
  - 1.6244822561740877
  - 1.9257887542247774
  - 1.8131620168685914
  - 1.5565211296081545
  - 1.6450411260128022
  - 1.5568291783332826
  - 1.501881468296051
  - 2.3015309870243073
  - 2.015554714202881
  - 1.788675308227539
  - 2.0601417124271393
  - 1.655886471271515
  - 1.8356582760810853
  - 1.6424319446086884
  - 1.4816561698913575
  - 1.5652734518051148
  - 1.5166847109794617
  - 1.8538005232810975
  - 1.8578773558139803
  validation_losses:
  - 9.796822547912598
  - 2.63338041305542
  - 1.758604645729065
  - 2.2934296131134033
  - 1.136083960533142
  - 0.9391258955001831
  - 0.4759805202484131
  - 0.4463086426258087
  - 1.1858271360397339
  - 0.9059450626373291
  - 2.052724838256836
  - 0.48137229681015015
  - 0.5189628005027771
  - 7.186191082000732
  - 0.7587548494338989
  - 0.6513388752937317
  - 1.4877946376800537
  - 0.5556075572967529
  - 0.6927803754806519
  - 0.9551688432693481
  - 0.4041801691055298
  - 0.6050340533256531
  - 0.4093782603740692
  - 0.4054689109325409
  - 0.4949162006378174
  - 0.39905405044555664
  - 0.412811279296875
  - 0.5863659977912903
  - 0.4146473705768585
  - 0.5153564810752869
  - 0.3948139250278473
  - 0.38912853598594666
  - 0.4146418571472168
  - 0.4045165181159973
  - 0.3920345604419708
  - 0.4883909821510315
  - 0.6315397620201111
  - 0.4073750376701355
  - 0.3803270161151886
  - 0.3847888708114624
  - 0.4005321264266968
  - 1.1050814390182495
  - 0.460306853055954
  - 0.4964572489261627
  - 0.400397390127182
  - 0.4236530661582947
  - 0.43853774666786194
  - 0.38725745677948
  - 0.4249075651168823
  - 0.3787173628807068
  - 0.4422001838684082
  - 0.38330408930778503
  - 0.38985568284988403
  - 0.398882657289505
  - 0.43182334303855896
  - 0.42314255237579346
  - 0.39157259464263916
  - 0.4371366500854492
  - 0.5668061971664429
  - 0.6199557781219482
  - 0.4165726602077484
  - 0.384798526763916
  - 0.389011025428772
  - 0.4305623173713684
  - 0.4288049042224884
  - 0.5089819431304932
  - 0.3879467844963074
  - 0.39642438292503357
  - 0.43921056389808655
  - 0.4017578661441803
  - 0.3838970363140106
  - 0.428558349609375
  - 0.3964967131614685
  - 0.40550822019577026
  - 0.5314076542854309
  - 0.408021479845047
  - 0.3929749131202698
  - 0.3885557949542999
  - 0.39689525961875916
  - 0.3777984380722046
  - 0.4358718991279602
  - 0.392917662858963
  - 0.45747190713882446
  - 0.41485005617141724
  - 0.3752574920654297
  - 0.4400234818458557
  - 0.3913923501968384
  - 0.3999459445476532
  - 0.485981822013855
  - 0.4939400553703308
  - 0.39338651299476624
  - 0.3821549415588379
  - 0.3828131854534149
  - 0.3783172070980072
  - 0.4021143317222595
  - 0.3796360492706299
  - 0.3777870535850525
  - 0.39354264736175537
  - 0.6299931406974792
  - 0.39419040083885193
loss_records_fold1:
  train_losses:
  - 1.6617193281650544
  - 1.9521502435207367
  - 2.679700839519501
  - 1.6448383867740632
  - 1.4334868907928469
  - 1.5625952184200287
  - 1.535173088312149
  - 1.475448340177536
  - 1.4729343205690384
  - 1.4224466860294342
  - 1.4418213695287705
  - 2.235246974229813
  - 1.6254631638526917
  - 1.8735517382621767
  - 1.5917567253112794
  - 1.9804974913597109
  - 1.545880377292633
  - 1.723479473590851
  - 1.5980791002511978
  - 1.523049569129944
  - 1.7371656119823458
  - 1.6732079684734344
  - 1.5278495311737061
  - 1.46493416428566
  validation_losses:
  - 0.39581236243247986
  - 0.4011170268058777
  - 0.4148778021335602
  - 0.40073344111442566
  - 0.40081509947776794
  - 0.512750506401062
  - 0.41312968730926514
  - 0.4000360369682312
  - 0.4015725553035736
  - 0.41759946942329407
  - 0.43654391169548035
  - 0.44505575299263
  - 0.45845580101013184
  - 0.3962043821811676
  - 0.4090263545513153
  - 0.48827457427978516
  - 0.39433756470680237
  - 0.41762447357177734
  - 0.39964786171913147
  - 0.3949818015098572
  - 0.400603711605072
  - 0.3998194932937622
  - 0.39894527196884155
  - 0.4078519344329834
loss_records_fold2:
  train_losses:
  - 1.4935747206211092
  - 1.6305070221424103
  - 4.317245799303055
  - 2.2856467425823213
  - 2.034980607032776
  - 2.2316231727600098
  - 1.738786166906357
  - 2.174964201450348
  - 3.485027897357941
  - 3.2894639611244205
  - 2.0706893980503085
  - 1.9128779590129854
  - 1.6107657432556153
  - 2.0142644345760345
  - 1.7416089951992035
  - 1.7569499075412751
  - 1.5983139097690584
  - 1.5396346986293794
  - 2.0329498529434207
  - 1.737023013830185
  - 1.566596043109894
  - 1.5843268990516663
  - 1.5453599810600283
  - 1.7014172852039338
  - 1.954568237066269
  - 1.7665642589330675
  - 2.5057116568088533
  - 2.308173859119415
  - 2.0893029659986495
  - 3.3411455869674684
  - 1.7404841959476471
  - 1.6353082060813904
  - 1.5508526712656021
  validation_losses:
  - 0.4102853834629059
  - 0.37689781188964844
  - 0.3776881992816925
  - 0.4214767515659332
  - 0.5712982416152954
  - 0.41521143913269043
  - 0.3722382187843323
  - 0.37227049469947815
  - 0.37540674209594727
  - 40.50523376464844
  - 4.714993000030518
  - 0.3783966898918152
  - 0.4212336838245392
  - 0.4157812297344208
  - 0.6020482182502747
  - 0.43885111808776855
  - 0.3791722357273102
  - 0.3795810639858246
  - 0.45476844906806946
  - 0.3814906179904938
  - 0.38312360644340515
  - 0.3784874975681305
  - 0.3959890902042389
  - 0.3788672089576721
  - 0.5413693785667419
  - 0.3882830739021301
  - 0.5062994360923767
  - 0.38976743817329407
  - 0.38984736800193787
  - 0.3752485513687134
  - 0.38108527660369873
  - 0.38945338129997253
  - 0.38443902134895325
loss_records_fold3:
  train_losses:
  - 1.5681910544633866
  - 1.9306247472763063
  - 1.5444846570491793
  - 1.5802825093269348
  - 1.553270149230957
  - 1.4616830706596375
  - 1.5614471197128297
  - 1.51951265335083
  - 1.5283176004886627
  - 1.82717222571373
  - 1.4837480306625368
  - 1.460928672552109
  - 1.443457642197609
  - 1.4870440602302553
  - 1.5162082374095918
  validation_losses:
  - 0.4106677770614624
  - 0.4433193802833557
  - 0.38191959261894226
  - 0.38559338450431824
  - 0.39219599962234497
  - 0.3897804021835327
  - 0.38260143995285034
  - 0.392708420753479
  - 0.41140347719192505
  - 0.38569962978363037
  - 0.38893812894821167
  - 0.38609951734542847
  - 0.38754037022590637
  - 0.3904222548007965
  - 0.3935064375400543
loss_records_fold4:
  train_losses:
  - 1.5048347592353821
  - 1.4318215370178224
  - 1.5269609808921816
  - 1.5778472006320954
  - 1.7306376159191132
  - 1.5604564785957338
  - 1.6021823167800904
  - 1.6157956898212433
  - 1.5395052313804627
  - 1.512747871875763
  - 1.5774765312671661
  - 1.495313960313797
  - 1.4972708523273468
  - 1.5344089448451996
  - 1.5119073152542115
  - 1.527157598733902
  - 1.5074013948440552
  - 1.4877942383289338
  - 1.5995125770568848
  - 1.5113414525985718
  - 1.4788227796554567
  - 1.4628099739551546
  - 1.5495703577995301
  - 1.515653520822525
  - 1.4555749237537385
  - 1.4721498459577562
  - 1.5509249329566956
  - 1.573366630077362
  - 1.8835803151130677
  - 2.6360413908958438
  - 1.7105741024017336
  - 1.53622664809227
  - 1.5324100673198702
  - 1.5338857531547547
  - 1.4833119094371796
  validation_losses:
  - 0.388228178024292
  - 0.37621432542800903
  - 0.44039517641067505
  - 0.39566391706466675
  - 0.40302029252052307
  - 0.4144177734851837
  - 0.4183925986289978
  - 0.3868374228477478
  - 0.40266159176826477
  - 0.3998526632785797
  - 0.4228833317756653
  - 0.3942956030368805
  - 0.3956877291202545
  - 0.3866208493709564
  - 0.4115305244922638
  - 0.3778779208660126
  - 0.3804580271244049
  - 0.38380560278892517
  - 0.37582728266716003
  - 0.39441606402397156
  - 0.3961201608181
  - 0.3934513032436371
  - 0.4229496419429779
  - 0.3796554207801819
  - 0.3809647858142853
  - 0.4209761321544647
  - 0.3900083601474762
  - 0.3777139186859131
  - 0.49805349111557007
  - 0.40359681844711304
  - 0.4008755087852478
  - 0.3992910087108612
  - 0.39020881056785583
  - 0.39660269021987915
  - 0.3858354687690735
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 33 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 35 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8490566037735849, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.023255813953488372, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.856555439632662
  mean_f1_accuracy: 0.004651162790697674
  total_train_time: '0:18:53.508420'
