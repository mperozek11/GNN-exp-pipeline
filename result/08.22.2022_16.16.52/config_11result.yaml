config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.167033'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_11fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_11fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_11fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_11fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_11fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_11fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_11fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_11fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_11fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_11fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 18.68641772270203
  - 4.963126122951508
  - 3.412383103370667
  - 1.2909477770328524
  - 1.1802077651023866
  - 1.0910289287567139
  - 1.0213716208934784
  - 1.0290861010551453
  - 0.9602236926555634
  - 0.9594119668006897
  - 1.0926315665245057
  - 1.0272200167179109
  - 0.8657853364944459
  - 1.937797021865845
  - 1.4238973021507264
  - 1.2582561075687408
  - 2.2725234985351563
  - 1.409916025400162
  - 0.9208011269569397
  - 0.8734633088111878
  - 0.7981458067893983
  - 0.8057004511356354
  - 0.8265692412853242
  - 0.8924574196338654
  - 2.179976886510849
  - 1.6774634063243867
  - 6.100826561450958
  - 3.7199078977108004
  - 1.2709617793560029
  - 1.6093925833702087
  - 1.5264835208654404
  - 1.3605096518993378
  - 1.8722010135650635
  - 1.4161296844482423
  - 1.2102887630462646
  - 0.890213692188263
  - 8.51158982515335
  - 1.4869286537170412
  - 0.9810905754566193
  - 3.0958259761333466
  - 1.7527881622314454
  - 1.0341631412506105
  - 2.7918304383754733
  - 1.4247088074684144
  - 2.1801012098789214
  - 0.9583354890346527
  - 0.9814366519451142
  - 0.9914214044809342
  - 0.9140711426734924
  - 0.9189614087343216
  - 1.0482626676559448
  - 1.855523145198822
  - 1.0587401807308197
  - 2.252790141105652
  - 1.4727146148681642
  - 1.6391928493976593
  - 1.1772512018680572
  - 0.9380082130432129
  - 1.3802631437778474
  - 0.8678197801113129
  - 0.9131976783275605
  - 0.8541827559471131
  - 2.4375820934772494
  - 0.7817634075880051
  - 0.8655758440494538
  - 0.8703999161720276
  - 0.9179951548576355
  - 0.8918282508850098
  - 1.3713160693645479
  - 1.159305787086487
  - 0.9752621650695801
  - 0.882422161102295
  - 0.9577029466629029
  - 1.8557343363761902
  - 0.928828114271164
  - 0.931343299150467
  - 0.7922786206007004
  - 0.7835838556289674
  - 0.7894812047481538
  - 0.8007450461387635
  - 0.7899331688880921
  - 0.7482035398483277
  - 0.8737438559532166
  - 0.879276567697525
  - 0.7566983938217163
  - 0.9824749052524567
  - 1.2228325545787813
  - 0.8497869729995728
  - 0.9306713461875916
  - 0.9207644283771516
  - 0.8168626129627228
  - 0.8488588988780976
  - 0.817741084098816
  - 0.8891640424728394
  - 0.8537134051322938
  - 0.9160744607448579
  - 0.8501531481742859
  - 0.7897506415843965
  - 1.212139642238617
  - 0.998077529668808
  validation_losses:
  - 5.805367946624756
  - 1.670420527458191
  - 1.254284381866455
  - 0.5388943552970886
  - 0.5327051281929016
  - 0.6332681179046631
  - 0.5608777403831482
  - 0.6002923846244812
  - 0.649008572101593
  - 0.5555689334869385
  - 0.39945632219314575
  - 0.4332886338233948
  - 0.38656553626060486
  - 0.835880696773529
  - 0.44947493076324463
  - 1.2489012479782104
  - 0.5206678509712219
  - 0.4216320812702179
  - 0.40620824694633484
  - 0.43518659472465515
  - 0.39780378341674805
  - 0.379604697227478
  - 0.4158748686313629
  - 0.4217056930065155
  - 0.3787025511264801
  - 0.4753477871417999
  - 1.135322093963623
  - 0.9100919961929321
  - 0.5400664806365967
  - 0.49107274413108826
  - 0.5940821170806885
  - 0.4195069968700409
  - 0.4226563274860382
  - 0.6839540004730225
  - 0.40768757462501526
  - 0.4137061834335327
  - 0.3849788010120392
  - 0.428256630897522
  - 0.4282633364200592
  - 0.3980411887168884
  - 0.4182978868484497
  - 0.8285788893699646
  - 0.552250325679779
  - 0.5317026972770691
  - 0.41554608941078186
  - 0.4128163754940033
  - 0.4650473892688751
  - 0.410641074180603
  - 0.43885374069213867
  - 0.3875733017921448
  - 0.3978930711746216
  - 0.4762236177921295
  - 0.6385911703109741
  - 1.0220593214035034
  - 0.44131115078926086
  - 0.7969674468040466
  - 0.38829708099365234
  - 0.3868042528629303
  - 0.3921172618865967
  - 0.3821251094341278
  - 0.3786875009536743
  - 0.39759260416030884
  - 0.5242019295692444
  - 0.5089403986930847
  - 0.4256204664707184
  - 0.46336379647254944
  - 0.4648124575614929
  - 0.6303703784942627
  - 0.45629820227622986
  - 0.554185688495636
  - 0.42943647503852844
  - 0.4887453615665436
  - 0.4991944134235382
  - 0.3912470042705536
  - 0.5171492099761963
  - 0.40966448187828064
  - 0.3855152428150177
  - 0.38308265805244446
  - 0.3816734850406647
  - 0.3998117446899414
  - 0.37968024611473083
  - 0.38199034333229065
  - 0.42178449034690857
  - 0.38594210147857666
  - 0.39634549617767334
  - 0.3790334463119507
  - 0.40110301971435547
  - 0.3997669816017151
  - 0.40632393956184387
  - 0.3982354402542114
  - 0.3884598910808563
  - 0.4593486487865448
  - 0.39191171526908875
  - 0.3812868595123291
  - 0.4017156660556793
  - 0.3869582414627075
  - 0.3854648172855377
  - 0.40462562441825867
  - 0.4156312346458435
  - 0.4071577787399292
loss_records_fold1:
  train_losses:
  - 0.8472476065158845
  - 0.866876745223999
  - 0.8309052288532257
  - 0.8338665664196014
  - 0.8354636251926423
  - 0.8170994758605957
  - 0.7991944313049317
  - 0.7588942706584931
  - 0.7517477571964264
  - 0.772781777381897
  - 0.8058707416057587
  - 0.8710327565670014
  - 0.8624868333339691
  - 0.8284672915935517
  - 1.1982938945293427
  - 0.8235526859760285
  - 0.9104941070079804
  - 1.4499423503875732
  - 1.026140570640564
  - 0.8326214015483857
  - 0.7961485862731934
  - 0.8456085979938508
  - 0.9411594808101654
  - 0.8381762027740479
  - 0.8752544164657593
  - 0.8318028211593629
  - 0.8216607511043549
  - 0.853754860162735
  - 0.7973668992519379
  - 0.7693404793739319
  - 0.7375994682312013
  - 0.7493043005466462
  - 0.8295857965946198
  validation_losses:
  - 0.4541887938976288
  - 0.4055841267108917
  - 0.41284579038619995
  - 0.40199366211891174
  - 0.40351635217666626
  - 0.41171109676361084
  - 0.3999815583229065
  - 0.4059578478336334
  - 0.40514811873435974
  - 0.4029987156391144
  - 0.42197534441947937
  - 0.4058295786380768
  - 0.45264875888824463
  - 0.41413381695747375
  - 0.4159683585166931
  - 0.3929668068885803
  - 0.4069594442844391
  - 0.4442861080169678
  - 0.42226022481918335
  - 0.4009166359901428
  - 0.40009990334510803
  - 0.4015445411205292
  - 0.3961411416530609
  - 0.460751473903656
  - 0.4045511782169342
  - 0.42029526829719543
  - 0.4899960160255432
  - 0.4117962419986725
  - 0.4144991338253021
  - 0.3994253873825073
  - 0.40592771768569946
  - 0.3993186354637146
  - 0.395702600479126
loss_records_fold2:
  train_losses:
  - 0.8147546648979187
  - 0.7575792193412781
  - 0.8052852511405946
  - 0.7562878906726838
  - 0.8510390222072601
  - 0.7763683199882507
  - 0.8397254645824432
  - 0.8019288659095765
  - 0.8513275623321533
  - 0.7829707682132722
  - 0.8031559348106385
  - 0.7796019971370698
  - 0.7737181305885316
  validation_losses:
  - 0.4002152383327484
  - 0.3763182461261749
  - 0.39313724637031555
  - 0.3734428286552429
  - 0.39860111474990845
  - 0.37550896406173706
  - 0.41759157180786133
  - 0.3790823817253113
  - 0.38167518377304077
  - 0.3753056526184082
  - 0.38347572088241577
  - 0.37743815779685974
  - 0.37362372875213623
loss_records_fold3:
  train_losses:
  - 0.8141158878803254
  - 0.7654767096042634
  - 0.8371538639068604
  - 0.8962924540042878
  - 1.125131994485855
  - 1.7047109842300416
  - 1.3702948510646822
  - 0.8562333226203919
  - 1.5760365247726442
  - 1.7777644515037538
  - 0.9353444576263428
  - 0.8569194853305817
  - 0.8672123193740845
  - 0.9491012871265412
  - 0.8112695813179016
  - 1.8002284705638887
  - 2.2467629611492157
  - 1.2627764761447908
  - 1.1686991512775422
  - 0.8712394654750825
  - 0.7879947245121003
  - 0.7933330595493318
  - 0.7990364313125611
  - 0.8437159478664399
  - 0.8951572477817535
  - 0.7664491176605225
  - 0.7690961062908173
  - 0.8843783974647522
  - 0.8299635529518128
  - 1.3530904054641724
  - 1.2329077422618866
  - 1.0690018653869628
  - 0.8796341478824616
  - 1.0328087985515595
  - 1.1431751608848573
  - 0.8927438467741013
  - 0.937524676322937
  - 0.8066579818725587
  - 0.8453449964523316
  validation_losses:
  - 0.3773444890975952
  - 0.3744051456451416
  - 0.3713359236717224
  - 0.3911919593811035
  - 0.38373562693595886
  - 0.37433382868766785
  - 0.37290844321250916
  - 0.39317405223846436
  - 0.38926127552986145
  - 0.4184628129005432
  - 0.3792552649974823
  - 0.37563005089759827
  - 0.3712645173072815
  - 0.3718191981315613
  - 0.387739360332489
  - 0.3740021288394928
  - 0.3785731792449951
  - 0.4378611743450165
  - 0.4831423759460449
  - 0.5184704065322876
  - 0.38773828744888306
  - 0.3897872269153595
  - 0.38379091024398804
  - 0.3845216631889343
  - 0.3984678089618683
  - 0.38713037967681885
  - 0.3827608525753021
  - 0.3836943209171295
  - 0.3853578269481659
  - 0.3968733549118042
  - 0.4336475133895874
  - 0.39711427688598633
  - 0.4162449240684509
  - 0.4154791533946991
  - 0.40437304973602295
  - 0.40513625741004944
  - 0.38158154487609863
  - 0.3865948021411896
  - 0.3841385245323181
loss_records_fold4:
  train_losses:
  - 0.7687459647655488
  - 0.817132580280304
  - 0.7951161742210389
  - 0.7763505101203919
  - 0.7729712069034577
  - 0.7420359015464784
  - 0.7509179532527924
  - 1.006844586133957
  - 0.8103005468845368
  - 0.839276272058487
  - 0.848237556219101
  - 0.8169710278511048
  - 0.7735203623771668
  - 0.7622938215732575
  validation_losses:
  - 0.4157484769821167
  - 0.4273107349872589
  - 0.4586733281612396
  - 0.4184202551841736
  - 0.4332016408443451
  - 0.41297096014022827
  - 0.41499683260917664
  - 0.4446883499622345
  - 0.44297105073928833
  - 0.4105931520462036
  - 0.3927451968193054
  - 0.39798012375831604
  - 0.39680108428001404
  - 0.3893311619758606
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 33 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
training_metrics:
  fold_eval_accs: '[0.8542024013722127, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.02298850574712644, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.004597701149425288
  total_train_time: '0:15:50.233922'
