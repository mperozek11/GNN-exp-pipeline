config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.858559'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_6fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_6fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_6fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_6fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_6fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_6fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_6fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_6fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_6fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_6fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.9428904712200166
  - 1.6559811174869539
  - 1.6486494839191437
  - 1.6174041450023653
  - 1.6153504610061646
  - 1.5867680311203003
  - 1.5763285756111145
  - 1.5921693086624147
  - 1.5736334800720215
  - 1.5724349558353425
  - 1.544720706343651
  - 1.5766286611557008
  - 1.5914915204048157
  - 1.5907548606395723
  - 1.5811102151870728
  - 1.5799657046794893
  - 1.5419609725475312
  - 1.4865539491176607
  - 1.5232399344444276
  - 1.5462017178535463
  - 1.5856280386447907
  - 1.55134516954422
  - 1.5815372943878174
  - 1.5916297912597657
  - 1.6124012589454653
  - 1.5532267153263093
  - 1.6271488845348359
  - 1.5702060699462892
  - 1.584046721458435
  - 1.5170360177755358
  - 1.5056702256202699
  - 1.5604689538478853
  - 1.5425424396991732
  - 1.5667801797389984
  - 1.5615718126296998
  validation_losses:
  - 0.4175330698490143
  - 0.4607003927230835
  - 0.40256983041763306
  - 0.38966119289398193
  - 0.39547809958457947
  - 0.4116884768009186
  - 0.3953310251235962
  - 0.38950130343437195
  - 0.3840624690055847
  - 0.40079018473625183
  - 0.3932582139968872
  - 0.4139154255390167
  - 0.38917839527130127
  - 0.3865413963794708
  - 0.413108766078949
  - 0.37860798835754395
  - 0.4031928479671478
  - 0.38497430086135864
  - 0.3911883533000946
  - 0.3844679594039917
  - 0.4000214636325836
  - 0.3910607099533081
  - 0.38908666372299194
  - 0.3915315270423889
  - 0.38212594389915466
  - 0.3931537866592407
  - 0.3904794156551361
  - 0.42328864336013794
  - 0.49816465377807617
  - 0.3936116397380829
  - 0.39168986678123474
  - 0.38781821727752686
  - 0.3956146538257599
  - 0.3882075846195221
  - 0.38732993602752686
loss_records_fold1:
  train_losses:
  - 1.5339327037334443
  - 1.5168592393398286
  - 1.5772523164749146
  - 1.485500591993332
  - 1.5275148987770082
  - 1.5401936173439026
  - 1.5163281172513963
  - 1.5871981203556063
  - 1.5400030672550202
  - 1.5432120084762575
  - 1.4575593322515488
  - 1.5824423611164093
  - 1.5234153985977175
  - 1.4935589134693146
  - 1.5440928399562837
  - 1.5133924663066864
  - 1.5709074556827547
  - 1.5174513816833497
  - 1.5074560761451723
  - 1.5466759979724884
  - 1.5502745687961579
  - 1.5188434243202211
  - 1.502179354429245
  - 1.522228753566742
  - 1.5370708346366884
  - 1.5392190992832184
  - 1.5716166436672212
  validation_losses:
  - 0.3824100196361542
  - 0.3875649869441986
  - 0.3959120810031891
  - 0.3867858946323395
  - 0.39861440658569336
  - 0.3928440511226654
  - 0.42647692561149597
  - 0.41668906807899475
  - 0.3902951776981354
  - 0.3845275044441223
  - 0.39343738555908203
  - 0.4487607479095459
  - 0.42198336124420166
  - 0.3934340476989746
  - 0.44457298517227173
  - 0.42406412959098816
  - 0.49140530824661255
  - 0.4771178960800171
  - 0.38721007108688354
  - 0.41548997163772583
  - 0.584007740020752
  - 0.4496977925300598
  - 0.40848636627197266
  - 0.3882751166820526
  - 0.3935510218143463
  - 0.399671733379364
  - 0.3909580707550049
loss_records_fold2:
  train_losses:
  - 1.5519939124584199
  - 1.5313671469688417
  - 1.501478374004364
  - 1.4969770312309265
  - 1.5292887449264527
  - 1.51203852891922
  - 1.4675613939762115
  - 1.5039522647857666
  - 1.5155211865901947
  - 1.5126363515853882
  - 1.5406530261039735
  - 1.5658290147781373
  - 1.5194870173931123
  - 1.4991563737392426
  - 1.5319316685199738
  - 1.5184525847434998
  - 1.5074550449848176
  - 1.5695646107196808
  - 1.543381828069687
  - 1.5384791374206543
  - 1.513972133398056
  - 1.5623365819454194
  - 1.5402932226657868
  - 1.446125477552414
  - 1.5333978950977327
  - 1.552245706319809
  - 1.510544329881668
  - 1.5245187103748323
  - 1.5258613586425782
  - 1.4896022796630861
  - 1.5663599371910095
  - 1.517826646566391
  - 1.5037703633308412
  - 1.5160516381263733
  - 1.5109940946102143
  - 1.5165447294712067
  validation_losses:
  - 0.3859648108482361
  - 0.38391539454460144
  - 0.39825376868247986
  - 0.41087716817855835
  - 0.4045019745826721
  - 0.40271568298339844
  - 0.3820127844810486
  - 0.45288562774658203
  - 0.4140753448009491
  - 0.38940441608428955
  - 0.4015442430973053
  - 0.44104239344596863
  - 0.4337818920612335
  - 0.466993123292923
  - 0.4486273527145386
  - 0.42177605628967285
  - 0.40524548292160034
  - 0.4052140414714813
  - 0.38371360301971436
  - 0.39584875106811523
  - 0.3852457106113434
  - 0.3824806809425354
  - 0.39557865262031555
  - 0.4157370328903198
  - 0.39469778537750244
  - 0.3877960443496704
  - 0.4296134114265442
  - 0.39772334694862366
  - 0.3965725898742676
  - 0.6367623209953308
  - 0.3914692997932434
  - 0.3975750505924225
  - 0.39194655418395996
  - 0.39203453063964844
  - 0.38950106501579285
  - 0.3874374330043793
loss_records_fold3:
  train_losses:
  - 1.570446252822876
  - 1.5213956117630005
  - 1.5392805576324464
  - 1.5546653509140016
  - 1.570442795753479
  - 1.5409026801586152
  - 1.547074830532074
  - 1.5374932110309603
  - 1.6348784625530244
  - 1.5377730488777162
  - 1.5484833002090455
  - 1.5321172237396241
  - 1.536000156402588
  - 1.5110864400863648
  - 1.5123882591724396
  - 1.4984996616840363
  - 1.5194282710552216
  - 1.4936023116111756
  - 1.573931521177292
  - 1.5891218006610872
  - 1.503346782922745
  - 1.5000319123268129
  - 1.4958803355693817
  - 1.6089624404907228
  - 1.539578837156296
  - 1.5482903242111208
  - 1.5362688064575196
  - 1.537605917453766
  - 1.5578430473804474
  - 1.5364816009998323
  - 1.5399992406368257
  - 1.5061830937862397
  - 1.5321381986141205
  - 1.5926230609416963
  - 1.5501589357852936
  - 1.4720739364624025
  - 1.5232204258441926
  - 1.5161572098731995
  - 1.5023103952407837
  - 1.5189854443073274
  - 1.505076950788498
  - 1.5303654789924623
  - 1.5214560508728028
  - 1.5167492210865021
  - 1.5065900325775148
  - 1.52499378323555
  - 1.4887614369392397
  - 1.5436681628227236
  - 1.4899677455425264
  - 1.5087292879819871
  - 1.5237497806549074
  - 1.5374618709087373
  - 1.514640986919403
  - 1.5334516286849977
  - 1.4823083996772768
  - 1.5652009844779968
  - 1.5239189207553865
  - 1.5130912363529205
  - 1.511357057094574
  - 1.5468222558498383
  - 1.5452845871448517
  - 1.5310107171535492
  - 1.4802984952926637
  - 1.5050406157970428
  - 1.6114112079143526
  - 1.5014043152332306
  - 1.4865119218826295
  - 1.5253073513507844
  - 1.4660163998603821
  - 1.533807820081711
  - 1.5482526361942293
  - 1.5433587908744812
  - 1.511347556114197
  - 1.4970972061157228
  - 1.4805667340755464
  - 1.5373446583747865
  - 1.491683715581894
  - 1.5397304832935335
  - 1.5525988161563875
  - 1.5027135193347931
  - 1.498821955919266
  - 1.5180260121822358
  - 1.4788495361804963
  - 1.5150884747505189
  - 1.529645997285843
  - 1.4628044217824936
  - 1.5220619142055511
  - 1.5248527169227601
  - 1.5488735258579256
  - 1.520780521631241
  - 1.5058558106422426
  - 1.4656489372253418
  - 1.554591852426529
  - 1.499154508113861
  - 1.471762102842331
  - 1.5134661674499512
  - 1.5045052766799927
  - 1.5407957911491394
  - 1.5327145099639894
  - 1.5090917348861694
  validation_losses:
  - 0.3761776387691498
  - 0.37159156799316406
  - 0.3824034631252289
  - 0.38081398606300354
  - 0.39358070492744446
  - 0.3713403046131134
  - 0.4137037694454193
  - 0.3787739872932434
  - 0.42512136697769165
  - 0.3659774959087372
  - 0.365730881690979
  - 0.3721030056476593
  - 0.3790023624897003
  - 0.37467819452285767
  - 0.396990031003952
  - 0.5106414556503296
  - 0.6375365257263184
  - 0.3711467981338501
  - 0.41263437271118164
  - 0.48388180136680603
  - 0.6333024501800537
  - 0.687625527381897
  - 0.7554017901420593
  - 0.38825181126594543
  - 0.3844177722930908
  - 0.3902527689933777
  - 0.46569961309432983
  - 0.47014114260673523
  - 0.5117732882499695
  - 0.3990631401538849
  - 0.4670974016189575
  - 0.5981571078300476
  - 0.9261078238487244
  - 0.5843538641929626
  - 0.741000771522522
  - 0.38798531889915466
  - 0.38737809658050537
  - 0.3827051818370819
  - 0.521086573600769
  - 0.38233762979507446
  - 0.3855258524417877
  - 0.4289475083351135
  - 0.519378662109375
  - 0.7359066009521484
  - 0.816688060760498
  - 0.9798858761787415
  - 0.49096691608428955
  - 0.9056028127670288
  - 0.4170569181442261
  - 0.3785887360572815
  - 0.39256781339645386
  - 0.39448776841163635
  - 0.715187132358551
  - 0.7557548880577087
  - 0.671532154083252
  - 0.6848719716072083
  - 0.40566641092300415
  - 0.5969398617744446
  - 0.3848824203014374
  - 0.7259737849235535
  - 0.8016839027404785
  - 0.38447311520576477
  - 0.3931042551994324
  - 0.4058225750923157
  - 0.588279128074646
  - 0.47666436433792114
  - 0.402485191822052
  - 0.49331095814704895
  - 1.67070472240448
  - 0.4156385064125061
  - 0.44975560903549194
  - 0.573672354221344
  - 0.8077188730239868
  - 0.8696087002754211
  - 0.902363657951355
  - 1.1150362491607666
  - 0.36911508440971375
  - 0.6981236934661865
  - 1.343428373336792
  - 1.0415889024734497
  - 1.2969284057617188
  - 1.5604372024536133
  - 0.5644224286079407
  - 0.3805526793003082
  - 0.4589695334434509
  - 0.46758657693862915
  - 0.6855641603469849
  - 1.1020455360412598
  - 2.016914129257202
  - 1.0766936540603638
  - 1.269173502922058
  - 1.5911210775375366
  - 1.583942174911499
  - 1.5371718406677246
  - 1.1089307069778442
  - 1.2031160593032837
  - 0.8813887238502502
  - 0.4189569354057312
  - 0.5803645849227905
  - 0.5798747539520264
loss_records_fold4:
  train_losses:
  - 1.524059844017029
  - 1.5241467952728271
  - 1.5726419508457186
  - 1.496320015192032
  - 1.5203060984611512
  - 1.5357782661914827
  - 1.5615518331527711
  - 1.4884487986564636
  - 1.5554934680461885
  - 1.4546942591667176
  - 1.5343609869480135
  - 1.4888927042484283
  - 1.5145645797252656
  - 1.4864071130752565
  - 1.5003500640392304
  - 1.5067841649055482
  - 1.476976388692856
  - 1.5719784140586854
  - 1.4747972130775453
  - 1.5248578548431397
  - 1.5727983236312868
  - 1.4952305138111115
  - 1.5237025380134583
  - 1.5009912788867952
  - 1.5026759684085846
  - 1.4788811922073366
  - 1.5033922255039216
  - 1.5423660159111023
  - 1.5140865623950959
  - 1.5506334722042086
  - 1.5534772992134096
  - 1.4975328624248505
  - 1.5039227306842804
  - 1.518145912885666
  - 1.5593204319477083
  - 1.5011771708726884
  - 1.487750691175461
  - 1.477971026301384
  - 1.523633760213852
  - 1.5511254012584688
  - 1.544272667169571
  - 1.5029377400875092
  - 1.555001896619797
  - 1.4978505432605744
  - 1.5291717052459717
  - 1.5151857733726501
  - 1.5103278100490571
  - 1.5196218430995942
  - 1.4473302602767946
  - 1.4951537668704988
  - 1.5120462775230408
  - 1.5473543584346772
  - 1.488250631093979
  - 1.5552784979343415
  - 1.4658627688884736
  - 1.4974180340766907
  - 1.5043346345424653
  - 1.5024349629879
  - 1.4870090544223786
  - 1.5222652912139893
  - 1.4801850616931915
  - 1.534616404771805
  - 1.5030905961990357
  - 1.4763366281986237
  - 1.4563908517360689
  - 1.563722735643387
  - 1.5030212104320526
  - 1.4919519424438477
  - 1.5500112712383272
  - 1.4643004179000856
  - 1.5069935739040377
  - 1.4990596532821656
  - 1.5333783686161042
  - 1.5167026102542878
  - 1.5481878399848938
  - 1.521449613571167
  - 1.5547415137290956
  - 1.5971399247646332
  - 1.4709260523319245
  - 1.5524531602859497
  - 1.4758943319320679
  - 1.508111470937729
  - 1.494797158241272
  - 1.5451280295848848
  - 1.4933172047138215
  - 1.5323359310626985
  - 1.5253195881843569
  - 1.515200161933899
  - 1.5609098613262178
  - 1.5564162611961365
  - 1.4811496198177339
  - 1.469770246744156
  - 1.4903703808784485
  - 1.4861072152853012
  - 1.472038096189499
  - 1.4877993285655977
  - 1.5143841087818146
  - 1.5227321207523348
  - 1.532077306509018
  - 1.5046095907688142
  validation_losses:
  - 0.38493669033050537
  - 0.40356186032295227
  - 0.3821730613708496
  - 0.43896952271461487
  - 0.47960105538368225
  - 0.5831454396247864
  - 0.41477763652801514
  - 0.44939273595809937
  - 0.5102864503860474
  - 0.4267652630805969
  - 0.5320366024971008
  - 0.45163971185684204
  - 0.48217034339904785
  - 0.410907119512558
  - 0.4438899755477905
  - 0.4851270020008087
  - 0.4235653877258301
  - 0.49825289845466614
  - 0.3998430669307709
  - 0.4528000056743622
  - 0.4453321099281311
  - 0.3687460124492645
  - 0.3866174519062042
  - 0.3813822567462921
  - 0.4010830223560333
  - 0.402480810880661
  - 0.36867716908454895
  - 0.42343705892562866
  - 0.3619501292705536
  - 0.4124981760978699
  - 0.40897753834724426
  - 0.3765917420387268
  - 0.37612372636795044
  - 0.3785434365272522
  - 0.3788333833217621
  - 0.42771202325820923
  - 0.39652159810066223
  - 0.3943895101547241
  - 0.37389424443244934
  - 0.3679914176464081
  - 0.4046681523323059
  - 0.3891282379627228
  - 0.3906153440475464
  - 0.37076789140701294
  - 0.3707520067691803
  - 0.37057122588157654
  - 0.3876058757305145
  - 0.40173766016960144
  - 0.3643425405025482
  - 0.3783133029937744
  - 0.4020831286907196
  - 0.39580726623535156
  - 0.386243611574173
  - 0.4117533266544342
  - 0.3827996253967285
  - 0.3831878900527954
  - 0.38733503222465515
  - 0.3814433515071869
  - 0.3805418908596039
  - 0.41178664565086365
  - 0.45289549231529236
  - 0.4055534899234772
  - 0.3675509989261627
  - 0.39827635884284973
  - 0.40460285544395447
  - 0.39925533533096313
  - 0.3838386833667755
  - 0.4075566530227661
  - 0.37402811646461487
  - 0.3729027807712555
  - 0.41738009452819824
  - 0.4157668352127075
  - 0.38548439741134644
  - 0.3804911673069
  - 0.40653499960899353
  - 0.421282023191452
  - 0.40562301874160767
  - 0.3854270577430725
  - 0.36407050490379333
  - 0.383968710899353
  - 0.37644821405410767
  - 0.37731772661209106
  - 0.38053998351097107
  - 0.4098283648490906
  - 0.389068603515625
  - 0.3866637647151947
  - 0.3695186972618103
  - 0.3751199543476105
  - 0.4125203788280487
  - 0.39192235469818115
  - 0.37969183921813965
  - 0.36911246180534363
  - 0.39322471618652344
  - 0.40190455317497253
  - 0.3607955276966095
  - 0.4703708589076996
  - 0.36297833919525146
  - 0.3705895245075226
  - 0.38319846987724304
  - 0.3853478729724884
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 35 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 36 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8490566037735849, 0.855917667238422, 0.8542024013722127,
    0.8539518900343642]'
  fold_eval_f1: '[0.0, 0.022222222222222223, 0.0, 0.02298850574712644, 0.12371134020618556]'
  mean_eval_accuracy: 0.854152299104643
  mean_f1_accuracy: 0.03378441363510685
  total_train_time: '0:24:52.104267'
