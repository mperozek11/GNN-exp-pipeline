config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:57:02.127325'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_110fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_110fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_110fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_110fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_110fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_110fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_110fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_110fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_110fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_110fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 30.89617898464203
  - 12.752237164974213
  - 8.10541684627533
  - 8.206437546014786
  - 6.730685013532639
  - 6.898253250122071
  - 5.877936375141144
  - 3.6728511095047
  - 2.4686669886112216
  - 2.028600549697876
  - 2.6217061996459963
  - 3.582342600822449
  - 2.767012929916382
  - 10.20397254228592
  - 8.26355015039444
  - 5.093347680568695
  - 3.3479761064052584
  - 2.2461859345436097
  validation_losses:
  - 3.4556970596313477
  - 2.997051954269409
  - 0.5971399545669556
  - 0.45244327187538147
  - 1.0552663803100586
  - 1.5494306087493896
  - 0.6792047619819641
  - 0.5332666039466858
  - 0.673201858997345
  - 0.6917847394943237
  - 0.957356333732605
  - 1.809198021888733
  - 0.9175320267677307
  - 0.8427785038948059
  - 0.720039427280426
  - 0.5294351577758789
  - 0.47535568475723267
  - 0.446241170167923
loss_records_fold1:
  train_losses:
  - 2.0647699296474458
  - 2.4467831015586854
  - 15.8706647336483
  - 10.305456966161728
  - 7.272931456565857
  - 14.412638622522355
  - 3.245268827676773
  - 2.184284317493439
  - 3.2986517131328585
  - 3.1114587724208835
  - 4.307973653078079
  - 5.061379534006119
  - 1.8686206281185151
  - 2.0893541634082795
  - 1.9022822618484498
  - 12.626426595449448
  - 2.8845019042491913
  - 4.149245393276215
  - 3.5939586579799654
  - 1.6721721827983858
  - 1.9129750788211823
  - 1.9419016599655152
  validation_losses:
  - 0.39867380261421204
  - 0.9262486100196838
  - 0.7490209937095642
  - 0.9503356218338013
  - 0.4624514579772949
  - 0.44521859288215637
  - 0.7797982692718506
  - 0.5354537963867188
  - 0.4579285681247711
  - 0.4882281422615051
  - 0.47180160880088806
  - 0.49784451723098755
  - 0.4901486337184906
  - 0.42615729570388794
  - 0.40854161977767944
  - 0.504026472568512
  - 0.41417062282562256
  - 0.4178875684738159
  - 0.418477326631546
  - 0.4098721146583557
  - 0.40232476592063904
  - 0.40535372495651245
loss_records_fold2:
  train_losses:
  - 2.053338718414307
  - 2.1062995910644533
  - 1.941677838563919
  - 4.375224828720093
  - 2.1225922763347627
  - 1.7319585025310518
  - 2.1024073094129565
  - 1.6482495188713076
  - 2.6872275650501254
  - 1.7580257952213287
  - 1.9369386553764345
  - 1.891537845134735
  - 1.947850012779236
  - 1.8929456174373627
  - 1.6682678759098053
  - 1.7025614917278291
  - 1.6686217844486237
  - 1.895688384771347
  - 1.8472215950489046
  - 4.51477438211441
  - 2.512666004896164
  - 1.9783307790756226
  - 2.2631011962890626
  - 1.6630559742450715
  - 5.60624766945839
  - 6.417359310388566
  - 1.9076467156410217
  - 4.091643625497818
  - 2.252440279722214
  - 4.7840279042720795
  - 13.840942174196243
  - 3.2390456378459933
  - 1.8330580055713654
  - 1.7878851532936098
  - 3.2676048934459687
  - 1.9836202919483186
  validation_losses:
  - 0.40984025597572327
  - 0.4056360423564911
  - 0.4071195125579834
  - 0.4332300126552582
  - 0.3907047510147095
  - 0.39971423149108887
  - 0.3819594383239746
  - 0.4056667685508728
  - 0.38826021552085876
  - 0.41416406631469727
  - 0.4192824959754944
  - 0.47794246673583984
  - 0.39254993200302124
  - 0.4116300940513611
  - 0.38224783539772034
  - 0.3915500044822693
  - 0.397357314825058
  - 0.40150025486946106
  - 0.38649919629096985
  - 0.4031788408756256
  - 0.39462751150131226
  - 0.3946017026901245
  - 0.39947065711021423
  - 0.40201905369758606
  - 1.0570722818374634
  - 0.5555551052093506
  - 0.48371854424476624
  - 0.42329296469688416
  - 0.4508725702762604
  - 0.5022241473197937
  - 0.4578680098056793
  - 0.4159979820251465
  - 0.39824849367141724
  - 0.40250706672668457
  - 0.4107362926006317
  - 0.3816298246383667
loss_records_fold3:
  train_losses:
  - 4.550695925951004
  - 1.6601334989070893
  - 1.892816358804703
  - 1.6914737820625305
  - 4.577787917852402
  - 2.6551661491394043
  - 1.737129878997803
  - 1.644862800836563
  - 3.172823923826218
  - 1.6443161249160767
  - 1.8414457738399506
  - 2.5313830435276032
  - 4.167220860719681
  - 2.0990562975406646
  - 2.0353630006313326
  - 1.71464980840683
  - 2.973469811677933
  - 1.748692810535431
  - 1.68443922996521
  - 2.148594391345978
  - 2.036241292953491
  - 1.5657153874635696
  - 1.6680569589138032
  - 1.7130474507808686
  - 1.6351959884166718
  - 1.575993424654007
  - 1.599206054210663
  - 1.9052616834640503
  - 1.6414532661437988
  - 1.5880888223648073
  - 1.6019260048866273
  - 1.676269602775574
  - 1.632292699813843
  - 1.6810816407203675
  - 2.14278244972229
  - 1.9763715684413912
  - 2.2027434349060058
  - 1.7972332835197449
  - 1.6807234346866609
  - 1.7273550391197205
  - 1.6031208217144013
  - 1.876774948835373
  - 1.6766854703426362
  - 1.6536889016628267
  - 1.587285763025284
  - 3.6605342626571655
  - 1.9279241800308229
  - 2.453061830997467
  validation_losses:
  - 0.40087416768074036
  - 0.4459689259529114
  - 0.469806969165802
  - 0.4956423342227936
  - 0.5142064690589905
  - 0.4066113531589508
  - 0.4007728397846222
  - 0.39659202098846436
  - 0.4051744341850281
  - 0.423616498708725
  - 0.629080593585968
  - 0.4104514420032501
  - 0.40971383452415466
  - 0.4141876697540283
  - 0.4351081848144531
  - 0.41716378927230835
  - 0.4097379148006439
  - 0.43433961272239685
  - 0.4101560115814209
  - 0.4197368323802948
  - 0.41272592544555664
  - 0.4502362608909607
  - 0.4351120591163635
  - 0.4071017801761627
  - 0.41467076539993286
  - 0.4285013675689697
  - 0.6106967329978943
  - 0.40658047795295715
  - 0.41350293159484863
  - 0.4087676405906677
  - 0.4005172848701477
  - 0.9910390377044678
  - 0.4126853048801422
  - 0.427700400352478
  - 0.45854997634887695
  - 0.40152060985565186
  - 0.41638168692588806
  - 0.41563093662261963
  - 0.39914068579673767
  - 0.41795316338539124
  - 0.40228140354156494
  - 0.41522207856178284
  - 0.41346970200538635
  - 0.4085710346698761
  - 0.4085276126861572
  - 0.40995290875434875
  - 0.40522727370262146
  - 0.39941564202308655
loss_records_fold4:
  train_losses:
  - 1.6131254136562347
  - 1.6785488903522492
  - 1.6817889392375946
  - 1.5900470197200776
  - 1.676598870754242
  - 1.6672174692153932
  - 1.581393188238144
  - 1.5686965465545655
  - 1.8571256697177887
  - 1.613042563199997
  - 1.608125936985016
  - 1.5629142999649048
  - 1.6727153599262239
  - 1.6782804250717165
  - 1.5611366569995881
  - 1.6167764365673065
  - 1.7210372507572176
  - 1.610998445749283
  - 1.7160891234874727
  - 2.6717435598373416
  - 1.732857871055603
  - 1.9205610930919648
  - 1.6045618414878846
  - 1.632328885793686
  - 1.786926591396332
  - 1.5881039559841157
  - 1.648400455713272
  - 1.6694696962833406
  - 1.6084868609905243
  - 1.5905142724514008
  - 1.5801331639289857
  - 1.588649082183838
  - 1.6572125792503358
  - 1.6513324081897736
  - 1.8672836124897003
  - 1.5742812514305116
  - 1.6318861246109009
  - 1.6288625001907349
  - 1.6320451498031616
  - 1.6288197696208955
  - 1.6093910157680513
  - 1.628614002466202
  - 1.7330289483070374
  - 1.657043892145157
  - 1.6612988054752351
  - 1.639080309867859
  - 1.693859660625458
  - 1.667627000808716
  - 1.6402747809886933
  - 1.5565010845661165
  - 1.6904384672641755
  - 1.6170874238014221
  - 1.7715082228183747
  - 1.70170875787735
  - 1.7752556562423707
  - 1.7426669716835024
  - 1.685223925113678
  - 1.6072898268699647
  - 1.5997548758983613
  - 1.5984687626361849
  - 1.618000113964081
  - 1.7058538913726808
  - 1.5949127197265627
  - 1.6756482660770418
  - 1.623518341779709
  - 1.5870017230510713
  - 1.6385289788246156
  validation_losses:
  - 0.40639859437942505
  - 0.4057195782661438
  - 0.4166600704193115
  - 0.40647101402282715
  - 0.5225279927253723
  - 0.4340795874595642
  - 0.42334282398223877
  - 0.41855284571647644
  - 0.43238893151283264
  - 0.4041346311569214
  - 0.4007934629917145
  - 0.398678183555603
  - 0.403818815946579
  - 0.4259435832500458
  - 0.40358465909957886
  - 0.4102098047733307
  - 0.4248671531677246
  - 0.4112352430820465
  - 0.3980441391468048
  - 0.42614829540252686
  - 0.4185194671154022
  - 0.4053526818752289
  - 0.39805859327316284
  - 0.4054541289806366
  - 0.3968040943145752
  - 0.4070909023284912
  - 0.4079649746417999
  - 0.42479264736175537
  - 0.4208143353462219
  - 0.40090498328208923
  - 0.4021877646446228
  - 0.4228213429450989
  - 0.44127699732780457
  - 0.40840837359428406
  - 0.4089778661727905
  - 0.394334614276886
  - 0.4121692478656769
  - 0.4107987582683563
  - 0.4039161205291748
  - 0.4086841344833374
  - 0.407509446144104
  - 0.40303125977516174
  - 0.4456743001937866
  - 0.40718114376068115
  - 0.40076228976249695
  - 0.40418541431427
  - 0.41001299023628235
  - 0.43508264422416687
  - 0.4057794511318207
  - 0.403410941362381
  - 0.4177417755126953
  - 0.41945531964302063
  - 0.41226068139076233
  - 0.42359039187431335
  - 0.4255131781101227
  - 0.41345828771591187
  - 0.40014055371284485
  - 0.42142337560653687
  - 0.40806934237480164
  - 0.4052011966705322
  - 0.4257960319519043
  - 0.40992558002471924
  - 0.4086100459098816
  - 0.4099494218826294
  - 0.41179463267326355
  - 0.40937769412994385
  - 0.3969576358795166
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 36 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 48 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 67 epochs
training_metrics:
  fold_eval_accs: '[0.8507718696397941, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.02247191011235955, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8572415459791456
  mean_f1_accuracy: 0.00449438202247191
  total_train_time: '0:17:14.147336'
