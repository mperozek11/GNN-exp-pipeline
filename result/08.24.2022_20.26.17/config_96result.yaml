config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:39:03.946126'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_96fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_96fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_96fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_96fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_96fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_96fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_96fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_96fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_96fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_96fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.347307732701302
  - 6.2092872917652135
  - 5.943481141328812
  - 6.102988886833192
  - 6.306648942828179
  - 5.9679857909679415
  - 6.071474146842957
  - 6.016967859864235
  - 5.960729134082794
  - 6.826531526446343
  - 5.7635013431310655
  - 5.7150164425373084
  - 5.93324742615223
  - 5.760323306918145
  - 5.70932993888855
  - 5.624917986989022
  - 5.689847394824028
  - 5.612367552518845
  - 5.6315032869577415
  - 5.605901911854744
  - 5.641018944978715
  - 5.6674902737140656
  - 5.740999615192414
  validation_losses:
  - 0.42666980624198914
  - 0.44411107897758484
  - 0.3925253748893738
  - 0.4013476073741913
  - 0.39624449610710144
  - 0.39987847208976746
  - 0.4056594669818878
  - 0.3962009847164154
  - 0.40291082859039307
  - 0.3876905143260956
  - 0.4082931876182556
  - 0.3928365707397461
  - 0.3863254189491272
  - 0.3899632692337036
  - 0.38755860924720764
  - 0.3889152407646179
  - 0.40217316150665283
  - 0.40761253237724304
  - 0.3923492133617401
  - 0.3898228108882904
  - 0.39263612031936646
  - 0.3924235701560974
  - 0.38773322105407715
loss_records_fold1:
  train_losses:
  - 5.6680445134639745
  - 5.719559589028359
  - 5.570246481895447
  - 5.623079317808152
  - 5.583349633216859
  - 5.6365612000226974
  - 5.545984250307083
  - 5.682407027482987
  - 5.655855986475945
  - 5.672390645742417
  - 5.648609328269959
  validation_losses:
  - 0.3849600851535797
  - 0.39203134179115295
  - 0.3962627053260803
  - 0.3859046995639801
  - 0.38429686427116394
  - 0.38678857684135437
  - 0.38890624046325684
  - 0.3860000967979431
  - 0.3918999433517456
  - 0.3888280391693115
  - 0.38877978920936584
loss_records_fold2:
  train_losses:
  - 5.6371063709259035
  - 5.606230381131173
  - 5.623273134231567
  - 5.611580136418343
  - 5.63598997592926
  - 5.704102194309235
  - 5.639989286661148
  - 5.608561927080155
  - 5.719754824042321
  - 5.627461516857148
  - 5.67525404393673
  - 5.635915634036064
  - 5.675877940654755
  - 5.583509397506714
  - 5.617065379023552
  validation_losses:
  - 0.3889942467212677
  - 0.38431423902511597
  - 0.38546547293663025
  - 0.38784974813461304
  - 0.3924298584461212
  - 0.5254283547401428
  - 0.39200159907341003
  - 0.38728150725364685
  - 0.43488216400146484
  - 0.39917588233947754
  - 0.39052918553352356
  - 0.38893964886665344
  - 0.3855437636375427
  - 0.3840677738189697
  - 0.3847876787185669
loss_records_fold3:
  train_losses:
  - 5.614032036066056
  - 5.600934144854546
  - 5.623708108067513
  - 5.61365514099598
  - 5.613250716030598
  - 5.633229237794876
  - 5.576468726992608
  - 5.621245697140694
  - 5.570953875780106
  - 5.602125823497772
  - 5.555536988377572
  - 5.52798222899437
  - 5.592172876000404
  - 5.575566446781159
  - 5.537200412154198
  - 5.58719409108162
  - 5.531194594502449
  - 5.578140082955361
  - 5.560388746857644
  - 5.601721486449242
  - 5.554227760434151
  - 5.516813334822655
  - 5.49366170167923
  - 5.454493120312691
  - 5.50585016310215
  - 5.507609996199609
  - 5.607463145256043
  - 5.7017000198364265
  - 5.5481806442141535
  - 5.576005926728249
  - 5.608883363008499
  - 5.583170838654041
  - 5.550894549489022
  - 5.644861146807671
  - 5.564668306708336
  - 5.581159314513207
  - 5.567477932572365
  - 5.584698033332825
  - 5.6086024940013885
  - 5.510461643338203
  - 5.540045756101609
  - 5.565272650122643
  - 5.504909038543701
  - 5.571112388372422
  - 5.555229735374451
  - 5.53610866367817
  - 5.624456471204758
  - 5.542758405208588
  - 5.572856992483139
  - 5.510257476568222
  - 5.560797929763794
  - 5.5699537158012395
  - 5.533304876089097
  - 5.482651773095132
  - 5.549816378951073
  - 5.498242563009263
  - 5.4949108302593235
  - 5.57294891178608
  - 5.546853160858155
  - 5.469299878180028
  - 5.53725822865963
  - 5.638683480024338
  - 5.573427459597588
  - 5.575015583634377
  - 5.574030315876008
  - 5.574002301692963
  - 5.543968290090561
  - 5.553029125928879
  - 5.536551377177239
  - 5.537978732585907
  - 5.45360532104969
  - 5.496404302120209
  - 5.523865485191346
  - 5.534525552392006
  - 5.5255347222089775
  - 5.537237051129342
  - 5.574565976858139
  - 5.5302268952131275
  - 5.623270317912102
  - 5.6244414240121845
  - 5.578225848078728
  - 5.489205819368363
  - 5.47831276357174
  - 5.5202014446258545
  - 5.476896843314171
  - 5.552328372001648
  - 5.535466295480728
  - 5.5387084007263185
  - 5.466164863109589
  - 5.795761209726334
  - 5.553403142094613
  - 5.672880885004997
  - 5.542797729372978
  - 5.470851641893387
  - 5.495576614141465
  - 5.548944908380509
  - 5.517670753598214
  - 5.5173151999712
  - 5.447562742233277
  - 5.460360449552536
  validation_losses:
  - 0.37824299931526184
  - 0.9059460163116455
  - 0.9392854571342468
  - 0.7218033671379089
  - 0.614185094833374
  - 1.1244127750396729
  - 1.211687684059143
  - 1.3462868928909302
  - 0.3921496868133545
  - 0.6652968525886536
  - 0.5126502513885498
  - 1.0402612686157227
  - 4.836754322052002
  - 1.4204325675964355
  - 1.6471601724624634
  - 0.967276930809021
  - 0.699002206325531
  - 1.2513808012008667
  - 1.6832481622695923
  - 1.3036549091339111
  - 1.9067004919052124
  - 3.7409865856170654
  - 2.250312089920044
  - 3.6884925365448
  - 0.375797837972641
  - 2.0655620098114014
  - 0.37258562445640564
  - 0.5324169397354126
  - 1.252357006072998
  - 1.3822109699249268
  - 39.18923568725586
  - 1.4530614614486694
  - 0.3713555335998535
  - 0.3684004545211792
  - 0.42356300354003906
  - 0.6218332052230835
  - 0.6162432432174683
  - 3.4799387454986572
  - 4.779290199279785
  - 4.760975360870361
  - 13.194300651550293
  - 5.845643997192383
  - 0.37598124146461487
  - 0.9618867635726929
  - 39.687461853027344
  - 30.001920700073242
  - 3.2904908657073975
  - 27.11916160583496
  - 18.667844772338867
  - 33.4084358215332
  - 20.61969566345215
  - 10.911027908325195
  - 29.595935821533203
  - 32.95061492919922
  - 17.6065731048584
  - 12.333261489868164
  - 18.04302978515625
  - 19.83418846130371
  - 49.50844955444336
  - 18.546903610229492
  - 22.49365234375
  - 24.212888717651367
  - 5.678447246551514
  - 1.9267287254333496
  - 1.124335527420044
  - 0.6755619645118713
  - 1.9274691343307495
  - 0.4387303590774536
  - 0.4915165603160858
  - 0.7290703654289246
  - 1.0346931219100952
  - 0.5915451049804688
  - 0.6071028709411621
  - 0.9136708974838257
  - 0.904541015625
  - 0.41559386253356934
  - 0.564378023147583
  - 0.393127977848053
  - 0.430926114320755
  - 0.3794892728328705
  - 0.37533506751060486
  - 5.910209655761719
  - 0.42505669593811035
  - 0.39110520482063293
  - 0.391107439994812
  - 0.47166693210601807
  - 4.751511573791504
  - 1.2092798948287964
  - 7.408150672912598
  - 0.40477219223976135
  - 0.38638603687286377
  - 0.3808387219905853
  - 0.4041003882884979
  - 0.39908090233802795
  - 0.40848681330680847
  - 0.3902267813682556
  - 1.1847944259643555
  - 0.4671652913093567
  - 1.215571403503418
  - 1.1822446584701538
loss_records_fold4:
  train_losses:
  - 5.575208410620689
  - 5.491370061039925
  - 5.553274947404862
  - 5.527497798204422
  - 5.50587198138237
  - 5.607618808746338
  - 5.491858142614365
  - 5.501357838511467
  - 5.4974630028009415
  - 5.5911492049694065
  - 5.498047950863839
  validation_losses:
  - 0.3893616795539856
  - 0.745373010635376
  - 0.36782634258270264
  - 0.36806201934814453
  - 0.3680902123451233
  - 0.3714437782764435
  - 0.37379348278045654
  - 0.3806867003440857
  - 0.37053629755973816
  - 0.36518943309783936
  - 0.3712674081325531
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8507718696397941,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.08421052631578949, 0.0]'
  mean_eval_accuracy: 0.8568984928059036
  mean_f1_accuracy: 0.016842105263157898
  total_train_time: '0:17:48.961261'
