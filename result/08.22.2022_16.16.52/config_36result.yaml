config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:56:21.381661'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_36fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_36fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_36fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_36fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_36fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_36fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_36fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_36fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_36fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_36fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.554775819182396
  - 6.527487149834633
  - 6.406332632899285
  - 6.131015723943711
  - 6.131636640429497
  - 6.258740848302842
  - 6.334289932250977
  - 6.1664370775222785
  - 6.248746749758721
  - 6.2814379900693895
  - 6.073045948147774
  - 6.042844396829605
  - 6.179733109474182
  - 6.119528198242188
  - 5.922218865156174
  - 6.199294170737267
  - 6.104852730035782
  - 5.877182719111443
  - 5.975422677397728
  - 5.936842119693757
  - 5.9332051366567615
  validation_losses:
  - 0.40221652388572693
  - 0.4815121591091156
  - 0.41502466797828674
  - 0.40237513184547424
  - 0.40369877219200134
  - 0.3904414772987366
  - 0.4319475591182709
  - 0.38998687267303467
  - 0.43026119470596313
  - 0.39049655199050903
  - 0.3935413658618927
  - 0.3994395136833191
  - 0.4118327498435974
  - 0.40597620606422424
  - 0.48396193981170654
  - 0.39506399631500244
  - 0.3909672498703003
  - 0.3952523469924927
  - 0.3919963240623474
  - 0.3932346701622009
  - 0.39865434169769287
loss_records_fold1:
  train_losses:
  - 5.959349793195725
  - 5.973919540643692
  - 6.022965452075005
  - 5.916986411809922
  - 6.064253950119019
  - 5.987942489981652
  - 5.897444489598275
  - 6.015363025665284
  - 5.906402388215065
  - 6.0555562525987625
  - 5.910378414392472
  validation_losses:
  - 0.40689533948898315
  - 0.4032709300518036
  - 0.38838064670562744
  - 0.3988288342952728
  - 0.3974546492099762
  - 0.3924581706523895
  - 0.39196982979774475
  - 0.39860162138938904
  - 0.4083399176597595
  - 0.39496687054634094
  - 0.3892258405685425
loss_records_fold2:
  train_losses:
  - 5.876672768592835
  - 5.917800453305245
  - 6.016653940081596
  - 5.9394760787487035
  - 5.977198138833046
  - 5.875136557221413
  - 5.950649738311768
  - 6.005576020479203
  - 6.0336764633655555
  - 6.230667459964753
  - 6.127819812297822
  - 5.9631125926971436
  - 5.854256916046143
  - 5.716959822177888
  - 5.999466207623482
  - 5.834685948491097
  - 5.821738928556442
  - 5.998392280936241
  - 5.846295189857483
  - 5.879786804318428
  - 5.927139329910279
  - 5.792929863929749
  - 5.8550749152898796
  validation_losses:
  - 0.38878747820854187
  - 0.3900250792503357
  - 0.4312284290790558
  - 0.3861331045627594
  - 0.3825443387031555
  - 0.3921053111553192
  - 0.39019566774368286
  - 0.39224398136138916
  - 0.39127206802368164
  - 0.542152464389801
  - 0.39419877529144287
  - 0.3900702893733978
  - 0.39401480555534363
  - 0.3901086449623108
  - 0.4017252027988434
  - 0.3829852044582367
  - 0.4018232226371765
  - 0.3909587264060974
  - 0.3916870653629303
  - 0.3863292634487152
  - 0.39191052317619324
  - 0.39025330543518066
  - 0.3927500247955322
loss_records_fold3:
  train_losses:
  - 6.001422932744027
  - 5.925380140542984
  - 6.010352754592896
  - 5.967591732740402
  - 6.0287669688463215
  - 5.916555672883987
  - 5.8021600395441055
  - 5.905069276690483
  - 6.041077643632889
  - 6.236809131503105
  - 5.938963869214058
  - 5.785369637608529
  - 5.9810676157474525
  - 5.947682005167008
  - 5.9274899661540985
  validation_losses:
  - 0.36415788531303406
  - 0.3864312767982483
  - 0.372942179441452
  - 0.37609565258026123
  - 0.37139615416526794
  - 0.38585928082466125
  - 0.37535029649734497
  - 0.38071030378341675
  - 0.9682077169418335
  - 0.380924254655838
  - 0.37829673290252686
  - 0.37148982286453247
  - 0.37486565113067627
  - 0.3790566921234131
  - 0.372955858707428
loss_records_fold4:
  train_losses:
  - 5.939320313930512
  - 5.968199071288109
  - 5.824348786473275
  - 5.866853523254395
  - 5.9711833745241165
  - 6.033604985475541
  - 5.907046636939049
  - 5.989690661430359
  - 5.7320153683424
  - 5.914910715818405
  - 5.884977015852929
  - 5.911550164222717
  - 5.7413278281688696
  - 5.853283280134201
  - 5.839966326951981
  - 5.856159636378289
  - 5.957215604186058
  - 5.806367662549019
  - 5.953806707262993
  - 5.834711191058159
  - 5.885703057050705
  - 5.954496464133263
  - 5.888386201858521
  - 5.7953508555889135
  - 5.9515638828277595
  - 5.711398240923882
  validation_losses:
  - 0.381497323513031
  - 0.37772831320762634
  - 0.38104087114334106
  - 0.3781071603298187
  - 0.3938227593898773
  - 0.38113442063331604
  - 0.3865240216255188
  - 0.3764667510986328
  - 0.392424613237381
  - 0.37932148575782776
  - 0.3841780424118042
  - 0.37885990738868713
  - 0.3726087510585785
  - 0.3853658139705658
  - 0.38253891468048096
  - 0.38545361161231995
  - 0.3787783980369568
  - 0.3836193382740021
  - 0.37016594409942627
  - 0.39162978529930115
  - 0.38297438621520996
  - 0.3803374767303467
  - 0.3744248151779175
  - 0.3753693401813507
  - 0.3847499489784241
  - 0.3818728029727936
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:10:12.744470'
