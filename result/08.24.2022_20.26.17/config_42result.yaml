config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:23:36.297053'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_42fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_42fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_42fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_42fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_42fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_42fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_42fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_42fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_42fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_42fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 28.20080604553223
  - 9.943247544765473
  - 7.573865520954133
  - 3.7391233861446382
  - 3.0627760708332064
  - 2.7778954803943634
  - 3.4862533748149875
  - 2.423894852399826
  - 2.085442477464676
  - 4.364235210418701
  - 3.4971418380737305
  - 1.8096603631973267
  - 1.8467652797698975
  - 1.8541091322898866
  - 1.6732008010149002
  - 1.5823531448841095
  - 4.6706122964620596
  - 2.0774568170309067
  - 3.5259238302707674
  - 2.3331636488437653
  - 1.8786646008491517
  - 2.766679757833481
  - 5.135605984926224
  - 4.258260428905487
  - 3.8086015760898593
  - 4.109073132276535
  - 3.1674137592315676
  - 1.9590050637722016
  - 1.805037432909012
  - 1.5472279965877533
  - 1.56913078725338
  validation_losses:
  - 3.660343885421753
  - 4.645028114318848
  - 3.061748504638672
  - 0.4962639808654785
  - 0.446954607963562
  - 0.6299033761024475
  - 0.9649681448936462
  - 0.68467116355896
  - 0.6593145728111267
  - 0.8613815307617188
  - 0.48342207074165344
  - 0.3854821026325226
  - 0.4253392815589905
  - 0.40087029337882996
  - 0.38525131344795227
  - 0.38308432698249817
  - 0.4490125775337219
  - 0.41293033957481384
  - 0.4886998236179352
  - 0.4132750332355499
  - 0.667543351650238
  - 0.7355222702026367
  - 0.4243064224720001
  - 0.3816485106945038
  - 0.4277937114238739
  - 0.4249383807182312
  - 0.40185728669166565
  - 0.4016534388065338
  - 0.3787596523761749
  - 0.3824222981929779
  - 0.3899625241756439
loss_records_fold1:
  train_losses:
  - 1.6827430427074432
  - 1.6402351677417757
  - 1.6741524875164033
  - 1.4797063410282136
  - 1.629767143726349
  - 2.509992176294327
  - 2.3350131809711456
  - 2.2730530738830566
  - 2.2398071467876437
  - 2.203712800145149
  - 3.015530300140381
  - 2.4417624831199647
  - 1.6811508893966676
  - 12.072161781787873
  - 2.256698536872864
  - 1.8949015617370606
  - 1.7208088278770448
  - 1.594226348400116
  - 2.2403797447681426
  - 1.8292217493057252
  - 1.6372859597206117
  - 2.0043980658054354
  - 1.5193635165691377
  - 1.4460655212402345
  - 1.4602502763271332
  - 1.4159830749034883
  - 1.4761765062808991
  - 1.6032710373401642
  - 1.6330482482910158
  - 2.1898957043886185
  - 1.4888293385505678
  - 1.8170099377632143
  - 1.502948135137558
  - 1.8231385290622713
  - 1.725879067182541
  - 1.6918631315231325
  - 2.278772324323654
  - 1.7225188195705414
  - 1.5516007065773012
  - 1.4859782099723817
  - 1.513694602251053
  - 1.444501292705536
  - 2.0419031143188477
  validation_losses:
  - 0.4739021956920624
  - 0.45647895336151123
  - 0.39727431535720825
  - 0.4001321494579315
  - 0.3945205509662628
  - 0.6601545214653015
  - 0.5908740758895874
  - 0.5692068338394165
  - 0.9144015312194824
  - 0.44037359952926636
  - 0.4417262077331543
  - 0.44733938574790955
  - 0.40663230419158936
  - 0.43311184644699097
  - 0.4739375412464142
  - 0.39886078238487244
  - 0.3973260819911957
  - 0.44972196221351624
  - 0.45406049489974976
  - 0.40480124950408936
  - 0.4397162199020386
  - 0.517762303352356
  - 0.393833190202713
  - 0.3927614688873291
  - 0.392021119594574
  - 0.40161293745040894
  - 0.4943106472492218
  - 0.39586853981018066
  - 0.39392557740211487
  - 0.3938538432121277
  - 0.39384523034095764
  - 0.4210048019886017
  - 0.39389166235923767
  - 0.40436798334121704
  - 0.3962293863296509
  - 0.4023890495300293
  - 0.5157741904258728
  - 0.4018999934196472
  - 0.39368531107902527
  - 0.3920542299747467
  - 0.39271166920661926
  - 0.3937625586986542
  - 0.39588290452957153
loss_records_fold2:
  train_losses:
  - 1.8825613856315613
  - 1.712398236989975
  - 2.398038733005524
  - 1.5251649022102356
  - 1.7447924196720124
  - 1.602716714143753
  - 1.5886956095695497
  - 2.3882916390895845
  - 2.3707330882549287
  - 1.8177619099617006
  - 1.5760984659194948
  - 1.5247031450271606
  - 1.9218120574951172
  - 1.609140905737877
  - 2.2457235872745516
  - 1.7714322060346603
  - 1.9201224148273468
  - 1.7883425652980804
  - 1.6223138749599457
  - 1.473351389169693
  - 1.557477378845215
  - 1.7538905024528504
  - 1.5328997552394867
  - 1.4702856421470643
  - 1.5658231794834139
  - 1.5284379482269288
  - 1.4612359941005708
  - 1.5440868675708772
  - 2.269806891679764
  - 1.724277910590172
  - 2.1525602221488955
  - 1.986387622356415
  - 1.518781876564026
  - 1.7955690503120423
  - 1.588569337129593
  - 2.9239273548126223
  - 1.6598499178886414
  - 1.5916946709156037
  - 1.5112085938453674
  - 1.5129044234752655
  - 1.4748398005962373
  - 1.4748292088508608
  - 1.5920939475297928
  - 1.5831603109836578
  - 1.6115021407604218
  - 1.501606982946396
  - 1.6358464807271957
  - 1.5332039594650269
  - 1.4731321811676026
  - 1.4634896874427796
  - 1.4962057113647462
  - 1.51359521150589
  - 1.501710903644562
  - 1.5192435681819916
  - 1.5385292947292328
  - 1.4907544016838075
  - 1.4805727124214174
  - 1.4515897154808046
  - 1.970903652906418
  - 1.5452734291553498
  - 1.5677172958850862
  - 1.5131014525890352
  - 3.2783010601997375
  - 2.352556657791138
  - 3.7711695194244386
  - 1.8618507385253906
  - 2.999549382925034
  - 1.8337649762630464
  - 1.9254194974899292
  - 1.549955588579178
  - 1.534816199541092
  - 2.1946031987667083
  - 2.013332682847977
  validation_losses:
  - 0.6002115607261658
  - 0.37613943219184875
  - 0.3856869339942932
  - 0.37404105067253113
  - 0.4339023530483246
  - 0.40642228722572327
  - 0.37896010279655457
  - 0.4278844892978668
  - 0.3831292688846588
  - 0.4488701820373535
  - 0.37575939297676086
  - 0.388175368309021
  - 0.3771577775478363
  - 0.3892019987106323
  - 0.45041966438293457
  - 0.3746340274810791
  - 0.3835335671901703
  - 0.38120707869529724
  - 0.37589678168296814
  - 0.3764284551143646
  - 0.39027923345565796
  - 0.3760837912559509
  - 0.3947472870349884
  - 0.38157159090042114
  - 0.3758631944656372
  - 0.37694355845451355
  - 0.3733138144016266
  - 0.38587531447410583
  - 0.4232792258262634
  - 0.4032291769981384
  - 0.37548425793647766
  - 0.4916848838329315
  - 0.3875696659088135
  - 0.4263037443161011
  - 0.41124463081359863
  - 0.3790677487850189
  - 0.3791150152683258
  - 0.37774863839149475
  - 0.4387337267398834
  - 0.3750280737876892
  - 0.3876839280128479
  - 0.3857729136943817
  - 0.39717164635658264
  - 0.3763152062892914
  - 0.37674766778945923
  - 0.4304623305797577
  - 0.39535051584243774
  - 0.3791310787200928
  - 0.4048854112625122
  - 0.3899291157722473
  - 0.3753179609775543
  - 0.38426777720451355
  - 0.3761690557003021
  - 0.40959078073501587
  - 0.4832802414894104
  - 0.3864050507545471
  - 0.3801327049732208
  - 0.37425315380096436
  - 0.43392083048820496
  - 0.37573811411857605
  - 0.375887393951416
  - 0.3867129385471344
  - 1.3611115217208862
  - 2.0755128860473633
  - 0.5141785144805908
  - 0.3806702196598053
  - 0.39635854959487915
  - 0.3777068555355072
  - 0.3780428469181061
  - 0.38212209939956665
  - 0.38682258129119873
  - 0.3841896653175354
  - 0.38279005885124207
loss_records_fold3:
  train_losses:
  - 2.2771232008934024
  - 1.5279433965682985
  - 1.5661677420139313
  - 1.540584769845009
  - 1.4692702353000642
  - 1.5486678183078766
  - 1.4817617595195771
  - 1.5604628086090089
  - 1.5623185098171235
  - 1.4922672808170319
  - 1.7878087520599366
  - 1.5613898038864136
  - 1.526446795463562
  - 1.7285516619682313
  - 1.5364644885063172
  - 1.4655794441699983
  - 1.4642146348953249
  - 1.4637967467308046
  - 1.5769635558128359
  - 1.4973089039325715
  - 1.9284022450447083
  - 1.9740326404571533
  - 1.4733954548835755
  - 1.49540890455246
  - 1.5217936456203462
  - 1.753287661075592
  - 1.576488095521927
  - 1.5456299901008608
  - 1.508034199476242
  - 1.4990487098693848
  - 1.7172564446926117
  - 1.5602991402149202
  validation_losses:
  - 0.38834279775619507
  - 0.4380805194377899
  - 0.4091939330101013
  - 0.3991812467575073
  - 0.3860872685909271
  - 0.38851118087768555
  - 0.38116711378097534
  - 0.38688546419143677
  - 0.3997228741645813
  - 0.406045138835907
  - 0.39126476645469666
  - 0.39032799005508423
  - 0.4162812829017639
  - 0.38477417826652527
  - 0.3874998092651367
  - 0.392398864030838
  - 0.3885645866394043
  - 0.3887086808681488
  - 0.40728285908699036
  - 0.3914225995540619
  - 0.37762007117271423
  - 0.3943333625793457
  - 0.41200748085975647
  - 0.4487048387527466
  - 0.4095381498336792
  - 0.42842310667037964
  - 0.40968117117881775
  - 0.4109070599079132
  - 0.39532729983329773
  - 0.4031943380832672
  - 0.3974478542804718
  - 0.3952379822731018
loss_records_fold4:
  train_losses:
  - 1.4821129739284515
  - 1.5541442453861238
  - 1.5937857508659363
  - 1.4719762831926346
  - 1.5011488616466524
  - 1.5251132428646088
  - 1.4989135921001435
  - 1.516829425096512
  - 1.71422655582428
  - 1.5099639177322388
  - 1.4977819144725801
  - 1.7332030892372132
  - 1.530241534113884
  - 1.6601722240447998
  - 1.5357104778289796
  - 1.5227511525154114
  - 1.5991095066070558
  - 1.600301480293274
  - 1.5637881189584732
  validation_losses:
  - 0.39635008573532104
  - 0.3955966532230377
  - 0.39977020025253296
  - 0.3895009160041809
  - 0.39629021286964417
  - 0.3899862766265869
  - 0.4122028648853302
  - 0.4053271412849426
  - 0.4366757273674011
  - 0.39118969440460205
  - 0.38993218541145325
  - 0.40734636783599854
  - 0.4316577911376953
  - 0.40014395117759705
  - 0.3922423720359802
  - 0.39376941323280334
  - 0.39468368887901306
  - 0.39831072092056274
  - 0.39392897486686707
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 31 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 43 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 73 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 32 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8542024013722127, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.0
  total_train_time: '0:17:36.389210'
