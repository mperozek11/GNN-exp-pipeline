config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:08:14.704289'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_47fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_47fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_47fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_47fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_47fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_47fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_47fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_47fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_47fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_47fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 24.98271760940552
  - 15.768133258819581
  - 15.058181953430177
  - 6.17449872493744
  - 4.457308769226074
  - 5.460803008079529
  - 5.317535245418549
  - 4.694863128662109
  - 2.9956502676010133
  - 2.306564563512802
  - 1.9513375878334047
  - 1.5611076354980469
  - 1.4864445328712463
  - 1.5353732347488405
  - 1.184199458360672
  - 1.0779721021652222
  - 1.1516198933124542
  - 1.8147394001483919
  - 1.2177599728107453
  - 1.30266535282135
  - 1.3180911540985107
  - 1.7503242611885073
  - 2.0544053077697755
  - 3.502492898702622
  - 5.9877073049545295
  - 1.0608170241117478
  - 1.4653186470270159
  - 1.4510313868522644
  - 1.4757300257682802
  - 1.0661086857318878
  - 1.0483706504106522
  - 5.324743330478668
  - 1.2600070238113403
  - 2.257317638397217
  - 1.1993660092353822
  - 1.6479003608226777
  - 3.008150374889374
  - 1.508285027742386
  - 2.118040755391121
  - 0.9726384520530701
  - 1.3374999701976777
  - 1.1499729454517365
  - 2.49381737112999
  - 1.123134970664978
  - 0.9432496011257172
  - 1.2863514125347137
  - 1.1228580236434937
  - 0.9499339520931245
  - 1.1243869841098786
  - 1.019126081466675
  - 1.439485949277878
  - 0.8740106582641602
  - 1.0413985311985017
  - 0.8691867649555207
  - 0.9197793483734131
  - 0.8349647104740143
  - 0.8296865403652192
  - 0.8508159935474396
  - 0.8226287305355072
  - 0.9243700087070466
  - 0.8334590286016464
  - 0.966979604959488
  - 0.8122537136077881
  - 1.2952878415584566
  - 1.431907194852829
  - 1.3767415881156921
  - 1.5516611874103547
  - 1.600366395711899
  - 1.6256451308727264
  - 1.1787513226270676
  - 1.0626936197280885
  - 0.8619698822498322
  - 2.3500992834568026
  - 0.958444094657898
  - 0.9606653153896332
  - 0.9456335037946701
  - 1.1060274600982667
  - 0.8849743962287904
  - 0.9050978720188141
  - 0.8813278704881669
  - 1.0785373151302338
  - 0.9417609035968781
  - 0.9349218428134919
  - 0.8869927048683167
  - 0.8983703196048737
  - 0.8555567383766175
  - 0.9146750152111054
  - 0.8491177916526795
  - 0.907865470647812
  - 1.00126571059227
  - 1.0256387054920197
  - 0.8639798939228058
  - 3.833639752864838
  - 2.19557386636734
  - 1.3212410032749178
  - 1.0282119154930116
  - 1.5030354917049409
  - 1.1555964589118959
  - 1.360116308927536
  - 1.0217315793037416
  validation_losses:
  - 6.349708080291748
  - 2.3847086429595947
  - 2.089398145675659
  - 2.2728047370910645
  - 1.7359858751296997
  - 1.200863003730774
  - 1.531207799911499
  - 1.2883816957473755
  - 1.1336965560913086
  - 0.6066708564758301
  - 0.6699471473693848
  - 0.5748177170753479
  - 0.567238450050354
  - 0.5083360075950623
  - 0.4273711144924164
  - 0.4114568829536438
  - 0.8099300265312195
  - 0.5909632444381714
  - 0.5125223994255066
  - 0.6420460343360901
  - 1.0613971948623657
  - 0.7255132794380188
  - 0.5007696151733398
  - 1.536758542060852
  - 0.49862751364707947
  - 0.5042703747749329
  - 0.5208736062049866
  - 0.517681896686554
  - 0.5244622826576233
  - 0.4960281252861023
  - 0.47539767622947693
  - 0.7284584045410156
  - 1.0047717094421387
  - 0.4365137815475464
  - 0.466501921415329
  - 0.40749746561050415
  - 0.40575194358825684
  - 0.5104458332061768
  - 0.44701096415519714
  - 0.43325284123420715
  - 0.4657168984413147
  - 0.40200355648994446
  - 0.4417051672935486
  - 0.44929754734039307
  - 0.395539790391922
  - 0.43931400775909424
  - 0.4007561206817627
  - 0.3977399468421936
  - 0.37844744324684143
  - 0.39170917868614197
  - 0.41939598321914673
  - 0.4139249920845032
  - 0.4854637384414673
  - 0.39490213990211487
  - 0.39558571577072144
  - 0.40640756487846375
  - 0.3986976146697998
  - 0.38183876872062683
  - 0.47430819272994995
  - 0.3995212912559509
  - 0.4114052951335907
  - 0.394910991191864
  - 0.45429128408432007
  - 0.39390239119529724
  - 0.5568519830703735
  - 0.4432249665260315
  - 1.1601734161376953
  - 0.6986732482910156
  - 0.47353395819664
  - 0.3963506817817688
  - 0.43908172845840454
  - 0.41883352398872375
  - 0.38907870650291443
  - 0.4252588748931885
  - 0.3747122585773468
  - 0.4233829975128174
  - 0.4121480882167816
  - 0.3904273509979248
  - 0.4020942151546478
  - 0.4095018208026886
  - 0.4101705849170685
  - 0.3949531018733978
  - 0.4254402220249176
  - 0.4990365505218506
  - 0.4622417092323303
  - 0.5475145578384399
  - 0.41208237409591675
  - 0.43590861558914185
  - 0.3877197802066803
  - 0.45832306146621704
  - 0.44251441955566406
  - 0.43184930086135864
  - 0.4499572515487671
  - 0.6317116022109985
  - 0.4407138228416443
  - 0.417677640914917
  - 0.42400461435317993
  - 0.49585673213005066
  - 0.49468541145324707
  - 0.4323107600212097
loss_records_fold1:
  train_losses:
  - 1.318616145849228
  - 1.154893058538437
  - 0.8568416893482209
  - 0.94817014336586
  - 0.8983152151107788
  - 0.8284327268600464
  - 0.8777277886867524
  - 1.0788482189178468
  - 0.8528144031763077
  - 0.8689746797084809
  - 0.8112264543771744
  - 2.2313065826892853
  - 3.2322108864784242
  - 9.584148794412613
  - 1.3143240094184876
  - 2.9817057251930237
  - 1.5360998392105103
  - 0.998112964630127
  - 0.9592784285545349
  - 0.9928358912467957
  - 0.855281549692154
  validation_losses:
  - 0.49598392844200134
  - 0.4242612421512604
  - 0.40490347146987915
  - 0.453178733587265
  - 0.4157666862010956
  - 0.46435919404029846
  - 0.3962923288345337
  - 0.39253678917884827
  - 0.4289993345737457
  - 0.41249576210975647
  - 0.40558192133903503
  - 0.7979000806808472
  - 0.49271804094314575
  - 0.5760981440544128
  - 1.959795594215393
  - 0.5441602468490601
  - 0.48479995131492615
  - 0.41178348660469055
  - 0.4205321967601776
  - 0.42306414246559143
  - 0.4133125841617584
loss_records_fold2:
  train_losses:
  - 1.1788671612739563
  - 0.9808962941169739
  - 0.9109053432941437
  - 1.0298203945159912
  - 0.8269733309745789
  - 0.9097129881381989
  - 0.8734242856502533
  - 1.3275560915470124
  - 1.0787512540817261
  - 0.8333193361759186
  - 0.8102260500192643
  - 6.500874662399292
  - 1.7876967996358872
  - 0.9131623089313508
  - 1.0956391155719758
  - 2.137401908636093
  - 1.4334117352962494
  - 0.9925492465496064
  - 0.9394078433513642
  - 2.7350786149501802
  - 1.674090838432312
  - 1.6979370057582857
  - 0.967416250705719
  - 0.9820910632610321
  - 2.9353193640708923
  - 1.508432561159134
  - 2.4460947751998905
  - 7.980912077426911
  - 6.429442000389099
  - 3.7851592510938645
  - 1.7297639369964601
  - 3.139063680171967
  - 1.2529768764972689
  - 2.1216502070426944
  - 10.551006317138672
  - 1.8192370235919952
  - 1.1999701142311097
  - 1.0408128917217254
  - 1.5532719612121584
  - 0.9825893223285675
  - 0.9421475440263749
  - 0.9813552260398866
  - 8.021995508670807
  - 1.1021309971809388
  - 1.1306577265262605
  - 1.0748683154582979
  - 1.000236701965332
  - 1.1176641941070558
  - 0.8973935693502426
  - 1.3156169950962067
  - 0.8877042055130006
  - 0.8585999757051468
  - 0.9629071235656739
  - 0.8776299417018891
  - 0.8589089989662171
  - 0.8608429551124573
  - 2.500421553850174
  - 0.8481399834156037
  - 0.8314266383647919
  validation_losses:
  - 0.40884870290756226
  - 0.4538397490978241
  - 0.4317186772823334
  - 0.4749562442302704
  - 0.38255661725997925
  - 0.38481590151786804
  - 0.3878275156021118
  - 0.40935176610946655
  - 0.38082996010780334
  - 0.3796427249908447
  - 0.38534823060035706
  - 0.40361487865448
  - 0.3805794417858124
  - 0.3951127827167511
  - 0.39193102717399597
  - 0.3916315734386444
  - 0.44628793001174927
  - 0.446790874004364
  - 0.3873558044433594
  - 0.4759189784526825
  - 0.3820701241493225
  - 0.4097767472267151
  - 0.47925999760627747
  - 0.45525580644607544
  - 0.4417837858200073
  - 0.51457679271698
  - 2.591956853866577
  - 5.325634956359863
  - 8.306151390075684
  - 1.1409814357757568
  - 1.1872246265411377
  - 0.6226174235343933
  - 0.8964400291442871
  - 0.5181993842124939
  - 0.4735581874847412
  - 0.5044358968734741
  - 0.4633888602256775
  - 0.44313564896583557
  - 0.40122759342193604
  - 0.4483182728290558
  - 0.4252417981624603
  - 0.3902207314968109
  - 0.4336041808128357
  - 0.41871196031570435
  - 0.4411375820636749
  - 0.42024022340774536
  - 0.4132128357887268
  - 0.42542776465415955
  - 2.1656546592712402
  - 0.3873739242553711
  - 0.4166230261325836
  - 0.3966716229915619
  - 0.4128795564174652
  - 0.39945197105407715
  - 0.4014127254486084
  - 0.40893352031707764
  - 0.40059733390808105
  - 0.4039023220539093
  - 0.39750945568084717
loss_records_fold3:
  train_losses:
  - 0.8721586346626282
  - 0.8586906373500824
  - 0.8443238198757173
  - 0.8831447124481202
  - 0.8618315696716309
  - 2.5742043972015383
  - 1.5488244652748109
  - 2.250966489315033
  - 0.924876880645752
  - 1.0974229574203491
  - 1.2098892271518709
  - 1.0959005594253541
  - 1.0057934880256654
  - 0.8998589813709259
  - 0.9480950713157654
  - 0.9090354681015015
  - 1.1421147882938385
  - 0.867083889245987
  - 0.8287221014499665
  - 0.9048220336437226
  - 0.8959383070468903
  - 0.8609945476055145
  validation_losses:
  - 0.4252583682537079
  - 0.40578535199165344
  - 0.4088438153266907
  - 0.4227217733860016
  - 0.4110261797904968
  - 0.4345604181289673
  - 0.4976749122142792
  - 0.4127052426338196
  - 0.49247562885284424
  - 0.43108484148979187
  - 0.458024799823761
  - 0.4004223644733429
  - 0.4139447510242462
  - 0.41520971059799194
  - 0.40447014570236206
  - 0.4242769181728363
  - 0.40621817111968994
  - 0.40175268054008484
  - 0.4025290012359619
  - 0.4013674557209015
  - 0.4046926498413086
  - 0.4089922308921814
loss_records_fold4:
  train_losses:
  - 0.8738687872886658
  - 3.358644509315491
  - 0.9431211054325104
  - 0.9239181518554688
  - 0.8360853999853135
  - 0.8240923702716828
  - 0.8905668020248414
  - 0.918093204498291
  - 0.8194931447505951
  - 0.8527483880519867
  - 0.8634442627429962
  - 1.232969492673874
  - 0.9028251647949219
  - 0.8720833182334901
  - 0.8552050113677979
  - 0.9026925265789032
  - 0.8127648055553437
  - 0.8479641497135163
  - 0.8081541657447815
  - 0.860209846496582
  - 0.8315873980522156
  - 0.8365730702877046
  - 0.9678905665874482
  - 0.8890515267848969
  - 0.8274084270000458
  - 0.7967678010463715
  - 2.308725744485855
  - 0.9049627602100373
  - 0.8735988497734071
  - 0.8415308237075806
  - 0.8186171740293503
  - 0.8409685790538788
  - 0.8110796809196472
  - 0.8738518714904786
  - 0.9486041367053986
  - 0.878648990392685
  - 0.8859242022037507
  - 0.814641335606575
  - 0.8442895829677582
  - 0.841875320672989
  - 0.9362004935741425
  - 0.8844158589839936
  - 0.872962886095047
  - 0.853079229593277
  - 0.8682814002037049
  - 0.8070344805717469
  - 0.8338405907154084
  - 0.8053870797157288
  - 0.8201521903276444
  - 0.822100591659546
  - 0.8344782948493958
  - 0.8754988133907319
  - 0.8522002995014191
  - 0.8497063219547272
  - 0.8296769142150879
  - 0.9387001395225525
  - 0.8284765124320984
  - 0.8323458373546601
  - 0.8237749159336091
  - 0.8888791799545288
  - 0.8317303359508514
  - 0.8419808894395828
  - 1.468942028284073
  - 1.0854899406433105
  - 1.120635062456131
  - 2.279134434461594
  - 0.8508367478847504
  - 0.9175856351852417
  - 0.8119806408882142
  - 0.826862794160843
  - 0.8221771061420441
  - 1.0254571914672852
  - 1.1044784486293793
  - 0.8835653841495514
  - 1.1299991965293885
  - 0.8973062038421631
  - 0.8292933255434036
  - 0.9207880258560182
  - 0.9397776603698731
  - 0.8821600258350373
  - 0.8629457294940949
  - 1.0397945523262024
  - 0.7968975484371186
  - 0.8625369429588319
  - 0.8354659140110017
  - 0.8327971816062928
  validation_losses:
  - 0.4221581518650055
  - 0.4110814332962036
  - 0.41903939843177795
  - 0.4225873649120331
  - 0.4232417345046997
  - 0.4442884922027588
  - 0.4272610545158386
  - 0.4379837214946747
  - 0.41712531447410583
  - 0.41361087560653687
  - 0.4227106273174286
  - 0.42670324444770813
  - 0.4303204119205475
  - 0.446928471326828
  - 0.4223478138446808
  - 0.41973474621772766
  - 0.4569037854671478
  - 0.42299777269363403
  - 0.4056994616985321
  - 0.4204494059085846
  - 0.41193094849586487
  - 0.45559442043304443
  - 0.43020257353782654
  - 0.4085365831851959
  - 0.4282023310661316
  - 0.4315655529499054
  - 0.41910651326179504
  - 0.40492942929267883
  - 0.4107828140258789
  - 0.4309060573577881
  - 0.4217984080314636
  - 0.4058314263820648
  - 0.41183289885520935
  - 0.43968603014945984
  - 0.4381083846092224
  - 0.41010692715644836
  - 0.4366441071033478
  - 0.4265764057636261
  - 0.40595147013664246
  - 0.42063674330711365
  - 0.42480385303497314
  - 0.4367462396621704
  - 0.4345299303531647
  - 0.40546804666519165
  - 0.4052405059337616
  - 0.4114275574684143
  - 0.43763628602027893
  - 0.4061364233493805
  - 0.40276038646698
  - 0.4165900647640228
  - 0.4056025743484497
  - 0.44426265358924866
  - 0.4349645972251892
  - 0.42447730898857117
  - 0.40394943952560425
  - 0.43350666761398315
  - 0.41503241658210754
  - 0.42649203538894653
  - 0.407803475856781
  - 0.41524195671081543
  - 0.3963838517665863
  - 0.4095946252346039
  - 0.4141373932361603
  - 0.4417659640312195
  - 0.46762117743492126
  - 0.44381818175315857
  - 0.4271203279495239
  - 0.41628456115722656
  - 0.4446201026439667
  - 0.42824679613113403
  - 0.4117227792739868
  - 0.4071202874183655
  - 0.43914714455604553
  - 0.41853317618370056
  - 0.4151691496372223
  - 0.4219587445259094
  - 0.42968612909317017
  - 0.46692827343940735
  - 0.43487977981567383
  - 0.44655942916870117
  - 0.42418569326400757
  - 0.4203878343105316
  - 0.4102042317390442
  - 0.4044211506843567
  - 0.4009109437465668
  - 0.4066683053970337
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 59 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 86 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8576329331046312, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.0
  total_train_time: '0:23:57.750887'
