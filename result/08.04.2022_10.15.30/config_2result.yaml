config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 10:15:54.078247'
fold_0_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_2fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_2fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_2fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_2fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_2fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_2fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_2fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_2fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_2fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_2fold_4_state_dict.pt
loss_records:
  train_losses:
  - 7.013122200965881
  - 7.338601768016815
  - 6.875879794359207
  - 6.797259524464607
  - 6.893489807844162
  - 6.880137547850609
  - 6.893311083316803
  - 6.944324374198914
  - 7.018887996673584
  - 7.101094663143158
  - 6.972210735082626
  - 7.006337940692902
  - 6.872283190488815
  - 6.8144740760326385
  - 6.911008298397064
  - 6.772433340549469
  - 6.852317154407501
  - 6.78970193862915
  - 6.624471411108971
  - 6.800814509391785
  - 6.7254467606544495
  - 6.865497142076492
  - 6.833894342184067
  - 7.106153696775436
  - 7.010302752256393
  - 7.147150546312332
  - 6.8238508850336075
  - 7.000731647014618
  - 6.930445522069931
  - 6.7356521636247635
  - 6.957441747188568
  - 6.865583807229996
  - 7.0415937304496765
  - 6.9718077480793
  - 7.017841875553131
  - 6.893933415412903
  - 6.919321358203888
  - 6.978504121303558
  - 6.834497392177582
  - 6.909787982702255
  - 7.127149909734726
  - 6.999634087085724
  - 6.976831436157227
  - 6.953151971101761
  - 6.995842754840851
  - 7.260805457830429
  - 6.861683756113052
  - 6.966736018657684
  - 7.00376558303833
  - 6.8191369622945786
  - 7.098746746778488
  - 7.048569947481155
  - 6.98482683300972
  - 6.779449999332428
  - 6.732418477535248
  - 6.756413817405701
  - 6.979879915714264
  - 6.989058315753937
  - 7.1587404906749725
  - 6.898639142513275
  - 7.013472944498062
  - 6.990698575973511
  - 6.9883202612400055
  - 6.840161025524139
  - 6.8184963166713715
  - 6.89303532242775
  - 6.707404166460037
  - 6.8508366495370865
  - 6.95183789730072
  - 6.89242896437645
  - 6.890337809920311
  - 6.757480353116989
  - 6.785553276538849
  - 6.896423578262329
  - 6.655548870563507
  - 6.794234901666641
  - 6.66493022441864
  - 6.959794253110886
  - 6.769317209720612
  - 6.980247646570206
  - 6.735222861170769
  - 6.585946634411812
  - 6.833218455314636
  - 6.910014480352402
  - 6.683277010917664
  - 7.04227939248085
  - 6.774787575006485
  - 6.703532472252846
  - 6.9132291078567505
  - 6.861612379550934
  - 6.7712159007787704
  - 6.973429322242737
  - 6.757668197154999
  - 6.925787538290024
  - 6.693902671337128
  - 6.764654904603958
  - 6.802279055118561
  - 6.8834280371665955
  - 6.878896862268448
  - 6.808794230222702
  validation_losses:
  - 0.5611168742179871
  - 0.4850916266441345
  - 0.5260587930679321
  - 0.5169337391853333
  - 0.7390041947364807
  - 0.8198046088218689
  - 0.4614662528038025
  - 0.46956220269203186
  - 0.45693475008010864
  - 0.6382745504379272
  - 0.5782633423805237
  - 0.5723522305488586
  - 0.8935486674308777
  - 0.9389666318893433
  - 0.6727615594863892
  - 0.734108567237854
  - 0.9287694096565247
  - 0.7129456996917725
  - 0.6336272954940796
  - 0.8061981201171875
  - 0.6846567392349243
  - 0.638438880443573
  - 0.5763134360313416
  - 0.45401808619499207
  - 0.47759538888931274
  - 0.6033382415771484
  - 0.717291533946991
  - 2.000648260116577
  - 0.7956075072288513
  - 0.5599582195281982
  - 0.46061673760414124
  - 0.3682233393192291
  - 0.388911634683609
  - 0.4217909276485443
  - 0.36880436539649963
  - 0.45314955711364746
  - 0.40220630168914795
  - 0.40140727162361145
  - 0.43551716208457947
  - 0.4089342951774597
  - 0.7222877740859985
  - 0.5237977504730225
  - 0.42642897367477417
  - 0.40604549646377563
  - 0.37132197618484497
  - 0.3689824938774109
  - 0.4514337182044983
  - 0.5547558665275574
  - 0.43633347749710083
  - 0.4258202314376831
  - 0.7972041964530945
  - 0.4172236919403076
  - 0.3906443119049072
  - 0.3912516236305237
  - 0.4303392469882965
  - 0.4464751183986664
  - 0.38632163405418396
  - 0.37509188055992126
  - 0.3715628385543823
  - 0.4398895502090454
  - 0.3726588487625122
  - 0.48506346344947815
  - 0.45145806670188904
  - 0.5346189737319946
  - 0.5014239549636841
  - 0.38095176219940186
  - 0.4976104199886322
  - 0.4640922248363495
  - 0.4730750322341919
  - 0.406807005405426
  - 0.3874034285545349
  - 0.4229276478290558
  - 0.5143768787384033
  - 0.4618157744407654
  - 0.41290974617004395
  - 0.4601391851902008
  - 0.5070509314537048
  - 0.5144404172897339
  - 0.37519213557243347
  - 0.40278467535972595
  - 0.44626331329345703
  - 0.5478096008300781
  - 0.3768230676651001
  - 0.38402846455574036
  - 0.40250834822654724
  - 0.4756166338920593
  - 0.43280285596847534
  - 0.5503230690956116
  - 1.5555065870285034
  - 0.46456414461135864
  - 1.4300580024719238
  - 1.8849495649337769
  - 1.6327402591705322
  - 1.5550473928451538
  - 3.4198169708251953
  - 1.0098376274108887
  - 0.8154997229576111
  - 1.7945013046264648
  - 0.4568604528903961
  - 1.743994951248169
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8250428816466552, 0.8473413379073756, 0.8456260720411664,
    0.8573883161512027]'
  fold_eval_f1: '[0.023529411764705882, 0.10526315789473682, 0.11881188118811879,
    0.24999999999999994, 0.06741573033707865]'
  mean_eval_accuracy: 0.8466063081702062
  mean_f1_accuracy: 0.11300403623692801
  total_train_time: '0:12:52.613131'
