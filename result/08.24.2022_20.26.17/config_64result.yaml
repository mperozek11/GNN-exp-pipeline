config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:53:58.038309'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_64fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_64fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_64fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_64fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_64fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_64fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_64fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_64fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_64fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_64fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.191236802935601
  - 6.092688581347466
  - 5.963181871175767
  - 5.8143254846334465
  - 5.878691107034683
  - 6.064227518439293
  - 5.9796414345502855
  - 5.771876734495163
  - 5.950656095147133
  - 5.860215318202973
  - 5.672702887654305
  - 5.894222053885461
  - 5.659862208366395
  - 5.843154200911522
  - 5.792079797387124
  - 5.733989942073823
  - 5.669551214575768
  - 5.629148915410042
  - 5.645285424590111
  - 5.626033431291581
  validation_losses:
  - 0.3964521288871765
  - 0.40180087089538574
  - 0.3941037356853485
  - 0.393564909696579
  - 0.3920896351337433
  - 0.3954925239086151
  - 0.4025730490684509
  - 0.39790040254592896
  - 0.39050722122192383
  - 0.4650757610797882
  - 0.4119671881198883
  - 0.41199904680252075
  - 0.39338716864585876
  - 0.46045780181884766
  - 0.4042646288871765
  - 0.3913929760456085
  - 0.3891465365886688
  - 0.39316293597221375
  - 0.38644158840179443
  - 0.3847796618938446
loss_records_fold1:
  train_losses:
  - 5.752446335554123
  - 5.629979622364044
  - 5.615875098109246
  - 5.603415861725807
  - 5.6177129447460175
  - 5.572585505247116
  - 5.735100239515305
  - 5.609490093588829
  - 5.587647032737732
  - 5.677745538949967
  - 5.650276297330857
  validation_losses:
  - 0.3899792432785034
  - 0.41236159205436707
  - 0.389773428440094
  - 0.39372554421424866
  - 0.3944106996059418
  - 0.38940781354904175
  - 0.383811891078949
  - 0.39216968417167664
  - 0.39397481083869934
  - 0.4021492898464203
  - 0.3880675137042999
loss_records_fold2:
  train_losses:
  - 5.61639843583107
  - 5.583215206861496
  - 5.607709559798241
  - 5.536272391676903
  - 5.4896937102079395
  - 5.556534466147423
  - 5.547007316350937
  - 5.6181029140949255
  - 5.540166187286378
  - 5.5963900059461595
  - 5.549285954236985
  - 5.551295569539071
  - 5.6446137011051185
  - 5.556444016098976
  - 5.514391961693764
  - 5.562825250625611
  - 5.539284151792526
  - 5.533458137512207
  - 5.628097587823868
  - 5.491578921675682
  - 5.568739581108094
  - 5.488531881570816
  - 5.5985903412103655
  - 5.4832892477512365
  - 5.552282884716988
  - 5.545896050333977
  - 5.489019107818604
  - 5.494630432128907
  - 5.518852838873864
  - 5.4789110898971565
  - 5.493394884467126
  - 5.520130777359009
  - 5.520647820830345
  - 5.500410288572311
  - 5.462180924415589
  - 5.551103243231774
  - 5.4883992135524755
  - 5.4727379322052006
  - 5.512986066937447
  - 5.492231398820877
  - 5.471462765336037
  - 5.550176945328713
  - 5.676112744212151
  - 5.506067326664925
  - 5.4841483563184745
  - 5.488228610157967
  - 5.55095491707325
  - 5.444550171494484
  - 5.461817038059235
  - 5.41761172413826
  - 5.47624773979187
  - 5.431595253944398
  - 5.432619506120682
  - 5.471588900685311
  - 5.578056654334069
  - 5.415352246165276
  - 5.421253091096879
  - 5.458382105827332
  - 5.455435177683831
  - 5.376400014758111
  - 5.537252002954483
  - 5.43240462243557
  - 5.560112354159355
  - 5.468183308839798
  - 5.471067383885384
  - 5.445102068781853
  - 5.466177609562874
  - 5.527237728238106
  - 5.500408095121384
  - 5.523169559240341
  - 5.437730890512467
  - 5.3842524826526645
  - 5.529750066995621
  - 5.4950381219387054
  - 5.526231217384339
  - 5.593988108634949
  - 5.436724072694779
  - 5.4461676478385925
  - 5.438070431351662
  - 5.440738815069199
  - 5.438984140753746
  - 5.436230584979057
  - 5.429516130685807
  - 5.405584326386452
  - 5.438346937298775
  - 5.45690747499466
  - 5.417508581280709
  - 5.401085230708123
  - 5.622251951694489
  - 5.525708815455437
  - 5.49826447069645
  - 5.394179660081864
  - 5.438109496235848
  - 5.510337409377098
  - 5.486690816283226
  - 5.385136049985886
  - 5.47102661728859
  - 5.463572919368744
  - 5.493722465634346
  - 5.490772816538811
  validation_losses:
  - 0.38313090801239014
  - 0.38449135422706604
  - 0.38110530376434326
  - 0.3845425248146057
  - 0.3845016658306122
  - 0.39329543709754944
  - 0.3853943347930908
  - 0.3999839723110199
  - 0.40968582034111023
  - 0.3814482092857361
  - 0.39106470346450806
  - 0.40044236183166504
  - 0.3880271315574646
  - 0.46354156732559204
  - 0.47101643681526184
  - 0.40734538435935974
  - 0.5049071907997131
  - 0.40381115674972534
  - 0.3965896964073181
  - 0.4205755889415741
  - 0.4196832478046417
  - 0.4205191135406494
  - 0.43615058064460754
  - 0.46355029940605164
  - 0.44948887825012207
  - 0.4478871822357178
  - 0.48733067512512207
  - 0.6544186472892761
  - 0.4827958047389984
  - 0.60459965467453
  - 0.41252997517585754
  - 0.6351010203361511
  - 0.46259331703186035
  - 0.45697131752967834
  - 0.6952479481697083
  - 0.4470079243183136
  - 0.5723385214805603
  - 0.5417699217796326
  - 0.3859405219554901
  - 0.4283139109611511
  - 0.39426472783088684
  - 0.4543517827987671
  - 0.45427370071411133
  - 0.4675774574279785
  - 0.4887593388557434
  - 0.6800477504730225
  - 0.39152562618255615
  - 0.39185643196105957
  - 0.5447415113449097
  - 0.5303179025650024
  - 0.5574102401733398
  - 0.6282854676246643
  - 0.5301344990730286
  - 0.5539404153823853
  - 0.4421406686306
  - 0.4600153863430023
  - 0.5255571603775024
  - 0.5526323318481445
  - 0.4362736940383911
  - 0.4443660080432892
  - 0.5114838480949402
  - 0.43882209062576294
  - 0.4835096299648285
  - 0.41313436627388
  - 0.4690150320529938
  - 0.4473239481449127
  - 0.41115888953208923
  - 0.4121023118495941
  - 0.49521195888519287
  - 0.49002134799957275
  - 1.897576928138733
  - 0.5403106212615967
  - 0.5888610482215881
  - 2.4831011295318604
  - 0.7673752307891846
  - 0.5673043131828308
  - 0.5737227201461792
  - 0.49087268114089966
  - 0.5044310688972473
  - 0.39882194995880127
  - 0.4825630187988281
  - 0.4254350960254669
  - 0.685540497303009
  - 0.4263750910758972
  - 0.43135306239128113
  - 0.4425409138202667
  - 0.4445738196372986
  - 0.4282170534133911
  - 0.4158024191856384
  - 0.44774219393730164
  - 0.48434025049209595
  - 0.5271521806716919
  - 0.40241745114326477
  - 0.4616936445236206
  - 0.5199915170669556
  - 0.5410724878311157
  - 0.4243592321872711
  - 0.5834996700286865
  - 2.1685194969177246
  - 1.2742196321487427
loss_records_fold3:
  train_losses:
  - 5.506476315855981
  - 5.55344686806202
  - 5.4575704604387285
  - 5.519840517640114
  - 5.465434840321541
  - 5.641891425848008
  - 5.488798955082894
  - 5.49154806137085
  - 5.5051487505435945
  - 5.5365926206111915
  - 5.419611683487893
  - 5.450171071290971
  - 5.5276431232690815
  - 5.496092316508293
  - 5.519253906607628
  - 5.462353876233101
  - 5.502664512395859
  - 5.437693697214127
  - 5.512164369225502
  - 5.421613717079163
  - 5.473242378234864
  - 5.431113696098328
  - 5.4732641816139225
  - 5.537645497918129
  - 5.495738074183464
  - 5.472557982802392
  - 5.3830044865608215
  - 5.434353247284889
  - 5.5126708298921585
  - 5.446862617135048
  - 5.4367131978273395
  - 5.499800682067871
  - 5.421587258577347
  - 5.448500728607178
  - 5.468655017018318
  - 5.4508765816688545
  - 5.473025470972061
  - 5.509885111451149
  - 5.424029526114464
  - 5.464681294560433
  - 5.405971336364747
  - 5.424452263116837
  - 5.522608670592309
  - 5.4512659281492235
  - 5.396963593363762
  - 5.441870039701462
  - 5.3956844270229345
  - 5.398980379104614
  - 5.305810838937759
  - 5.536635002493859
  - 5.436035713553429
  - 5.444382157921791
  - 5.497815448045731
  - 5.422738590836525
  - 5.39707405269146
  - 5.334277188777924
  - 5.34519092142582
  - 5.4250670909881595
  - 5.427404737472535
  - 5.413268303871155
  - 5.341693717241288
  - 5.505024096369744
  - 5.485507357120515
  - 5.429812678694725
  - 5.365479293465615
  - 5.417807227373124
  - 5.475350794196129
  - 5.393177199363709
  - 5.413059335947037
  - 5.400233665108681
  - 5.324138203263283
  - 5.386868658661843
  - 5.359945169091225
  - 5.440538415312767
  - 5.334336522221566
  - 5.333995085954666
  - 5.471430468559266
  - 5.483957442641259
  - 5.337431234121323
  - 5.511833617091179
  - 5.388655555248261
  - 5.437736150622368
  - 5.425211596488953
  - 5.423874109983444
  - 5.328447926044465
  - 5.4036929488182075
  - 5.402359023690224
  - 5.4477265000343325
  - 5.347559425234795
  - 5.3912351265549665
  - 5.407000294327736
  - 5.510292285680771
  - 5.364595738053322
  - 5.291135531663895
  - 5.445247399806977
  - 5.418229219317436
  - 5.359984290599823
  - 5.258506751060486
  - 5.668598750233651
  - 5.5109054982662204
  validation_losses:
  - 2.890035390853882
  - 5.893118381500244
  - 4.933897018432617
  - 3.939087152481079
  - 2.467947006225586
  - 0.530540943145752
  - 0.6654795408248901
  - 0.5366379022598267
  - 0.953208863735199
  - 1.3926724195480347
  - 5.011184215545654
  - 5.112742900848389
  - 2.644573926925659
  - 3.9239449501037598
  - 2.504967451095581
  - 3.4301328659057617
  - 0.989682674407959
  - 1.2327220439910889
  - 1.5786175727844238
  - 2.831883668899536
  - 4.329699516296387
  - 7.068445205688477
  - 7.747713088989258
  - 2.073485851287842
  - 4.151791095733643
  - 1.6527386903762817
  - 4.320799350738525
  - 3.1894969940185547
  - 3.500514030456543
  - 3.563992500305176
  - 1.084086537361145
  - 0.7810341119766235
  - 2.1247081756591797
  - 2.1856234073638916
  - 4.283663272857666
  - 4.002423286437988
  - 3.093075752258301
  - 1.2612882852554321
  - 0.7343969941139221
  - 1.0875617265701294
  - 2.5101590156555176
  - 3.4181625843048096
  - 2.0386860370635986
  - 5.206503868103027
  - 2.6557891368865967
  - 2.66422176361084
  - 9.302406311035156
  - 9.874014854431152
  - 0.7696173787117004
  - 0.8715292811393738
  - 2.0078773498535156
  - 1.1705801486968994
  - 1.7850459814071655
  - 6.001666069030762
  - 9.802895545959473
  - 21.148975372314453
  - 2.853963851928711
  - 11.910802841186523
  - 17.020057678222656
  - 6.897360801696777
  - 0.7496752142906189
  - 19.267427444458008
  - 31.111757278442383
  - 14.885149955749512
  - 19.963953018188477
  - 18.804285049438477
  - 10.57226276397705
  - 4.557513236999512
  - 0.8719111084938049
  - 16.987075805664062
  - 6.066057205200195
  - 10.802078247070312
  - 1.2847415208816528
  - 2.137831926345825
  - 2.152545928955078
  - 2.6478559970855713
  - 1.3057726621627808
  - 3.808715343475342
  - 4.025594711303711
  - 2.109766960144043
  - 7.0755295753479
  - 11.423468589782715
  - 4.35624885559082
  - 6.171570301055908
  - 1.4479396343231201
  - 16.811250686645508
  - 10.670587539672852
  - 1.972144603729248
  - 4.462497711181641
  - 2.350954055786133
  - 5.039781093597412
  - 9.075081825256348
  - 4.055354595184326
  - 9.575383186340332
  - 6.829922676086426
  - 8.638818740844727
  - 6.710108280181885
  - 8.171785354614258
  - 6.160353183746338
  - 5.213904857635498
loss_records_fold4:
  train_losses:
  - 5.493624237179756
  - 5.389847433567048
  - 5.377591469883919
  - 5.472721832990647
  - 5.406396979093552
  - 5.389035443961621
  - 5.447816467285157
  - 5.443307355046272
  - 5.379748496413232
  - 5.389175486564636
  - 5.3709009975194935
  - 5.413029432296753
  - 5.378384476900101
  - 5.4378247439861305
  - 5.390934881567955
  - 5.355673828721047
  - 5.478214812278748
  - 5.399494138360024
  - 5.342974993586541
  - 5.39746200144291
  - 5.467398464679718
  - 5.5022310465574265
  - 5.4342245906591415
  - 5.377489131689072
  - 5.427155178785324
  - 5.409620389342308
  - 5.3474358260631565
  - 5.37281041443348
  - 5.328406548500062
  - 5.342060285806657
  - 5.370887565612794
  - 5.5219359010458
  - 5.378481769561768
  - 5.391237264871598
  - 5.371089833974839
  - 5.459537839889527
  - 5.489628648757935
  - 5.374310478568077
  - 5.428609761595727
  - 5.375635644793511
  - 5.367212855815888
  - 5.51618672311306
  - 5.457241263985634
  - 5.39238495528698
  - 5.3752849996089935
  - 5.3958752125501634
  - 5.332502594590188
  - 5.369858151674271
  - 5.335715928673745
  - 5.356526783108712
  - 5.368592387437821
  - 5.345989057421685
  - 5.326860654354096
  - 5.306770560145378
  - 5.397486740350724
  - 5.352100816369057
  - 5.3800673454999925
  - 5.416101053357124
  - 5.359949290752411
  - 5.534599569439888
  - 5.598161759972573
  - 5.417425405979157
  - 5.577690553665161
  - 5.476960891485215
  - 5.464513868093491
  - 5.402211597561837
  - 5.420420509576798
  - 5.34310873746872
  - 5.466131922602654
  - 5.5081157922744755
  - 5.365956577658654
  - 5.4283999621868135
  - 5.554892772436142
  - 5.491441026329994
  - 5.388222202658653
  - 5.4936160236597065
  - 5.378341475129128
  - 5.437644904851914
  - 5.42167307138443
  - 5.3650724530220035
  - 5.484438276290894
  - 5.346020016074181
  - 5.465484270453453
  - 5.507805049419403
  - 5.458516070246697
  - 5.43812410235405
  - 5.366004928946495
  - 5.362378951907158
  - 5.3860662460327156
  - 5.359468261897565
  - 5.406436705589295
  - 5.418763598799706
  - 5.394704720377923
  - 5.4153101503849035
  - 5.3932468593120575
  - 5.368562541902065
  - 5.384994810819626
  - 5.371256098151207
  - 5.380258563160897
  - 5.351104235649109
  validation_losses:
  - 0.686053454875946
  - 0.7116982936859131
  - 0.8447917103767395
  - 0.5612146258354187
  - 0.5719107389450073
  - 0.9783145189285278
  - 0.7950981855392456
  - 0.7475871443748474
  - 0.5548714995384216
  - 0.37115752696990967
  - 0.6563733220100403
  - 0.36821871995925903
  - 1.0558996200561523
  - 0.9361814856529236
  - 0.831875741481781
  - 0.8307101130485535
  - 0.6684027314186096
  - 0.9966817498207092
  - 0.9060559868812561
  - 0.7364016175270081
  - 0.9237830638885498
  - 0.5637436509132385
  - 0.7364004850387573
  - 0.6927639245986938
  - 1.0971468687057495
  - 0.4528172016143799
  - 0.4887244403362274
  - 0.38181522488594055
  - 0.5327617526054382
  - 0.5736677646636963
  - 0.5960449576377869
  - 0.5669273138046265
  - 1.401078462600708
  - 0.41061943769454956
  - 0.7873736619949341
  - 0.9049471020698547
  - 0.5059875845909119
  - 0.46966806054115295
  - 0.41352230310440063
  - 0.4341476559638977
  - 0.46125200390815735
  - 0.3735126256942749
  - 0.44136661291122437
  - 0.5329257249832153
  - 0.8724579215049744
  - 0.5561050772666931
  - 0.43934011459350586
  - 0.47449183464050293
  - 0.5868550539016724
  - 0.5823090672492981
  - 0.5558212995529175
  - 0.5277400612831116
  - 0.5612566471099854
  - 0.40274739265441895
  - 0.3848472237586975
  - 0.5748217701911926
  - 0.6618970632553101
  - 0.5346009135246277
  - 0.5593816637992859
  - 0.6126781702041626
  - 0.3855382204055786
  - 0.3688408434391022
  - 0.3633735477924347
  - 0.36528754234313965
  - 0.38281750679016113
  - 0.38736042380332947
  - 0.4764128029346466
  - 0.547404408454895
  - 0.45372727513313293
  - 0.3761618435382843
  - 0.4853186309337616
  - 0.4333973228931427
  - 0.38880836963653564
  - 0.40194642543792725
  - 0.378265380859375
  - 0.40911316871643066
  - 0.38574644923210144
  - 0.38732054829597473
  - 0.38118988275527954
  - 0.4380524158477783
  - 0.43600648641586304
  - 0.43312501907348633
  - 0.4472380578517914
  - 0.3978755474090576
  - 0.42403483390808105
  - 0.41215312480926514
  - 0.45211678743362427
  - 0.40989983081817627
  - 0.41529756784439087
  - 0.49181878566741943
  - 0.8343952894210815
  - 0.5600858926773071
  - 0.6143270134925842
  - 1.2520904541015625
  - 0.6073639988899231
  - 0.5506995916366577
  - 0.4111035168170929
  - 0.48912641406059265
  - 0.36336326599121094
  - 0.37048473954200745
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8524871355060034, 0.8147512864493996,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.0, 0.0851063829787234, 0.30769230769230765, 0.0]'
  mean_eval_accuracy: 0.8479785208631736
  mean_f1_accuracy: 0.07855973813420622
  total_train_time: '0:34:27.741237'
