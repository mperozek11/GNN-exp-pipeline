config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:15:34.567672'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_94fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_94fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_94fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_94fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_94fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_94fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_94fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_94fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_94fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_94fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 23.806085896492007
  - 10.167945683002472
  - 3.28657802939415
  - 4.107797139883042
  - 6.849042463302613
  - 4.843496930599213
  - 5.021558725833893
  - 3.715213167667389
  - 4.606064707040787
  - 3.271734756231308
  - 2.8236255764961244
  - 3.224424546957016
  - 3.400413799285889
  - 3.982550621032715
  - 4.245261353254318
  - 14.822459173202516
  - 8.488425779342652
  - 3.8978713989257816
  - 1.8853072226047516
  - 1.68586842417717
  - 2.5664617598056796
  - 1.861300939321518
  - 5.288225448131562
  - 4.387996500730515
  - 2.7589888632297517
  - 1.9369363725185396
  - 2.9162902712821963
  - 3.002942663431168
  - 3.1791881382465363
  - 1.7866381704807281
  - 2.236374604701996
  - 2.4316652297973635
  - 2.158183389902115
  - 3.9337724626064303
  - 3.7935029447078707
  - 2.123130178451538
  - 1.950321787595749
  - 1.5740868210792542
  - 2.1541811883449555
  - 3.0883137702941896
  - 3.978621953725815
  - 1.9138704955577852
  - 1.650841408967972
  - 7.126365929841995
  - 4.667221987247467
  - 2.069074332714081
  - 3.7884855568408966
  - 5.396813154220581
  - 2.5512278497219087
  - 2.3737836599349977
  - 1.929545122385025
  - 3.627603566646576
  - 2.1733068704605105
  - 1.7850127279758454
  - 1.7117549598217012
  - 2.037193846702576
  - 2.5786204516887667
  - 3.71823565363884
  - 2.282333564758301
  - 1.7708402454853058
  - 2.1000454008579257
  - 2.36170192360878
  - 3.287232917547226
  - 5.370336377620697
  - 3.5285143315792085
  - 4.340527456998825
  - 2.00035679936409
  - 1.9332031965255738
  - 1.9187216818332673
  - 2.506141293048859
  - 1.936328399181366
  - 1.7264971971511842
  - 1.8180728018283845
  - 1.7757622301578522
  - 2.2020963430404663
  - 1.9757772266864777
  - 1.6455660581588747
  - 2.094166749715805
  - 1.6775358259677888
  - 1.6916557729244233
  - 1.8640416026115418
  - 1.6010223388671876
  - 1.8600980281829835
  - 2.057912343740463
  - 1.9265066623687745
  - 1.6896588563919068
  - 2.9939806640148166
  - 1.7530939102172853
  - 1.6467515349388124
  - 1.771744728088379
  - 1.6093680918216706
  - 1.6543788492679596
  - 1.6521784961223602
  validation_losses:
  - 12.13121509552002
  - 0.7285950183868408
  - 0.47448235750198364
  - 0.95782470703125
  - 0.7684424519538879
  - 0.9819172620773315
  - 0.5052430629730225
  - 0.6393808126449585
  - 0.585696280002594
  - 0.8453863263130188
  - 0.7440303564071655
  - 0.544232189655304
  - 0.6011955738067627
  - 0.4037001430988312
  - 0.4425923526287079
  - 0.5857489109039307
  - 0.4353790283203125
  - 0.4178849160671234
  - 0.3781158924102783
  - 0.4349948465824127
  - 0.4025833010673523
  - 0.4337502121925354
  - 0.4376200735569
  - 0.44339486956596375
  - 0.4164851903915405
  - 0.42255866527557373
  - 0.4002246558666229
  - 0.4490323066711426
  - 0.39881807565689087
  - 0.43554824590682983
  - 0.48861759901046753
  - 0.4084419012069702
  - 0.4071146547794342
  - 0.4242175221443176
  - 0.400147020816803
  - 0.40539464354515076
  - 0.38455504179000854
  - 0.39818552136421204
  - 0.41132766008377075
  - 0.4231453239917755
  - 0.4103234112262726
  - 0.4129343032836914
  - 0.3989894390106201
  - 0.42356911301612854
  - 1.4590307474136353
  - 0.518531322479248
  - 0.4521853029727936
  - 0.49377161264419556
  - 0.4411223828792572
  - 0.42528122663497925
  - 0.4094700217247009
  - 0.40819600224494934
  - 0.41850465536117554
  - 0.40212640166282654
  - 0.4049796760082245
  - 0.4128960967063904
  - 0.40840208530426025
  - 0.4079318046569824
  - 0.42035728693008423
  - 0.42187589406967163
  - 0.4116207957267761
  - 0.39764147996902466
  - 0.42842090129852295
  - 0.6925628781318665
  - 0.4820128083229065
  - 0.4604793190956116
  - 0.39849722385406494
  - 0.4514656662940979
  - 0.4070325791835785
  - 0.4305257201194763
  - 0.4039939343929291
  - 0.4170199930667877
  - 0.39731475710868835
  - 0.416227787733078
  - 0.38939979672431946
  - 0.4099539518356323
  - 0.4037437438964844
  - 0.42641526460647583
  - 0.4196670651435852
  - 0.39803776144981384
  - 0.41113054752349854
  - 0.39474382996559143
  - 0.4015297293663025
  - 0.4044860005378723
  - 0.39793530106544495
  - 0.3927161991596222
  - 0.43974485993385315
  - 0.3951486647129059
  - 0.39895012974739075
  - 0.4062195122241974
  - 0.40744549036026
  - 0.4050038754940033
  - 0.4014110267162323
loss_records_fold1:
  train_losses:
  - 1.622612488269806
  - 1.581489270925522
  - 1.688232058286667
  - 1.7630267560482027
  - 1.6725621938705446
  - 1.6707667231559755
  - 1.7338420629501343
  - 1.758525037765503
  - 1.6860825002193451
  - 1.603298842906952
  - 1.7916282415390015
  - 1.6396244168281555
  - 1.9168405771255494
  - 1.6565515398979187
  - 1.668513035774231
  - 1.6267871141433716
  - 1.5849957406520845
  - 1.634952110052109
  - 1.6544681787490845
  - 1.6549982130527496
  - 1.6352297067642212
  - 2.1537489533424377
  - 1.70227410197258
  - 1.6463634461164476
  - 1.596469223499298
  - 1.6213343858718874
  - 1.749996566772461
  - 1.61827250123024
  - 1.6360748767852784
  - 1.6281076192855837
  - 1.6790698587894441
  - 1.6676095068454744
  - 1.8351538419723512
  - 1.7826315581798555
  - 1.7556587755680084
  - 1.5891334891319275
  - 1.6231965243816378
  - 1.656607648730278
  - 1.6353178441524507
  - 1.6644803047180177
  - 1.6239779829978944
  - 1.6163362860679626
  - 1.7367757260799408
  - 1.6139818131923676
  - 1.5676201999187471
  - 1.5469046056270601
  - 1.66315358877182
  - 1.6389684796333315
  - 1.6618560194969179
  - 1.8543948829174042
  - 1.633031862974167
  - 1.6596818447113038
  - 1.6389614760875704
  - 1.6039184570312501
  - 1.6806178629398347
  - 1.6789257526397705
  - 1.5923053324222565
  - 1.615533882379532
  - 1.7917509198188784
  - 1.6161602139472961
  - 1.6111073136329652
  - 1.911892718076706
  - 1.8123246788978578
  - 2.105185973644257
  - 3.225652885437012
  - 1.6255818903446198
  - 1.7161454975605013
  - 1.6042115628719331
  - 1.7483958065509797
  - 1.5912787318229675
  - 1.6097538471221924
  - 1.9170023262500764
  - 1.9254978001117706
  - 1.6242530584335328
  - 2.1051896512508392
  - 1.6906101524829866
  - 1.70640407204628
  - 1.6577543914318085
  - 1.622298538684845
  - 1.5867081403732302
  validation_losses:
  - 0.4046071469783783
  - 0.4209703803062439
  - 0.47634780406951904
  - 0.48212215304374695
  - 0.4154815077781677
  - 0.4209131896495819
  - 0.42589548230171204
  - 0.43359676003456116
  - 0.4528099000453949
  - 0.4330151081085205
  - 0.4399257004261017
  - 0.424398809671402
  - 0.4226263165473938
  - 0.4347022473812103
  - 0.40265148878097534
  - 0.4669661819934845
  - 0.41642263531684875
  - 0.41745564341545105
  - 0.41116324067115784
  - 0.4259621500968933
  - 0.424669086933136
  - 0.43515852093696594
  - 0.41073521971702576
  - 0.4141266345977783
  - 0.41323786973953247
  - 0.45249247550964355
  - 0.45092281699180603
  - 0.4101140797138214
  - 0.423711359500885
  - 0.41056862473487854
  - 0.452908456325531
  - 0.40982678532600403
  - 0.421213835477829
  - 0.4363333284854889
  - 0.41271263360977173
  - 0.4445714056491852
  - 0.4102916717529297
  - 0.44567108154296875
  - 0.4227900803089142
  - 0.41829773783683777
  - 0.4150688648223877
  - 0.4112517535686493
  - 0.4159255623817444
  - 0.4466433525085449
  - 0.40617358684539795
  - 0.3998667895793915
  - 0.413301020860672
  - 0.4509618878364563
  - 0.4336088299751282
  - 0.4313472509384155
  - 0.40249091386795044
  - 0.4186584949493408
  - 0.40753281116485596
  - 0.4304156005382538
  - 0.415243923664093
  - 0.40526601672172546
  - 0.418320894241333
  - 0.40589839220046997
  - 0.4088706374168396
  - 0.427561491727829
  - 0.42469602823257446
  - 0.4404483437538147
  - 0.4456929564476013
  - 0.4250969886779785
  - 0.40587857365608215
  - 0.4416755139827728
  - 0.4087999761104584
  - 0.3971285820007324
  - 0.42079848051071167
  - 0.45239850878715515
  - 0.42760661244392395
  - 0.4082506000995636
  - 0.4205266833305359
  - 0.44082584977149963
  - 0.431593120098114
  - 0.4374516010284424
  - 0.4433814287185669
  - 0.4198955297470093
  - 0.42162370681762695
  - 0.4124700725078583
loss_records_fold2:
  train_losses:
  - 1.641498762369156
  - 1.61449111700058
  - 1.7898938596248628
  - 1.6229856550693513
  - 1.6704712033271791
  - 2.141683340072632
  - 1.7053685247898103
  - 1.6847769796848298
  - 1.6388307690620423
  - 1.623386797308922
  - 1.7121168911457063
  - 1.692275094985962
  - 1.6270223438739777
  - 1.664820420742035
  - 1.669720858335495
  - 1.5944688618183136
  validation_losses:
  - 0.39455434679985046
  - 0.4222598969936371
  - 0.40901249647140503
  - 0.4274808168411255
  - 0.3886476457118988
  - 0.41475534439086914
  - 0.3994622230529785
  - 0.3960398733615875
  - 0.40343934297561646
  - 0.41435256600379944
  - 0.41092827916145325
  - 0.4101843535900116
  - 0.40842440724372864
  - 0.41671469807624817
  - 0.4032488465309143
  - 0.3992809057235718
loss_records_fold3:
  train_losses:
  - 1.593333399295807
  - 1.6116180360317232
  - 1.6141060888767242
  - 1.8081480681896211
  - 1.7659727334976196
  - 1.6288267254829407
  - 1.621720039844513
  - 1.6786693334579468
  - 1.6323278188705446
  - 1.6284896790981294
  - 1.6533138498663904
  - 1.6347423553466798
  - 1.81824991106987
  - 1.6329549968242647
  - 1.6004157364368439
  - 1.971243852376938
  - 1.7322582662105561
  - 1.6573923110961915
  - 1.5671512722969057
  - 1.592477548122406
  - 1.5933895349502565
  - 1.6595807313919069
  - 1.6136710166931154
  - 1.6584612488746644
  - 1.6978016257286073
  - 1.721322113275528
  - 1.622661727666855
  - 1.6410113990306856
  - 1.6394525468349457
  - 1.6943792760372163
  validation_losses:
  - 0.41717037558555603
  - 0.408086895942688
  - 0.40997007489204407
  - 0.4184040427207947
  - 0.4800896644592285
  - 0.4186961054801941
  - 0.4376632869243622
  - 0.4016486704349518
  - 0.43443432450294495
  - 0.4083499610424042
  - 0.42326289415359497
  - 0.4081660807132721
  - 0.4288201928138733
  - 0.4161754548549652
  - 0.4060403108596802
  - 0.41398885846138
  - 0.40610578656196594
  - 0.4414713382720947
  - 0.40295279026031494
  - 0.4034924805164337
  - 0.4311617314815521
  - 0.3974383473396301
  - 0.40057551860809326
  - 0.47264066338539124
  - 0.41615360975265503
  - 0.4140687584877014
  - 0.4197602868080139
  - 0.41195055842399597
  - 0.40223371982574463
  - 0.39992883801460266
loss_records_fold4:
  train_losses:
  - 1.669795447587967
  - 1.6846431016921999
  - 1.664827209711075
  - 1.653971689939499
  - 1.6428075790405274
  - 1.5864414930343629
  - 1.596266406774521
  - 1.7241719663143158
  - 1.7392996728420258
  - 1.6551664531230927
  - 1.6551219165325166
  - 1.6512037217617035
  - 1.5907812356948854
  - 1.8218850612640383
  - 1.6574358224868775
  - 1.6776433527469636
  - 1.6348335146903992
  - 1.6660991847515108
  - 1.6821570038795473
  - 1.6209091365337374
  - 1.6746133089065554
  - 1.6508966505527498
  - 1.6250963509082794
  - 1.6146689832210541
  - 1.5978554248809815
  - 1.6121587216854096
  - 1.6004068553447723
  - 1.6891567230224611
  - 1.7445627510547639
  - 2.273009920120239
  - 1.6564213156700136
  - 1.6874301433563232
  - 1.653407806158066
  - 1.6225124508142472
  - 2.0217627704143526
  - 1.6448644995689392
  - 2.6462882041931155
  - 1.7283605992794038
  - 1.5986584007740021
  - 1.6090669155120851
  - 1.624174201488495
  - 1.6178488969802858
  validation_losses:
  - 0.4060303568840027
  - 0.40044650435447693
  - 0.4045938551425934
  - 0.4342215061187744
  - 0.4175543189048767
  - 0.4153808355331421
  - 0.40104708075523376
  - 0.41160616278648376
  - 0.40311938524246216
  - 0.39985284209251404
  - 0.4450961649417877
  - 0.4232417941093445
  - 0.42635318636894226
  - 0.4055239260196686
  - 0.41466131806373596
  - 0.42805880308151245
  - 0.4160596430301666
  - 0.40897679328918457
  - 0.42327582836151123
  - 0.4081745147705078
  - 0.45667827129364014
  - 0.43988341093063354
  - 0.4022476375102997
  - 0.41159331798553467
  - 0.43363434076309204
  - 0.4166143834590912
  - 0.42019033432006836
  - 0.4132179617881775
  - 0.4024752080440521
  - 0.4058109521865845
  - 0.4365595579147339
  - 0.40559571981430054
  - 0.409349262714386
  - 0.5816285014152527
  - 0.4090462923049927
  - 0.4286583662033081
  - 0.41279521584510803
  - 0.4141838550567627
  - 0.414815217256546
  - 0.40495285391807556
  - 0.4091680943965912
  - 0.40247997641563416
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 93 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 80 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 42 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:22:36.256487'
