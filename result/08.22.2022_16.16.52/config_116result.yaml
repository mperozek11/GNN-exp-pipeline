config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:51:06.849377'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_116fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_116fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_116fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_116fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_116fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_116fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_116fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_116fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_116fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_116fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 7.021779099106789
  - 6.552443271875382
  - 6.728800493478776
  - 7.117262268066407
  - 7.178704971075058
  - 6.927822750806809
  - 6.789022043347359
  - 6.493829745054246
  - 6.455305916070938
  - 6.672439920902253
  - 6.643072485923767
  - 6.3310635656118395
  - 6.750687059760094
  - 6.1734105288982395
  - 6.073806828260422
  - 6.025666764378548
  - 6.005830273032188
  - 6.046644908189774
  - 5.949489879608155
  - 6.020554268360138
  - 5.949852603673936
  - 5.893446934223175
  - 6.575633466243744
  - 6.121288362145425
  - 6.097987908124924
  - 6.045383796095848
  - 5.981489574909211
  - 5.9383938878774645
  validation_losses:
  - 0.5590859055519104
  - 0.43386247754096985
  - 0.4770205318927765
  - 0.4063604772090912
  - 0.6245837211608887
  - 0.4004150927066803
  - 0.4023255407810211
  - 0.414869487285614
  - 0.4055570065975189
  - 0.407663494348526
  - 0.6396780014038086
  - 0.391010582447052
  - 0.39099201560020447
  - 0.39844033122062683
  - 0.3854895830154419
  - 0.39133426547050476
  - 0.40437209606170654
  - 0.4020571708679199
  - 0.38748037815093994
  - 0.4041658043861389
  - 0.3880842328071594
  - 0.3981676399707794
  - 0.39412182569503784
  - 0.40196242928504944
  - 0.41018640995025635
  - 0.4031769037246704
  - 0.39334383606910706
  - 0.3955898880958557
loss_records_fold1:
  train_losses:
  - 6.07853339612484
  - 6.015155899524689
  - 6.0494434386491776
  - 5.972565114498138
  - 5.958456605672836
  - 6.082539144158364
  - 6.082925102114678
  - 6.0377250790596015
  - 5.988070476055146
  - 6.020491109788418
  - 6.000753378868104
  - 6.045193135738373
  - 5.935162723064423
  - 5.981413754820824
  - 6.055838197469711
  - 5.91477707028389
  validation_losses:
  - 0.3998013734817505
  - 0.40841588377952576
  - 0.3921126127243042
  - 0.3988885283470154
  - 0.41193127632141113
  - 0.39655038714408875
  - 0.4035189747810364
  - 0.39005136489868164
  - 0.4035930335521698
  - 0.4298710227012634
  - 0.40119653940200806
  - 0.3941671848297119
  - 0.39554163813591003
  - 0.3906077444553375
  - 0.39158326387405396
  - 0.39866721630096436
loss_records_fold2:
  train_losses:
  - 5.8974624365568165
  - 5.906053110957146
  - 5.962339821457864
  - 5.973995214700699
  - 5.917475897073746
  - 5.797445160150528
  - 5.884508791565896
  - 5.8464693367481235
  - 6.011375164985657
  - 6.068854245543481
  - 5.975912645459175
  - 5.962292364239693
  - 5.951679542660713
  - 5.970881724357605
  - 5.955626112222672
  - 5.858035969734193
  - 5.9090355247259145
  - 5.935171535611153
  - 5.94703950881958
  - 6.00828472673893
  - 5.893987441062928
  - 5.862710365653038
  - 5.999852225184441
  - 5.899690583348274
  - 5.972236698865891
  - 5.915893319249154
  - 5.989092150330544
  - 5.974131521582604
  - 5.962021481990814
  - 5.932761457562447
  - 5.811171853542328
  - 5.953084722161293
  - 5.7400078058242805
  - 5.727431347966195
  - 5.903962036967278
  - 5.902748355269432
  - 5.8700332105159765
  - 5.810313168168069
  - 5.902435354888439
  - 5.991873088479043
  - 5.91600815653801
  - 5.941033944487572
  - 5.8011351972818375
  - 5.793587961792946
  - 5.780281108617783
  - 5.865152847766876
  - 6.035072633624077
  - 5.8631757467985155
  - 5.816890677809716
  - 5.814164501428604
  - 5.922505480051041
  - 5.797341400384903
  - 5.9260730892419815
  - 5.86921273469925
  - 5.8670917659997945
  - 5.9809648364782335
  - 5.911301285028458
  - 5.876304897665978
  - 5.9362040579319
  - 5.890274879336357
  - 5.8422037929296495
  - 5.799331659078598
  - 5.836983025074005
  - 5.792075368762017
  - 5.899295315146446
  - 5.81847976744175
  - 5.79517247080803
  - 5.850427430868149
  - 5.798502817749977
  - 5.824736306071282
  - 5.604623806476593
  - 5.676494321227074
  - 5.855592885613442
  - 5.6787419646978385
  - 5.949128901958466
  - 5.898546627163888
  - 5.992224448919297
  - 5.8568947374820715
  - 5.768932762742043
  - 5.86516886651516
  - 5.802363151311875
  - 5.69554649591446
  - 5.803088393807411
  - 5.8238167822361
  - 5.599433207511902
  - 5.71296888589859
  - 5.806756401062012
  - 5.7902441233396535
  - 5.737497153878213
  - 5.807286876440049
  - 5.801222717761994
  - 5.964849841594696
  - 5.747535791993141
  - 5.704303261637688
  - 5.746990996599198
  - 5.967914462089539
  - 5.784092342853547
  - 5.8416028231382375
  - 5.933972564339638
  - 5.7244691222906114
  validation_losses:
  - 7.4592485427856445
  - 0.523004949092865
  - 0.3997339606285095
  - 0.3952926993370056
  - 0.40642493963241577
  - 0.4307193160057068
  - 0.4202359914779663
  - 2.5179998874664307
  - 0.4010626971721649
  - 6.851297855377197
  - 0.3895307183265686
  - 0.3970909118652344
  - 0.3985527753829956
  - 0.4015227258205414
  - 0.4214411675930023
  - 0.38810378313064575
  - 0.3883773386478424
  - 0.41735824942588806
  - 0.5046048164367676
  - 0.3857623040676117
  - 0.3970833122730255
  - 0.40361225605010986
  - 0.4761636257171631
  - 4.445748329162598
  - 0.4082429111003876
  - 0.3883924186229706
  - 0.39794477820396423
  - 0.40122494101524353
  - 0.4143398404121399
  - 0.41265153884887695
  - 0.3946840167045593
  - 1.2834140062332153
  - 0.49515530467033386
  - 0.4829787015914917
  - 13.965863227844238
  - 0.6350067853927612
  - 0.5301671624183655
  - 0.5149372220039368
  - 1.908789873123169
  - 2.9499731063842773
  - 0.5792306065559387
  - 1.2581706047058105
  - 1.0399510860443115
  - 1.417475938796997
  - 0.5313851833343506
  - 0.6716955304145813
  - 0.398476243019104
  - 0.7018709182739258
  - 0.7015776634216309
  - 0.42418164014816284
  - 0.4421434998512268
  - 0.4303820729255676
  - 0.4132125675678253
  - 0.3886033892631531
  - 0.4853188991546631
  - 0.4513697624206543
  - 0.3943573534488678
  - 0.43606260418891907
  - 8.139200210571289
  - 4.554145336151123
  - 0.8442972302436829
  - 23.76715660095215
  - 0.4771134555339813
  - 0.40183696150779724
  - 0.6214971542358398
  - 0.4790553152561188
  - 0.41368111968040466
  - 0.6164596080780029
  - 0.5038003325462341
  - 1.2302201986312866
  - 2.2372100353240967
  - 2.314589262008667
  - 7.032015800476074
  - 5.750644683837891
  - 0.3832636773586273
  - 18.191251754760742
  - 0.5278616547584534
  - 7.723263740539551
  - 3.057858943939209
  - 0.4013083279132843
  - 2.8242838382720947
  - 2.5648138523101807
  - 0.7013720870018005
  - 2.583239793777466
  - 3.054145097732544
  - 4.599399566650391
  - 0.856513500213623
  - 3.484225273132324
  - 0.39159899950027466
  - 0.4651872515678406
  - 1.4795187711715698
  - 4.169975757598877
  - 4.877972602844238
  - 1.019680380821228
  - 3.5021026134490967
  - 0.7378668785095215
  - 0.5410372018814087
  - 0.4967638850212097
  - 0.48450544476509094
  - 0.48951274156570435
loss_records_fold3:
  train_losses:
  - 5.87065373659134
  - 5.827430167794228
  - 5.9156088650226595
  - 5.852590963244438
  - 5.763295072317124
  - 5.901429125666619
  - 5.819355690479279
  - 5.82562869489193
  - 5.789007100462914
  - 5.740608954429627
  - 5.854674991965294
  - 5.85371407866478
  - 5.9613624095916755
  - 5.840449553728104
  - 5.931218713521957
  - 5.860926780104638
  - 5.8125939756631855
  - 5.750303792953492
  - 5.7708476215600975
  - 5.76519228219986
  - 6.029399165511132
  - 5.760223749279977
  - 5.871358522772789
  - 5.892389044165611
  - 5.885531067848206
  - 5.94267108142376
  - 5.829425299167633
  - 5.781047198176385
  - 5.8775465309619905
  - 5.7307087004184725
  - 5.8097556143999105
  - 5.853234741091729
  - 5.964685997366906
  - 5.822696882486344
  - 5.782019919157029
  - 5.831216269731522
  - 5.76549930870533
  - 5.938293692469597
  - 5.7448432594537735
  - 5.831606671214104
  - 5.78681287765503
  - 5.782496976852418
  - 5.701300796866417
  - 5.856019479036331
  - 5.762863859534264
  - 5.799899795651436
  - 5.670091059803963
  - 6.027525913715363
  - 5.751844507455826
  - 5.95794515311718
  - 5.942504087090493
  - 5.888826370239258
  - 5.8194445163011554
  - 5.854564192891122
  - 5.847093939781189
  - 5.798643949627877
  - 5.90115604698658
  - 5.695090675354004
  - 5.918233373761177
  - 5.778929960727692
  - 5.871660777926445
  - 5.877966043353081
  - 5.922378093004227
  - 5.899609145522118
  - 5.642693218588829
  - 5.783985713124276
  - 5.722432568669319
  - 5.77242757678032
  - 5.9164708539843565
  - 5.800253909826279
  - 5.647141152620316
  - 5.733988854289056
  - 5.849969977140427
  - 5.829464858770371
  - 5.814319676160813
  - 5.859373289346696
  - 5.9292121738195425
  - 5.80555903017521
  - 5.83609608411789
  - 5.888343900442123
  - 6.028593453764916
  - 5.775577241182328
  - 5.757284072041512
  - 6.027966395020485
  - 5.961567178368568
  - 5.695669355988503
  - 5.843510472774506
  - 5.907947197556496
  - 5.867884695529938
  - 5.883354830741883
  - 5.888406112790108
  - 5.925003474950791
  - 5.844669687747956
  - 5.886286363005638
  - 5.685630908608437
  - 5.652147921919823
  - 5.804260113835335
  - 5.793301883339883
  - 5.63511030972004
  - 5.771572318673134
  validation_losses:
  - 0.5703610777854919
  - 0.7848706841468811
  - 0.5472183227539062
  - 0.6595273017883301
  - 0.8356322050094604
  - 0.5978726744651794
  - 0.7718483209609985
  - 0.7847172617912292
  - 0.6967177391052246
  - 0.5471383333206177
  - 0.5142034292221069
  - 0.7451979517936707
  - 2.8739678859710693
  - 1.084067702293396
  - 13.299814224243164
  - 1.8734296560287476
  - 3.642240285873413
  - 14.637537956237793
  - 8.2631254196167
  - 0.6798871755599976
  - 4.257070064544678
  - 0.5065611004829407
  - 5.047304153442383
  - 1.413826584815979
  - 1.6983383893966675
  - 0.6631689071655273
  - 0.6307265162467957
  - 0.43354669213294983
  - 0.41668179631233215
  - 0.8808669447898865
  - 0.5255893468856812
  - 0.44894710183143616
  - 4.021756649017334
  - 0.48290807008743286
  - 0.5115198493003845
  - 0.5524080991744995
  - 0.494788259267807
  - 0.4219147861003876
  - 0.5780636668205261
  - 0.7448410391807556
  - 1.6085257530212402
  - 0.4870902895927429
  - 0.6575859785079956
  - 0.6543923616409302
  - 0.5166596174240112
  - 0.5981261134147644
  - 1.8969848155975342
  - 1.5654321908950806
  - 4.153176307678223
  - 2.2536842823028564
  - 2.5160939693450928
  - 4.653580188751221
  - 3.105738878250122
  - 4.235477924346924
  - 0.45048069953918457
  - 2.10438871383667
  - 0.5259655117988586
  - 0.7495452761650085
  - 0.8816980719566345
  - 0.4302346706390381
  - 0.6498364210128784
  - 0.5684478282928467
  - 1.6782786846160889
  - 3.8548049926757812
  - 11.176199913024902
  - 13.382641792297363
  - 0.47898533940315247
  - 5.544639587402344
  - 1.6871156692504883
  - 1.3688071966171265
  - 0.663433849811554
  - 1.4717799425125122
  - 0.550398051738739
  - 0.5758391618728638
  - 0.5196069478988647
  - 0.7135826349258423
  - 0.7240089774131775
  - 0.5382152199745178
  - 0.5870403051376343
  - 7.43290376663208
  - 0.41220641136169434
  - 0.8087283372879028
  - 3.020284652709961
  - 13.824216842651367
  - 1.1844027042388916
  - 1.6332006454467773
  - 1.9987236261367798
  - 1.0682364702224731
  - 0.5930982828140259
  - 1.4676297903060913
  - 2.3399646282196045
  - 0.5680349469184875
  - 0.4169354736804962
  - 0.43963995575904846
  - 0.41403114795684814
  - 4.575700283050537
  - 0.6327888369560242
  - 0.47227707505226135
  - 0.5395204424858093
  - 0.40371719002723694
loss_records_fold4:
  train_losses:
  - 5.732981508970261
  - 5.779993242025376
  - 5.755420508980752
  - 5.701152893900872
  - 5.763306912779808
  - 5.839041823148728
  - 5.899432766437531
  - 5.815192008018494
  - 5.833501890301704
  - 5.944040527939797
  - 5.947695940732956
  - 5.917418357729912
  - 5.937118580937386
  - 5.784693521261215
  - 5.942596000432968
  - 5.845524087548256
  - 5.829799067974091
  - 5.786847028136254
  - 5.9651545822620395
  - 5.8013112366199495
  - 5.639024418592453
  - 5.78132992386818
  - 5.874681442975998
  - 5.748515003919602
  - 5.82832879126072
  - 5.840908619761468
  - 5.763906237483025
  - 5.736469277739525
  - 5.757045829296112
  - 5.806045949459076
  - 5.803820410370827
  - 5.892103120684624
  - 5.616776916384698
  - 5.7405638813972475
  - 5.656572577357292
  - 5.727025347948075
  - 5.837049996852875
  - 5.806038799881936
  - 5.718100216984749
  - 5.782306277751923
  - 5.7368299782276155
  - 5.838826298713684
  - 5.7939531177282335
  - 5.766531670093537
  - 5.831566953659058
  - 5.773734286427498
  - 5.662584653496743
  - 5.786866268515587
  - 6.004887732863427
  - 5.677024334669113
  - 5.780991712212563
  - 5.82381075322628
  - 5.750335434079171
  - 5.944574058055878
  - 5.843844339251518
  - 5.855683076381684
  - 5.857689085602761
  - 5.85972540974617
  - 5.789554503560066
  - 5.912061208486557
  - 5.806768307089806
  - 5.827726191282273
  - 5.780517932772637
  - 5.919936299324036
  - 5.813136118650437
  - 5.858495685458184
  - 5.869409218430519
  - 5.9322314947843555
  - 5.733639767765999
  - 5.876926890015603
  - 5.803488492965698
  - 5.859532022476197
  - 5.80003214776516
  - 5.750945681333542
  - 5.831856779754162
  - 5.756068074703217
  - 5.634419280290604
  - 5.73592289686203
  - 5.770344400405884
  - 5.780112481117249
  - 5.827867761254311
  - 5.818837726116181
  - 5.718171828985215
  - 5.745885905623436
  - 5.8174425840377815
  - 5.778278249502183
  - 5.721910306811333
  - 5.769429802894592
  - 5.665859562158585
  - 5.798588725924493
  - 5.721856209635735
  - 5.8066190659999855
  - 5.886415898799896
  - 5.819166487455369
  - 5.733467638492584
  - 5.832534849643707
  - 5.756542819738389
  - 5.860354688763619
  - 5.785765421390534
  - 5.893402901291847
  validation_losses:
  - 0.4231400787830353
  - 0.42989447712898254
  - 0.44207093119621277
  - 0.4306943714618683
  - 0.3978005051612854
  - 0.40841183066368103
  - 0.4570530652999878
  - 0.44200390577316284
  - 0.40284255146980286
  - 0.4239514470100403
  - 0.5012745261192322
  - 0.4393076002597809
  - 0.4608515202999115
  - 0.37221163511276245
  - 0.4106000065803528
  - 0.41669854521751404
  - 0.39924660325050354
  - 0.4333394467830658
  - 0.3955584764480591
  - 0.3978129029273987
  - 0.4627878665924072
  - 0.37061306834220886
  - 0.4408392310142517
  - 0.45816653966903687
  - 0.48257991671562195
  - 0.5480519533157349
  - 0.40237900614738464
  - 0.473519504070282
  - 0.48080700635910034
  - 0.4137515425682068
  - 0.38967543840408325
  - 0.4112117886543274
  - 0.4329267144203186
  - 0.5686490535736084
  - 0.48157998919487
  - 0.4740159809589386
  - 0.45291295647621155
  - 0.5352293848991394
  - 0.43973544239997864
  - 0.40386098623275757
  - 0.39069294929504395
  - 0.5329795479774475
  - 0.5320720672607422
  - 0.4724980592727661
  - 0.4191368520259857
  - 0.46192821860313416
  - 1.6072858572006226
  - 0.42141854763031006
  - 0.39942285418510437
  - 0.4348287880420685
  - 0.408600389957428
  - 0.44798746705055237
  - 0.4285513162612915
  - 0.49702101945877075
  - 0.4608151614665985
  - 0.39147335290908813
  - 0.4169866740703583
  - 0.4081049859523773
  - 0.42820683121681213
  - 0.42651471495628357
  - 0.41543343663215637
  - 0.39926499128341675
  - 0.5160720944404602
  - 0.40271997451782227
  - 0.41695207357406616
  - 0.49705609679222107
  - 0.5206605195999146
  - 0.636196494102478
  - 0.5240830779075623
  - 0.4241296648979187
  - 0.5164241194725037
  - 0.4160423278808594
  - 0.4791457951068878
  - 0.6568109393119812
  - 0.5266382098197937
  - 0.4974261522293091
  - 0.5706610083580017
  - 0.9817079305648804
  - 0.629594624042511
  - 0.5326692461967468
  - 0.7397010326385498
  - 0.4735506474971771
  - 0.6321842074394226
  - 0.5286303162574768
  - 0.4594504237174988
  - 0.5577487945556641
  - 0.8518916964530945
  - 0.7735110521316528
  - 0.5445517897605896
  - 0.4816795885562897
  - 0.5327931642532349
  - 0.5418129563331604
  - 0.4989614188671112
  - 3.80673885345459
  - 1.4512989521026611
  - 0.581193745136261
  - 0.6604560613632202
  - 0.7658776640892029
  - 0.5168230533599854
  - 0.41681405901908875
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.855917667238422, 0.8490566037735849,
    0.8384879725085911]'
  fold_eval_f1: '[0.0, 0.0, 0.14285714285714288, 0.02222222222222222, 0.11320754716981131]'
  mean_eval_accuracy: 0.851745621945972
  mean_f1_accuracy: 0.05565738244983528
  total_train_time: '0:35:57.115788'
