config:
  aggregation: mean
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:35:10.093873'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_51fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_51fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_51fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_51fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_51fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_51fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_51fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_51fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_51fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_51fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 0.9732615768909455
  - 0.7888162970542908
  - 0.9089021563529969
  - 0.8068524241447449
  - 0.7798089623451233
  - 0.8042805194854736
  - 0.8292492151260377
  - 0.7704207062721253
  - 0.7403011500835419
  - 0.7917256534099579
  - 0.8267526924610138
  validation_losses:
  - 0.4076767563819885
  - 0.4058126211166382
  - 0.48006659746170044
  - 0.40332266688346863
  - 0.40354594588279724
  - 0.3998311161994934
  - 0.3985750079154968
  - 0.4023011028766632
  - 0.3926044702529907
  - 0.3982299864292145
  - 0.38752302527427673
loss_records_fold1:
  train_losses:
  - 0.7744359016418457
  - 0.7615555286407472
  - 0.7570432722568512
  - 0.7276852011680603
  - 0.8425299763679505
  - 0.8008110105991364
  - 0.7863645553588867
  - 0.7857522189617158
  - 0.7388667404651642
  - 0.7549184799194336
  - 0.8144479513168336
  validation_losses:
  - 0.3955422341823578
  - 0.39436277747154236
  - 0.39431706070899963
  - 0.39323461055755615
  - 0.4171864688396454
  - 0.39309123158454895
  - 0.39183905720710754
  - 0.39173129200935364
  - 0.3936119079589844
  - 0.3901422917842865
  - 0.38904500007629395
loss_records_fold2:
  train_losses:
  - 0.7975602328777314
  - 0.7951467871665955
  - 0.7969021916389466
  - 0.7528074860572815
  - 0.7579812526702882
  - 0.7856258928775788
  - 0.7703753173351289
  - 0.7739039361476898
  - 0.7439289927482605
  - 0.7248372942209245
  - 0.7538483738899231
  - 0.7684099674224854
  - 0.7418696969747544
  - 0.7269701123237611
  - 0.8019237756729126
  - 0.7230109095573426
  - 0.775695425271988
  - 0.7583677053451539
  - 0.7745286583900453
  - 0.7400130927562714
  validation_losses:
  - 0.48286500573158264
  - 0.3894135355949402
  - 0.3855321705341339
  - 0.3878132700920105
  - 0.41636142134666443
  - 0.42288070917129517
  - 0.38507184386253357
  - 0.3925081193447113
  - 0.38528895378112793
  - 0.40183180570602417
  - 0.38551661372184753
  - 0.3923141062259674
  - 0.382396399974823
  - 0.4370798170566559
  - 0.4011075496673584
  - 0.3908683657646179
  - 0.3927573561668396
  - 0.38381659984588623
  - 0.3916079103946686
  - 0.3858562707901001
loss_records_fold3:
  train_losses:
  - 0.7786013901233674
  - 0.7867931008338929
  - 0.7557674884796143
  - 0.7820673942565919
  - 0.7398136407136917
  - 0.7730076313018799
  - 0.7717039227485657
  - 0.7756232500076294
  - 0.780981057882309
  - 0.7908686339855194
  - 0.7434451282024384
  - 0.8059977412223817
  - 0.783074140548706
  - 0.790776151418686
  - 0.7559870243072511
  - 0.773072475194931
  - 0.7562614679336548
  validation_losses:
  - 0.5757859349250793
  - 0.376586377620697
  - 0.37118223309516907
  - 0.38524729013442993
  - 0.3726881146430969
  - 0.3759040832519531
  - 0.37104108929634094
  - 0.37006253004074097
  - 0.3726917803287506
  - 0.3861196041107178
  - 0.527136504650116
  - 0.5081837773323059
  - 0.37432700395584106
  - 0.3827263116836548
  - 0.3741060793399811
  - 0.3733496069908142
  - 0.37155619263648987
loss_records_fold4:
  train_losses:
  - 0.7472084343433381
  - 0.7733284771442414
  - 0.7690195441246033
  - 0.7315174520015717
  - 0.7315649032592774
  - 0.746067625284195
  - 0.7520645797252655
  - 0.7333722889423371
  - 0.7638300836086274
  - 0.8694970071315766
  - 0.7820545792579652
  - 0.7364744246006012
  - 0.7397317409515382
  - 0.7845888197422028
  - 0.7492500662803651
  - 0.7267336070537568
  - 0.7589295983314515
  - 0.7706323683261872
  - 0.740755820274353
  - 0.7319492042064667
  - 0.7367386102676392
  - 0.7525293588638307
  - 0.7615045309066772
  - 0.7506635516881943
  - 0.7904288470745087
  - 0.805520087480545
  - 0.7321547627449037
  - 0.75210200548172
  - 0.7496991574764252
  - 0.7618898570537568
  - 0.7518961608409882
  - 0.7211073368787766
  - 0.7269385129213334
  - 0.7578563094139099
  - 0.7631217658519746
  - 0.7210028678178788
  validation_losses:
  - 0.3810587525367737
  - 0.40281954407691956
  - 0.41636502742767334
  - 0.3806455135345459
  - 0.38481417298316956
  - 0.379576176404953
  - 0.38368791341781616
  - 0.38314640522003174
  - 0.4006313979625702
  - 0.3782130181789398
  - 0.3899221122264862
  - 0.3805220425128937
  - 0.3822007477283478
  - 0.3913024961948395
  - 0.38803213834762573
  - 0.3811689615249634
  - 0.3923131227493286
  - 0.3747108578681946
  - 0.386351078748703
  - 0.37845760583877563
  - 0.3838947117328644
  - 0.4088341295719147
  - 0.40988776087760925
  - 0.37888067960739136
  - 0.385170578956604
  - 0.38063299655914307
  - 0.37846410274505615
  - 0.4147427976131439
  - 0.3825187385082245
  - 0.3947184383869171
  - 0.37855178117752075
  - 0.3844669461250305
  - 0.3837146461009979
  - 0.38752877712249756
  - 0.3796190917491913
  - 0.37480416893959045
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 36 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582701160604291
  mean_f1_accuracy: 0.0
  total_train_time: '0:08:13.833862'
