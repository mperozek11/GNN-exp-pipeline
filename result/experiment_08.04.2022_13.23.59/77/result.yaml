config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 21:42:45.878910'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/77/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/77/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/77/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/77/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/77/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/77/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/77/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/77/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/77/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/77/fold_4_state_dict.pt
loss_records_fold2:
  train_losses:
  - 8.230638533830643
  - 8.16633939743042
  - 8.39560779929161
  - 8.24449673295021
  - 8.253621906042099
  - 8.250324994325638
  - 8.232871532440186
  - 8.293463945388794
  - 8.38802021741867
  - 8.89608883857727
  - 8.292705029249191
  - 8.23515397310257
  - 8.156932473182678
  - 8.260405600070953
  - 8.1478610932827
  - 8.221735060214996
  - 8.281098544597626
  - 8.210193574428558
  - 8.449157267808914
  - 8.330803036689758
  - 8.231005877256393
  - 8.132316619157791
  - 8.128313064575195
  - 8.293785870075226
  - 8.327454805374146
  - 8.239693492650986
  - 8.266343414783478
  - 8.268919587135315
  - 8.231785863637924
  - 8.229740589857101
  - 8.271752148866653
  - 8.256737887859344
  - 8.543854653835297
  - 8.432964146137238
  - 8.451982140541077
  - 8.270249277353287
  - 8.339765161275864
  - 8.43007516860962
  - 8.18739864230156
  - 8.307050883769989
  - 8.155512809753418
  - 8.39678093791008
  - 8.152148842811584
  - 8.577281147241592
  - 8.895535737276077
  - 8.256253451108932
  - 8.055478304624557
  - 8.495191663503647
  - 8.421194106340408
  - 8.418857008218765
  - 8.213468477129936
  - 8.353426218032837
  - 8.210979491472244
  - 8.37879604101181
  - 8.5858433842659
  - 8.538927525281906
  - 8.174051612615585
  - 8.330515652894974
  - 8.289938539266586
  - 8.134978502988815
  - 8.040154337882996
  - 8.675164014101028
  - 8.186372250318527
  - 8.34248998761177
  - 8.089649885892868
  - 8.295261412858963
  - 7.847013205289841
  - 8.921379685401917
  - 8.39041218161583
  - 8.252781748771667
  - 8.285059690475464
  - 8.362940728664398
  - 8.37915125489235
  - 8.42033377289772
  - 8.299967616796494
  - 8.544644206762314
  - 8.22501590847969
  - 8.396658658981323
  - 8.272679805755615
  - 8.31126856803894
  - 8.132786333560944
  - 8.350350797176361
  - 8.457569569349289
  - 8.270006239414215
  - 8.371386796236038
  - 8.21385172009468
  - 8.252568066120148
  - 8.128331363201141
  - 8.159204602241516
  - 8.235681504011154
  - 8.154665052890778
  - 8.330815464258194
  - 8.26649883389473
  - 8.261089831590652
  - 8.099202886223793
  - 8.677946329116821
  - 8.124705821275711
  - 8.19489187002182
  - 8.17592641711235
  - 8.396462112665176
  validation_losses:
  - 0.4060378074645996
  - 0.40880027413368225
  - 0.40990644693374634
  - 0.4359223246574402
  - 0.4163304567337036
  - 0.4036732316017151
  - 0.39985862374305725
  - 181057093632.0
  - 17014344122368.0
  - 125750736846848.0
  - 227628736839680.0
  - 253885616750592.0
  - 257812559036416.0
  - 240857940754432.0
  - 307974488719360.0
  - 252255240454144.0
  - 249093087559680.0
  - 243612071755776.0
  - 249249618984960.0
  - 243914967613440.0
  - 244701433167872.0
  - 247000767397888.0
  - 253501905043456.0
  - 257251193389056.0
  - 245467682177024.0
  - 250623857524736.0
  - 262899176046592.0
  - 252910189412352.0
  - 255603167461376.0
  - 245531150385152.0
  - 305192490762240.0
  - 263533304479744.0
  - 241152783548416.0
  - 249960889057280.0
  - 239698249252864.0
  - 249169172234240.0
  - 244622093713408.0
  - 247933396058112.0
  - 247919185756160.0
  - 244946581848064.0
  - 249451465670656.0
  - 239242361962496.0
  - 252255408226304.0
  - 254881042530304.0
  - 247919185756160.0
  - 245534354833408.0
  - 243369540321280.0
  - 238131240501248.0
  - 248352910344192.0
  - 249006382907392.0
  - 246928893804544.0
  - 246988956237824.0
  - 249805884358656.0
  - 242757004165120.0
  - 247630114324480.0
  - 325077551808512.0
  - 255037456515072.0
  - 242007079387136.0
  - 254255738912768.0
  - 243425458782208.0
  - 247408571187200.0
  - 254358314811392.0
  - 244537452658688.0
  - 270300998533120.0
  - 247668836139008.0
  - 244479772590080.0
  - 241679235809280.0
  - 252348001681408.0
  - 253906219171840.0
  - 312101381865472.0
  - 254549155643392.0
  - 240231160741888.0
  - 243787745984512.0
  - 245966519140352.0
  - 256732844523520.0
  - 251032114298880.0
  - 248692078542848.0
  - 248246928670720.0
  - 243643143159808.0
  - 258858652008448.0
  - 258807582162944.0
  - 252580030578688.0
  - 246252017025024.0
  - 259298517057536.0
  - 248177739431936.0
  - 240577811578880.0
  - 250832616423424.0
  - 253724756803584.0
  - 244650984079360.0
  - 253754402144256.0
  - 240907936858112.0
  - 244088678907904.0
  - 239875416653824.0
  - 246787143106560.0
  - 250499823566848.0
  - 242258972508160.0
  - 253794701017088.0
  - 253358493401088.0
  - 255713578319872.0
  - 256761785221120.0
loss_records_fold3:
  train_losses:
  - 8.180596500635147
  - 8.209513872861862
  - 8.366154104471207
  - 8.60480722784996
  - 8.574138343334198
  - 8.294809937477112
  - 8.168808341026306
  - 8.175865828990936
  - 8.249254018068314
  - 8.139376819133759
  - 8.3209787607193
  - 8.40919828414917
  - 8.236961424350739
  - 8.40945789217949
  - 8.058344811201096
  - 8.374808341264725
  - 8.31296506524086
  - 8.109021335840225
  - 8.523319751024246
  - 8.339092373847961
  - 8.586429506540298
  - 8.529425352811813
  - 8.29509088397026
  - 8.347698539495468
  - 8.309886008501053
  - 8.300265729427338
  - 8.08942461013794
  - 8.23596528172493
  - 8.076007410883904
  - 8.334153771400452
  - 8.25935384631157
  - 8.340845823287964
  - 8.105730324983597
  - 8.115024715662003
  - 8.335294783115387
  - 8.024604260921478
  - 8.154655128717422
  - 8.354499846696854
  - 8.193637013435364
  - 8.068835288286209
  - 8.328203797340393
  - 8.593395471572876
  - 8.471728771924973
  - 8.314254760742188
  - 8.484845578670502
  - 8.22959554195404
  - 8.581310242414474
  - 8.321385502815247
  - 8.288123220205307
  - 8.456372082233429
  - 8.240354478359222
  - 8.372044533491135
  - 8.220926493406296
  - 8.365204572677612
  - 8.394548505544662
  - 8.06667110323906
  - 8.299282997846603
  - 8.431179255247116
  - 8.333935767412186
  - 8.497111529111862
  - 8.39500105381012
  - 8.161690771579742
  - 8.435095429420471
  - 8.197836935520172
  - 8.263546735048294
  - 8.355039536952972
  - 8.307025671005249
  - 8.310407847166061
  - 8.554737657308578
  - 8.158999502658844
  - 8.09162575006485
  - 8.295310109853745
  - 8.115671217441559
  - 8.332979202270508
  - 8.989464819431305
  - 10.718957334756851
  - 12.19221544265747
  - 8.622848004102707
  - 8.754423767328262
  - 8.324505895376205
  - 8.289466202259064
  - 8.125435620546341
  - 8.187488183379173
  - 8.25407987833023
  - 8.44617161154747
  - 8.445487946271896
  - 8.355378806591034
  - 8.390008807182312
  - 8.137158781290054
  - 8.242692768573761
  - 8.125941693782806
  - 8.27556312084198
  - 8.4336439371109
  - 8.160436391830444
  - 8.022806912660599
  - 8.249868124723434
  - 8.40207126736641
  - 8.08301991224289
  - 8.17250069975853
  - 7.95631530880928
  validation_losses:
  - 311439654912000.0
  - 322475137171456.0
  - 306056282505216.0
  - 387916580782080.0
  - 320915795607552.0
  - 304260986175488.0
  - 306185064415232.0
  - 302006564552704.0
  - 304888990924800.0
  - 298985860366336.0
  - 304995123593216.0
  - 311855763423232.0
  - 308887806803968.0
  - 309404041740288.0
  - 310741655617536.0
  - 314177595899904.0
  - 303962586611712.0
  - 308626853986304.0
  - 312683651923968.0
  - 311070958813184.0
  - 307927344742400.0
  - 308252084535296.0
  - 318309455101952.0
  - 303884975210496.0
  - 303897927221248.0
  - 304300110643200.0
  - 304743968669696.0
  - 302718690263040.0
  - 302351403450368.0
  - 306438433931264.0
  - 320715744083968.0
  - 302805059371008.0
  - 301836846235648.0
  - 301283063889920.0
  - 295400334426112.0
  - 307668505853952.0
  - 308238662762496.0
  - 374250061955072.0
  - 316319677284352.0
  - 313847621615616.0
  - 314724029825024.0
  - 299978836672512.0
  - 299969474985984.0
  - 303321294307328.0
  - 321049208029184.0
  - 303724484362240.0
  - 313386684383232.0
  - 386459748007936.0
  - 324796768321536.0
  - 307986668978176.0
  - 310477246693376.0
  - 310259209994240.0
  - 300883061506048.0
  - 376709031395328.0
  - 328377093324800.0
  - 305372376072192.0
  - 306989464813568.0
  - 317830566248448.0
  - 312493968719872.0
  - 309871253651456.0
  - 305956323852288.0
  - 304261455937536.0
  - 310119590002688.0
  - 309182783815680.0
  - 316160293732352.0
  - 317583370747904.0
  - 318806530457600.0
  - 314552667340800.0
  - 304412551544832.0
  - 301949287137280.0
  - 315921990156288.0
  - 311143604158464.0
  - 313396213841920.0
  - 0.40868356823921204
  - 15.13686466217041
  - 1.8091704845428467
  - 0.4780620038509369
  - 0.4070533812046051
  - 0.4263671636581421
  - 0.43612605333328247
  - 1.1854994297027588
  - 10.919102668762207
  - 52.927764892578125
  - 313.9175720214844
  - 7224.22509765625
  - 181816.375
  - 132492879200256.0
  - 2.112863205117133e+16
  - 7.807641245169315e+26
  - 1.2626218418855847e+28
  - 3.170873587274576e+28
  - 3.684021276994261e+28
  - 4.05259135565765e+28
  - 3.9575827727437683e+28
  - 4.114528497737024e+28
  - 3.97614190913977e+28
  - 3.671254359207823e+28
  - 4.042962450399079e+28
  - 3.8047861189423622e+28
  - 4.203218318178502e+28
loss_records_fold4:
  train_losses:
  - 8.42519560456276
  - 8.421838909387589
  - 8.104382544755936
  - 8.034932255744934
  - 8.299712300300598
  - 8.658541351556778
  - 8.135215103626251
  - 8.284783899784088
  - 8.333649933338165
  - 11.469061017036438
  - 8.647960871458054
  - 8.354051321744919
  - 8.053220421075821
  - 8.315013200044632
  - 8.206397950649261
  - 8.367574840784073
  - 8.441597312688828
  - 8.229538053274155
  - 8.458469927310944
  - 8.192788630723953
  - 8.499368697404861
  - 8.408494979143143
  - 8.205430954694748
  - 8.196232914924622
  - 8.44411164522171
  - 8.208185404539108
  - 8.215598553419113
  - 8.139535129070282
  - 8.42938506603241
  - 8.511869847774506
  - 8.519109934568405
  - 8.365353286266327
  - 8.742108255624771
  - 8.154365569353104
  - 8.118582934141159
  - 8.131344050168991
  - 8.364584177732468
  - 8.58455502986908
  - 8.53873085975647
  - 8.027896344661713
  - 8.39681851863861
  - 8.215872153639793
  - 8.662238240242004
  - 8.577241450548172
  - 8.216037034988403
  - 8.292042255401611
  - 8.444423347711563
  - 8.214033216238022
  - 8.541964292526245
  - 8.162484914064407
  - 8.34287703037262
  - 8.390144050121307
  - 8.316677123308182
  - 8.13131132721901
  - 8.193588376045227
  - 8.287808924913406
  - 8.363414704799652
  - 8.275636076927185
  - 8.098562508821487
  - 8.234916627407074
  - 8.39932730793953
  - 8.476845502853394
  - 8.324821561574936
  - 8.202771455049515
  - 8.395696938037872
  - 8.151814103126526
  - 8.066206961870193
  - 8.295495748519897
  - 8.089646220207214
  - 8.24089628458023
  - 8.363181352615356
  - 8.257898807525635
  - 8.460179090499878
  - 8.248858988285065
  - 8.454624325037003
  - 8.062452048063278
  - 8.264819383621216
  - 8.179339170455933
  - 8.140787094831467
  - 8.231887847185135
  - 8.383211344480515
  - 8.266218155622482
  - 8.238630652427673
  - 8.070699483156204
  - 8.307796150445938
  - 8.080492809414864
  - 8.196691393852234
  - 8.232052117586136
  - 8.25621822476387
  - 8.42954495549202
  - 8.458107352256775
  - 8.290170818567276
  - 8.155401140451431
  - 8.004076927900314
  - 8.060942441225052
  - 8.114264398813248
  - 8.31903862953186
  - 8.145860701799393
  - 8.2886201441288
  - 8.628816217184067
  validation_losses:
  - 2.007867823221047e+28
  - 2.2178306122441834e+28
  - 1.9491496822547218e+28
  - 1.982913185897295e+28
  - 1.896182321132073e+28
  - 2.1116057005785136e+28
  - 2.0669049600432902e+28
  - 1.9101863808778609e+28
  - 1.9715391301049792e+28
  - 1051.0416259765625
  - 30220070.0
  - 23255134208.0
  - 10923806818304.0
  - 5449500676063232.0
  - 6.031012043618779e+18
  - 1.4193524882372485e+21
  - 1.4790380523358428e+23
  - 3.239552054159861e+24
  - 1.1086749713334216e+24
  - 8.968457412780491e+18
  - 4.379395715264361e+19
  - 7.99056617463884e+19
  - 9.157148891318478e+19
  - 9.297220515432025e+19
  - 9.280129706689875e+19
  - 9.269978135732945e+19
  - 9.266962835044932e+19
  - 9.225122459366195e+19
  - 9.225513005896381e+19
  - 9.281120146764176e+19
  - 9.203952022680345e+19
  - 9.232774180686214e+19
  - 9.162589274852714e+19
  - 9.202476038271218e+19
  - 9.087533092703515e+19
  - 9.078276964016246e+19
  - 8.977098144809694e+19
  - 9.008801023280336e+19
  - 8.954154415770567e+19
  - 8.98005099323725e+19
  - 8.98416228711583e+19
  - 8.989652808380291e+19
  - 9.017550497009526e+19
  - 8.91899203391429e+19
  - 8.987246197329415e+19
  - 8.97400719772169e+19
  - 8.990386402538344e+19
  - 8.990625656268548e+19
  - 9.020424180599882e+19
  - 8.976048770912145e+19
  - 8.909867846622354e+19
  - 8.890926339908331e+19
  - 8.929949326992055e+19
  - 8.960241312141935e+19
  - 8.944371401111267e+19
  - 9.002158213829965e+19
  - 9.00526587349471e+19
  - 8.956506491044705e+19
  - 8.938706717204965e+19
  - 8.917905716426047e+19
  - 8.944164692925245e+19
  - 8.963860904420573e+19
  - 8.982864863395054e+19
  - 8.982828799413663e+19
  - 8.976894075451579e+19
  - 8.971006850391815e+19
  - 8.968733939954876e+19
  - 8.920181265690893e+19
  - 9.02314041412514e+19
  - 8.948568896701465e+19
  - 9.002810883932212e+19
  - 8.902062193674446e+19
  - 8.987658734092157e+19
  - 8.958277144570076e+19
  - 9.00465982268548e+19
  - 8.976203582149335e+19
  - 1.1047454790460742e+20
  - 9.205581059108058e+19
  - 8.989338787859399e+19
  - 8.92844255625735e+19
  - 8.899591371144508e+19
  - 8.948462463975896e+19
  - 8.954585424328655e+19
  - 8.999347862109369e+19
  - 8.966765374336506e+19
  - 9.001780861439312e+19
  - 8.986131732343502e+19
  - 8.974976527172737e+19
  - 8.940385011753602e+19
  - 8.979609429367534e+19
  - 8.984727875897157e+19
  - 9.016384135074782e+19
  - 8.968748893313014e+19
  - 9.017943682367619e+19
  - 9.019541932469754e+19
  - 8.977174670818987e+19
  - 8.95077319761283e+19
  - 8.933671833559053e+19
  - 8.976596767507428e+19
  - 8.995121339412198e+19
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:05:38.119031'
