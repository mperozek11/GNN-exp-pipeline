config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 07:42:59.622161'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/151/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/151/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/151/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/151/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/151/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/151/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/151/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/151/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/151/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/151/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 6.097931891679764
  - 7.890450447797775
  - 10.83583876490593
  - 30.603888899087906
  - 4.095674574375153
  - 15.123392581939697
  - 6.770016312599182
  - 12.917640715837479
  - 26.790047526359558
  - 7.836454331874847
  - 11.342003256082535
  - 9.703250646591187
  - 5.1230760514736176
  - 6.657878488302231
  - 5.019743114709854
  - 23.424020409584045
  - 6.145817518234253
  - 5.5331423580646515
  - 9.365174680948257
  - 27.855417490005493
  - 4.440459191799164
  - 9.481019914150238
  - 7.8790149092674255
  - 12.23120579123497
  - 5.220770508050919
  - 5.965195715427399
  - 5.712536096572876
  - 5.0228758454322815
  - 4.584683254361153
  - 5.321373134851456
  - 6.11011728644371
  - 5.756865561008453
  - 5.900320321321487
  - 5.1584878116846085
  - 4.681758940219879
  - 5.199908226728439
  - 4.3630411028862
  - 3.8499457389116287
  - 4.574816197156906
  - 4.348564922809601
  - 4.126724481582642
  - 4.118914932012558
  - 4.007115989923477
  - 4.353549063205719
  - 6.014403611421585
  - 5.449137598276138
  - 4.150952935218811
  - 4.1940856873989105
  - 28.180038779973984
  - 8.973169714212418
  - 4.338103026151657
  - 4.073931008577347
  - 4.003693133592606
  - 3.9056578874588013
  - 3.940115660429001
  - 4.695896625518799
  - 5.181339621543884
  - 4.140244722366333
  - 4.798926830291748
  - 4.3919877409935
  - 5.086693286895752
  - 4.178316354751587
  - 5.501061052083969
  - 4.403957962989807
  - 5.146721005439758
  - 4.054319381713867
  - 3.962409943342209
  - 3.872856765985489
  - 4.124070346355438
  - 4.259546369314194
  - 4.443199872970581
  - 4.049649238586426
  - 3.9048827290534973
  - 4.390783786773682
  - 4.415647089481354
  - 4.175366818904877
  - 4.176933258771896
  - 4.193485289812088
  - 4.472064703702927
  - 4.084492743015289
  - 5.647438824176788
  - 4.832092374563217
  - 4.106143623590469
  - 5.008218020200729
  - 5.63061985373497
  - 9.881632149219513
  - 8.551643311977386
  - 5.073992699384689
  - 5.185383766889572
  - 6.3275106549263
  - 7.956049591302872
  - 22.2277392745018
  - 5.251055687665939
  - 4.654822409152985
  - 4.0802963078022
  - 5.68687516450882
  - 4.175072550773621
  - 5.41957888007164
  - 9.541764944791794
  - 4.189370661973953
  validation_losses:
  - 0.40437158942222595
  - 0.4078252613544464
  - 0.42101389169692993
  - 0.44465765357017517
  - 0.4198029041290283
  - 0.4076623320579529
  - 1.063740611076355
  - 0.5243059396743774
  - 0.9126710891723633
  - 0.47328874468803406
  - 1.3405393362045288
  - 0.4001677930355072
  - 0.6281346678733826
  - 0.5668902397155762
  - 0.432220995426178
  - 0.48993486166000366
  - 0.4172942340373993
  - 0.6260735988616943
  - 0.47761690616607666
  - 0.4559859037399292
  - 0.4384090006351471
  - 0.754896342754364
  - 0.7482954263687134
  - 0.42286449670791626
  - 0.49782493710517883
  - 0.788494884967804
  - 0.6282218098640442
  - 0.5485756993293762
  - 0.40416136384010315
  - 0.4678514897823334
  - 0.8167123794555664
  - 0.6485397219657898
  - 0.7774121761322021
  - 0.4811105728149414
  - 0.42286059260368347
  - 0.419767290353775
  - 0.407804012298584
  - 0.5370242595672607
  - 0.4096717834472656
  - 0.4760720431804657
  - 0.4464426636695862
  - 0.40571314096450806
  - 0.404906302690506
  - 0.4206983745098114
  - 0.4343000650405884
  - 0.40305638313293457
  - 0.4285237193107605
  - 0.44582006335258484
  - 0.43281289935112
  - 0.42117124795913696
  - 0.410808265209198
  - 0.40855637192726135
  - 0.4033636450767517
  - 0.42472365498542786
  - 0.47349610924720764
  - 0.4555112421512604
  - 0.49678629636764526
  - 0.49338093400001526
  - 0.40932774543762207
  - 0.6446954011917114
  - 0.5398520231246948
  - 0.5629991888999939
  - 0.4748227596282959
  - 0.6443818807601929
  - 0.4184327721595764
  - 0.40443453192710876
  - 0.4088817536830902
  - 0.4776501953601837
  - 0.4213355481624603
  - 0.5390576124191284
  - 0.4291664659976959
  - 0.4343646466732025
  - 0.4437722861766815
  - 0.5330320000648499
  - 0.41325607895851135
  - 0.43088459968566895
  - 0.4150184392929077
  - 0.420516699552536
  - 0.4744308888912201
  - 0.40693166851997375
  - 5.592945098876953
  - 0.9847180843353271
  - 0.4135266840457916
  - 0.6060316562652588
  - 0.5512465834617615
  - 1.2804261445999146
  - 0.7586050629615784
  - 0.4156567454338074
  - 0.4033674895763397
  - 0.6715295314788818
  - 0.7427109479904175
  - 0.4969351291656494
  - 0.45177167654037476
  - 0.40827012062072754
  - 0.4112536907196045
  - 0.41886672377586365
  - 0.7468132376670837
  - 0.7377572059631348
  - 0.4197133481502533
  - 0.4317491352558136
loss_records_fold4:
  train_losses:
  - 6.549141377210617
  - 5.765147507190704
  - 4.5844902992248535
  - 4.039736330509186
  - 5.8089247941970825
  - 4.0571287870407104
  - 4.123908817768097
  - 4.584330976009369
  - 4.095560938119888
  - 4.004080384969711
  - 5.984629601240158
  - 13.826636016368866
  - 8.477815851569176
  - 4.560674875974655
  - 6.009492635726929
  - 6.464038729667664
  - 5.828539669513702
  - 236.956666380167
  - 20.713862001895905
  - 15.955178260803223
  - 34.509253680706024
  - 3.789629504084587
  - 4.3667654395103455
  - 9.157397150993347
  - 6.327749311923981
  - 4.708142012357712
  - 6.52606076002121
  - 5.226015746593475
  - 10.393655139952898
  - 4.8887854516506195
  - 5.2273930311203
  - 25.749052494764328
  - 4.561679154634476
  - 8.583858102560043
  - 4.203461110591888
  - 4.281542152166367
  - 5.199254184961319
  - 4.685797154903412
  - 4.590038299560547
  - 4.5421046912670135
  - 6.686682432889938
  - 4.261590152978897
  - 37.471252769231796
  - 21.736145049333572
  - 31.259408831596375
  - 11.166269361972809
  - 11.037143886089325
  - 6.337747544050217
  - 5.452524170279503
  - 6.932513058185577
  - 7.339422106742859
  - 5.859073728322983
  - 4.528479337692261
  - 5.328709840774536
  - 3.970087379217148
  - 3.8892236948013306
  - 4.215679377317429
  - 10.00132241845131
  - 5.4514254331588745
  - 4.0498612225055695
  - 4.7285833060741425
  - 7.017618268728256
  - 4.062683492898941
  - 3.9984050691127777
  - 3.9972424507141113
  - 29.43633922934532
  - 11.15853875875473
  - 6.185776203870773
  - 4.495966136455536
  - 4.249019086360931
  - 3.9702811539173126
  - 4.572031944990158
  - 4.435464292764664
  - 4.037851274013519
  - 4.6889894008636475
  - 3.914511948823929
  - 4.284655660390854
  - 4.314849615097046
  - 4.604958951473236
  - 4.583648651838303
  - 4.906954795122147
  - 4.4670349434018135
  - 4.67928272485733
  - 3.8160095512866974
  - 47.793090492486954
  - 26.456918388605118
  - 4.076027378439903
  - 4.207746982574463
  - 4.9874536991119385
  - 4.787658989429474
  - 4.092438697814941
  - 4.017583727836609
  - 3.995860904455185
  - 3.9834679663181305
  - 3.9279531836509705
  - 5.59662002325058
  - 5.312209933996201
  - 5.622736066579819
  - 4.609465405344963
  - 4.833135098218918
  validation_losses:
  - 0.5946272611618042
  - 0.5665062069892883
  - 0.5811558961868286
  - 0.6283989548683167
  - 0.5798709988594055
  - 0.6387428045272827
  - 0.7414414882659912
  - 0.5499854683876038
  - 0.5342611074447632
  - 0.6176904439926147
  - 0.5400871634483337
  - 0.49911007285118103
  - 0.5189743638038635
  - 0.669526219367981
  - 0.6360180377960205
  - 0.8337639570236206
  - 1.7111772298812866
  - 0.9217147827148438
  - 306.7939147949219
  - 92.79572296142578
  - 0.8272086381912231
  - 0.5625871419906616
  - 0.6371858716011047
  - 0.6321975588798523
  - 12.178157806396484
  - 1.4656003713607788
  - 0.6786675453186035
  - 1.1167354583740234
  - 1.291012167930603
  - 1.2128905057907104
  - 0.9658935070037842
  - 0.8408182263374329
  - 0.7105985283851624
  - 0.785861074924469
  - 0.7434828877449036
  - 0.7420889139175415
  - 0.7402896285057068
  - 0.8384401798248291
  - 0.7181653380393982
  - 0.7044277787208557
  - 0.5956869721412659
  - 0.638835072517395
  - 0.6240208148956299
  - 2.924616575241089
  - 0.9448956251144409
  - 0.9205425381660461
  - 1.069822072982788
  - 0.9090480804443359
  - 0.7721344828605652
  - 0.5702898502349854
  - 0.7085414528846741
  - 0.6105202436447144
  - 0.5253981351852417
  - 0.5536370277404785
  - 0.5411597490310669
  - 0.530183732509613
  - 0.852928638458252
  - 5.274724960327148
  - 0.49698495864868164
  - 0.5998568534851074
  - 0.5448542237281799
  - 0.4726828932762146
  - 0.450467973947525
  - 0.4171874225139618
  - 0.43576398491859436
  - 0.4216155409812927
  - 0.6560311913490295
  - 0.6852310299873352
  - 0.6712258458137512
  - 0.5797503590583801
  - 0.586076021194458
  - 0.6035203337669373
  - 0.619721531867981
  - 0.5558075308799744
  - 0.6219716668128967
  - 0.6348404288291931
  - 0.6310789585113525
  - 0.6210572719573975
  - 0.6170746684074402
  - 0.5559864640235901
  - 0.5460177063941956
  - 0.6200750470161438
  - 0.5863387584686279
  - 0.6599048376083374
  - 0.5785945653915405
  - 0.5658493638038635
  - 0.428625226020813
  - 0.40431246161460876
  - 0.4291708767414093
  - 0.46321865916252136
  - 0.46966123580932617
  - 0.49949923157691956
  - 0.48086193203926086
  - 0.4869062900543213
  - 0.5506401062011719
  - 0.8464716672897339
  - 0.6500550508499146
  - 0.6965566277503967
  - 0.4714047610759735
  - 0.4561743140220642
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:02:21.781417'
