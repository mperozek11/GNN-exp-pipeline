config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:46:43.041645'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_17fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_17fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_17fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_17fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_17fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_17fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_17fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_17fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_17fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_17fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.3297841548919678
  - 2.9483188539743423
  - 2.9585439652204517
  - 2.927916878461838
  - 2.966019278764725
  - 2.8420149773359302
  - 2.8814677357673646
  - 2.9244443595409395
  - 2.9293423116207125
  - 2.8994320541620255
  - 2.915834477543831
  validation_losses:
  - 0.48149996995925903
  - 0.4134598970413208
  - 0.40949729084968567
  - 0.38804882764816284
  - 0.40194523334503174
  - 0.38763508200645447
  - 0.3946666121482849
  - 0.3942224085330963
  - 0.3854299485683441
  - 0.39381441473960876
  - 0.38607120513916016
loss_records_fold1:
  train_losses:
  - 2.83244688808918
  - 2.9198044389486313
  - 2.8118623048067093
  - 2.839365229010582
  - 2.815149360895157
  - 2.947906455397606
  - 2.8258701264858246
  - 2.790759000182152
  - 2.7853407442569735
  - 2.9096065670251847
  - 2.894891059398651
  - 2.829824134707451
  - 2.7926695942878723
  - 2.8760900795459747
  - 2.796448737382889
  - 2.8053264141082765
  - 2.7901862233877184
  - 2.8319363355636598
  - 2.824524211883545
  - 2.814030480384827
  - 2.8302805721759796
  - 2.7628154903650284
  - 2.8459176152944567
  - 2.7734153479337693
  - 2.758025342226029
  - 2.801237922906876
  - 2.728648227453232
  - 2.7451912939548495
  - 2.783544284105301
  - 2.7723177015781406
  - 2.7480560302734376
  - 2.75814907848835
  - 2.733957028388977
  - 2.7521616131067277
  - 2.7491130739450456
  - 2.780010771751404
  - 2.7442333102226257
  - 2.7401925683021546
  - 2.75670217871666
  - 2.7826611310243607
  - 2.7505045741796494
  - 2.7534000098705294
  - 2.7207139402627947
  - 2.756211906671524
  - 2.746739685535431
  - 2.741296884417534
  - 2.7276476979255677
  - 2.7066647171974183
  - 2.7684634357690814
  - 2.705268603563309
  - 2.8434899508953095
  - 2.790449315309525
  - 2.743655985593796
  - 2.78430752158165
  - 2.8019180178642276
  - 2.730423241853714
  - 2.76142053604126
  - 2.7446931123733522
  - 2.7560926973819733
  - 2.7272712647914887
  - 2.68925541639328
  - 2.700540152192116
  - 2.7681919395923615
  - 2.747013604640961
  - 2.7422640323638916
  - 2.7240655422210693
  - 2.762018096446991
  - 2.7153331279754642
  - 2.6812374621629718
  - 2.707172963023186
  - 2.7456931948661807
  - 2.726638561487198
  - 2.717892023921013
  - 2.714098107814789
  - 2.6917790472507477
  - 2.7266924858093264
  - 2.716957125067711
  - 2.71725489795208
  - 2.8104988038539886
  - 2.821077746152878
  - 2.7812852680683138
  - 2.7106498748064043
  - 2.6704305171966554
  - 2.7338294446468354
  - 2.6971754997968675
  - 2.692126303911209
  - 2.78610360622406
  - 2.674871796369553
  - 2.6956367880105976
  - 2.7234877049922943
  - 2.7069931209087374
  - 2.733203834295273
  - 2.7153905898332598
  - 2.7579125225543977
  - 2.699963051080704
  - 2.74418962597847
  - 2.708405622839928
  - 2.705521395802498
  - 2.6866445899009705
  - 2.685919398069382
  validation_losses:
  - 0.3895100951194763
  - 0.4190388023853302
  - 0.5580716729164124
  - 0.3871215581893921
  - 0.46778663992881775
  - 0.3879546523094177
  - 0.39240288734436035
  - 0.41949307918548584
  - 0.5406882762908936
  - 0.40374642610549927
  - 0.38944339752197266
  - 0.3919098377227783
  - 0.39343714714050293
  - 0.41188952326774597
  - 0.3987760841846466
  - 0.40280404686927795
  - 0.39040523767471313
  - 0.40247583389282227
  - 0.39661574363708496
  - 0.40638870000839233
  - 0.5152178406715393
  - 0.46987250447273254
  - 0.4824994206428528
  - 0.49857908487319946
  - 0.5260250568389893
  - 0.46816855669021606
  - 0.5782087445259094
  - 0.5698790550231934
  - 0.6567584872245789
  - 0.44077202677726746
  - 0.5224552154541016
  - 0.5281797051429749
  - 0.582198977470398
  - 0.5450645089149475
  - 0.5058770179748535
  - 0.6890262365341187
  - 0.7986263632774353
  - 0.5230099558830261
  - 0.6297082901000977
  - 0.4609067738056183
  - 0.7721928358078003
  - 0.632300078868866
  - 0.6201520562171936
  - 0.6765185594558716
  - 0.873920202255249
  - 0.5908111929893494
  - 1.4877506494522095
  - 0.9710096716880798
  - 0.7661507725715637
  - 1.0132933855056763
  - 0.4178197979927063
  - 0.4182000160217285
  - 0.47492220997810364
  - 0.5638413429260254
  - 0.5064046382904053
  - 0.5020797252655029
  - 0.5332362055778503
  - 0.5038015842437744
  - 0.6244764924049377
  - 0.7638832926750183
  - 0.7207189202308655
  - 1.0973527431488037
  - 1.0094549655914307
  - 0.5913403034210205
  - 0.546058714389801
  - 0.5631908774375916
  - 0.8682970404624939
  - 0.6861633062362671
  - 0.6903528571128845
  - 0.8024569749832153
  - 0.661232054233551
  - 0.688447892665863
  - 0.7036283612251282
  - 0.7118481397628784
  - 0.8173857927322388
  - 0.6766763925552368
  - 0.9021135568618774
  - 0.7415429949760437
  - 0.6574352383613586
  - 0.5137697458267212
  - 0.6034740805625916
  - 0.6187159419059753
  - 0.6682125926017761
  - 0.6963178515434265
  - 0.6332696080207825
  - 0.6237513422966003
  - 0.7206140756607056
  - 0.6779919862747192
  - 0.7723044157028198
  - 0.6670329570770264
  - 0.6263555884361267
  - 0.7063899040222168
  - 0.641029417514801
  - 0.6678085923194885
  - 0.7721864581108093
  - 0.8265900015830994
  - 0.8720442652702332
  - 0.8136038184165955
  - 0.8083174824714661
  - 0.8254756331443787
loss_records_fold2:
  train_losses:
  - 2.7212358206510547
  - 2.7560936093330386
  - 2.8041376858949665
  - 2.7675893843173984
  - 2.689184653759003
  - 2.763886621594429
  - 2.731248378753662
  - 2.7229291737079624
  - 2.7071471214294434
  - 2.6954033225774765
  - 2.710123673081398
  - 2.776101565361023
  - 2.7368977189064028
  - 2.7396864891052246
  - 2.732167845964432
  - 2.6875931113958362
  - 2.711798551678658
  - 2.721299317479134
  - 2.726623567938805
  - 2.842986074090004
  - 2.7420498341321946
  - 2.7735810697078707
  - 2.782564228773117
  - 2.7771723687648775
  - 2.7537473380565647
  - 2.7215853065252307
  - 2.775881463289261
  - 2.7689401775598528
  - 2.744600933790207
  - 2.735465168952942
  - 2.7270826876163485
  - 2.744582080841065
  - 2.7493874579668045
  - 2.726022845506668
  - 2.695900017023087
  - 2.713975316286087
  - 2.778765815496445
  - 2.7137114197015766
  - 2.734691533446312
  - 2.7134596049785618
  - 2.6928222835063935
  - 2.664286583662033
  - 2.6495974361896515
  - 2.6802541822195054
  - 2.7058703571558
  - 2.6684871643781665
  - 2.693718746304512
  - 2.6729465961456302
  - 2.6427462220191957
  - 2.7854896545410157
  - 2.705917435884476
  - 2.7655039459466937
  - 2.748313742876053
  - 2.7290602862834934
  - 2.6913476169109347
  - 2.6884660810232166
  - 2.677439343929291
  - 2.7064412117004397
  - 2.675763300061226
  - 2.673007971048355
  - 2.6883095979690554
  - 2.653245756030083
  - 2.655479258298874
  - 2.6837549537420275
  - 2.711417701840401
  - 2.6960221588611604
  - 2.6603793859481812
  - 2.7148745566606522
  - 2.7201569795608522
  - 2.64955128133297
  - 2.7238034486770633
  - 2.6936541318893434
  - 2.692844817042351
  - 2.692713475227356
  - 2.6874663442373277
  - 2.6307686358690265
  - 2.672615110874176
  - 2.65125749707222
  - 2.6791732192039492
  - 2.671511322259903
  - 2.649168369174004
  - 2.663922882080078
  - 2.675445255637169
  - 2.6526320308446887
  - 2.6733977019786836
  - 2.678572487831116
  - 2.6879821717739105
  - 2.6569700270891192
  - 2.6411294847726823
  - 2.659930831193924
  - 2.658340507745743
  - 2.687223157286644
  - 2.657837763428688
  - 2.6473037689924244
  - 2.643196934461594
  - 2.666194033622742
  - 2.737405300140381
  - 2.674987882375717
  - 2.715235006809235
  - 2.617313462495804
  validation_losses:
  - 0.64532870054245
  - 0.5651782155036926
  - 0.44531527161598206
  - 0.49202466011047363
  - 0.5321053266525269
  - 0.6259415149688721
  - 0.5070075392723083
  - 0.5771864652633667
  - 0.5264941453933716
  - 0.55841064453125
  - 0.6392461061477661
  - 0.46548163890838623
  - 0.49963951110839844
  - 0.5532264113426208
  - 0.5997464656829834
  - 0.6332932114601135
  - 0.6147577166557312
  - 0.5857881903648376
  - 0.4926161766052246
  - 0.38954898715019226
  - 0.37803196907043457
  - 0.41170403361320496
  - 0.3799455463886261
  - 0.3983783721923828
  - 0.41114044189453125
  - 0.4039781987667084
  - 0.3819389343261719
  - 0.396551251411438
  - 0.4228152930736542
  - 0.40195488929748535
  - 0.5385950803756714
  - 0.48008811473846436
  - 0.38770192861557007
  - 0.394941121339798
  - 0.39881014823913574
  - 0.44713109731674194
  - 0.47308027744293213
  - 0.40657082200050354
  - 0.42350468039512634
  - 0.40938836336135864
  - 0.49944546818733215
  - 0.49939239025115967
  - 0.5106900930404663
  - 0.43185484409332275
  - 0.4121171236038208
  - 0.48313096165657043
  - 0.45615771412849426
  - 0.4701393246650696
  - 0.5622128248214722
  - 0.4208569824695587
  - 0.6170622110366821
  - 0.6911122798919678
  - 0.49551334977149963
  - 0.5045943260192871
  - 0.4303623139858246
  - 0.5605517625808716
  - 0.5950042009353638
  - 0.5689127445220947
  - 0.5147609114646912
  - 0.43942320346832275
  - 0.4750831127166748
  - 0.6001035571098328
  - 0.4556654691696167
  - 0.6943154335021973
  - 0.48462343215942383
  - 0.4569857120513916
  - 0.8351518511772156
  - 0.43580132722854614
  - 0.568092942237854
  - 0.6313657164573669
  - 0.5851344466209412
  - 0.6821611523628235
  - 0.4341870844364166
  - 0.49724718928337097
  - 0.4431666433811188
  - 0.48650622367858887
  - 0.5109692811965942
  - 0.5348459482192993
  - 0.454199880361557
  - 0.4542964994907379
  - 0.44499048590660095
  - 0.5318251848220825
  - 0.5555765628814697
  - 0.5499105453491211
  - 0.5524487495422363
  - 0.40132787823677063
  - 0.5040452480316162
  - 0.4891829788684845
  - 0.5590984225273132
  - 0.5360509157180786
  - 0.49952179193496704
  - 0.6011338829994202
  - 0.5798742771148682
  - 0.5608824491500854
  - 0.5125521421432495
  - 0.45691609382629395
  - 0.5557035207748413
  - 0.40866410732269287
  - 0.46304768323898315
  - 0.7462171912193298
loss_records_fold3:
  train_losses:
  - 2.669469302892685
  - 2.678410190343857
  - 2.790222826600075
  - 2.7097500145435336
  - 2.698418074846268
  - 2.6870642453432083
  - 2.7100866198539735
  - 2.6673144847154617
  - 2.6565744996070864
  - 2.649959009885788
  - 2.6428521245718004
  - 2.640640449523926
  - 2.656707248091698
  - 2.668159407377243
  - 2.6722761124372485
  - 2.643721064925194
  - 2.7174210965633394
  - 2.6921344608068467
  - 2.664768168330193
  - 2.6706814944744113
  - 2.6913079619407654
  - 2.7014966994524006
  - 2.705961546301842
  - 2.6333508610725405
  - 2.6765028685331345
  - 2.6578477352857592
  - 2.643371030688286
  - 2.6844756960868836
  - 2.6207219779491426
  - 2.646502044796944
  - 2.6683956652879717
  - 2.7226320654153824
  - 2.731690853834152
  - 2.718520897626877
  - 2.6885818600654603
  - 2.659077200293541
  - 2.6634527236223224
  - 2.6598194748163224
  - 2.7114102184772495
  - 2.7002334594726562
  - 2.7200281739234926
  - 2.6558743447065356
  - 2.7481293976306915
  - 2.6421650767326357
  - 2.6678690969944
  - 2.6820356190204624
  - 2.640385437011719
  - 2.708362564444542
  - 2.686925134062767
  - 2.6777850151062013
  - 2.647040864825249
  - 2.6138864248991016
  - 2.639916726946831
  - 2.6891496032476425
  - 2.6816706985235217
  - 2.7055742859840395
  - 2.6318200141191483
  - 2.6413652777671817
  - 2.7462801039218903
  - 2.6870006293058397
  - 2.69589661359787
  - 2.683050847053528
  - 2.6552554428577424
  - 2.610307914018631
  - 2.651127961277962
  - 2.6636290818452837
  - 2.673890966176987
  - 2.6772135466337206
  - 2.613971242308617
  - 2.6381838962435724
  - 2.6585493803024294
  - 2.612025406956673
  - 2.616566240787506
  - 2.625146219134331
  - 2.643261128664017
  - 2.596423837542534
  - 2.6287274152040485
  - 2.6525188714265826
  - 2.576059803366661
  - 2.6618784576654435
  - 2.637362450361252
  - 2.62449865937233
  - 2.67272079885006
  - 2.6763696134090424
  - 2.637020578980446
  - 2.6325322270393374
  - 2.6174263060092926
  - 2.6274439841508865
  - 2.6255035519599916
  - 2.6057740390300754
  - 2.6202602684497833
  - 2.658866745233536
  - 2.6835135519504547
  - 2.6472747027873993
  - 2.628525796532631
  - 2.66965828537941
  - 2.617250657081604
  - 2.5982401728630067
  - 2.5796593576669693
  - 2.5876644402742386
  validation_losses:
  - 0.4295593202114105
  - 0.4167695641517639
  - 0.3622503876686096
  - 0.3986953794956207
  - 0.4034905731678009
  - 0.4506579637527466
  - 0.48408031463623047
  - 0.5126582980155945
  - 0.517852246761322
  - 0.45582783222198486
  - 0.6647364497184753
  - 0.43894699215888977
  - 0.4503640830516815
  - 0.4211480915546417
  - 0.5719054937362671
  - 1.1234512329101562
  - 1.4490407705307007
  - 0.4174244701862335
  - 0.47104206681251526
  - 0.43230411410331726
  - 0.45553094148635864
  - 1.1048563718795776
  - 0.6166108846664429
  - 0.7442396879196167
  - 1.548325777053833
  - 1.1131237745285034
  - 0.8448384404182434
  - 0.5766140818595886
  - 0.5338056683540344
  - 0.9244017601013184
  - 0.9906188249588013
  - 1.2536507844924927
  - 0.6791183948516846
  - 0.5932554602622986
  - 0.7144729495048523
  - 0.7060174942016602
  - 0.8278982043266296
  - 0.7719894051551819
  - 0.4009782671928406
  - 0.536196231842041
  - 0.7927157878875732
  - 0.5006820559501648
  - 0.6518906354904175
  - 0.5913555026054382
  - 0.8168059587478638
  - 0.7643508911132812
  - 0.43629923462867737
  - 1.0799038410186768
  - 0.9376983046531677
  - 0.8168164491653442
  - 1.291920781135559
  - 1.215743899345398
  - 1.1681687831878662
  - 0.5312144756317139
  - 0.5042786598205566
  - 0.4559691250324249
  - 1.5176879167556763
  - 0.6740695238113403
  - 0.604335606098175
  - 0.546194314956665
  - 0.5980277061462402
  - 0.9087251424789429
  - 0.38995712995529175
  - 0.7058027982711792
  - 0.6297366619110107
  - 0.616534948348999
  - 0.4905085861682892
  - 0.5722022652626038
  - 0.5395298600196838
  - 1.3348487615585327
  - 0.5279771089553833
  - 0.6048144102096558
  - 1.0107206106185913
  - 0.6859459280967712
  - 0.5904846787452698
  - 0.4856016933917999
  - 0.5562566518783569
  - 0.614499032497406
  - 0.6458950042724609
  - 0.5559818744659424
  - 0.49291327595710754
  - 0.4329938292503357
  - 0.6686692833900452
  - 0.5712265372276306
  - 0.7288314700126648
  - 0.6360944509506226
  - 0.5133131742477417
  - 0.7322109341621399
  - 0.6980130076408386
  - 0.6033352017402649
  - 0.7125436067581177
  - 0.5009985566139221
  - 0.5721840262413025
  - 0.45402783155441284
  - 0.5786635875701904
  - 0.4529564082622528
  - 0.7892153859138489
  - 0.5998085141181946
  - 0.9766927361488342
  - 0.6729629039764404
loss_records_fold4:
  train_losses:
  - 2.6808309793472294
  - 2.6599247038364413
  - 2.647638511657715
  - 2.645110011100769
  - 2.7303364217281345
  - 2.658634230494499
  - 2.6461422443389893
  - 2.6492825686931614
  - 2.637163555622101
  - 2.6472124695777897
  - 2.6328162252902985
  - 2.6741298139095306
  - 2.6459203958511353
  - 2.6165060937404636
  - 2.6420965373516085
  - 2.6096524089574817
  - 2.628794857859612
  - 2.641455420851708
  - 2.6603901505470278
  - 2.655700942873955
  - 2.5922477036714557
  - 2.6160130709409715
  - 2.6788202404975894
  - 2.6754213720560074
  - 2.6268102139234544
  - 2.647498589754105
  - 2.627573545277119
  - 2.690275540947914
  - 2.621250915527344
  - 2.622041004896164
  - 2.654391807317734
  - 2.6154044717550278
  - 2.6801032662391666
  - 2.758483862876892
  - 2.7077587991952896
  - 2.7175569951534273
  - 2.674704515933991
  - 2.7097053050994875
  - 2.6911345034837724
  - 2.712144169211388
  - 2.6524168848991394
  - 2.6635183453559876
  - 2.5918641924858097
  - 2.6715502172708514
  - 2.679962792992592
  - 2.6776748418807985
  - 2.606775537133217
  - 2.607089897990227
  - 2.5945629477500916
  - 2.629117745161057
  - 2.6004010438919067
  - 2.701678913831711
  - 2.650965797901154
  - 2.697336381673813
  - 2.65556492805481
  - 2.606546270847321
  - 2.6405735999345783
  - 2.598437908291817
  - 2.6185116559267048
  - 2.638653826713562
  - 2.6066439628601077
  - 2.651652765274048
  - 2.6072712361812593
  - 2.6143335461616517
  - 2.5745495915412904
  - 2.5458708852529526
  - 2.632781705260277
  - 2.599421089887619
  - 2.644340080022812
  - 2.6110707551240924
  - 2.669969666004181
  - 2.58962721824646
  - 2.6678424119949344
  - 2.6346636533737184
  - 2.5806634008884433
  - 2.645746409893036
  - 2.5924195468425753
  - 2.629236578941345
  - 2.6036173671483995
  - 2.645222726464272
  - 2.619479897618294
  - 2.591240790486336
  - 2.5916308522224427
  - 2.6334482699632646
  - 2.6126553386449816
  - 2.551101189851761
  - 2.6563543289899827
  - 2.735451409220696
  - 2.8038121312856674
  - 2.7139479517936707
  - 2.6796130388975143
  - 2.7511417478322984
  - 2.7555979788303375
  - 2.6793476760387422
  - 2.6910306185483934
  - 2.669459947943688
  - 2.599998986721039
  - 2.626911163330078
  - 2.626686751842499
  - 2.5902893900871278
  validation_losses:
  - 0.6182234287261963
  - 0.7245444655418396
  - 0.7514950633049011
  - 0.5008254647254944
  - 1.2114803791046143
  - 0.5170858502388
  - 0.633679986000061
  - 0.5887714624404907
  - 0.6157707571983337
  - 0.564428985118866
  - 0.4926687479019165
  - 0.5000231862068176
  - 0.5915225148200989
  - 0.5051719546318054
  - 0.6378819942474365
  - 0.7204485535621643
  - 0.694379448890686
  - 0.5731902122497559
  - 0.5309914350509644
  - 0.5449278354644775
  - 0.6862373352050781
  - 0.6613079309463501
  - 0.5871720314025879
  - 0.5054197907447815
  - 0.5296686887741089
  - 0.6090154051780701
  - 0.5175142884254456
  - 0.5107598900794983
  - 0.6128407716751099
  - 0.6141756772994995
  - 0.5811530947685242
  - 0.5840145945549011
  - 0.8115384578704834
  - 0.3924347460269928
  - 0.37999439239501953
  - 0.46089303493499756
  - 0.4227435886859894
  - 0.4309120774269104
  - 0.4177354574203491
  - 0.3997470438480377
  - 0.5199097394943237
  - 0.4387597143650055
  - 0.49379250407218933
  - 0.6261549592018127
  - 0.398040771484375
  - 0.5555947422981262
  - 0.6407797932624817
  - 0.5539011359214783
  - 0.5711725354194641
  - 0.6059132218360901
  - 0.6112234592437744
  - 0.8380802273750305
  - 0.35108813643455505
  - 0.5276591181755066
  - 0.5534409880638123
  - 0.5754607915878296
  - 0.5416876077651978
  - 0.6577669978141785
  - 0.4441430866718292
  - 0.6128717064857483
  - 0.7709178924560547
  - 0.6289862394332886
  - 0.5273786187171936
  - 0.6482576727867126
  - 0.5767197608947754
  - 0.601995587348938
  - 0.5666926503181458
  - 0.5980291962623596
  - 0.6453547477722168
  - 0.5604631900787354
  - 0.6804741024971008
  - 0.6060463786125183
  - 0.5099143385887146
  - 0.5832816362380981
  - 0.510471761226654
  - 0.5236555337905884
  - 0.5498083233833313
  - 0.6193342208862305
  - 0.6271690726280212
  - 0.7273806929588318
  - 0.5913575291633606
  - 0.5602819919586182
  - 0.6405230760574341
  - 0.5401371121406555
  - 0.5874763131141663
  - 0.7139007449150085
  - 0.6121076345443726
  - 0.5447571277618408
  - 0.4994083344936371
  - 0.5345670580863953
  - 0.48910439014434814
  - 0.4662386476993561
  - 0.5368399620056152
  - 0.5133451819419861
  - 0.44393104314804077
  - 0.5561126470565796
  - 0.44555193185806274
  - 0.4559391140937805
  - 0.49619364738464355
  - 0.485460102558136
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8336192109777015, 0.7735849056603774, 0.8439108061749572,
    0.8367697594501718]'
  fold_eval_f1: '[0.0, 0.1415929203539823, 0.34653465346534656, 0.20869565217391303,
    0.24000000000000002]'
  mean_eval_accuracy: 0.8291035230735678
  mean_f1_accuracy: 0.18736464519864837
  total_train_time: '0:36:44.479758'
