config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:32:21.474501'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_64fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_64fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_64fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_64fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_64fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_64fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_64fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_64fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_64fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_64fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.112374192476273
  - 6.219090795516968
  - 6.185378530621529
  - 5.917741164565086
  - 6.683507399260998
  - 5.87253315448761
  - 5.7253393024206165
  - 5.695661622285844
  - 5.87094663977623
  - 5.7246336370706565
  - 5.644946831464768
  - 5.728011730313302
  - 5.629425197839737
  - 5.63006835281849
  - 5.618578580021858
  - 5.704673285782338
  - 5.628829953074455
  - 5.614178332686425
  - 5.633398354053497
  - 5.599621456861496
  - 5.597463965415955
  - 5.635312476754189
  - 5.584886205196381
  - 5.575511729717255
  - 5.6155566543340685
  - 5.604527640342713
  - 5.626291516423226
  - 5.798708212375641
  - 5.673771423101425
  validation_losses:
  - 0.4052982032299042
  - 0.41928377747535706
  - 0.3976890444755554
  - 0.3952440917491913
  - 0.3874092400074005
  - 0.3887416124343872
  - 0.40373003482818604
  - 0.389156699180603
  - 0.4404413104057312
  - 0.4261496067047119
  - 0.4013628661632538
  - 0.4130977988243103
  - 0.38984212279319763
  - 0.38904622197151184
  - 0.4027164876461029
  - 0.38686999678611755
  - 0.38954585790634155
  - 0.40580621361732483
  - 0.3882471024990082
  - 0.3869997560977936
  - 0.38594624400138855
  - 0.3991986811161041
  - 0.4280853271484375
  - 0.3960250914096832
  - 0.39349880814552307
  - 0.39415261149406433
  - 0.39503276348114014
  - 0.39148271083831787
  - 0.39335331320762634
loss_records_fold1:
  train_losses:
  - 5.726185500621796
  - 5.696375212073327
  - 5.594347086548805
  - 5.564898300170899
  - 5.573025065660477
  - 5.585733589529991
  - 5.582833364605904
  - 5.554682895541191
  - 5.552249982953072
  - 5.576071080565453
  - 5.6112251207232475
  validation_losses:
  - 0.39804568886756897
  - 0.3924553394317627
  - 0.3902851939201355
  - 0.38776305317878723
  - 0.3858652412891388
  - 0.38755330443382263
  - 0.3880654275417328
  - 0.3893405497074127
  - 0.3909863531589508
  - 0.3955841660499573
  - 0.3891671597957611
loss_records_fold2:
  train_losses:
  - 5.669471773505212
  - 5.642283868789673
  - 5.61300320327282
  - 5.639961639046669
  - 5.573720201849937
  - 5.566999590396882
  - 5.586261838674545
  - 5.559808611869812
  - 5.609125962853432
  - 5.6110274493694305
  - 5.545525336265564
  - 5.597302505373955
  - 5.5618248075246814
  - 5.625598675012589
  - 5.5266917824745185
  - 5.5913483083248146
  - 5.556645470857621
  - 5.494735145568848
  - 5.582672980427742
  - 5.5460853099823
  - 5.533437579870224
  - 5.545249211788178
  - 5.582806134223938
  - 5.530954745411873
  - 5.516747838258744
  - 5.4598294183611875
  - 5.573675131797791
  - 5.544545316696167
  - 5.499338975548745
  - 5.500821897387505
  - 5.508775758743287
  - 5.513707306981087
  - 5.523885875940323
  - 5.511369812488557
  - 5.507587260007859
  - 5.480608567595482
  - 5.522573450207711
  - 5.560973262786866
  - 5.440718457102776
  - 5.473610362410546
  - 5.445253023505211
  - 5.51484509408474
  - 5.462627157568932
  - 5.475108420848847
  - 5.494159451127053
  - 5.55721082687378
  - 5.436073458194733
  - 5.441684073209763
  - 5.478627699613572
  - 5.551031920313836
  - 5.471863421797753
  - 5.518998566269875
  - 5.565094769001007
  - 5.639679726958275
  validation_losses:
  - 0.383238285779953
  - 0.3853267729282379
  - 0.38505488634109497
  - 0.3863103687763214
  - 0.38680148124694824
  - 0.3878553509712219
  - 0.38716480135917664
  - 0.5722991824150085
  - 0.3874489665031433
  - 0.39461684226989746
  - 0.7553292512893677
  - 0.401852011680603
  - 0.42350882291793823
  - 0.40158379077911377
  - 0.7594759464263916
  - 1.1514374017715454
  - 1.3740851879119873
  - 0.5889642834663391
  - 0.4212510883808136
  - 2.6245787143707275
  - 0.57696932554245
  - 0.3938698172569275
  - 0.5074920058250427
  - 0.48539313673973083
  - 0.5114485621452332
  - 0.6717140078544617
  - 0.46462616324424744
  - 0.46468421816825867
  - 0.7232056856155396
  - 0.41751742362976074
  - 0.5405488014221191
  - 0.49158427119255066
  - 0.3846340477466583
  - 0.5159063339233398
  - 0.5545276999473572
  - 0.40461090207099915
  - 0.6547037363052368
  - 0.4384417235851288
  - 0.40837785601615906
  - 0.6110065579414368
  - 0.5653442740440369
  - 0.4996384084224701
  - 0.7332316040992737
  - 0.4509722888469696
  - 0.5643306970596313
  - 0.5029439330101013
  - 0.48521682620048523
  - 0.5989976525306702
  - 0.5758523941040039
  - 0.39988434314727783
  - 0.3877803683280945
  - 0.39716216921806335
  - 0.38588958978652954
  - 0.3914150595664978
loss_records_fold3:
  train_losses:
  - 5.563225066661835
  - 5.463829046487809
  - 5.561228880286217
  - 5.514945748448373
  - 5.566874903440476
  - 5.457225948572159
  - 5.471397328376771
  - 5.47142837047577
  - 5.464662489295006
  - 5.488458672165871
  - 5.5703668951988226
  - 5.485852810740472
  - 5.610871973633767
  - 5.522863638401032
  - 5.458015707135201
  - 5.5681192278862
  - 5.56176261305809
  - 5.4967601060867315
  - 5.5346794664859775
  - 5.439148539304734
  - 5.5689394026994705
  - 5.5392713189125065
  - 5.665216726064682
  - 5.544631901383401
  - 5.583447125554085
  - 5.587131851911545
  - 5.505277559161186
  - 5.44738661646843
  - 5.529564259946347
  - 5.45271123945713
  - 5.581467026472092
  - 5.531663072109223
  - 5.53595706820488
  - 5.525591853260995
  - 5.523922109603882
  - 5.486999288201332
  - 5.482326495647431
  - 5.532489621639252
  - 5.518349593877793
  - 5.503415611386299
  - 5.51961687207222
  - 5.510450175404549
  - 5.614065024256707
  - 5.4527997255325324
  - 5.551485291123391
  - 5.523086839914322
  - 5.554834565520287
  - 5.4536029398441315
  - 5.480393740534783
  - 5.521705085039139
  - 5.551370716094971
  - 5.5356305867433555
  - 5.470908832550049
  - 5.522523912787438
  - 5.4492649167776115
  - 5.534134873747826
  - 5.476243317127228
  - 5.520122990012169
  - 5.502913534641266
  - 5.491651192307472
  - 5.534872287511826
  - 5.500770920515061
  - 5.557074204087257
  - 5.45250977575779
  - 5.521567136049271
  - 5.613902294635773
  - 5.540339076519013
  - 5.488816758990288
  - 5.524838334321976
  - 5.478626674413682
  - 5.546968558430672
  - 5.543351060152054
  - 5.624233260750771
  - 5.607171681523323
  - 5.509232100844383
  - 5.522169050574303
  - 5.544764989614487
  - 5.457521277666093
  - 5.47578704059124
  - 5.448808270692826
  - 5.5067527294158936
  - 5.448038166761399
  - 5.538603973388672
  - 5.442617076635361
  - 5.457445120811463
  - 5.414046314358711
  - 5.62681023478508
  - 5.5166125208139425
  - 5.574389880895615
  - 5.546898511052132
  - 5.521069273352623
  - 5.515991950035096
  - 5.431762146949769
  - 5.41587048470974
  - 5.48592963218689
  - 5.500583064556122
  - 5.529031798243523
  - 5.438693529367447
  - 5.514141434431076
  - 5.530494040250779
  validation_losses:
  - 0.4350169897079468
  - 0.471533864736557
  - 0.8331130743026733
  - 0.46675214171409607
  - 0.6398230791091919
  - 0.6649481654167175
  - 0.7182020545005798
  - 0.8619903326034546
  - 0.8305302262306213
  - 0.4183347523212433
  - 0.40500256419181824
  - 0.7352800369262695
  - 0.7365615367889404
  - 0.6268688440322876
  - 0.6233906745910645
  - 0.7755074501037598
  - 0.7536802887916565
  - 0.4616505801677704
  - 0.4795188903808594
  - 0.5214378833770752
  - 0.4320288300514221
  - 0.6474382281303406
  - 0.7064762115478516
  - 0.5586739778518677
  - 0.5332054495811462
  - 0.5507542490959167
  - 0.6352574825286865
  - 0.7908539175987244
  - 0.5469403266906738
  - 0.8008649945259094
  - 0.6167011260986328
  - 0.6585806608200073
  - 0.5098984837532043
  - 0.39466115832328796
  - 0.5881208181381226
  - 0.5942233204841614
  - 0.9499951601028442
  - 0.6157721877098083
  - 0.4703814685344696
  - 0.6092855334281921
  - 0.7085936069488525
  - 0.7066279053688049
  - 0.702171802520752
  - 0.9403459429740906
  - 0.4181802272796631
  - 0.5626686215400696
  - 0.7485539317131042
  - 1.684898853302002
  - 1.8223649263381958
  - 1.1107957363128662
  - 0.7626732587814331
  - 0.8768673539161682
  - 0.7710738778114319
  - 0.5595660209655762
  - 0.7862035632133484
  - 0.4029979407787323
  - 0.48011332750320435
  - 1.032820224761963
  - 0.8015418648719788
  - 0.6961798667907715
  - 0.42299625277519226
  - 0.8901937007904053
  - 0.5596354603767395
  - 0.7187820672988892
  - 0.7830543518066406
  - 0.6622589230537415
  - 0.7761465311050415
  - 0.6791507601737976
  - 0.4354804754257202
  - 0.5762064456939697
  - 0.7921622395515442
  - 0.39855125546455383
  - 0.3799784183502197
  - 1.0302263498306274
  - 2.139195442199707
  - 1.2140071392059326
  - 0.9436732530593872
  - 0.6751985549926758
  - 1.1566367149353027
  - 0.9050948619842529
  - 1.406901478767395
  - 0.9726119041442871
  - 0.9480745792388916
  - 2.17000150680542
  - 0.7714672684669495
  - 2.0614817142486572
  - 0.4853929281234741
  - 0.42460137605667114
  - 0.5917981266975403
  - 0.6075825095176697
  - 0.5582178831100464
  - 0.37615469098091125
  - 0.6104000210762024
  - 0.7094123959541321
  - 0.5729082226753235
  - 0.5108798742294312
  - 0.44132235646247864
  - 0.6698371767997742
  - 0.5065692663192749
  - 0.5678123831748962
loss_records_fold4:
  train_losses:
  - 5.48313621878624
  - 5.493687090277672
  - 5.497605356574059
  - 5.486391299962998
  - 5.468256506323815
  - 5.4919353306293495
  - 5.509658259153366
  - 5.460537850856781
  - 5.480723878741265
  - 5.473477694392205
  - 5.47876496911049
  - 5.702896875143051
  - 5.556011453270912
  - 5.411463388800621
  - 5.586145281791687
  - 5.450402638316155
  - 5.509260499477387
  - 5.497401401400566
  - 5.497378084063531
  - 5.549353379011155
  - 5.470705592632294
  - 5.512682044506073
  - 5.528638178110123
  - 5.497561085224152
  - 5.500021061301232
  - 5.47735433280468
  - 5.471035178005696
  - 5.565075576305389
  - 5.622084200382233
  - 5.5069893360137945
  - 5.534157380461693
  - 5.57565338909626
  - 5.515540951490403
  - 5.521982681751251
  - 5.47035925090313
  - 5.494039341807365
  validation_losses:
  - 0.4179685711860657
  - 0.4235874116420746
  - 0.4295941889286041
  - 0.43025317788124084
  - 0.3854532837867737
  - 0.40409332513809204
  - 0.3981919288635254
  - 0.4109330475330353
  - 0.38438019156455994
  - 0.4237064719200134
  - 0.41466471552848816
  - 0.4438823163509369
  - 0.4132852256298065
  - 0.43295496702194214
  - 0.4299677312374115
  - 0.4707714021205902
  - 0.4186845123767853
  - 0.4041506350040436
  - 0.4107389748096466
  - 0.44903889298439026
  - 0.3857688903808594
  - 0.37110888957977295
  - 0.4359200894832611
  - 0.46439895033836365
  - 0.3872132897377014
  - 0.37524843215942383
  - 0.40567079186439514
  - 0.37011730670928955
  - 0.3750530183315277
  - 0.39233094453811646
  - 0.39688563346862793
  - 0.3764922320842743
  - 0.3776019215583801
  - 0.3810792565345764
  - 0.3842315971851349
  - 0.3904595971107483
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 54 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 36 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8487972508591065]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.08333333333333334]'
  mean_eval_accuracy: 0.8565519030020099
  mean_f1_accuracy: 0.01666666666666667
  total_train_time: '0:24:06.088921'
