config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cpu
  dropout: 0.0
  epochs: 30
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.05
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 5
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-09 14:57:18.428849'
fold_0_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_4fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_4fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_4fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_4fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_4fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_4fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_4fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_4fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_4fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_4fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 180.56648765951397
  - 22.41786365956068
  - 13.562779662013055
  - 10.789809328317643
  - 11.577778908237816
  - 9.999527907371522
  - 12.720296743512154
  - 10.79298527389765
  - 11.112799802422524
  - 14.78742558658123
  - 7.248825272917748
  - 8.119270603358746
  - 7.382709999382496
  - 7.8566773474216465
  - 7.6689374387264255
  - 7.365262304246426
  - 10.468447893857956
  - 8.25184120759368
  - 6.94659476429224
  - 10.307444712519647
  - 7.881551007926465
  - 7.053112044930458
  - 8.380228337645532
  - 8.457243360579014
  - 7.707588398456574
  - 9.297105565667152
  - 7.436721813678742
  - 8.90743134766817
  - 6.872108185291291
  - 8.34598437100649
  validation_losses:
  - 0.5310789346694946
  - 0.8252617120742798
  - 0.4291233420372009
  - 0.7085036039352417
  - 0.39519038796424866
  - 0.5031121373176575
  - 1.646606683731079
  - 1746.0482177734375
  - 1581.00390625
  - 1894588.375
  - 8182316425805824.0
  - 0.4054900109767914
  - 0.43104785680770874
  - 0.8663102984428406
  - 0.39327919483184814
  - 1.2339876890182495
  - 0.7566450834274292
  - 0.3937227129936218
  - 0.7070158123970032
  - 0.642558753490448
  - 0.4020124077796936
  - 0.4293263852596283
  - 0.5695480704307556
  - 0.787029504776001
  - 0.7881823778152466
  - 0.45497411489486694
  - 0.9043514132499695
  - 0.44492360949516296
  - 0.4228461682796478
  - 0.5620401501655579
loss_records_fold1:
  train_losses:
  - 7.5867645889520645
  - 8.438602894544601
  - 6.950457355380059
  - 7.2086149811744695
  - 8.554159235581755
  - 6.4713075220584875
  - 7.837269073724747
  - 8.135572576522828
  - 6.994573749601841
  - 7.222344762086869
  - 7.536243657767773
  - 8.55711205676198
  - 8.54622605741024
  - 6.425163432955742
  - 8.303047397732735
  - 7.685352176427841
  - 7.8697544381022455
  - 6.9614836275577545
  - 7.6088028192520145
  - 8.543285066634416
  - 6.682745596766472
  - 6.905723121762276
  - 8.476013321429491
  - 7.363397121429443
  - 8.541743898391724
  - 6.748930722475052
  - 10.003586150333286
  - 7.443342073261738
  - 8.330011431872846
  - 6.764335578680039
  validation_losses:
  - 0.4127438962459564
  - 0.48050904273986816
  - 0.4161864221096039
  - 0.4086633622646332
  - 0.41037169098854065
  - 0.4155607521533966
  - 0.4473710358142853
  - 0.6856400370597839
  - 0.4128720164299011
  - 0.4185265302658081
  - 0.7265585064888
  - 0.7413115501403809
  - 0.4306035041809082
  - 0.643484890460968
  - 0.6137582063674927
  - 0.4343731999397278
  - 0.5013666152954102
  - 0.4363623261451721
  - 0.7805755138397217
  - 0.5243938565254211
  - 0.53448086977005
  - 0.934720516204834
  - 0.6123009324073792
  - 0.4259384870529175
  - 0.7909495830535889
  - 0.4223363697528839
  - 0.42977529764175415
  - 0.4104389548301697
  - 0.7420544624328613
  - 0.4117491543292999
loss_records_fold2:
  train_losses:
  - 7.10705228894949
  - 10.048494711518288
  - 12.77440488934517
  - 7.828981372714043
  - 7.366596323251724
  - 6.860164357721806
  - 10.46371409893036
  - 6.6355462044477465
  - 8.083470579981805
  - 8.83102540075779
  - 7.5141255289316184
  - 9.464226505160331
  - 7.8892294928431514
  - 6.688700503110886
  - 7.611809116601944
  - 7.418431669473648
  - 8.840312391519547
  - 7.646815624833107
  - 7.194051143527031
  - 7.235342372581363
  - 7.959070292115212
  - 8.816319037973882
  - 7.302747315168381
  - 6.491122058033944
  - 9.159442114830018
  - 8.63213484287262
  - 8.387740439176559
  - 6.847331932187081
  - 8.105088683962823
  - 7.5915952086448675
  validation_losses:
  - 0.7876096963882446
  - 0.9741078019142151
  - 0.41090667247772217
  - 0.40371719002723694
  - 0.387602835893631
  - 0.5406800508499146
  - 0.4130132794380188
  - 0.47863757610321045
  - 0.5524114966392517
  - 0.448739618062973
  - 0.38538897037506104
  - 0.9120069742202759
  - 1.1491938829421997
  - 0.498861163854599
  - 0.4221749007701874
  - 0.5325413942337036
  - 0.38864248991012573
  - 0.3952137231826782
  - 0.40763145685195923
  - 0.483340322971344
  - 0.3834182620048523
  - 0.3968247175216675
  - 0.835218071937561
  - 0.4410496652126312
  - 0.5571624040603638
  - 0.5606721043586731
  - 0.4801643192768097
  - 0.39346569776535034
  - 0.6072310209274292
  - 0.5274443030357361
loss_records_fold3:
  train_losses:
  - 13.49449515901506
  - 8.996345888078213
  - 6.928243769705296
  - 7.196748152375221
  - 8.276723369956017
  - 6.532504129409791
  - 7.690869152545929
  - 6.9195452913641935
  - 9.790749007463456
  - 6.826600805670023
  - 7.823404750227929
  - 8.336707890033722
  - 7.910200157761574
  - 10.002428123354912
  - 7.035307801514865
  - 6.651871350407601
  - 8.168562850356102
  - 8.125494748353958
  - 6.5590303361415865
  - 6.922171834111214
  validation_losses:
  - 1.4205182790756226
  - 0.42409491539001465
  - 0.5928458571434021
  - 0.4283609092235565
  - 0.403847336769104
  - 0.3994870185852051
  - 0.5103670954704285
  - 0.5221737027168274
  - 0.41164201498031616
  - 0.5748215913772583
  - 0.7013007402420044
  - 0.4447033405303955
  - 0.774101972579956
  - 0.4287368953227997
  - 0.4889835715293884
  - 0.6939523220062256
  - 0.479957640171051
  - 0.4602569043636322
  - 0.4098314046859741
  - 0.4102054238319397
loss_records_fold4:
  train_losses:
  - 6.669984146952629
  - 8.56261957883835
  - 7.3390773177146915
  - 8.934374606609344
  - 8.600605481863022
  - 7.229509410262108
  - 7.555072438716889
  - 7.532584992051125
  - 8.08427619189024
  - 7.478115043044091
  - 9.090848150849343
  - 6.749534134566784
  - 8.197672265768052
  validation_losses:
  - 0.43824490904808044
  - 0.5786784291267395
  - 0.5677153468132019
  - 0.4145602583885193
  - 0.7572247982025146
  - 0.7310118079185486
  - 0.4034411907196045
  - 0.420522540807724
  - 0.7551950812339783
  - 0.5683058500289917
  - 0.47281622886657715
  - 0.4403679668903351
  - 0.4082096219062805
training fold messages:
  fold 0 training message: completed 30 epochs without stopping early
  fold 1 training message: completed 30 epochs without stopping early
  fold 2 training message: completed 30 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:05:07.703729'
