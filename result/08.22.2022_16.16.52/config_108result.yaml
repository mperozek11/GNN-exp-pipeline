config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:44:39.738355'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_108fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_108fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_108fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_108fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_108fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_108fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_108fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_108fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_108fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_108fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 119.87298864722253
  - 73.13665215075017
  - 55.89301480650902
  - 67.96385462731124
  - 33.54309929609299
  - 36.42387847155333
  - 33.599547773599625
  - 34.35411668419838
  - 20.921949216723444
  - 33.7271023452282
  - 25.857350260019302
  - 29.360778608918192
  - 37.33069885373116
  - 26.857936614751818
  - 18.168700379133224
  - 11.742155528068544
  - 16.64853964447975
  - 10.573678606748581
  - 12.869771778583527
  - 14.030601164698602
  - 10.863863420486451
  - 10.157411819696428
  - 7.891868001222611
  - 9.752304282784463
  - 8.408472526073457
  - 8.44709507226944
  - 7.5832201719284065
  - 8.111867371201516
  - 7.6186116516590126
  - 8.423026835918426
  - 11.249698063731195
  - 9.302337849140168
  - 8.151752027869225
  - 8.84060489833355
  - 8.446079811453819
  - 7.458883991837502
  - 8.134573367238046
  - 6.418283146619797
  - 6.704692730307579
  - 7.470522165298462
  - 7.001418310403825
  - 6.791145157814026
  - 6.632680326700211
  - 6.703979542851449
  - 6.646995547413827
  - 6.640483644604683
  - 7.2235339969396595
  - 6.569125464558602
  - 6.7988993555307395
  - 8.522366273403168
  - 7.774399942159653
  - 27.18212940990925
  - 9.59261804819107
  - 8.796730184555054
  - 7.636484915018082
  - 7.634317806363106
  - 6.7577764898538595
  - 6.62563622891903
  - 6.633560766279698
  - 6.753356727957726
  - 6.888807737827301
  - 6.54879773259163
  - 6.6724674820899965
  - 6.56571408212185
  - 6.299548757076264
  - 6.445550137758255
  - 6.9500712841749195
  - 7.695567101240158
  - 6.482372125983239
  - 6.38485643863678
  - 6.5304160714149475
  - 6.759041598439217
  - 6.703725671768189
  - 6.728392161428928
  - 6.54774189889431
  - 7.06320478618145
  - 7.053230968117714
  - 6.670157104730606
  - 6.779767796397209
  - 6.428461819887161
  - 6.445736819505692
  - 6.799933505058289
  validation_losses:
  - 2.1111221313476562
  - 1.45659339427948
  - 1.0094853639602661
  - 0.9004641175270081
  - 0.5727865099906921
  - 0.5878499746322632
  - 1.0815376043319702
  - 0.7808355093002319
  - 0.5626800656318665
  - 0.7366107702255249
  - 0.5382391214370728
  - 0.4687631130218506
  - 0.3972753882408142
  - 0.4087367057800293
  - 0.4478670358657837
  - 0.408511757850647
  - 0.39035654067993164
  - 0.42467954754829407
  - 0.41094714403152466
  - 0.410200297832489
  - 0.4583984315395355
  - 0.43022266030311584
  - 0.44192543625831604
  - 0.3927975594997406
  - 0.4147740602493286
  - 0.41580262780189514
  - 0.4449758231639862
  - 0.5102125406265259
  - 0.42346206307411194
  - 0.41047582030296326
  - 0.3985517621040344
  - 0.4107230603694916
  - 0.41140738129615784
  - 0.43045175075531006
  - 0.4773213267326355
  - 4.38949728012085
  - 0.40598174929618835
  - 0.4207133948802948
  - 0.48659032583236694
  - 0.4151861071586609
  - 0.43456771969795227
  - 0.4078182876110077
  - 0.42597565054893494
  - 0.4086318016052246
  - 0.4038434624671936
  - 0.4264853894710541
  - 0.397844135761261
  - 0.4230170249938965
  - 0.48642498254776
  - 1.3934952020645142
  - 18.430606842041016
  - 0.7249055504798889
  - 0.4102068841457367
  - 0.39544418454170227
  - 0.4429006278514862
  - 0.4078764319419861
  - 0.42078039050102234
  - 0.41409915685653687
  - 0.5110437273979187
  - 0.4423348903656006
  - 0.4753413796424866
  - 0.4091815948486328
  - 0.4415651857852936
  - 0.40872764587402344
  - 0.43050214648246765
  - 0.4261902868747711
  - 0.42058080434799194
  - 0.4139274060726166
  - 0.41741642355918884
  - 0.4004863202571869
  - 0.42990490794181824
  - 0.5798825621604919
  - 0.41559305787086487
  - 0.41585108637809753
  - 0.4131787121295929
  - 0.8617046475410461
  - 0.4449521601200104
  - 0.44268763065338135
  - 0.43047064542770386
  - 0.42222678661346436
  - 0.4029882550239563
  - 0.4078625738620758
loss_records_fold1:
  train_losses:
  - 6.520128375291825
  - 6.273156714439392
  - 6.397817957401276
  - 6.604287931323052
  - 6.404344069957734
  - 6.485152512788773
  - 6.4501933574676515
  - 6.087644627690316
  - 6.448543798923493
  - 6.60164555311203
  - 6.551333937048913
  - 6.400180527567864
  - 6.49713491499424
  - 6.891404914855958
  - 6.54311210513115
  - 6.480841934680939
  - 6.414380264282227
  - 6.2372420072555546
  - 6.282302677631378
  - 6.14659059047699
  - 6.547449225187302
  - 6.654430568218231
  - 6.452324545383454
  - 7.080753764510155
  - 7.930456310510635
  - 7.891957724094391
  - 8.677346366643906
  - 6.629397505521775
  - 6.319602999091149
  - 6.656499552726746
  - 6.546150821447373
  - 6.394492653012276
  - 6.5698731511831285
  - 6.669669765233994
  - 6.23992637693882
  - 6.307606434822083
  - 6.43696495294571
  - 6.413218837976456
  - 6.257479676604271
  - 6.368909063935281
  - 6.288926067948342
  - 6.4807000160217285
  - 6.224569487571717
  - 6.437217396497727
  - 6.522235333919525
  - 7.036765715479851
  - 6.431407251954079
  - 6.632646203041077
  - 9.056047508120537
  - 14.452191653847695
  - 7.876526296138763
  - 7.021767407655716
  - 6.562439024448395
  - 6.633689308166504
  - 6.801599350571633
  - 6.279964855313302
  - 6.466585808992386
  - 6.406787160038949
  - 6.220154497027398
  - 6.323260575532913
  - 6.291949155926705
  - 6.43784551024437
  - 6.338658112287522
  - 6.769981861114502
  - 6.518729653954506
  - 6.440338480472565
  - 6.31782013475895
  - 6.336885018646718
  - 6.601561069488525
  - 6.668826547265053
  - 6.234963148832321
  - 6.562332257628441
  - 6.428576216101646
  - 6.515964761376381
  - 6.554388147592545
  - 6.330228412151337
  - 6.281363546848297
  - 6.433628484606743
  - 6.310106498003006
  - 6.520428496599198
  - 6.431586059927941
  - 6.5109062731266025
  - 6.585828292369843
  - 6.358486205339432
  - 6.665583595633507
  - 6.294726699590683
  - 6.556966835260392
  - 6.414212796092034
  - 6.458812201023102
  - 6.370328637957574
  - 6.882296210527421
  - 6.480441051721574
  - 6.503166502714158
  - 6.430572271347046
  - 6.317311626672745
  - 6.179074865579605
  - 6.483198046684265
  - 6.3908648014068605
  - 6.3398414880037315
  - 6.222816354036332
  validation_losses:
  - 0.43332725763320923
  - 0.4168328642845154
  - 0.41473469138145447
  - 0.4295995235443115
  - 0.45670023560523987
  - 0.42048344016075134
  - 0.4168640375137329
  - 0.41915491223335266
  - 0.47233349084854126
  - 1.2577424049377441
  - 1176.509521484375
  - 0.4076140224933624
  - 0.4429216682910919
  - 0.45812472701072693
  - 0.4338204860687256
  - 0.4104428291320801
  - 0.42289748787879944
  - 0.40586596727371216
  - 0.41458192467689514
  - 0.4258250296115875
  - 0.4601682424545288
  - 37.7464485168457
  - 46.21116638183594
  - 0.4562719762325287
  - 0.4420880675315857
  - 0.43032336235046387
  - 0.42992961406707764
  - 0.4308893084526062
  - 2933.14697265625
  - 74303408.0
  - 113747736.0
  - 14336638.0
  - 12730857.0
  - 638081600.0
  - 523326.53125
  - 165769664.0
  - 77785336.0
  - 40340.05078125
  - 43468272.0
  - 205639440.0
  - 19038148.0
  - 199372176.0
  - 184985664.0
  - 0.5012800097465515
  - 0.41878384351730347
  - 0.9674645066261292
  - 5054.73583984375
  - 36.41961669921875
  - 1.3197274208068848
  - 0.4459843635559082
  - 18.36916732788086
  - 8116.21826171875
  - 26613.02734375
  - 117.49967956542969
  - 56.517520904541016
  - 0.47689858078956604
  - 0.5283543467521667
  - 0.426211416721344
  - 741235200.0
  - 5620954159906816.0
  - 0.4444023668766022
  - 0.4069960415363312
  - 0.4517797827720642
  - 0.448648601770401
  - 0.4099451005458832
  - 0.43767234683036804
  - 0.4140741229057312
  - 0.5294370651245117
  - 0.4204552471637726
  - 0.4098292291164398
  - 0.4372982084751129
  - 0.44310227036476135
  - 0.423690527677536
  - 0.4289151132106781
  - 0.521527111530304
  - 0.41244879364967346
  - 0.43889686465263367
  - 0.41400331258773804
  - 0.43256109952926636
  - 0.4115135371685028
  - 0.51915043592453
  - 0.41391074657440186
  - 0.4120490849018097
  - 0.4280186593532562
  - 0.5073174834251404
  - 0.4110963046550751
  - 0.43511393666267395
  - 0.4044138491153717
  - 0.5554142594337463
  - 0.4149666130542755
  - 0.45003780722618103
  - 0.42611873149871826
  - 0.46005669236183167
  - 0.416576623916626
  - 0.42748764157295227
  - 0.47484132647514343
  - 0.40797334909439087
  - 0.42151716351509094
  - 0.4194910526275635
  - 0.41592779755592346
loss_records_fold2:
  train_losses:
  - 6.561472409963608
  - 6.3670952171087265
  - 6.218211364746094
  - 6.569018703699112
  - 6.682851132750511
  - 6.444542127847672
  - 6.476735058426858
  - 6.51738515496254
  - 6.560122808814049
  - 6.316474029421807
  - 6.558106333017349
  - 6.55082106590271
  - 6.577143505215645
  - 6.584804916381836
  - 6.582756751775742
  - 6.362953937053681
  - 6.41729274392128
  - 6.500952658057213
  - 6.495762497186661
  - 6.666073849797249
  - 7.068031063675881
  - 6.661391997337342
  - 6.2752711296081545
  - 6.526523977518082
  - 6.3562543332576755
  - 6.4443844854831696
  - 6.377103036642075
  - 6.769937664270401
  - 6.736253771185876
  - 6.383398017287255
  - 6.2537896811962135
  - 6.463977667689324
  - 6.610096111893654
  - 6.4670123100280765
  validation_losses:
  - 0.460766077041626
  - 0.4033704996109009
  - 0.399249792098999
  - 0.40188390016555786
  - 0.40542393922805786
  - 0.41228389739990234
  - 0.40455764532089233
  - 0.40589380264282227
  - 0.4065553843975067
  - 0.4169805645942688
  - 0.4074319303035736
  - 0.42611363530158997
  - 0.44030675292015076
  - 0.41706568002700806
  - 0.4067768156528473
  - 0.4299467206001282
  - 0.39241647720336914
  - 0.42370226979255676
  - 0.4113883674144745
  - 0.450803279876709
  - 0.41919752955436707
  - 0.4110001027584076
  - 0.41403326392173767
  - 0.4447696805000305
  - 0.4183606505393982
  - 0.4052363336086273
  - 0.4253023564815521
  - 0.4566337466239929
  - 0.4370824992656708
  - 0.40076738595962524
  - 0.40501588582992554
  - 0.41496750712394714
  - 0.41365665197372437
  - 0.39874234795570374
loss_records_fold3:
  train_losses:
  - 6.411330929398537
  - 6.5769665241241455
  - 6.568875080347062
  - 6.83594326376915
  - 6.365704077482224
  - 6.234449410438538
  - 6.327126201987267
  - 6.602038761973382
  - 6.559674465656281
  - 6.685557410120964
  - 6.2354765206575395
  - 6.4860425114631655
  - 6.349612802267075
  - 6.204659536480904
  - 6.455195596814156
  - 6.662752333283425
  - 6.360479789972306
  - 6.367904374003411
  - 6.556069898605347
  - 6.476964309811592
  - 6.493375271558762
  - 6.830589789152146
  - 6.310463932156563
  - 6.4933409810066225
  - 6.527147650718689
  - 6.602581098675728
  - 6.468919372558594
  - 6.270604571700097
  - 6.469582065939903
  - 6.412623327970505
  - 6.432580158114433
  - 6.684302785992623
  - 6.847278192639351
  - 6.4608238548040395
  - 6.359523913264275
  - 9.376607409119606
  - 14.610723772644997
  - 17.380590543150902
  - 14.495141291618348
  - 10.342551237344743
  - 12.609891968965531
  - 10.99255936741829
  - 7.316652402281761
  - 7.486857292056084
  - 6.923412933945656
  - 8.249804735183716
  - 6.970761653780937
  - 6.85390644967556
  - 6.556955316662789
  - 6.753242078423501
  - 6.564713525772095
  - 6.616436260938645
  - 6.414645862579346
  - 6.387185189127923
  - 6.455130532383919
  - 6.554104623198509
  - 6.340166068077088
  - 6.521026760339737
  - 6.3454857736825945
  - 6.914279341697693
  - 6.512468612194062
  - 6.338190281391144
  - 6.541728052496911
  - 6.437413907051087
  - 6.499056971073151
  - 6.5039521813392644
  - 6.253054079413414
  - 8.73595319390297
  - 7.8435342609882355
  - 9.65922453403473
  - 14.385392054915428
  - 7.778352528810501
  - 6.662406793236733
  - 6.935649666190148
  - 6.90291574895382
  - 6.432752943038941
  - 6.627653539180756
  - 6.440600258111954
  - 6.589417609572411
  - 6.429177728295326
  - 6.606677526235581
  - 6.777818685770035
  - 6.5901988565921785
  - 6.377411594986916
  - 6.456955218315125
  - 6.3245112001895905
  - 6.387784531712533
  - 6.590904420614243
  - 6.339283159375191
  - 6.295568951964379
  - 6.724755862355233
  - 6.578101927042008
  - 6.5914450079202656
  - 6.370532354712487
  - 6.483648249506951
  - 6.414855700731278
  - 6.285173872113228
  - 6.470597329735757
  - 6.563596493005753
  - 6.470658886432648
  validation_losses:
  - 0.4867153465747833
  - 0.42537546157836914
  - 0.4099388122558594
  - 0.42646893858909607
  - 0.4086655378341675
  - 0.4079538583755493
  - 0.4179104268550873
  - 0.4278739392757416
  - 0.4990488886833191
  - 0.44967353343963623
  - 0.4262005388736725
  - 0.4467337131500244
  - 0.4043053090572357
  - 0.40998128056526184
  - 0.44004756212234497
  - 0.4207385182380676
  - 0.41904416680336
  - 0.41259336471557617
  - 0.4155738353729248
  - 0.43559718132019043
  - 0.47717955708503723
  - 0.4105495512485504
  - 0.41477149724960327
  - 0.4779500961303711
  - 0.403920441865921
  - 0.4171832501888275
  - 0.41748109459877014
  - 0.42758968472480774
  - 0.43003004789352417
  - 0.4363919496536255
  - 0.41644084453582764
  - 0.46100279688835144
  - 0.4230758845806122
  - 0.41860252618789673
  - 0.41278916597366333
  - 0.4163565933704376
  - 0.40080526471138
  - 0.41697850823402405
  - 0.4039415121078491
  - 0.41316676139831543
  - 0.401669979095459
  - 0.42522475123405457
  - 0.4198550283908844
  - 0.4118407070636749
  - 0.45832598209381104
  - 0.4315764605998993
  - 0.41114068031311035
  - 0.4171648919582367
  - 0.411695271730423
  - 0.39837846159935
  - 0.43226858973503113
  - 0.45672231912612915
  - 0.4327205419540405
  - 0.43176278471946716
  - 0.4359937608242035
  - 0.4271697402000427
  - 2569.93798828125
  - 892401.25
  - 578422528.0
  - 31850248.0
  - 0.4093545973300934
  - 0.46014705300331116
  - 0.4281160533428192
  - 0.4149814546108246
  - 0.4155427813529968
  - 265022.96875
  - 0.42616206407546997
  - 0.4875973165035248
  - 0.41772615909576416
  - 0.40980663895606995
  - 0.472131609916687
  - 0.40903738141059875
  - 0.40249672532081604
  - 0.42814794182777405
  - 0.4188064932823181
  - 0.42814579606056213
  - 0.40649041533470154
  - 0.43901190161705017
  - 0.4072754979133606
  - 0.413860559463501
  - 0.49145403504371643
  - 0.4302830398082733
  - 0.4327391982078552
  - 0.3994801938533783
  - 0.44560715556144714
  - 0.4079147279262543
  - 0.40946218371391296
  - 0.4277014136314392
  - 0.4201887547969818
  - 0.405050128698349
  - 0.4363023638725281
  - 0.4035443365573883
  - 0.40113162994384766
  - 0.4246523082256317
  - 0.41326358914375305
  - 0.40843722224235535
  - 0.42975515127182007
  - 0.4147571325302124
  - 0.4019884765148163
  - 0.4341897666454315
loss_records_fold4:
  train_losses:
  - 6.500894665718079
  - 6.631147634983063
  - 6.461913663148881
  - 6.344940060377121
  - 6.350096371769905
  - 6.473821073770523
  - 6.1744442075490955
  - 6.6828288555145265
  - 6.877003607153893
  - 6.66139960885048
  - 6.7550385981798176
  - 6.349172207713128
  - 6.556527683138848
  - 6.4039685100317
  - 7.064973881840706
  - 6.2861258625984195
  - 8.135895782709122
  - 8.077465215325356
  - 7.061179980635643
  - 6.556690472364426
  - 6.680269289016724
  - 6.929906785488129
  - 6.457788750529289
  - 6.743664032220841
  - 6.542002740502358
  - 6.687411886453629
  - 6.509517043828964
  - 6.514784333109856
  - 6.539384965598583
  - 6.44552760720253
  - 6.368891727924347
  - 6.402536076307297
  - 6.565750944614411
  - 6.299798750877381
  - 6.417694944143296
  - 6.90640841126442
  - 6.398209381103516
  - 6.577892714738846
  - 6.42324156165123
  - 6.328621280193329
  - 6.247677493095399
  - 6.228070536255837
  - 6.426365914940835
  - 6.399134624004365
  - 6.739559924602509
  - 6.674923297762871
  - 6.473462787270546
  - 6.301232191920281
  - 6.481770244240761
  - 6.414024657011033
  - 6.6584721386432655
  - 6.406244942545891
  - 6.657319363951683
  - 6.649894651770592
  - 6.385942563414574
  - 6.420389428734779
  - 6.3526245117187505
  - 6.400433450937271
  - 6.3422928839921955
  - 6.506320071220398
  - 6.353480556607247
  - 6.463585063815117
  - 6.308088231086732
  - 6.330697336792946
  - 6.551899629831315
  - 6.511919829249383
  - 6.485650649666787
  - 6.701858589053154
  - 6.550902026891709
  - 6.418598622083664
  - 6.561590775847435
  - 6.2531412750482565
  - 6.3511674314737325
  - 6.316545502841473
  - 6.571253091096878
  - 6.417211645841599
  - 6.6457706689834595
  - 6.323352447152138
  - 6.593134164810181
  - 6.536217936873436
  - 6.240921825170517
  - 6.486311513185502
  - 6.438649913668633
  - 6.211431410908699
  - 6.426361477375031
  - 6.468947261571884
  - 6.8448263883590705
  - 6.352485689520837
  - 6.400970384478569
  - 6.426901862025261
  - 6.514598247408867
  - 6.604072690010071
  - 6.478477934002877
  - 6.6727610558271415
  - 6.643985483050347
  - 6.2626597672700886
  - 6.436678747832776
  - 6.632371154427529
  - 6.512611937522888
  - 6.808549922704697
  validation_losses:
  - 0.4475019574165344
  - 0.39906078577041626
  - 0.4115559756755829
  - 0.4092477560043335
  - 0.40831685066223145
  - 0.42082664370536804
  - 0.42987847328186035
  - 0.44075196981430054
  - 0.43047812581062317
  - 0.40315505862236023
  - 0.421141117811203
  - 0.43059995770454407
  - 0.41836294531822205
  - 0.4228668510913849
  - 0.4576699137687683
  - 6.118175029754639
  - 0.4054561257362366
  - 0.43875038623809814
  - 0.41783004999160767
  - 0.45962199568748474
  - 0.546481192111969
  - 0.43230438232421875
  - 0.4021981358528137
  - 0.4056284725666046
  - 0.4070494472980499
  - 0.4736766517162323
  - 0.43913352489471436
  - 0.4136103391647339
  - 0.4826006293296814
  - 0.39911335706710815
  - 0.406429648399353
  - 0.4083620607852936
  - 0.42098334431648254
  - 0.5021152496337891
  - 0.441677451133728
  - 0.42076483368873596
  - 0.4094047248363495
  - 0.4123038649559021
  - 0.4311301112174988
  - 0.4006687104701996
  - 0.41011229157447815
  - 0.41234323382377625
  - 0.42626041173934937
  - 0.4039316177368164
  - 0.4059736132621765
  - 0.4044143259525299
  - 0.41781216859817505
  - 0.41175323724746704
  - 0.41228094696998596
  - 0.4383123815059662
  - 0.42678362131118774
  - 0.40439531207084656
  - 0.43397894501686096
  - 0.4300422966480255
  - 0.4543900191783905
  - 0.40460044145584106
  - 0.4162798225879669
  - 0.4109865128993988
  - 0.4120262563228607
  - 0.39974603056907654
  - 0.44525524973869324
  - 0.40429115295410156
  - 0.40541872382164
  - 0.40121135115623474
  - 0.4374673068523407
  - 0.4247748553752899
  - 0.40454837679862976
  - 0.4726647138595581
  - 0.4482738673686981
  - 0.414206862449646
  - 0.4481702446937561
  - 0.4305419325828552
  - 0.42573946714401245
  - 0.40242189168930054
  - 0.4561014771461487
  - 0.4182930588722229
  - 0.4248572289943695
  - 0.3967687487602234
  - 0.45629286766052246
  - 0.42298126220703125
  - 0.4056803584098816
  - 0.43800118565559387
  - 0.40753066539764404
  - 0.4267951250076294
  - 0.43835902214050293
  - 0.4102862775325775
  - 0.4091275632381439
  - 0.40065014362335205
  - 0.4406052529811859
  - 0.4697737693786621
  - 0.42140713334083557
  - 0.4043397605419159
  - 0.4031337797641754
  - 0.4190541207790375
  - 0.42035967111587524
  - 0.4305064380168915
  - 0.41642582416534424
  - 0.4077731966972351
  - 0.4335824251174927
  - 0.39123430848121643
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 82 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:43:16.178320'
