config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:05:51.254085'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_84fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_84fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_84fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_84fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_84fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_84fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_84fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_84fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_84fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_84fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.987664371728897
  - 6.937444031238556
  - 6.514821344614029
  - 6.5339666485786445
  - 7.090453749895096
  - 6.713528487086297
  - 6.19585445523262
  - 6.461032673716545
  - 6.673854133486748
  - 6.474239221215249
  - 6.179798111319542
  - 6.252462017536164
  - 6.063913115859032
  - 6.147979101538659
  - 6.062676885724068
  - 6.043615752458573
  validation_losses:
  - 0.411471426486969
  - 0.41868671774864197
  - 0.41197431087493896
  - 0.40308046340942383
  - 0.410544216632843
  - 0.4171901047229767
  - 0.42171552777290344
  - 0.3895094692707062
  - 0.4091470539569855
  - 0.4321121275424957
  - 0.4113626182079315
  - 0.4122088849544525
  - 0.40467125177383423
  - 0.39109089970588684
  - 0.38721132278442383
  - 0.3961116373538971
loss_records_fold1:
  train_losses:
  - 6.078763544559479
  - 5.999447935819626
  - 6.030280610918999
  - 6.074135911464691
  - 6.167524522542954
  - 6.146928411722183
  - 6.18864595592022
  - 6.054979759454728
  - 5.95444917678833
  - 6.13399296104908
  - 6.0234838217496876
  - 6.01264717578888
  - 6.020500415563584
  - 5.905474403500557
  validation_losses:
  - 0.4052923321723938
  - 0.40395888686180115
  - 0.41456273198127747
  - 0.39369532465934753
  - 0.41263672709465027
  - 0.4233357310295105
  - 0.4017556607723236
  - 0.4273855686187744
  - 0.3911144435405731
  - 0.3977757692337036
  - 0.3912263512611389
  - 0.3937726318836212
  - 0.3926212787628174
  - 0.39692550897598267
loss_records_fold2:
  train_losses:
  - 6.140878155827522
  - 5.974414330720902
  - 6.092133194208145
  - 6.069186407327653
  - 5.9258165746927265
  - 5.987865102291107
  - 5.936594441533089
  - 6.018252989649773
  - 6.072717255353928
  - 6.047016623616219
  - 5.964115509390831
  - 5.9242578029632575
  - 5.970507273077965
  - 5.853944039344788
  - 6.044711554050446
  validation_losses:
  - 0.39492300152778625
  - 0.4050065875053406
  - 0.3998555541038513
  - 0.3925006687641144
  - 0.40797746181488037
  - 0.3995036780834198
  - 0.39557740092277527
  - 0.39247336983680725
  - 0.40890613198280334
  - 0.3923603296279907
  - 0.4018637537956238
  - 0.39889395236968994
  - 0.4043673574924469
  - 0.40098315477371216
  - 0.3938657343387604
loss_records_fold3:
  train_losses:
  - 6.051257753372193
  - 6.070979110896587
  - 6.138037952780724
  - 6.135365420579911
  - 6.115295132994652
  - 6.052898266911507
  - 6.073081794381142
  - 6.192974430322647
  - 6.068472567200661
  - 5.99234913289547
  - 6.0043707549572
  - 5.954437392950059
  - 5.94934895336628
  - 6.04819655418396
  - 5.990387496352196
  - 6.057337793707848
  - 5.985475486516953
  - 6.067258980870247
  - 6.031477618217469
  validation_losses:
  - 0.38015079498291016
  - 0.38406819105148315
  - 0.3729839026927948
  - 0.3912888169288635
  - 0.3802490532398224
  - 0.386482298374176
  - 0.38360899686813354
  - 0.3726612329483032
  - 0.402383029460907
  - 0.39561915397644043
  - 0.38317570090293884
  - 0.3686755895614624
  - 0.3814511001110077
  - 0.38292548060417175
  - 0.3770466148853302
  - 0.38472941517829895
  - 0.38428574800491333
  - 0.39185506105422974
  - 0.3968874514102936
loss_records_fold4:
  train_losses:
  - 6.11737865805626
  - 6.030991047620773
  - 6.040698325634003
  - 5.983889138698578
  - 6.2735978156328205
  - 6.0310389518737795
  - 6.04883354306221
  - 6.015717029571533
  - 6.19282066822052
  - 5.939452722668648
  - 6.120176693797112
  - 6.098366969823838
  - 6.000932350754738
  - 6.144875755906106
  - 6.085449320077896
  - 5.972836101055146
  - 6.048442181944847
  validation_losses:
  - 0.39454156160354614
  - 0.3886586129665375
  - 0.38817504048347473
  - 0.39298519492149353
  - 0.38222381472587585
  - 0.38448473811149597
  - 0.38160961866378784
  - 0.3937584161758423
  - 0.3843643367290497
  - 0.37826061248779297
  - 0.3906005620956421
  - 0.38493067026138306
  - 0.3828234076499939
  - 0.38799673318862915
  - 0.3873752951622009
  - 0.3905976414680481
  - 0.3886412978172302
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:08:18.187778'
