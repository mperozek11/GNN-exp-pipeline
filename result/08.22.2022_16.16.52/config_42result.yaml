config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:03:28.345906'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_42fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_42fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_42fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_42fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_42fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_42fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_42fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_42fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_42fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_42fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 29.472913551330567
  - 9.693957483768465
  - 8.076905876398087
  - 4.623448544740677
  - 3.6812410891056064
  - 3.0041331112384797
  - 3.413174331188202
  - 2.2223930597305297
  - 1.6656376153230668
  - 2.173642361164093
  - 2.221869993209839
  - 1.8332981288433077
  - 2.1421989977359774
  - 1.8680987596511842
  - 1.5076225131750107
  - 1.5562111616134644
  - 2.830104419589043
  - 3.0307240277528766
  - 3.1963293194770817
  - 1.808205634355545
  - 1.7370801866054535
  - 2.2957121014595034
  - 3.357715106010437
  - 8.142965108156204
  - 3.366809141635895
  - 2.793761719763279
  - 2.843925425410271
  - 3.0523221611976625
  - 2.087548214197159
  - 1.6705975532531738
  - 1.8724046587944032
  - 2.9941076278686527
  - 2.164986449480057
  - 2.969498574733734
  - 2.1778410732746125
  - 1.5556756794452669
  - 1.9964037358760836
  - 2.28209308385849
  - 2.2783948123455047
  - 1.8977684348821642
  - 2.215127331018448
  - 1.856959652900696
  - 2.403210234642029
  - 4.961278653144837
  - 2.515712130069733
  - 3.01635040640831
  - 2.1527454614639283
  - 1.5837004840373994
  - 1.9483969628810884
  - 1.7947445750236513
  - 1.6253817915916444
  - 1.5307552993297577
  - 1.551042699813843
  - 1.4897679150104524
  - 1.545057010650635
  - 1.7975064396858216
  - 1.6820719778537752
  - 1.529033923149109
  - 1.8245683819055558
  - 1.599053865671158
  - 1.458887130022049
  - 2.3421760499477386
  - 1.867869919538498
  - 1.8813340842723847
  - 2.3052554666996
  - 2.145973759889603
  - 1.9287319719791414
  - 1.5690019547939302
  - 1.8129488646984102
  - 1.6132297843694687
  - 1.773227906227112
  - 1.9657180905342102
  validation_losses:
  - 1.3598307371139526
  - 1.353622555732727
  - 1.4855737686157227
  - 0.6969831585884094
  - 1.068799614906311
  - 0.45176413655281067
  - 0.6272241473197937
  - 0.5444342494010925
  - 0.5730762481689453
  - 0.4265584349632263
  - 0.39008915424346924
  - 0.4744778871536255
  - 0.40018799901008606
  - 0.40738654136657715
  - 0.42382776737213135
  - 0.41358068585395813
  - 0.5289664268493652
  - 0.7853996157646179
  - 0.5222170352935791
  - 0.5447579026222229
  - 0.4229239821434021
  - 0.7563801407814026
  - 0.44818371534347534
  - 0.41599881649017334
  - 0.591910719871521
  - 0.44583365321159363
  - 0.473409503698349
  - 0.4018391966819763
  - 0.38778483867645264
  - 0.38844338059425354
  - 0.4049648344516754
  - 0.4574541747570038
  - 0.43822991847991943
  - 0.5539673566818237
  - 0.39059045910835266
  - 0.4317447245121002
  - 0.4074491560459137
  - 0.6397508382797241
  - 0.48985424637794495
  - 0.45886823534965515
  - 0.3963281214237213
  - 0.38485392928123474
  - 0.57574862241745
  - 0.5973790287971497
  - 0.48999857902526855
  - 0.4058639705181122
  - 0.38604438304901123
  - 0.4064804017543793
  - 0.4016314446926117
  - 0.4464573562145233
  - 0.39495259523391724
  - 0.3857952952384949
  - 0.39896079897880554
  - 0.3872697651386261
  - 0.4038126468658447
  - 0.40820789337158203
  - 0.3978070020675659
  - 0.39898306131362915
  - 0.39537176489830017
  - 0.41975903511047363
  - 0.3894694745540619
  - 0.4108400046825409
  - 0.4296261668205261
  - 0.3836023211479187
  - 0.40400978922843933
  - 0.4210381507873535
  - 0.40555229783058167
  - 0.41143015027046204
  - 0.38702860474586487
  - 0.38309067487716675
  - 0.3929315507411957
  - 0.3897748291492462
loss_records_fold1:
  train_losses:
  - 1.728016608953476
  - 2.6938898622989655
  - 1.535776746273041
  - 1.4962019085884095
  - 1.7512363612651827
  - 1.646989715099335
  - 1.5345445722341537
  - 1.553904926776886
  - 2.217324787378311
  - 1.7147370338439942
  - 1.6868719041347504
  - 1.8963781297206879
  - 1.5722074896097185
  - 1.701707148551941
  - 1.6155361354351045
  - 1.5452350676059723
  - 1.486730867624283
  - 1.4902903556823732
  - 1.6983582317829133
  - 1.727600556612015
  - 1.6238263845443726
  - 1.4953847974538803
  - 2.2522893369197847
  - 1.750530630350113
  - 2.0507148146629333
  - 1.632285463809967
  - 3.983760839700699
  - 2.9745135605335236
  - 2.5426515102386475
  - 1.6980587959289553
  - 1.6357972085475923
  - 1.6825472533702852
  - 1.4962692201137544
  - 1.441435343027115
  - 1.5455842018127441
  - 2.0511442571878433
  - 1.842893296480179
  - 1.4936150908470154
  - 1.5563983023166656
  - 2.1494106233119967
  - 2.840839368104935
  - 2.516851556301117
  - 1.8004220366477968
  - 1.6737161815166475
  - 1.6743708908557893
  - 1.5507168352603913
  - 1.5811751246452332
  - 1.5767642438411713
  - 1.4313571721315386
  - 1.748417076468468
  - 1.6782645791769029
  - 3.272283613681793
  - 1.5630257785320283
  - 1.565962800383568
  - 1.6425397098064423
  - 2.620514738559723
  - 2.2359844326972964
  - 1.6112387537956239
  - 1.6522056579589846
  - 1.6098466753959657
  - 1.4956871330738069
  - 1.501004868745804
  - 1.799254608154297
  - 1.5703635573387147
  - 1.666914236545563
  - 1.6201969861984253
  - 1.7220155060291291
  - 1.5690899074077607
  - 1.4724631488323212
  - 1.467047217488289
  - 1.5928425133228303
  - 1.6966895520687104
  - 1.4846865475177766
  - 1.4366755843162538
  - 1.4315230488777162
  - 1.4531428456306459
  - 1.4999247312545778
  - 1.4802814662456514
  - 1.516283941268921
  - 1.5019104063510895
  - 1.4930073022842407
  - 1.423263818025589
  - 1.4949809670448304
  - 1.5089852154254915
  - 1.4535206437110901
  - 1.482571253180504
  - 1.4542264997959138
  - 1.4668211549520493
  - 1.4847174763679505
  - 1.4381468862295153
  - 1.547198909521103
  - 1.5665678918361665
  - 1.4738567352294922
  - 1.4613829374313356
  - 1.434538045525551
  - 1.474263447523117
  - 1.4540376842021943
  - 1.4670397698879243
  - 1.4757275700569155
  - 1.445245897769928
  validation_losses:
  - 0.4024633765220642
  - 0.40057966113090515
  - 0.399659663438797
  - 0.41916000843048096
  - 0.39798712730407715
  - 0.5034927129745483
  - 0.41119807958602905
  - 0.4026166796684265
  - 0.4089237153530121
  - 0.4726216197013855
  - 0.7573347091674805
  - 0.5352963209152222
  - 0.4257180392742157
  - 0.4067271947860718
  - 0.4264145493507385
  - 0.3998470902442932
  - 0.4033939242362976
  - 0.41396600008010864
  - 0.45666012167930603
  - 0.44161537289619446
  - 0.39929646253585815
  - 0.41976243257522583
  - 0.39417755603790283
  - 0.4136074185371399
  - 0.397048681974411
  - 0.4317951500415802
  - 0.40541642904281616
  - 0.4013078510761261
  - 0.4163723289966583
  - 0.44205591082572937
  - 0.408476859331131
  - 0.40345898270606995
  - 0.40482014417648315
  - 0.44010621309280396
  - 0.4131312668323517
  - 0.5767574310302734
  - 1.6798721551895142
  - 0.4071634113788605
  - 0.5181154012680054
  - 0.6000862717628479
  - 0.40018391609191895
  - 0.4489860236644745
  - 0.4152714014053345
  - 0.3951239585876465
  - 0.4317525029182434
  - 0.43062078952789307
  - 0.4469854235649109
  - 0.39963340759277344
  - 0.4105567932128906
  - 0.42406484484672546
  - 0.40111443400382996
  - 0.3964002728462219
  - 0.40280476212501526
  - 0.41476741433143616
  - 0.42680445313453674
  - 0.40541964769363403
  - 0.6300704479217529
  - 0.39802026748657227
  - 0.4090534448623657
  - 0.3997138440608978
  - 0.4256075322628021
  - 0.39597031474113464
  - 0.39668312668800354
  - 0.40429234504699707
  - 0.5808707475662231
  - 0.42764362692832947
  - 0.4910171627998352
  - 0.4039579927921295
  - 0.3972949683666229
  - 0.4104502201080322
  - 0.3972296714782715
  - 0.4052218198776245
  - 0.41446802020072937
  - 0.41123637557029724
  - 0.39952385425567627
  - 0.4183136820793152
  - 0.39917850494384766
  - 0.41538071632385254
  - 0.41424527764320374
  - 0.40101465582847595
  - 0.40442875027656555
  - 0.4355596899986267
  - 0.4105071425437927
  - 0.4036034643650055
  - 0.39710819721221924
  - 0.4121328592300415
  - 0.40033599734306335
  - 0.40274763107299805
  - 0.4105200171470642
  - 0.40075570344924927
  - 0.430406391620636
  - 0.40178239345550537
  - 0.4196970760822296
  - 0.4581717848777771
  - 0.40877026319503784
  - 0.4019145667552948
  - 0.397532194852829
  - 0.45836564898490906
  - 0.42587730288505554
  - 0.4109438955783844
loss_records_fold2:
  train_losses:
  - 1.5123266220092775
  - 1.4963825166225435
  - 1.4948794305324555
  - 1.4670339643955232
  - 1.4966417193412782
  - 1.5085044503211975
  - 1.5495944380760194
  - 1.5104960203170776
  - 1.5352995038032533
  - 1.433027195930481
  - 1.4821333527565004
  - 1.5219600081443787
  - 1.5592767298221588
  - 1.6145382583141328
  - 1.4955527722835542
  - 1.4602860629558565
  - 1.8500426471233369
  - 1.6086395740509034
  - 1.5869216084480287
  - 1.72811821103096
  - 1.5671085059642793
  - 2.013443595170975
  - 1.9948435187339784
  - 1.7278460383415224
  - 1.603776103258133
  - 1.532095804810524
  - 1.535988587141037
  - 1.5512201309204103
  - 1.5715259492397309
  - 1.5448125064373017
  - 1.5855536580085756
  - 1.523356831073761
  - 1.5092646837234498
  - 1.7042649149894715
  - 1.5664935111999512
  - 1.7735941290855408
  validation_losses:
  - 0.3736537992954254
  - 0.39865368604660034
  - 0.3821541965007782
  - 0.37505078315734863
  - 0.38892850279808044
  - 0.4650130569934845
  - 0.37650012969970703
  - 0.38719797134399414
  - 0.3747461140155792
  - 0.38156041502952576
  - 0.3739144504070282
  - 0.373788982629776
  - 0.42268821597099304
  - 0.3768288791179657
  - 0.3767181634902954
  - 0.37887799739837646
  - 2.2319440841674805
  - 0.38437244296073914
  - 0.3957792818546295
  - 0.3843061327934265
  - 0.3814876675605774
  - 0.42741695046424866
  - 0.3933684229850769
  - 0.39019301533699036
  - 0.3805515170097351
  - 0.3829469382762909
  - 0.3879118263721466
  - 0.41603270173072815
  - 0.38467052578926086
  - 0.4129035770893097
  - 0.3877356946468353
  - 0.3888077139854431
  - 0.3832114338874817
  - 0.3856194019317627
  - 0.3821338415145874
  - 0.383484423160553
loss_records_fold3:
  train_losses:
  - 1.7369187474250793
  - 1.8377962172031403
  - 1.4752734631299973
  - 1.5613246619701386
  - 2.0501115143299105
  - 1.7261218786239625
  - 1.8703093469142915
  - 2.0157517731189727
  - 1.985169941186905
  - 1.923646640777588
  - 1.7056814372539522
  - 1.5821422338485718
  - 1.592480117082596
  - 1.5386640131473541
  - 1.765156477689743
  - 1.522269481420517
  - 1.531850665807724
  - 1.5159714281558991
  - 1.5305270701646805
  - 1.5252034366130829
  - 1.6278147995471954
  - 1.776261568069458
  - 1.5095140039920807
  - 1.505083829164505
  validation_losses:
  - 0.43251344561576843
  - 0.3965877890586853
  - 0.39817705750465393
  - 0.41585251688957214
  - 0.44825899600982666
  - 0.4117077887058258
  - 0.4006940424442291
  - 0.39144623279571533
  - 0.3914305567741394
  - 0.40614113211631775
  - 0.3904964327812195
  - 0.3905823528766632
  - 0.40147820115089417
  - 0.3957691788673401
  - 0.4009561240673065
  - 0.3986735939979553
  - 0.39443525671958923
  - 0.41870519518852234
  - 0.40636539459228516
  - 0.3944443464279175
  - 0.3935941159725189
  - 0.3951354920864105
  - 0.3945672810077667
  - 0.3951612412929535
loss_records_fold4:
  train_losses:
  - 1.5178060770034791
  - 1.5008384883403778
  - 1.4850270718336107
  - 1.5535407841205597
  - 1.5726503670215608
  - 1.5589072525501253
  - 1.5166785717010498
  - 1.475135099887848
  - 1.514344644546509
  - 1.575069469213486
  - 1.595770072937012
  - 1.5184859812259675
  - 1.5420238941907884
  - 1.5203283846378328
  - 1.5533222496509553
  - 1.4868566751480103
  - 1.4892992913722993
  - 1.4923911601305009
  - 1.496775421500206
  validation_losses:
  - 0.3969961404800415
  - 0.39666858315467834
  - 0.3994136154651642
  - 0.42626893520355225
  - 0.39882925152778625
  - 0.4047507047653198
  - 0.39057666063308716
  - 0.39070072770118713
  - 0.3904607892036438
  - 0.40115052461624146
  - 0.39384323358535767
  - 0.3930990397930145
  - 0.41751646995544434
  - 0.3971730172634125
  - 0.3939859867095947
  - 0.39166179299354553
  - 0.4002414643764496
  - 0.3938329219818115
  - 0.39168432354927063
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 72 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 36 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:21:47.228164'
