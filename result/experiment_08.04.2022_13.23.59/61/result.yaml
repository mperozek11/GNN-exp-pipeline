config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 19:55:53.495676'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/61/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/61/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/61/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/61/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/61/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/61/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/61/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/61/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/61/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/61/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 163.89350305497646
  - 180.39138329029083
  - 15.06232687830925
  - 10.152481615543365
  - 10.04157269001007
  - 9.448496997356415
  - 8.872700840234756
  - 40.251685321331024
  - 22.625900000333786
  - 55.835506826639175
  - 14.80047819018364
  - 11.328600078821182
  - 11.499130427837372
  - 76.59274259209633
  - 41.572877526283264
  - 19.997301697731018
  - 33.46247175335884
  - 14.791920483112335
  - 25.25863191485405
  - 28.49589902162552
  - 12.392515391111374
  - 20.253355503082275
  - 8.719367504119873
  - 8.980074793100357
  - 45.162521719932556
  - 21.29913967847824
  - 29.05692282319069
  - 12.80420470237732
  - 10.725167542696
  - 12.925730139017105
  - 11.151788786053658
  - 19.95912927389145
  - 13.013671547174454
  - 10.89599034190178
  - 10.42947632074356
  - 8.805242270231247
  - 10.423113584518433
  - 19.041012555360794
  - 10.404139280319214
  - 9.272148102521896
  - 8.964361369609833
  - 10.722067534923553
  - 10.67126876115799
  - 9.258025199174881
  - 9.176800787448883
  - 20.49828040599823
  - 13.08914053440094
  - 9.826897129416466
  - 12.322063475847244
  - 27.557790488004684
  - 15.624502807855606
  - 10.678331047296524
  - 11.418224722146988
  - 12.24701076745987
  - 13.068892151117325
  - 11.248372614383698
  - 10.559915393590927
  - 10.325858682394028
  - 10.676986187696457
  - 10.396785318851471
  - 10.14466056227684
  - 9.748109072446823
  - 10.597120821475983
  - 11.632025480270386
  - 27.172699242830276
  - 19.0331591963768
  - 17.92704525589943
  - 10.23055100440979
  - 14.237744450569153
  - 12.180105119943619
  - 13.80928847193718
  - 13.17441377043724
  - 9.476493924856186
  - 14.009807139635086
  - 8.484978437423706
  - 8.690776973962784
  - 24.78471802175045
  - 11.249842792749405
  - 17.577156126499176
  - 38.91840958595276
  - 9.254613488912582
  - 12.519490242004395
  - 24.88921719789505
  - 24.383407056331635
  - 14.895848542451859
  - 12.914470791816711
  - 65.26911363005638
  - 44.20459482073784
  - 16.09949091076851
  - 23.602605015039444
  - 14.108394145965576
  - 18.18763428926468
  - 13.944712936878204
  - 22.942760080099106
  - 20.076836943626404
  - 11.938514232635498
  - 12.76435649394989
  - 13.94364446401596
  - 23.416594564914703
  - 27.518160670995712
  validation_losses:
  - 77.53145599365234
  - 0.4972805380821228
  - 0.4519871175289154
  - 0.4645019471645355
  - 0.4119924306869507
  - 0.4510987102985382
  - 0.44412824511528015
  - 0.5283872485160828
  - 0.42007869482040405
  - 2.310051679611206
  - 0.43969595432281494
  - 0.4454902708530426
  - 0.8904980421066284
  - 0.4328855276107788
  - 6.609093189239502
  - 0.4955904483795166
  - 0.4887716472148895
  - 0.9397948384284973
  - 0.6704854369163513
  - 0.5368923544883728
  - 0.44718989729881287
  - 0.5007391571998596
  - 0.4145817756652832
  - 0.4942058026790619
  - 0.6736008524894714
  - 0.44988617300987244
  - 0.9574719071388245
  - 0.4073870778083801
  - 0.4283207654953003
  - 0.4551878571510315
  - 0.6357019543647766
  - 0.71689772605896
  - 0.47687071561813354
  - 0.4247361123561859
  - 0.42690762877464294
  - 0.4160021245479584
  - 0.8368970155715942
  - 0.6627902984619141
  - 0.48830026388168335
  - 0.5348657369613647
  - 0.5888717174530029
  - 0.5917052626609802
  - 0.3964097797870636
  - 0.42825040221214294
  - 0.6968510746955872
  - 0.5860699415206909
  - 0.5676201581954956
  - 0.5800213813781738
  - 0.6548580527305603
  - 0.8954423069953918
  - 1.1858149766921997
  - 0.9237367510795593
  - 0.978565514087677
  - 0.9899320602416992
  - 0.6846417188644409
  - 0.7126269936561584
  - 0.5144983530044556
  - 0.6831432580947876
  - 0.9294260144233704
  - 0.6429204940795898
  - 0.43067672848701477
  - 0.5301066637039185
  - 0.4782680571079254
  - 0.6945527195930481
  - 0.8944929838180542
  - 4.274298667907715
  - 0.5327837467193604
  - 7.675967216491699
  - 27.704486846923828
  - 22.43593978881836
  - 1.1679720878601074
  - 0.4138295650482178
  - 0.4403911232948303
  - 0.43021082878112793
  - 0.4458405375480652
  - 0.5256484150886536
  - 316.61767578125
  - 2.396793842315674
  - 0.7904179096221924
  - 0.48103195428848267
  - 0.6033332943916321
  - 0.39999648928642273
  - 1.4697535037994385
  - 1.579944133758545
  - 0.6822583079338074
  - 0.6466678380966187
  - 0.6319343447685242
  - 0.9839211106300354
  - 0.6287263631820679
  - 0.5878373384475708
  - 1.8491313457489014
  - 4.742936611175537
  - 0.7027943730354309
  - 0.5224531888961792
  - 0.8142856359481812
  - 0.46319833397865295
  - 0.4723176956176758
  - 0.6028850674629211
  - 0.5146830081939697
  - 2.4466567039489746
loss_records_fold2:
  train_losses:
  - 10.064763754606247
  - 9.264394134283066
  - 10.752341568470001
  - 11.597626835107803
  - 10.370933085680008
  - 12.533917546272278
  - 17.68692472577095
  - 14.796363949775696
  - 40.72655189037323
  - 60.50334611535072
  - 8.89833402633667
  - 14.168097823858261
  - 10.173985421657562
  - 22.31827160716057
  - 25.849032521247864
  - 23.884456157684326
  - 27.957977786660194
  - 30.918162882328033
  - 23.09968739748001
  - 59.614619463682175
  - 27.523556351661682
  - 31.454178988933563
  - 10.196468234062195
  - 13.95209738612175
  - 12.269475042819977
  - 16.627018809318542
  - 11.886453628540039
  - 12.80553463101387
  - 12.868639439344406
  - 10.399829357862473
  - 12.876960530877113
  - 13.00293830037117
  - 10.27350041270256
  - 18.684911727905273
  - 15.454930454492569
  - 9.674319043755531
  - 16.744602024555206
  - 27.183861821889877
  - 11.324751019477844
  - 15.225349873304367
  - 56.815635204315186
  - 26.280262410640717
  - 10.318630546331406
  - 11.015032142400742
  - 16.531658202409744
  - 10.016367197036743
  - 8.428089410066605
  - 12.126735270023346
  - 15.748788207769394
  - 10.716546952724457
  - 12.687744408845901
  - 14.999455988407135
  - 11.46406665444374
  - 15.388833343982697
  - 14.102607905864716
  - 14.316039204597473
  - 12.28503230214119
  - 11.545361906290054
  - 10.652482211589813
  - 11.392634570598602
  - 12.61008769273758
  - 16.206947058439255
  - 12.418780475854874
  - 9.153075784444809
  - 12.52907457947731
  - 12.967362374067307
  - 10.500178307294846
  - 11.006670236587524
  - 11.302023023366928
  - 8.922670140862465
  - 12.371964633464813
  - 8.73158985376358
  - 10.061473935842514
  - 9.436679929494858
  - 14.846143782138824
  - 11.68307039141655
  - 10.103962987661362
  - 10.002456545829773
  - 8.885815590620041
  - 14.783878326416016
  - 41.33354169130325
  - 31.115128964185715
  - 13.434104174375534
  - 19.112102389335632
  - 19.945924013853073
  - 153.1500197649002
  - 12.914925545454025
  - 14.25106105208397
  - 10.47649022936821
  - 10.015791922807693
  - 9.003080278635025
  - 12.373862624168396
  - 13.370242685079575
  - 22.256360411643982
  - 12.678687900304794
  - 58.20676761865616
  - 11.962498188018799
  - 15.94934332370758
  - 16.868773490190506
  - 12.378957211971283
  validation_losses:
  - 0.4005574584007263
  - 0.4793775677680969
  - 0.8682382106781006
  - 0.5143033862113953
  - 0.8242079019546509
  - 0.7471268177032471
  - 0.8988730311393738
  - 0.7017284035682678
  - 2.5828206539154053
  - 0.4468229115009308
  - 0.6053692102432251
  - 0.7120730876922607
  - 0.44938355684280396
  - 0.46833255887031555
  - 0.5819628834724426
  - 0.7840127348899841
  - 0.507476270198822
  - 4.0876359939575195
  - 0.47262099385261536
  - 0.40454182028770447
  - 0.41426217555999756
  - 0.6030336618423462
  - 0.8252450823783875
  - 0.5821053981781006
  - 0.5949501991271973
  - 0.5224502682685852
  - 0.7267253398895264
  - 0.41825225949287415
  - 0.41957852244377136
  - 0.4763161242008209
  - 0.46115025877952576
  - 0.9653772711753845
  - 0.5462664365768433
  - 0.41020604968070984
  - 0.392928808927536
  - 0.5439936518669128
  - 0.603103756904602
  - 0.4493585228919983
  - 0.6202583312988281
  - 0.5583462715148926
  - 0.42347973585128784
  - 0.49422281980514526
  - 0.8035454750061035
  - 1.4794660806655884
  - 0.48340359330177307
  - 0.45469826459884644
  - 0.6127344369888306
  - 0.8143354654312134
  - 0.4812370538711548
  - 0.5852898955345154
  - 0.932429313659668
  - 0.5527052879333496
  - 0.48824504017829895
  - 0.6248005032539368
  - 0.7150118947029114
  - 0.6702054738998413
  - 0.5812467336654663
  - 0.4066358506679535
  - 0.4347889721393585
  - 0.42420485615730286
  - 0.6108191013336182
  - 1.1363102197647095
  - 0.40977323055267334
  - 0.4332736134529114
  - 0.9574976563453674
  - 0.5100307464599609
  - 0.5541291832923889
  - 0.435131698846817
  - 0.6171622276306152
  - 0.444874107837677
  - 0.3966934382915497
  - 0.5389566421508789
  - 0.4328823685646057
  - 0.9336050748825073
  - 0.49983543157577515
  - 0.881103515625
  - 0.4308224320411682
  - 0.4528278410434723
  - 0.5092442631721497
  - 4.491623878479004
  - 0.48303329944610596
  - 0.4534306526184082
  - 0.4133213460445404
  - 0.6073279976844788
  - 0.40558427572250366
  - 0.45185133814811707
  - 0.4027189612388611
  - 0.44411176443099976
  - 0.42123186588287354
  - 0.41518858075141907
  - 0.42667657136917114
  - 0.39081183075904846
  - 0.494782030582428
  - 0.6206518411636353
  - 0.6567338109016418
  - 0.7058788537979126
  - 0.432913601398468
  - 0.8088425397872925
  - 0.611384391784668
  - 0.8179668188095093
loss_records_fold3:
  train_losses:
  - 10.45962455868721
  - 9.391904354095459
  - 20.15899121761322
  - 9.353816360235214
  - 15.724494636058807
  - 9.940855622291565
  - 9.34700831770897
  - 10.327574461698532
  - 10.122979700565338
  - 23.61958158016205
  - 18.17062796652317
  - 14.829606473445892
  - 10.597029477357864
  - 11.339018702507019
  - 11.326461970806122
  - 10.09227967262268
  - 12.814922153949738
  - 16.433182954788208
  - 11.52186232805252
  - 9.395836681127548
  - 19.750515341758728
  - 30.748272627592087
  - 9.770784974098206
  - 10.602733537554741
  - 8.428943276405334
  - 8.814567625522614
  - 10.891766369342804
  - 9.661437690258026
  - 9.040265023708344
  - 9.80743083357811
  - 16.861901342868805
  - 14.685588032007217
  - 12.181303709745407
  - 10.33552473783493
  - 9.950552552938461
  - 8.730737119913101
  - 10.382737308740616
  - 10.793133914470673
  - 14.813378542661667
  - 12.550256907939911
  - 11.038633584976196
  - 12.470346212387085
  - 10.474678188562393
  - 8.859058231115341
  - 10.647491037845612
  - 11.892183512449265
  - 11.811520099639893
  - 10.801781058311462
  - 10.346083581447601
  - 10.40037976205349
  - 9.136011689901352
  - 9.63755515217781
  - 9.562237083911896
  - 11.470615684986115
  - 12.728255599737167
  - 9.978288292884827
  - 8.592585891485214
  - 10.601856619119644
  - 9.577396899461746
  - 8.618989914655685
  - 8.72850638628006
  - 10.556746393442154
  - 9.877290695905685
  - 14.709315776824951
  - 12.633667007088661
  - 9.94683575630188
  - 9.704973131418228
  - 9.841244280338287
  - 9.429947257041931
  - 11.626101464033127
  - 12.205204218626022
  - 17.319698214530945
  - 15.367098838090897
  - 15.39110453426838
  - 11.748078376054764
  - 9.155130311846733
  - 9.325013250112534
  - 10.535891681909561
  - 12.437745034694672
  - 10.388687878847122
  - 12.005901336669922
  - 12.076310902833939
  - 9.43544489145279
  - 9.409536302089691
  - 13.399694681167603
  - 11.526897519826889
  - 9.011231482028961
  - 8.974507957696915
  - 9.701143562793732
  - 9.986697793006897
  - 11.963786542415619
  - 13.219589233398438
  - 9.586493641138077
  - 8.413468956947327
  - 9.160641193389893
  - 9.446634769439697
  - 9.872255086898804
  - 8.606400698423386
  - 9.23839983344078
  - 9.548921018838882
  validation_losses:
  - 0.6391883492469788
  - 0.4242461025714874
  - 0.5182309150695801
  - 0.43454450368881226
  - 0.5328611135482788
  - 169152576.0
  - 1168459392.0
  - 4914250240.0
  - 7138484736.0
  - 6667243008.0
  - 6575974400.0
  - 7023025152.0
  - 6817227776.0
  - 6881069056.0
  - 6656961536.0
  - 7027679232.0
  - 6434228224.0
  - 6714112000.0
  - 7050016256.0
  - 0.9257940649986267
  - 0.7356813549995422
  - 4.788283824920654
  - 0.4695647358894348
  - 0.5656819939613342
  - 0.40805724263191223
  - 0.7199309468269348
  - 0.5174426436424255
  - 0.4823758900165558
  - 0.5037400722503662
  - 0.6017646789550781
  - 1.2929319143295288
  - 0.4862715005874634
  - 0.5599401593208313
  - 0.4667065441608429
  - 0.47772711515426636
  - 0.5242365002632141
  - 0.5126094818115234
  - 1.3861072063446045
  - 2.792225670116147e+16
  - 1.7412398026210673e+18
  - 4.2227957925574345e+19
  - 2.3784568681743463e+20
  - 3.82800761039423e+20
  - 0.5629716515541077
  - 0.4822772145271301
  - 0.6462515592575073
  - 0.5559263825416565
  - 0.6666683554649353
  - 0.4600410461425781
  - 0.549821138381958
  - 0.49228227138519287
  - 0.47415104508399963
  - 1.2624982595443726
  - 0.6596415042877197
  - 3.917957214240768e+16
  - 2.31423090854409e+18
  - 5.17519848199747e+19
  - 2.2664486510825728e+20
  - 3.717147283441855e+20
  - 4.0598304012131264e+20
  - 4.334252670538576e+20
  - 3.857896734483693e+20
  - 3.078774350792318e+20
  - 2.1861651267721363e+20
  - 2.270721968994622e+20
  - 2.082306665787299e+20
  - 2.0558010468397587e+20
  - 0.5428749322891235
  - 0.5958013534545898
  - 0.5159487128257751
  - 0.5657986998558044
  - 0.8878181576728821
  - 1.3237583637237549
  - 0.950002908706665
  - 0.5260031819343567
  - 0.44841745495796204
  - 0.5183207392692566
  - 0.4822884202003479
  - 5765157149999104.0
  - 5.841018770801295e+17
  - 1.5698393813804384e+19
  - 1.218524261702288e+20
  - 2.5902103172319124e+20
  - 3.1593683771864383e+20
  - 2.8609315424573378e+20
  - 2.9521528325190294e+20
  - 3.16896984048576e+20
  - 3.106111552374178e+20
  - 3.310086968215926e+20
  - 2.6024015262388322e+20
  - 2.316764062388207e+20
  - 2.38092593148568e+20
  - 2.523142394921762e+20
  - 2.540583640009917e+20
  - 2.0274735811054587e+20
  - 1.5326393405951024e+20
  - 1.7740011424528623e+20
  - 1.6620812381350317e+20
  - 1.8505514289628132e+20
  - 1.8422625186642657e+20
loss_records_fold4:
  train_losses:
  - 12.506367146968842
  - 14.820565670728683
  - 18.213410526514053
  - 13.67204549908638
  - 9.776619791984558
  - 8.285035759210587
  - 9.80122122168541
  - 9.571032464504242
  - 9.058104276657104
  - 8.741681635379791
  - 8.661168336868286
  - 9.919869840145111
  - 9.565379589796066
  - 9.460321813821793
  - 9.790954262018204
  - 9.250902473926544
  - 11.611284255981445
  - 10.654956847429276
  - 10.327659100294113
  - 8.606829524040222
  - 14.63571572303772
  - 9.977226108312607
  - 9.653155118227005
  - 8.837573915719986
  - 11.696318686008453
  - 11.379266381263733
  - 13.469897121191025
  - 12.540119856595993
  - 16.826539516448975
  - 10.224125057458878
  - 9.27194806933403
  - 11.062662035226822
  - 10.507231771945953
  - 9.893765777349472
  - 9.188605070114136
  - 10.71550652384758
  - 9.062362909317017
  - 9.032119572162628
  - 8.5228151679039
  - 9.037040710449219
  - 9.219229519367218
  - 9.409997701644897
  - 24.291281700134277
  - 14.386000037193298
  - 13.747033834457397
  - 13.038341790437698
  - 9.41180357336998
  - 9.534263581037521
  - 9.649853616952896
  - 9.145816624164581
  - 9.018609553575516
  - 8.741564989089966
  - 10.465066641569138
  - 14.283442854881287
  - 12.175657749176025
  - 10.246488094329834
  - 10.515753120183945
  - 9.438795626163483
  - 12.791219681501389
  - 11.943002253770828
  - 12.121903717517853
  - 9.771652102470398
  - 8.993880599737167
  - 10.240887105464935
  - 9.443055778741837
  - 9.28448674082756
  - 10.593880474567413
  - 9.757190942764282
  - 10.714463740587234
  - 11.419673681259155
  - 10.395508855581284
  - 11.13792410492897
  - 10.128138720989227
  - 17.960393130779266
  - 17.69995903968811
  - 13.657767713069916
  - 10.939957857131958
  - 15.846357822418213
  - 19.01985067129135
  - 12.313986241817474
  - 11.142918825149536
  - 13.026323318481445
  - 9.787391364574432
  - 12.861728936433792
  - 13.554742932319641
  - 10.067675352096558
  - 8.859888762235641
  - 9.552389472723007
  - 11.12276440858841
  - 8.771017670631409
  - 9.322639107704163
  - 10.165039032697678
  - 9.929791122674942
  - 9.219530612230301
  - 9.575675547122955
  - 8.576495051383972
  - 9.438545376062393
  - 10.460501223802567
  - 9.660844266414642
  - 9.636833310127258
  validation_losses:
  - 41786220544.0
  - 47867424768.0
  - 0.5354492664337158
  - 0.5414588451385498
  - 0.4126642942428589
  - 0.39595484733581543
  - 0.4800078868865967
  - 0.46241945028305054
  - 155181647921152.0
  - 2.0467832005328896e+16
  - 1.0634497079692493e+18
  - 0.4996524155139923
  - 0.4897599518299103
  - 0.4253186583518982
  - 0.44615912437438965
  - 0.48885589838027954
  - 0.5109586715698242
  - 0.5312423706054688
  - 0.4334818720817566
  - 69109416.0
  - 8.744570594525184e+17
  - 1.4086259078833635e+19
  - 0.5511211156845093
  - 0.4269597828388214
  - 0.48111748695373535
  - 0.8234195113182068
  - 43167617384448.0
  - 4311910673547264.0
  - 2.0916787943256883e+17
  - 3.9999218719014257e+18
  - 3.1031048278768615e+19
  - 0.4787629246711731
  - 0.42642831802368164
  - 0.4594537317752838
  - 0.544650673866272
  - 0.5825443267822266
  - 45373443801088.0
  - 7671638371860480.0
  - 4.566455271156613e+17
  - 0.4152725040912628
  - 0.4320681691169739
  - 0.7473931312561035
  - 0.9890630841255188
  - 0.6607632637023926
  - 0.8972176313400269
  - 338113649967104.0
  - 3.109724040776909e+16
  - 0.5059086680412292
  - 0.481123685836792
  - 1256340.875
  - 0.4082263708114624
  - 0.4098435640335083
  - 0.6799001097679138
  - 0.7359195351600647
  - 536019367624704.0
  - 6.346031934681907e+16
  - 2.602582780730671e+18
  - 2.50272573942745e+19
  - 7.058143370694099e+19
  - 5.566181298397407e+19
  - 6.257671197980151e+19
  - 5.86538743942223e+19
  - 5.34030334704757e+19
  - 5.147515418037977e+19
  - 5.3467913452607504e+19
  - 5.351100111427679e+19
  - 6.190267616760273e+19
  - 5.10493836956863e+19
  - 7.012929693341345e+19
  - 5.8251189255665615e+19
  - 3.3977029156696752e+19
  - 2.7950592730716963e+19
  - 3.39125076153556e+19
  - 3.1360585107752616e+19
  - 1.887575811770496e+19
  - 0.645004153251648
  - 0.4522288739681244
  - 0.861770510673523
  - 0.5520548224449158
  - 0.42920348048210144
  - 21032526151680.0
  - 1.2057405571465216e+16
  - 4.535804666545439e+17
  - 8.061955155655918e+18
  - 4.020932494572179e+19
  - 6.68839344324047e+19
  - 7.870357008178741e+19
  - 7.0999551590705136e+19
  - 5.642909178025476e+19
  - 5.661339191930257e+19
  - 5.142928695331547e+19
  - 4.4817378177731e+19
  - 5.1908484907025826e+19
  - 5.012059983542485e+19
  - 5.208715994458594e+19
  - 5.538494276196054e+19
  - 5.721292922164976e+19
  - 6.2224371279657435e+19
  - 7.208823962601731e+19
  - 6.230832119146139e+19
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.7512864493996569, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.152046783625731, 0.0, 0.0]'
  mean_eval_accuracy: 0.8370014087578763
  mean_f1_accuracy: 0.030409356725146202
  total_train_time: '0:06:19.743441'
