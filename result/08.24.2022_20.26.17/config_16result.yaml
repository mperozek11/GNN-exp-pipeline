config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:41:53.459946'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_16fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_16fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_16fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_16fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_16fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_16fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_16fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_16fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_16fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_16fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.319135642051697
  - 5.937397663295269
  - 5.887151136994362
  - 5.782647967338562
  - 5.8905689701437955
  - 5.761214616894723
  - 5.877289071679115
  - 5.78042471408844
  - 5.855487361550331
  - 5.701032054424286
  - 5.763140934705735
  - 5.708627071976662
  - 5.745088413357735
  - 5.578455421328545
  - 5.621898072957993
  - 5.665643501281739
  - 5.633272472023965
  - 5.66283275783062
  - 5.759597444534302
  - 5.610244801640511
  - 5.636278912425041
  validation_losses:
  - 0.41414836049079895
  - 0.3862124979496002
  - 0.38860201835632324
  - 0.39759618043899536
  - 0.4080547094345093
  - 0.3939688503742218
  - 0.3889646530151367
  - 0.39956557750701904
  - 0.3849240243434906
  - 0.4406956136226654
  - 0.385333776473999
  - 0.39686456322669983
  - 0.4020390510559082
  - 0.3872988224029541
  - 0.4256761372089386
  - 0.39164963364601135
  - 0.3886059820652008
  - 0.38798829913139343
  - 0.39328861236572266
  - 0.3906298875808716
  - 0.39529159665107727
loss_records_fold1:
  train_losses:
  - 5.673038762807846
  - 5.632286277413368
  - 5.552291840314865
  - 5.634318470954895
  - 5.615603864192963
  - 5.589556050300598
  - 5.6398430317640305
  - 5.637401604652405
  - 5.63863135278225
  - 5.590846851468086
  - 5.553546226024628
  validation_losses:
  - 0.3915250301361084
  - 0.3928859829902649
  - 0.3962380886077881
  - 0.3873644769191742
  - 0.38965579867362976
  - 0.3903835713863373
  - 0.385944664478302
  - 0.39194807410240173
  - 0.386980265378952
  - 0.3843908905982971
  - 0.38824620842933655
loss_records_fold2:
  train_losses:
  - 5.7864177405834205
  - 5.691995185613632
  - 5.561825689673424
  - 5.608190920948982
  - 5.654895377159119
  - 5.620109209418297
  - 5.628113472461701
  - 5.612393483519554
  - 5.5338229775428776
  - 5.627972075343132
  - 5.597804698348046
  validation_losses:
  - 0.39346230030059814
  - 0.3890371024608612
  - 0.39007094502449036
  - 0.3856561481952667
  - 0.3933272063732147
  - 0.3877250552177429
  - 0.3859040439128876
  - 0.38980183005332947
  - 0.39241865277290344
  - 0.3932584822177887
  - 0.3875458240509033
loss_records_fold3:
  train_losses:
  - 5.707519665360451
  - 5.725653758645058
  - 5.64720785021782
  - 5.619503480195999
  - 5.629148849844933
  - 5.655724155902863
  - 5.673086401820183
  - 5.702920603752137
  - 5.6556674450635915
  - 5.672006577253342
  - 5.634261527657509
  - 5.624418652057648
  - 5.683569979667664
  - 5.686676615476609
  - 5.656804838776589
  - 5.643575835227967
  - 5.619142791628838
  validation_losses:
  - 0.38245052099227905
  - 0.37367358803749084
  - 0.37761446833610535
  - 0.37111803889274597
  - 0.3790510296821594
  - 0.4036136865615845
  - 0.3765830397605896
  - 0.37409016489982605
  - 0.37773269414901733
  - 0.3725016117095947
  - 0.38568398356437683
  - 0.3801189959049225
  - 0.3755459189414978
  - 0.3782690465450287
  - 0.37478330731391907
  - 0.3730637729167938
  - 0.3761652410030365
loss_records_fold4:
  train_losses:
  - 5.631653559207916
  - 5.625427332520485
  - 5.587716564536095
  - 5.587508779764176
  - 5.613549882173539
  - 5.584762066602707
  - 5.607995486259461
  - 5.600310683250427
  - 5.612219285964966
  - 5.6116915747523315
  - 5.616035509109498
  validation_losses:
  - 0.3800501823425293
  - 0.3795948922634125
  - 0.3801063001155853
  - 0.38080868124961853
  - 0.380159854888916
  - 0.3775098919868469
  - 0.3823825716972351
  - 0.38057801127433777
  - 0.3809520900249481
  - 0.38076362013816833
  - 0.3791258931159973
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:07:05.101828'
