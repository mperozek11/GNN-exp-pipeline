config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.880631'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_11fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_11fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_11fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_11fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_11fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_11fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_11fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_11fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_11fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_11fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 19.245674324035647
  - 7.019359707832336
  - 2.6571540713310244
  - 3.382460153102875
  - 2.1947162985801696
  - 3.423037600517273
  - 1.6867133975028992
  - 1.5087682485580445
  - 1.776375871896744
  - 1.5016612231731417
  - 1.9623772561550141
  - 1.5929214239120484
  - 1.4806174397468568
  - 1.5926982522010804
  - 2.0932212054729464
  - 1.1076048970222474
  - 2.069882893562317
  - 1.7278048038482667
  - 1.1664307951927186
  - 1.0484433710575105
  - 0.9907664120197297
  - 0.858903056383133
  - 0.8611472249031067
  - 0.9115777611732483
  - 2.3006980061531066
  - 1.1600594341754913
  - 2.3713497400283816
  - 2.3908519744873047
  - 1.3466644406318666
  - 1.0045787274837494
  - 1.5771457612514497
  - 2.8535904407501222
  - 2.039823567867279
  - 1.2956409692764284
  - 1.1618721425533296
  - 0.8804356217384339
  - 3.903850257396698
  - 1.1537676513195039
  - 0.9656334102153779
  - 1.6864619135856629
  - 1.5899031221866609
  - 1.0501313745975496
  - 2.30328027009964
  - 1.1707765221595765
  - 1.492539143562317
  - 0.9458508223295212
  - 0.9592969238758088
  - 0.9787406787276268
  - 0.934389054775238
  - 0.8799713253974915
  - 0.8991418480873108
  - 1.1292375326156616
  - 0.8653892695903779
  - 1.5016522705554962
  - 1.7941608667373659
  - 1.1498103439807892
  - 0.9688180446624757
  - 0.8755055069923401
  - 1.2371545255184175
  - 1.1186060547828676
  - 1.51534760594368
  - 0.8867243826389313
  - 0.9216520488262177
  - 0.8115571677684784
  - 0.8440605580806733
  - 0.9143014371395112
  - 0.9737320601940156
  - 0.9227705419063569
  - 1.306855171918869
  - 0.7991496860980988
  - 0.8555798828601837
  - 1.103649240732193
  - 0.9168341398239136
  - 1.3565882086753847
  - 0.9156529784202576
  - 0.9338033676147461
  - 0.8117803871631623
  - 0.7986249685287476
  - 0.813387966156006
  - 0.9255312025547028
  - 0.7821982383728028
  - 0.7545829832553864
  - 0.8396130859851838
  - 0.8410312473773957
  - 0.7929412484169007
  - 0.935958594083786
  - 1.2034770488739015
  - 1.0832462549209596
  - 1.4054897964000703
  - 1.0525357067584993
  - 0.9730511128902436
  - 0.953203946352005
  - 0.8479836821556092
  - 0.8875241577625275
  - 0.9355864107608796
  - 1.055992990732193
  - 0.9090210735797882
  - 0.8456393778324127
  - 1.3711583435535433
  - 1.027761161327362
  validation_losses:
  - 4.977656841278076
  - 1.8308583498001099
  - 0.801044762134552
  - 2.236499786376953
  - 0.9950529336929321
  - 2.953843355178833
  - 0.7964751124382019
  - 1.573900580406189
  - 0.9060961008071899
  - 0.5072712302207947
  - 0.8273944854736328
  - 0.7486979365348816
  - 0.4964345395565033
  - 2.888321876525879
  - 0.7132636308670044
  - 0.506655752658844
  - 0.6188441514968872
  - 0.4762462377548218
  - 0.4362388551235199
  - 0.5477354526519775
  - 0.4672677516937256
  - 0.38542288541793823
  - 0.38275107741355896
  - 0.4271911084651947
  - 0.40977245569229126
  - 0.44383054971694946
  - 0.8895086050033569
  - 0.6033319234848022
  - 0.4744511842727661
  - 0.4916890263557434
  - 0.7235046029090881
  - 1.1599433422088623
  - 0.5963000655174255
  - 0.47620755434036255
  - 0.42623573541641235
  - 0.4178584814071655
  - 0.6876355409622192
  - 0.40026548504829407
  - 0.432420551776886
  - 0.5489857792854309
  - 0.6284393668174744
  - 0.5971340537071228
  - 0.5147914886474609
  - 1.4141980409622192
  - 0.42761942744255066
  - 0.43450218439102173
  - 0.45432955026626587
  - 0.458136647939682
  - 0.4251864552497864
  - 0.3881172835826874
  - 0.42130914330482483
  - 0.5042555928230286
  - 0.540838897228241
  - 0.42645156383514404
  - 1.0138293504714966
  - 0.7914191484451294
  - 0.4020748734474182
  - 0.5458115339279175
  - 0.41458794474601746
  - 0.4153849184513092
  - 0.41901251673698425
  - 0.4181162416934967
  - 0.4536173641681671
  - 0.4302058815956116
  - 0.43188127875328064
  - 0.38783666491508484
  - 0.40198907256126404
  - 0.38819554448127747
  - 0.4445492625236511
  - 0.3927244544029236
  - 0.43836063146591187
  - 0.42548123002052307
  - 0.42083820700645447
  - 0.48783281445503235
  - 0.5082939863204956
  - 0.39139115810394287
  - 0.38370904326438904
  - 0.382796972990036
  - 0.39276784658432007
  - 0.3752746284008026
  - 0.39280009269714355
  - 0.38088032603263855
  - 0.4717123508453369
  - 0.3876882493495941
  - 0.37720924615859985
  - 0.6407546401023865
  - 0.39277613162994385
  - 0.649966835975647
  - 0.5555832386016846
  - 0.4702788293361664
  - 0.5232741832733154
  - 0.4666089713573456
  - 0.3816222846508026
  - 0.37943124771118164
  - 0.40771323442459106
  - 0.42555251717567444
  - 0.4289218485355377
  - 0.40459898114204407
  - 0.38884440064430237
  - 0.3806557059288025
loss_records_fold1:
  train_losses:
  - 0.8511480152606965
  - 1.024291855096817
  - 1.1435709834098817
  - 0.8437970280647278
  - 0.9533248364925385
  - 0.8378542959690094
  - 0.8064834415912628
  - 0.7678305804729462
  - 0.7518833547830582
  - 0.7586715161800385
  - 0.781402850151062
  - 0.8878872990608215
  - 1.127854198217392
  - 1.0174589335918427
  - 1.7319184422492981
  - 1.0246440559625627
  - 1.0330458581447601
  - 1.564653116464615
  - 1.1515240669250488
  - 0.933151513338089
  - 0.7924925357103348
  - 0.879478919506073
  - 1.30363444685936
  - 0.8473860383033753
  - 0.8332085967063905
  - 0.7937164664268495
  - 0.7782013714313507
  - 0.7866202473640442
  - 0.7855112314224244
  - 0.7704359352588654
  - 0.7647066593170166
  - 1.1605988442897797
  - 1.1844101130962372
  - 1.1804498553276062
  - 1.0704563736915589
  - 1.0104381322860718
  - 1.0892750382423402
  - 1.4513731241226198
  - 1.7774381756782534
  - 0.9867274641990662
  - 0.8966983437538147
  - 1.0602157592773438
  - 0.8455534517765045
  - 0.9467628777027131
  - 1.306093853712082
  - 1.2170521914958954
  - 0.8846489310264588
  - 2.2612978518009186
  - 1.1699955821037293
  - 0.8639292389154435
  - 1.2485445439815521
  - 5.051769185066224
  - 1.0224978923797607
  - 0.8707228004932404
  - 0.9058779180049896
  - 1.2822795867919923
  - 2.5845182955265047
  - 1.5388498663902284
  - 0.9412657618522644
  - 0.8434583485126496
  - 0.7801794350147248
  - 0.8273955225944519
  - 0.8346399962902069
  - 0.887494570016861
  - 0.8571587026119233
  - 0.9742163836956025
  - 0.803905576467514
  - 1.3979301750659943
  - 1.7311031103134156
  - 0.9134558379650116
  - 0.9706108152866364
  validation_losses:
  - 0.48775362968444824
  - 0.4026520848274231
  - 0.3990570604801178
  - 0.4576883614063263
  - 0.4869884252548218
  - 0.5037671327590942
  - 0.4241979122161865
  - 0.4166465997695923
  - 0.3986152112483978
  - 0.4002017676830292
  - 0.4245009422302246
  - 0.4252875745296478
  - 0.5222412943840027
  - 0.7447437047958374
  - 0.45439863204956055
  - 0.48294419050216675
  - 0.407264769077301
  - 0.4689178466796875
  - 0.5175384879112244
  - 0.42991265654563904
  - 0.39363908767700195
  - 0.4007444679737091
  - 0.5183861255645752
  - 0.464470237493515
  - 0.3973180949687958
  - 0.4077024757862091
  - 0.4025637209415436
  - 0.3988687992095947
  - 0.40993189811706543
  - 0.3989071547985077
  - 0.4384230971336365
  - 0.5069961547851562
  - 0.4442993998527527
  - 0.43554604053497314
  - 0.39905276894569397
  - 0.39908745884895325
  - 0.5583722591400146
  - 0.9677662253379822
  - 0.4330706298351288
  - 0.47990682721138
  - 0.3954394459724426
  - 0.4098605513572693
  - 0.4769147038459778
  - 0.5058249235153198
  - 0.9271115660667419
  - 0.5160201787948608
  - 0.5336813926696777
  - 0.5414198040962219
  - 0.44568970799446106
  - 0.48486796021461487
  - 0.47619369626045227
  - 0.40930044651031494
  - 0.3997626006603241
  - 0.41176271438598633
  - 0.4989486038684845
  - 0.4156995117664337
  - 0.4913480877876282
  - 0.6057544946670532
  - 0.400710791349411
  - 0.4000221788883209
  - 0.400489866733551
  - 0.41167157888412476
  - 0.41896647214889526
  - 0.4006321132183075
  - 0.4233252704143524
  - 0.3975023925304413
  - 0.40116578340530396
  - 0.4073682725429535
  - 0.39898115396499634
  - 0.3996046185493469
  - 0.4000724256038666
loss_records_fold2:
  train_losses:
  - 0.9145410776138306
  - 2.0718883097171785
  - 0.879807785153389
  - 1.0068510308861733
  - 0.8532597839832307
  - 0.8958303451538087
  - 0.8101355731487274
  - 0.8536489188671113
  - 1.237783706188202
  - 0.9878905415534973
  - 0.8740789592266083
  - 2.0724357187747957
  - 0.850339424610138
  - 1.0273443400859834
  - 0.9192230343818665
  - 1.0730996191501618
  - 1.0488854765892028
  - 0.9994107902050019
  - 1.1296266674995423
  - 0.8779358506202698
  - 0.8340513646602631
  - 0.887877556681633
  - 0.9325081229209901
  - 0.8718448698520661
  - 1.034435933828354
  - 0.7616336464881898
  - 0.8739624083042146
  - 0.8747071146965028
  - 0.8780124574899674
  - 0.8524063885211945
  - 0.7737145930528642
  - 0.8366046845912933
  - 1.2228684604167939
  - 0.8824827432632447
  - 0.8018382251262666
  - 0.8055851876735688
  - 1.0768392443656922
  - 0.8557845175266267
  - 1.0748065829277038
  - 0.8491285860538483
  - 0.7849078238010407
  - 0.9221883118152618
  - 0.7688583672046662
  validation_losses:
  - 0.37423941493034363
  - 0.3890763521194458
  - 0.6243418455123901
  - 0.37550362944602966
  - 0.38220372796058655
  - 0.37575045228004456
  - 0.38943904638290405
  - 0.38896992802619934
  - 0.3821544349193573
  - 0.41471606492996216
  - 0.3772154748439789
  - 0.3898380696773529
  - 0.463379442691803
  - 0.3730848431587219
  - 0.42114272713661194
  - 0.5300523638725281
  - 0.4036574363708496
  - 0.3817872107028961
  - 0.37663739919662476
  - 0.3773114085197449
  - 0.3900891840457916
  - 0.37313419580459595
  - 0.39159080386161804
  - 0.3934405446052551
  - 0.38970494270324707
  - 0.37627652287483215
  - 0.391325980424881
  - 0.3933045268058777
  - 0.3717334270477295
  - 0.4454221725463867
  - 0.4411771893501282
  - 0.39424368739128113
  - 0.4891664385795593
  - 0.37872225046157837
  - 0.39322781562805176
  - 0.37526383996009827
  - 0.44247081875801086
  - 0.37837520241737366
  - 0.37865373492240906
  - 0.37342342734336853
  - 0.3742567002773285
  - 0.3796840012073517
  - 0.3721655607223511
loss_records_fold3:
  train_losses:
  - 1.1146403312683106
  - 0.8808693408966065
  - 0.8230686604976655
  - 0.9300684571266175
  - 0.786598426103592
  - 0.8191401243209839
  - 0.8467109620571137
  - 1.6790981769561768
  - 0.8364363968372346
  - 0.8022366642951966
  - 0.8174522161483765
  - 1.0820087671279908
  - 0.7550917506217957
  - 0.8293633818626405
  - 0.8155749261379243
  - 0.7823025405406953
  - 0.787274044752121
  - 0.8213060796260834
  - 0.793127030134201
  - 0.7800630390644074
  validation_losses:
  - 0.38001734018325806
  - 0.38039442896842957
  - 0.37887880206108093
  - 0.3796261250972748
  - 0.38668298721313477
  - 0.3803187906742096
  - 0.40695688128471375
  - 0.4054819345474243
  - 0.3804585635662079
  - 0.39144477248191833
  - 0.382667601108551
  - 0.391695111989975
  - 0.37950968742370605
  - 0.4779561758041382
  - 0.3955039083957672
  - 0.3791922628879547
  - 0.3803076148033142
  - 0.37580811977386475
  - 0.3800569474697113
  - 0.3758631944656372
loss_records_fold4:
  train_losses:
  - 0.7446538746356964
  - 0.7374206423759461
  - 0.7509524881839753
  - 0.7908599555492402
  - 0.8163842380046845
  - 0.7576450526714326
  - 0.7587341904640198
  - 0.7711976706981659
  - 0.7885211646556854
  - 0.7826822698116302
  - 0.8048511564731599
  - 0.7680187702178956
  - 0.7297013700008392
  - 0.7680864691734315
  - 0.7771822810173035
  - 0.8036910772323609
  - 0.7293035000562669
  - 0.7941496789455414
  - 0.7573107123374939
  - 0.7620174169540406
  - 0.7652036964893342
  - 0.7341542273759842
  - 0.7278498828411103
  - 0.7670587360858918
  - 0.7613525688648224
  - 0.8931817352771759
  - 0.837206256389618
  - 0.7965372502803802
  - 0.7425180077552795
  - 0.7643594086170197
  - 0.7562196910381318
  - 0.7394756853580475
  - 0.9051636636257172
  - 0.7470775783061981
  - 0.8122494161128998
  - 0.7416868478059769
  - 0.7687139332294465
  - 0.8189407527446747
  - 0.8222963094711304
  - 0.7980172157287598
  - 0.8568021893501282
  - 1.125222408771515
  - 0.8138464570045472
  - 0.8669292330741882
  - 0.9341487765312195
  - 0.8800566911697388
  - 0.8393262624740601
  - 0.9377046287059785
  - 0.758589056134224
  - 0.8106931388378144
  - 0.8756384670734406
  - 0.9117146670818329
  - 0.8466307163238526
  - 0.7671824276447297
  - 0.7819293856620789
  - 0.7752892673015594
  - 0.734201055765152
  validation_losses:
  - 0.39346909523010254
  - 0.39665451645851135
  - 0.4262358844280243
  - 0.39693379402160645
  - 0.4134884476661682
  - 0.38820740580558777
  - 0.3852345049381256
  - 0.390327125787735
  - 0.38580796122550964
  - 0.39273741841316223
  - 0.4144468605518341
  - 0.40112829208374023
  - 0.392941415309906
  - 0.44677117466926575
  - 0.3876897394657135
  - 0.4265185594558716
  - 0.38682353496551514
  - 0.39071232080459595
  - 0.39029425382614136
  - 0.38229161500930786
  - 0.4032599925994873
  - 0.39184099435806274
  - 0.38287147879600525
  - 0.40723055601119995
  - 0.3888096213340759
  - 0.7975903153419495
  - 0.4137742817401886
  - 0.4113251566886902
  - 0.3914448320865631
  - 0.4241761863231659
  - 0.38551509380340576
  - 0.39069029688835144
  - 0.4168872833251953
  - 0.5522271394729614
  - 0.41608983278274536
  - 0.3954605460166931
  - 0.40675055980682373
  - 0.4144091010093689
  - 0.44694021344184875
  - 0.4822506308555603
  - 0.5762903690338135
  - 0.5247347354888916
  - 0.3875619173049927
  - 0.3962028920650482
  - 0.46832528710365295
  - 0.4308817386627197
  - 0.3949006199836731
  - 0.40725386142730713
  - 0.4264013171195984
  - 0.41320332884788513
  - 0.5709646344184875
  - 0.4484720826148987
  - 0.39231809973716736
  - 0.3991609215736389
  - 0.40552666783332825
  - 0.38848403096199036
  - 0.386986643075943
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 71 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 43 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 57 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:23:20.892405'
