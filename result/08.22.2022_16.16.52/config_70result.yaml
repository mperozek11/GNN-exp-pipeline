config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:49:19.057622'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_70fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_70fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_70fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_70fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_70fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_70fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_70fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_70fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_70fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_70fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.8882461309432985
  - 1.64018132686615
  - 1.6252276539802553
  - 1.5800494849681854
  - 1.6617437601089478
  - 1.6041218936443329
  - 1.6136751115322114
  - 1.5150173097848894
  - 1.6227306842803957
  - 1.6027005910873413
  - 1.6256641030311585
  - 1.6316172182559967
  - 1.6064718663692474
  - 1.584404504299164
  - 1.5543207824230194
  - 1.6927647352218629
  - 1.800118225812912
  - 1.5937886893749238
  - 1.5415882587432863
  - 1.5474935054779053
  - 1.6112169206142426
  - 1.5849283993244172
  - 1.5955245077610016
  - 1.5979939222335817
  - 1.5599870085716248
  - 1.5369548857212068
  validation_losses:
  - 0.45728376507759094
  - 0.40245184302330017
  - 0.39763933420181274
  - 0.3980538249015808
  - 0.4089750647544861
  - 0.41024500131607056
  - 0.39223384857177734
  - 0.38080641627311707
  - 0.39885005354881287
  - 0.40474632382392883
  - 0.3997037708759308
  - 0.39633163809776306
  - 0.3973289132118225
  - 0.39027509093284607
  - 0.44045284390449524
  - 0.3934680223464966
  - 0.4106379747390747
  - 0.393442302942276
  - 0.3799956738948822
  - 0.39118969440460205
  - 0.39691561460494995
  - 0.3958032727241516
  - 0.3949839174747467
  - 0.3988226652145386
  - 0.390205979347229
  - 0.38642191886901855
loss_records_fold1:
  train_losses:
  - 1.5700967073440553
  - 1.6148623883724214
  - 1.5897003948688508
  - 1.566943883895874
  - 1.5735547661781313
  - 1.5035490036010744
  - 1.620762038230896
  - 1.6186853289604188
  - 1.6002526700496675
  - 1.5620411813259125
  - 1.532416218519211
  - 1.606561690568924
  - 1.4998861223459246
  - 1.5387865722179415
  - 1.5914911389350892
  - 1.6196812689304352
  - 1.580430793762207
  - 1.575184589624405
  - 1.5592702329158783
  - 1.5705125093460084
  - 1.5451500415802002
  - 1.5191618025302889
  - 1.5480963587760925
  - 1.568750262260437
  validation_losses:
  - 0.39488181471824646
  - 0.4069981276988983
  - 0.3961334824562073
  - 0.4016874432563782
  - 0.5480676889419556
  - 0.5155986547470093
  - 0.42929670214653015
  - 0.4002567231655121
  - 0.3973356783390045
  - 0.39321988821029663
  - 0.41145092248916626
  - 0.4065835177898407
  - 0.3991177976131439
  - 0.4149543344974518
  - 0.4034622609615326
  - 0.41402408480644226
  - 0.39780864119529724
  - 0.40929684042930603
  - 0.3973499536514282
  - 0.3919675350189209
  - 0.3963488042354584
  - 0.3910815715789795
  - 0.39093834161758423
  - 0.38729128241539
loss_records_fold2:
  train_losses:
  - 1.5754754424095154
  - 1.5985971450805665
  - 1.549641263484955
  - 1.5613134920597078
  - 1.6040283739566803
  - 1.5997865021228792
  - 1.526636415719986
  - 1.6052464425563813
  - 1.5131830096244814
  - 1.5990475773811341
  - 1.570502132177353
  validation_losses:
  - 0.6210242509841919
  - 0.49772021174430847
  - 0.3928054869174957
  - 0.40419867634773254
  - 0.39809417724609375
  - 0.39348867535591125
  - 0.3987920880317688
  - 0.38984501361846924
  - 0.38901400566101074
  - 0.39336442947387695
  - 0.3956710994243622
loss_records_fold3:
  train_losses:
  - 1.5548788487911225
  - 1.5816693902015686
  - 1.5387826383113863
  - 1.5363899350166321
  - 1.52935688495636
  - 1.5530901670455934
  - 1.6155378222465515
  - 1.5676137924194338
  - 1.6073379576206208
  - 1.565198075771332
  - 1.5994834184646607
  - 1.558091002702713
  - 1.5195840656757356
  - 1.6007960438728333
  - 1.5759728372097017
  - 1.537121468782425
  - 1.61675683259964
  validation_losses:
  - 0.3856430947780609
  - 0.40484049916267395
  - 0.3900022506713867
  - 0.3987622559070587
  - 0.3885398209095001
  - 0.3855816423892975
  - 0.4856360852718353
  - 0.4038192927837372
  - 0.39108502864837646
  - 0.4389723837375641
  - 0.45001620054244995
  - 0.41709327697753906
  - 0.3908795118331909
  - 0.38245508074760437
  - 0.3878065049648285
  - 0.383894145488739
  - 0.38904082775115967
loss_records_fold4:
  train_losses:
  - 1.583676278591156
  - 1.5239806592464449
  - 1.827074307203293
  - 1.6394108057022097
  - 1.6284080743789673
  - 1.5024970412254335
  - 1.5483045220375062
  - 1.5853404700756073
  - 1.5868002116680147
  - 1.5148318707942963
  - 1.5263297438621521
  - 1.6220207214355469
  - 1.7136849761009216
  - 1.646474677324295
  - 1.6196415305137635
  - 1.5794639050960542
  - 1.5177592873573305
  - 1.6059961378574372
  - 1.581251472234726
  - 1.5702503681182862
  - 1.5836786329746246
  - 1.5363104581832887
  - 1.61714483499527
  - 1.5275743663311006
  - 1.5516334652900696
  - 1.541641390323639
  - 1.5793951213359834
  - 1.5432607948780062
  - 1.5702862918376923
  - 1.5570928037166596
  - 1.586606204509735
  - 1.5756395637989045
  - 1.5698028981685639
  - 1.5416952848434449
  - 1.5510573744773866
  - 1.5830706417560578
  - 1.581146413087845
  validation_losses:
  - 0.3955838978290558
  - 0.385972797870636
  - 0.4153563678264618
  - 0.4003903567790985
  - 0.38780415058135986
  - 0.39034637808799744
  - 0.4000295400619507
  - 0.3867398202419281
  - 0.4009266793727875
  - 0.38994351029396057
  - 0.4110240042209625
  - 0.3996662199497223
  - 0.43960514664649963
  - 0.4078420400619507
  - 0.40083199739456177
  - 0.4264748692512512
  - 0.4007055461406708
  - 0.42361313104629517
  - 0.38160642981529236
  - 0.38864174485206604
  - 0.41578611731529236
  - 0.39228513836860657
  - 0.39208734035491943
  - 0.37888437509536743
  - 0.3917054533958435
  - 0.39507144689559937
  - 0.39237862825393677
  - 0.3946415185928345
  - 0.39989522099494934
  - 0.3985667824745178
  - 0.4146900177001953
  - 0.38304847478866577
  - 0.3800271451473236
  - 0.38889074325561523
  - 0.3938390910625458
  - 0.3935421407222748
  - 0.3799569606781006
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 37 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:09:59.656040'
