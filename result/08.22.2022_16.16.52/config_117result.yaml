config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:51:21.465298'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_117fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_117fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_117fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_117fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_117fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_117fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_117fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_117fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_117fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_117fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.8532772302627567
  - 3.3721681714057925
  - 3.2620906293392182
  - 3.214995354413986
  - 3.2788843452930454
  - 3.2542185187339783
  - 3.2305880129337314
  - 3.2538554191589357
  - 3.6544773519039158
  - 3.2254399955272675
  - 3.1704390943050385
  - 3.0539745867252353
  - 3.2871260881423954
  - 3.689263027906418
  - 3.176427471637726
  - 3.1133302688598636
  - 3.047610855102539
  - 3.0501153945922854
  - 3.3437100112438203
  - 3.2597359836101534
  - 3.2154509067535404
  - 3.2402599453926086
  - 3.3457170724868774
  - 3.05896058678627
  - 3.1508180081844332
  - 3.1622478485107424
  - 3.2607145547866825
  - 3.445839536190033
  - 3.1483486235141758
  - 3.0208971083164218
  - 3.0690463334321976
  - 3.1258774995803833
  - 3.054837274551392
  - 3.1022358477115635
  validation_losses:
  - 1.0914634466171265
  - 0.4032416343688965
  - 0.3967781364917755
  - 0.39833521842956543
  - 0.4204248785972595
  - 0.429796040058136
  - 0.40296947956085205
  - 0.4147196412086487
  - 0.43487629294395447
  - 0.3929479122161865
  - 0.4183286428451538
  - 0.4185168743133545
  - 0.4218969941139221
  - 0.4037201404571533
  - 0.4060257077217102
  - 0.39316463470458984
  - 0.40371280908584595
  - 0.40429526567459106
  - 0.406009703874588
  - 0.4313475787639618
  - 0.3994731605052948
  - 0.4006580412387848
  - 0.42727187275886536
  - 0.4373854994773865
  - 0.4139193296432495
  - 0.41162925958633423
  - 0.38928234577178955
  - 0.40131720900535583
  - 0.39710307121276855
  - 0.391130656003952
  - 0.3882918953895569
  - 0.3952792286872864
  - 0.3894718587398529
  - 0.3947407901287079
loss_records_fold1:
  train_losses:
  - 3.081888711452484
  - 3.0695731699466706
  - 3.027555710077286
  - 3.0458539545536043
  - 3.0328699141740803
  - 3.112075233459473
  - 3.0604964554309846
  - 3.0167167484760284
  - 3.0803046464920047
  - 3.0534494757652286
  - 3.02875269651413
  - 3.23605672121048
  - 3.1138268589973452
  - 3.0897311806678776
  - 3.036936765909195
  validation_losses:
  - 0.4175868332386017
  - 0.3973345458507538
  - 0.3882603049278259
  - 0.4194439947605133
  - 0.3971756398677826
  - 0.4070475399494171
  - 0.4194943308830261
  - 0.4704403579235077
  - 0.5757695436477661
  - 0.3957984745502472
  - 0.4030895531177521
  - 0.38753148913383484
  - 0.39311274886131287
  - 0.38897401094436646
  - 0.3968204855918884
loss_records_fold2:
  train_losses:
  - 3.2057451725006105
  - 3.1496817111968998
  - 3.038680738210678
  - 3.053726297616959
  - 3.0691063493490223
  - 2.9913608133792877
  - 3.0178270399570466
  - 3.1094030618667605
  - 3.0726820647716524
  - 3.0686303198337557
  - 2.982626885175705
  - 3.105550003051758
  - 3.177815318107605
  - 3.0900743246078495
  validation_losses:
  - 0.39893069863319397
  - 0.3943801820278168
  - 0.39477357268333435
  - 0.4006235599517822
  - 0.39461222290992737
  - 0.3902629315853119
  - 0.3790871202945709
  - 0.4017517864704132
  - 0.3967946469783783
  - 0.3862350881099701
  - 0.3949791193008423
  - 0.3908678889274597
  - 0.3913256525993347
  - 0.38782617449760437
loss_records_fold3:
  train_losses:
  - 3.0464400529861453
  - 2.9654871344566347
  - 3.005862206220627
  - 3.07548617720604
  - 3.0435476779937747
  - 3.0668342411518097
  - 2.998162838816643
  - 2.966411253809929
  - 2.9851443052291873
  - 2.978000831604004
  - 2.961178070306778
  - 2.9635765969753267
  - 3.0591104328632355
  - 3.0153257966041567
  - 3.016107952594757
  validation_losses:
  - 0.3866758942604065
  - 0.39286860823631287
  - 0.379560649394989
  - 0.39568397402763367
  - 0.3786776661872864
  - 0.3716108798980713
  - 0.37519803643226624
  - 0.37239575386047363
  - 0.3888777792453766
  - 0.36971667408943176
  - 0.3679157495498657
  - 0.37338876724243164
  - 0.37948697805404663
  - 0.3802681267261505
  - 0.3797292411327362
loss_records_fold4:
  train_losses:
  - 2.9672421455383304
  - 2.9530199408531193
  - 2.939562913775444
  - 3.0236531823873523
  - 3.020060104131699
  - 2.9726602971553806
  - 3.039102140069008
  - 3.0043508529663088
  - 2.9456326544284823
  - 3.025446093082428
  - 2.9430622637271884
  - 3.0282283365726474
  - 2.9675836861133575
  - 2.891826665401459
  validation_losses:
  - 0.37228357791900635
  - 0.4202924966812134
  - 0.3923582136631012
  - 0.3809869587421417
  - 0.37673845887184143
  - 0.3758493661880493
  - 0.3691945970058441
  - 0.37960124015808105
  - 0.38358965516090393
  - 0.38138553500175476
  - 0.3869558572769165
  - 0.38095831871032715
  - 0.378541499376297
  - 0.3802366256713867
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8576329331046312,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:09:10.849755'
