config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:55:28.692897'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_109fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_109fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_109fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_109fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_109fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_109fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_109fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_109fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_109fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_109fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 53.07031126022339
  - 32.56589438319207
  - 19.833954519033433
  - 10.262741380929947
  - 17.275565916299822
  - 16.61806468963623
  - 13.382429242134094
  - 13.159494781494141
  - 21.752425289154054
  - 14.344798243045808
  - 9.334460681676864
  - 22.996015024185184
  - 24.31485124230385
  - 30.53486816287041
  - 15.678630340099335
  - 12.934044569730759
  - 10.477345919609071
  - 6.507160419225693
  - 8.853706192970277
  - 7.9745163500309
  - 6.11248868405819
  - 7.341972553730011
  - 8.02228741645813
  - 5.849227732419968
  - 5.318661451339722
  - 7.665847808122635
  - 9.752685540914536
  - 5.981088894605637
  - 12.141988131403924
  - 5.13905600309372
  - 9.27009711265564
  - 6.046306800842285
  - 4.613521808385849
  - 4.207269424200058
  - 5.651584416627884
  - 5.64499905705452
  - 6.266152459383012
  - 4.905397880077363
  - 3.8155435621738434
  - 4.92504290342331
  - 4.629000824689865
  - 5.905157351493836
  - 5.388087922334671
  - 3.6407167613506317
  - 6.6115748465061195
  - 4.471604418754578
  - 6.281951308250427
  - 4.083591389656067
  - 4.219932043552399
  - 4.201605719327927
  - 4.361945539712906
  - 5.816090452671052
  - 6.144359463453293
  - 5.7874462604522705
  - 6.872433978319169
  - 4.36128860116005
  - 3.2956529021263123
  - 4.741261315345764
  - 4.885027861595154
  - 3.7060992419719696
  - 3.5343179225921633
  - 3.3206132441759113
  - 3.820531040430069
  - 3.782862812280655
  - 3.2455032706260685
  - 3.352070334553719
  - 3.3954174101352694
  - 3.5796702563762666
  - 3.7043161332607273
  - 3.144535773992539
  - 3.6583276927471164
  - 3.689827257394791
  - 3.6994094610214234
  - 3.442707055807114
  - 3.076968824863434
  - 3.7102301299571994
  - 3.3004137873649597
  - 3.1614128530025485
  - 3.594756403565407
  - 4.23405869603157
  - 3.340226060152054
  - 3.809343454241753
  - 3.430380344390869
  - 3.210815092921257
  - 3.7577889919281007
  - 3.364771413803101
  - 3.1934952437877655
  - 3.2582065761089325
  - 3.3353524565696717
  - 3.5596166253089905
  - 3.108210426568985
  - 3.157005780935288
  - 3.3516655564308167
  - 3.3332682847976685
  - 3.176464796066284
  - 3.232137137651444
  - 3.2756399512290955
  - 3.3261811971664432
  - 3.46582293510437
  - 3.3896074235439304
  validation_losses:
  - 6.975234508514404
  - 1.1838687658309937
  - 1.0581746101379395
  - 0.6238869428634644
  - 1.674744725227356
  - 1.6224133968353271
  - 0.5741254091262817
  - 1.3458120822906494
  - 0.6275420188903809
  - 0.7053385972976685
  - 0.48773089051246643
  - 0.7945196628570557
  - 0.4419046938419342
  - 0.5555334687232971
  - 0.7633394002914429
  - 0.7098456621170044
  - 0.7499251365661621
  - 0.5904185771942139
  - 0.41590407490730286
  - 0.5652295351028442
  - 0.4638197720050812
  - 0.4058161675930023
  - 0.4617069959640503
  - 0.3927411437034607
  - 0.4094681143760681
  - 0.45604407787323
  - 0.43123573064804077
  - 0.39446237683296204
  - 0.41661909222602844
  - 0.42471376061439514
  - 0.4174308180809021
  - 0.40942755341529846
  - 0.3822528123855591
  - 0.39778223633766174
  - 0.40836817026138306
  - 0.4113367199897766
  - 0.4203681945800781
  - 0.42749711871147156
  - 0.39616942405700684
  - 0.8250635266304016
  - 0.41566237807273865
  - 0.43941766023635864
  - 0.4347280263900757
  - 0.4955294728279114
  - 0.39418768882751465
  - 0.5371235609054565
  - 0.38786277174949646
  - 0.3911467492580414
  - 0.49102291464805603
  - 0.441928893327713
  - 0.41151973605155945
  - 0.4651799201965332
  - 1.01706063747406
  - 0.7340477108955383
  - 0.4489224851131439
  - 0.42260175943374634
  - 0.4063531160354614
  - 0.39529862999916077
  - 0.42072606086730957
  - 0.38856348395347595
  - 0.41024670004844666
  - 0.40284299850463867
  - 0.4603051245212555
  - 0.4267347455024719
  - 0.3876784145832062
  - 0.3869680166244507
  - 0.3975009024143219
  - 0.4050200283527374
  - 0.39525866508483887
  - 0.41016140580177307
  - 0.39628866314888
  - 0.39655041694641113
  - 0.4242355227470398
  - 0.4013177454471588
  - 0.41462939977645874
  - 0.39263880252838135
  - 0.40424636006355286
  - 0.40212157368659973
  - 0.3882516026496887
  - 0.3994474709033966
  - 0.5556032657623291
  - 0.42093896865844727
  - 0.4045827388763428
  - 0.42420852184295654
  - 0.4061203598976135
  - 0.39371439814567566
  - 0.40288010239601135
  - 0.43297287821769714
  - 0.3928637206554413
  - 0.38833773136138916
  - 0.4004283845424652
  - 0.4039313495159149
  - 0.42196908593177795
  - 0.41305625438690186
  - 0.44097739458084106
  - 0.4215727746486664
  - 0.4621407389640808
  - 0.412315309047699
  - 0.9920204281806946
  - 0.4619031250476837
loss_records_fold1:
  train_losses:
  - 3.4556769251823427
  - 3.362677609920502
  - 3.3973647356033325
  - 3.312845915555954
  - 3.8689932316541675
  - 3.3353723227977756
  - 3.403858941793442
  - 3.2780617713928226
  - 3.1349934279918674
  - 3.1808733671903613
  - 3.108714121580124
  - 3.124761658906937
  - 3.2405384480953217
  - 3.201956027746201
  - 3.1225251972675325
  - 4.236715185642242
  - 3.306797289848328
  - 3.2977765142917637
  - 3.5969887137413026
  - 3.3299001991748813
  - 3.7695949494838716
  - 3.4095818519592287
  - 3.3263761043548588
  - 3.288982087373734
  - 3.2935108780860904
  - 3.2010145008563997
  - 3.713726216554642
  - 3.5972429513931274
  - 3.4531135976314546
  - 3.4896328628063205
  - 3.5272952854633335
  - 3.7462501466274265
  - 3.1819536447525025
  - 3.31119424700737
  - 3.2392924755811694
  - 3.1655738890171055
  - 4.11420168876648
  - 4.320569932460785
  - 5.152740716934204
  - 3.5196585178375246
  - 3.429905489087105
  - 3.557797133922577
  - 3.617276364564896
  - 3.1775459289550785
  - 3.5656157910823825
  - 3.5152068316936496
  - 3.1367610096931458
  - 3.076428654789925
  - 3.1303038001060486
  - 3.3590693414211277
  - 3.3269072949886325
  - 3.2149801880121234
  - 3.1743351817131042
  - 3.128508612513542
  - 3.203546088933945
  - 3.159656471014023
  - 3.106920725107193
  - 3.218413302302361
  - 3.2649485707283024
  - 3.214445322751999
  - 3.155295115709305
  - 3.051912593841553
  - 3.1830897212028506
  - 3.0983174800872804
  - 3.2823749840259553
  - 3.1320558667182925
  - 3.106446099281311
  - 3.096186101436615
  - 3.121976351737976
  - 3.2096133530139923
  - 3.324408060312271
  - 3.552430593967438
  - 3.234297662973404
  - 3.2068139106035236
  - 3.3036718726158143
  - 3.1817282497882844
  - 3.1742913842201235
  - 3.238739049434662
  - 3.2186702847480775
  - 3.285491442680359
  - 3.2510853677988054
  - 3.27308184504509
  - 3.136977878212929
  - 3.1322815060615543
  - 3.1124301433563235
  - 3.175304701924324
  - 3.107107734680176
  - 3.222915542125702
  - 3.362343978881836
  - 3.270862287282944
  - 3.480677950382233
  - 3.2523106366395953
  - 3.404416471719742
  - 3.2092250466346743
  validation_losses:
  - 0.41826486587524414
  - 0.4105331599712372
  - 0.43348148465156555
  - 0.43711450695991516
  - 0.4046212434768677
  - 0.41381317377090454
  - 0.45402440428733826
  - 0.408443808555603
  - 0.41736361384391785
  - 0.39755454659461975
  - 0.4240110218524933
  - 0.445227712392807
  - 0.4295337200164795
  - 0.3992639183998108
  - 0.48336872458457947
  - 3.992964029312134
  - 0.424665629863739
  - 0.4463628828525543
  - 3.762272357940674
  - 0.43025830388069153
  - 0.41875070333480835
  - 0.4252491891384125
  - 0.49126896262168884
  - 0.4059390127658844
  - 0.6550648808479309
  - 0.41117197275161743
  - 0.4337337017059326
  - 0.4403860867023468
  - 0.4070284068584442
  - 0.43594175577163696
  - 0.42136773467063904
  - 0.4412989914417267
  - 0.40724635124206543
  - 0.40867388248443604
  - 0.4114200174808502
  - 0.42845913767814636
  - 1.3418442010879517
  - 0.5394015312194824
  - 0.4161940813064575
  - 0.4288579225540161
  - 0.4392591416835785
  - 0.4525049328804016
  - 0.4329635798931122
  - 0.4044877290725708
  - 0.44236132502555847
  - 0.4642256200313568
  - 0.39720064401626587
  - 0.40617066621780396
  - 0.5028526186943054
  - 0.5120474696159363
  - 0.42279258370399475
  - 0.4360770583152771
  - 0.4181359112262726
  - 0.41534072160720825
  - 0.43932515382766724
  - 0.41324684023857117
  - 0.417205810546875
  - 0.4092935621738434
  - 0.4292393922805786
  - 0.42454683780670166
  - 0.4042198359966278
  - 0.40752920508384705
  - 0.39852043986320496
  - 0.4627799391746521
  - 0.42958033084869385
  - 0.41173824667930603
  - 0.4100414514541626
  - 0.44119980931282043
  - 0.4879879057407379
  - 0.4626324772834778
  - 0.4185159504413605
  - 0.4867105185985565
  - 0.4109499156475067
  - 0.44656965136528015
  - 0.41220173239707947
  - 0.41902396082878113
  - 0.4333702325820923
  - 0.45391976833343506
  - 0.40982866287231445
  - 0.4130609333515167
  - 0.4282219409942627
  - 0.44572940468788147
  - 0.4444444179534912
  - 0.43242204189300537
  - 0.41598570346832275
  - 0.4035834074020386
  - 0.4081901013851166
  - 1.8815308809280396
  - 0.6159800887107849
  - 0.4260648787021637
  - 0.42504850029945374
  - 0.41536054015159607
  - 0.425187349319458
  - 0.4221291244029999
loss_records_fold2:
  train_losses:
  - 3.2366748720407488
  - 3.282957023382187
  - 3.4122340381145477
  - 3.3735440373420715
  - 3.3764164805412293
  - 3.1120502948760986
  - 3.1571474462747577
  - 3.1171615183353425
  - 3.1411579787731174
  - 3.2123592376708987
  - 3.3234348058700562
  - 3.2319685101509097
  - 3.2106351763010026
  - 6.406809875369072
  - 11.709086233377457
  - 9.908120912313462
  - 5.369765008985997
  - 5.005973374843598
  - 5.032282119989396
  - 3.779748243093491
  - 4.177216446399689
  - 3.7956543743610385
  - 3.5481612861156466
  - 3.6190767824649814
  - 3.482370436191559
  - 4.106589937210083
  - 4.349195224046707
  - 4.886750811338425
  - 3.5100144982337955
  - 3.980323684215546
  - 3.3803286731243136
  - 3.98473174571991
  validation_losses:
  - 0.39933231472969055
  - 0.4115142524242401
  - 0.4097665548324585
  - 0.452110230922699
  - 0.41224393248558044
  - 0.39607635140419006
  - 0.4057399034500122
  - 0.3925565183162689
  - 0.4175793528556824
  - 0.4033221900463104
  - 0.42096269130706787
  - 0.42032384872436523
  - 0.3994603455066681
  - 0.46839067339897156
  - 1.7728049755096436
  - 0.39068862795829773
  - 0.37763625383377075
  - 0.3859429955482483
  - 0.5028742551803589
  - 0.3852841854095459
  - 0.40432408452033997
  - 0.386325865983963
  - 0.4087269902229309
  - 0.4656479060649872
  - 0.40822094678878784
  - 0.43178021907806396
  - 0.39604923129081726
  - 0.39767777919769287
  - 0.38779258728027344
  - 0.3901139199733734
  - 0.39813733100891113
  - 0.39428114891052246
loss_records_fold3:
  train_losses:
  - 3.6308506786823274
  - 3.2437004446983337
  - 3.4851845026016237
  - 3.394789230823517
  - 3.2568027436733247
  - 3.35310515165329
  - 3.2717241525650027
  - 3.3031100153923036
  - 3.368809998035431
  - 3.4619528084993365
  - 3.225643414258957
  - 3.39468292593956
  - 3.2033678472042086
  - 3.329856896400452
  - 3.5662600100040436
  - 3.207620269060135
  validation_losses:
  - 0.41963452100753784
  - 0.45442673563957214
  - 0.40518996119499207
  - 0.40538689494132996
  - 0.4261525869369507
  - 0.40188920497894287
  - 0.41312333941459656
  - 0.4071635603904724
  - 0.44866621494293213
  - 0.5035234093666077
  - 0.41253232955932617
  - 0.40135568380355835
  - 0.4070510268211365
  - 0.4068829119205475
  - 0.40535956621170044
  - 0.4093022644519806
loss_records_fold4:
  train_losses:
  - 3.43768767118454
  - 3.2615504264831543
  - 3.441001731157303
  - 3.2527244210243227
  - 3.168734320998192
  - 3.2497281789779664
  - 3.167224878072739
  - 3.503426042199135
  - 3.4869369864463806
  - 3.346433162689209
  - 3.3706019937992098
  - 3.2963265001773836
  - 3.2717714190483096
  - 3.39709067940712
  - 3.5226505219936373
  - 3.1478318095207216
  - 3.3207994610071183
  - 3.225403195619583
  - 3.0799660682678223
  - 3.2217484295368197
  - 3.1777371525764466
  - 3.1393599212169647
  - 3.221100053191185
  - 3.1627946436405185
  - 3.223922109603882
  - 3.2675940096378326
  - 3.2134070992469788
  - 3.2161534279584885
  - 3.0809513479471207
  - 3.169070038199425
  - 3.334168010950089
  - 3.2408212363719944
  - 3.238680762052536
  - 3.3097851872444153
  - 3.345834964513779
  - 3.151153725385666
  - 3.349149310588837
  - 3.242758423089981
  - 3.1586091399192813
  - 3.129301315546036
  - 3.2327736914157867
  - 3.220335304737091
  - 3.168314462900162
  - 3.2162651598453524
  - 3.2459048449993135
  - 3.0859778046607973
  - 3.295911228656769
  - 3.1476512193679813
  - 3.2222588419914246
  - 3.2030242800712587
  - 3.2317403852939606
  - 3.3366619110107423
  - 3.124390587210655
  - 3.364383578300476
  - 3.0809737473726275
  - 3.117669838666916
  - 3.2094497561454776
  validation_losses:
  - 0.43328022956848145
  - 0.42941802740097046
  - 0.40769144892692566
  - 0.40068092942237854
  - 0.41974762082099915
  - 0.408620148897171
  - 0.39603057503700256
  - 0.41929376125335693
  - 0.4128125309944153
  - 0.4147219657897949
  - 0.3916875720024109
  - 0.40429607033729553
  - 0.41066738963127136
  - 0.38654085993766785
  - 0.4065762758255005
  - 0.40271174907684326
  - 0.39891719818115234
  - 0.3871556520462036
  - 0.39128369092941284
  - 0.40883320569992065
  - 0.40981554985046387
  - 0.4040943384170532
  - 0.42070454359054565
  - 0.3974493443965912
  - 0.3919731676578522
  - 0.3972342014312744
  - 0.407858669757843
  - 0.39068859815597534
  - 0.38463324308395386
  - 0.4389314353466034
  - 0.41983070969581604
  - 0.406297504901886
  - 0.38857483863830566
  - 0.3946663439273834
  - 0.39851653575897217
  - 0.4211128056049347
  - 0.40949854254722595
  - 0.4178330600261688
  - 0.39258286356925964
  - 0.38479429483413696
  - 0.40301424264907837
  - 0.4029030203819275
  - 0.4428037106990814
  - 0.5280328989028931
  - 0.39424630999565125
  - 0.3952358663082123
  - 0.4131942093372345
  - 0.40358197689056396
  - 0.3955903649330139
  - 0.4096794128417969
  - 0.4229048788547516
  - 0.4217957556247711
  - 0.4002714455127716
  - 0.3988751769065857
  - 0.39227408170700073
  - 0.3871418833732605
  - 0.38447991013526917
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 94 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 32 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 57 epochs
training_metrics:
  fold_eval_accs: '[0.8370497427101201, 0.855917667238422, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.02061855670103093, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8541540674199689
  mean_f1_accuracy: 0.0041237113402061865
  total_train_time: '0:29:15.351505'
