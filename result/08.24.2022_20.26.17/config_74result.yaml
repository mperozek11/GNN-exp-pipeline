config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:10:55.804692'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_74fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_74fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_74fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_74fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_74fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_74fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_74fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_74fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_74fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_74fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 20.561820173263552
  - 6.164786517620087
  - 3.609646809101105
  - 4.343300670385361
  - 2.848797851800919
  - 4.3574002444744115
  - 4.964634144306183
  - 2.5888841018080715
  - 4.885731554031373
  - 2.9878633558750156
  - 2.576897704601288
  - 2.924914014339447
  - 2.886225527524948
  - 1.993449068069458
  - 2.4281249880790714
  - 4.692123329639435
  - 12.153129249811172
  - 6.244889706373215
  - 2.0625313073396683
  - 1.8192715466022493
  - 2.315739357471466
  - 1.8804325819015504
  - 4.1971518993377686
  - 3.142027491331101
  - 1.706166172027588
  - 1.7850439310073853
  - 1.861683088541031
  - 1.7274984180927277
  - 2.6465973973274233
  - 1.5466236472129822
  - 1.6848566859960556
  - 2.527229601144791
  - 1.812600165605545
  - 2.7267969787120823
  - 2.239811560511589
  - 1.817072355747223
  - 1.8236663579940797
  - 1.6204229772090912
  - 1.9594248682260513
  - 2.082727515697479
  - 3.1202858209609987
  - 1.5479682683944702
  - 1.5848443537950516
  - 6.377352166175843
  - 1.808908009529114
  - 1.544704979658127
  - 1.942729723453522
  - 5.773760369420052
  - 1.878578132390976
  - 1.6213604748249055
  - 1.7848459601402284
  - 2.7890834331512453
  - 2.156656616926193
  - 1.6652717232704164
  - 1.5167773187160494
  - 1.7575917780399324
  - 2.093492144346237
  - 1.8839271366596222
  validation_losses:
  - 7.803752422332764
  - 0.7985368967056274
  - 0.5198070406913757
  - 0.6852025389671326
  - 2.742727041244507
  - 0.9963946342468262
  - 0.6071335673332214
  - 1.1312371492385864
  - 0.9618393182754517
  - 0.7366306185722351
  - 0.4726397693157196
  - 1.198938250541687
  - 0.5839770436286926
  - 0.43925395607948303
  - 0.4287419617176056
  - 0.38021233677864075
  - 0.6814728379249573
  - 0.5085208415985107
  - 0.4094334840774536
  - 0.4143093228340149
  - 0.3847218155860901
  - 0.389269083738327
  - 0.5477766990661621
  - 0.4343407452106476
  - 0.388855516910553
  - 0.3801359534263611
  - 0.3818606436252594
  - 0.38775312900543213
  - 0.4004940688610077
  - 0.3845197856426239
  - 0.498203307390213
  - 0.3857450783252716
  - 0.3806411325931549
  - 0.5126436352729797
  - 0.4142323136329651
  - 0.3942277431488037
  - 0.41607964038848877
  - 0.37862950563430786
  - 0.3970138728618622
  - 0.6978889107704163
  - 0.38179659843444824
  - 0.3836226761341095
  - 0.40881580114364624
  - 0.3813590109348297
  - 0.4028036594390869
  - 0.40050989389419556
  - 0.40457990765571594
  - 0.4338805377483368
  - 0.4721525311470032
  - 0.45286643505096436
  - 0.393736332654953
  - 0.40883001685142517
  - 0.3860916197299957
  - 0.39169204235076904
  - 0.38927778601646423
  - 0.39027807116508484
  - 0.3835522532463074
  - 0.3846278488636017
loss_records_fold1:
  train_losses:
  - 3.0778772830963135
  - 2.095098489522934
  - 1.791266268491745
  - 1.5817070841789247
  - 3.2827789127826694
  - 2.3806275308132174
  - 1.6621280491352082
  - 1.6131263256072998
  - 1.5881063461303713
  - 1.6741947293281556
  - 1.654011631011963
  - 1.6877847015857697
  - 1.5120164513587953
  - 1.517674618959427
  - 1.4946476340293886
  - 1.4962436497211458
  validation_losses:
  - 0.4044349193572998
  - 0.4156917333602905
  - 0.43770748376846313
  - 0.4148232042789459
  - 0.4020906090736389
  - 0.4235500693321228
  - 0.40104255080223083
  - 0.40179911255836487
  - 0.4007589817047119
  - 0.42093783617019653
  - 0.3997463583946228
  - 0.39901405572891235
  - 0.3971720039844513
  - 0.3981879651546478
  - 0.39946499466896057
  - 0.4020889401435852
loss_records_fold2:
  train_losses:
  - 1.5065688967704773
  - 1.5612183749675752
  - 1.4788069665431978
  - 1.527135717868805
  - 1.835965871810913
  - 1.620383220911026
  - 1.5149030268192292
  - 1.504675340652466
  - 1.5298050701618195
  - 1.5843872785568238
  - 2.0984467029571534
  - 1.5270224034786226
  - 1.568346044421196
  - 1.5847664535045625
  - 1.6133673131465913
  - 1.5777791500091554
  - 1.4871456503868103
  - 1.5362797379493713
  - 2.0757576763629912
  validation_losses:
  - 0.37836700677871704
  - 0.40067535638809204
  - 0.38133591413497925
  - 0.41481560468673706
  - 0.6248154044151306
  - 0.38197195529937744
  - 0.4200935959815979
  - 0.3841913044452667
  - 0.38174617290496826
  - 0.40213921666145325
  - 0.3902908265590668
  - 0.38410624861717224
  - 0.4103666841983795
  - 0.38619548082351685
  - 0.39075666666030884
  - 0.38653966784477234
  - 0.3904261887073517
  - 0.39172831177711487
  - 0.38675007224082947
loss_records_fold3:
  train_losses:
  - 2.4824857115745544
  - 1.5740395724773408
  - 1.8412677526474
  - 2.585989844799042
  - 1.505206423997879
  - 2.1650322288274766
  - 1.566722643375397
  - 1.9264605164527895
  - 1.5701982200145723
  - 2.130111926794052
  - 2.288322687149048
  - 2.3479587972164153
  - 1.6739534318447113
  - 1.5210474610328675
  - 2.2968021869659423
  - 2.4428845942020416
  - 2.073152095079422
  - 3.142074537277222
  - 2.3189025580883027
  - 1.9705845892429352
  - 1.530937984585762
  - 1.6073986530303956
  - 1.5406706869602205
  - 1.5950673818588257
  - 1.4855389207601548
  - 1.5492897272109987
  - 1.4765772700309754
  - 2.14374213218689
  - 1.482410478591919
  - 1.4823771476745606
  - 1.4666111052036286
  - 1.5411010265350342
  - 1.5640435099601746
  - 1.5110628426074983
  - 1.573059105873108
  - 1.4824881792068483
  - 1.4942795217037201
  - 1.8456347167491913
  - 1.6988452553749085
  - 1.5384633421897889
  - 1.5553203523159027
  - 1.5560367107391357
  - 1.4737260848283769
  - 1.6546653628349306
  - 1.856614673137665
  - 1.5636976510286331
  - 2.29438379406929
  - 1.8185509741306305
  - 1.8716143250465394
  - 1.6336673438549043
  - 1.626204639673233
  - 1.882507359981537
  - 1.6135628283023835
  validation_losses:
  - 0.4165072739124298
  - 0.391853004693985
  - 0.3973677158355713
  - 0.39199700951576233
  - 0.39559414982795715
  - 0.4096389412879944
  - 0.39816561341285706
  - 0.4029426574707031
  - 0.3975520431995392
  - 0.6737149357795715
  - 0.3982885777950287
  - 0.3912011384963989
  - 0.4014517366886139
  - 0.39523279666900635
  - 0.393917441368103
  - 0.5200637578964233
  - 0.4406084716320038
  - 0.5151842832565308
  - 0.4286888539791107
  - 0.4372706711292267
  - 0.40261024236679077
  - 0.4129577875137329
  - 0.39235782623291016
  - 0.4272323548793793
  - 0.4084399342536926
  - 0.3921397924423218
  - 0.38878658413887024
  - 0.41770991683006287
  - 0.39256882667541504
  - 0.394449919462204
  - 0.39782068133354187
  - 0.3918861150741577
  - 0.4136887192726135
  - 0.4671015441417694
  - 0.40032002329826355
  - 0.3911263346672058
  - 0.40742769837379456
  - 0.4248862564563751
  - 0.41042888164520264
  - 0.3878161907196045
  - 0.38572919368743896
  - 0.39122557640075684
  - 0.4033374786376953
  - 0.3945609927177429
  - 0.3914375603199005
  - 0.3898121416568756
  - 0.4329538345336914
  - 0.41241639852523804
  - 0.3930221199989319
  - 0.38882237672805786
  - 0.38957950472831726
  - 0.39050188660621643
  - 0.39787355065345764
loss_records_fold4:
  train_losses:
  - 1.5464913666248323
  - 1.5203964412212372
  - 1.5175580978393555
  - 1.4958082854747774
  - 1.5191335797309877
  - 1.4829940974712372
  - 1.490189814567566
  - 1.4880240797996522
  - 1.4828180134296418
  - 1.5532285928726197
  - 1.4494567543268204
  - 1.4793662667274476
  - 1.547364342212677
  - 1.580616080760956
  - 1.5993980526924134
  - 1.5661401748657227
  - 1.5654678463935854
  - 1.649305146932602
  - 1.5109946310520173
  - 1.4827612042427063
  - 1.5028921008110048
  - 1.5048085093498231
  - 1.5173221349716188
  - 1.6025084495544435
  - 1.5357441246509553
  - 1.51486451625824
  - 1.4866247832775117
  - 1.4736866712570191
  - 1.4121679991483689
  validation_losses:
  - 0.3927105963230133
  - 0.4470086991786957
  - 0.39610522985458374
  - 0.40230444073677063
  - 0.38565582036972046
  - 0.39312708377838135
  - 0.38952064514160156
  - 0.3904739320278168
  - 0.400783896446228
  - 0.3936249017715454
  - 0.3858831524848938
  - 0.3945125937461853
  - 0.39947184920310974
  - 0.4268451929092407
  - 0.3990202248096466
  - 0.38793808221817017
  - 0.3980405032634735
  - 0.39302411675453186
  - 0.4061076045036316
  - 0.3893405795097351
  - 0.3970238268375397
  - 0.39332249760627747
  - 0.4048960506916046
  - 0.4127800762653351
  - 0.3877359628677368
  - 0.3861240744590759
  - 0.39018958806991577
  - 0.39189785718917847
  - 0.3944752812385559
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 58 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 53 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:15:07.516338'
