config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 19:00:41.256386'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_122fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_122fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_122fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_122fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_122fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_122fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_122fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_122fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_122fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_122fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 21.570991039276123
  - 18.41455669403076
  - 9.01241638660431
  - 5.303364354372025
  - 6.684543073177338
  - 5.4247002661228185
  - 5.8265229105949405
  - 4.334396409988403
  - 3.0713010847568514
  - 1.9474145531654359
  - 2.665012151002884
  - 2.275885319709778
  - 1.766324681043625
  - 7.89373579621315
  - 6.252168875932694
  - 3.3046213388442993
  - 3.5088841259479526
  - 1.929858535528183
  - 2.7265786051750185
  - 3.9191323995590213
  - 3.1600337624549866
  - 4.301078605651855
  - 2.0398642599582675
  - 2.5567996144294742
  - 4.491079246997834
  - 2.0513196885585785
  - 4.557229292392731
  - 3.655761730670929
  - 3.0907210588455203
  - 2.3470333695411685
  - 1.7406679749488831
  - 2.109718042612076
  - 2.095195358991623
  - 1.902578490972519
  - 2.2576271831989287
  - 6.444084882736206
  - 2.9498647272586824
  - 12.511970114707948
  - 2.5665492922067643
  - 2.420320153236389
  - 1.8256637632846833
  - 4.975657278299332
  - 3.9115046560764313
  - 6.975096362829209
  - 2.2151133477687837
  - 2.5056937754154207
  - 2.5595823228359222
  - 2.7441940248012546
  - 2.457974076271057
  - 2.2817509710788726
  - 3.492602849006653
  - 3.993794858455658
  - 3.2668433487415314
  - 3.585775935649872
  - 2.511115223169327
  - 3.2012779474258424
  - 2.0338228523731234
  - 4.875136154890061
  - 2.828516894578934
  - 5.686154085397721
  - 3.2758047342300416
  - 2.2432843446731567
  - 2.089674597978592
  - 1.9971886575222015
  - 1.8327394366264345
  - 1.543511861562729
  - 1.6633512377738953
  - 1.5485523581504823
  - 2.23374525308609
  - 1.617023915052414
  - 1.912773895263672
  - 1.7509121835231782
  - 1.5868391811847689
  - 1.6124952852725984
  - 1.5899369299411774
  - 1.63779074549675
  - 1.6519420444965363
  - 1.7542482793331147
  - 1.632923984527588
  - 1.8994901537895204
  - 1.697750747203827
  - 1.5503682255744935
  - 1.613742882013321
  - 1.696789014339447
  - 1.589311695098877
  - 1.6397772312164307
  - 1.4818078547716143
  - 1.484460324048996
  - 1.9050752520561218
  - 1.6647256433963777
  validation_losses:
  - 7.28335428237915
  - 2.82647967338562
  - 1.1138063669204712
  - 0.6316323280334473
  - 0.7911819219589233
  - 2.860602855682373
  - 1.0696570873260498
  - 0.5856641530990601
  - 0.5132631659507751
  - 0.46981698274612427
  - 1.7210410833358765
  - 0.492099791765213
  - 0.4422511160373688
  - 3.3054816722869873
  - 0.6730983257293701
  - 0.6394398212432861
  - 1.2431972026824951
  - 0.46509307622909546
  - 0.41743144392967224
  - 0.4163631796836853
  - 0.6120559573173523
  - 0.48320576548576355
  - 0.6622297763824463
  - 0.453406423330307
  - 0.48061516880989075
  - 0.7118820548057556
  - 0.4750864803791046
  - 0.5271037817001343
  - 0.5241454839706421
  - 0.4960310459136963
  - 0.3985174298286438
  - 0.38302016258239746
  - 0.40671640634536743
  - 0.4016142785549164
  - 0.41668060421943665
  - 0.4611266851425171
  - 0.4854203462600708
  - 0.8217811584472656
  - 0.4478674530982971
  - 0.4311492145061493
  - 0.3909805417060852
  - 0.8993862271308899
  - 0.44569510221481323
  - 0.43749740719795227
  - 0.5707679986953735
  - 0.4491625428199768
  - 0.43180450797080994
  - 0.42754679918289185
  - 0.40547922253608704
  - 0.3915746212005615
  - 0.4538426697254181
  - 0.4209316074848175
  - 0.39563485980033875
  - 0.41393786668777466
  - 0.4432069957256317
  - 0.4609004557132721
  - 0.45578595995903015
  - 0.38757774233818054
  - 0.4031558632850647
  - 0.47884950041770935
  - 0.38673311471939087
  - 0.6257525682449341
  - 0.42870479822158813
  - 0.49413928389549255
  - 0.389555960893631
  - 0.3908069431781769
  - 0.40167152881622314
  - 0.3883337676525116
  - 0.4050605595111847
  - 0.3876079320907593
  - 0.4003259837627411
  - 0.43849778175354004
  - 0.3924272358417511
  - 0.38880154490470886
  - 0.40448328852653503
  - 0.4293220043182373
  - 0.39190390706062317
  - 0.3878312110900879
  - 0.38780754804611206
  - 0.39394494891166687
  - 0.40496698021888733
  - 0.40600115060806274
  - 0.3902166187763214
  - 0.41112780570983887
  - 0.39222049713134766
  - 0.3912525773048401
  - 0.3849947452545166
  - 0.385002076625824
  - 0.394493043422699
  - 0.39966097474098206
loss_records_fold1:
  train_losses:
  - 1.836549073457718
  - 2.5079013943672184
  - 2.4426078855991364
  - 2.3528773546218873
  - 1.8064915239810944
  - 1.5788693249225618
  - 1.7378082633018495
  - 2.129524314403534
  - 1.6049163758754732
  - 1.8850174605846406
  - 1.5624622702598572
  - 1.594211345911026
  - 2.0889604508876802
  - 1.9248903512954714
  - 1.5575745284557343
  - 2.0578557193279265
  - 1.691052430868149
  - 1.5316887080669404
  - 1.4571310460567475
  - 1.4433495938777925
  - 1.4550823092460634
  - 2.411897897720337
  - 1.597023242712021
  - 1.702941209077835
  - 1.566586822271347
  - 3.1516417026519776
  - 1.554195147752762
  - 1.6329533338546753
  - 1.592449590563774
  - 1.5194987773895265
  - 1.8349314630031586
  - 1.5848203599452972
  - 1.5536998748779298
  - 1.8985488712787628
  - 1.5127863347530366
  - 1.5043029367923737
  - 1.4754329919815063
  - 1.5711508065462114
  - 1.5152495175600054
  - 3.123319220542908
  - 2.3153190433979036
  - 1.71788047850132
  - 1.7019523084163666
  - 3.3232530415058137
  - 1.6628416895866396
  - 1.7071508407592775
  - 1.4448160380125046
  - 1.462221223115921
  - 2.132727509737015
  - 1.5294985949993134
  - 1.7257524013519288
  - 1.4885209679603577
  - 2.141780787706375
  - 1.476543003320694
  - 1.5109563022851944
  - 1.6327397644519808
  - 1.7850840330123903
  - 1.5872475564479829
  - 1.4781172692775728
  - 1.6983230471611024
  - 1.5169917762279512
  - 1.6665301203727723
  - 1.5975541710853578
  - 1.4857590675354004
  - 1.6595194578170778
  - 1.6985509753227235
  - 1.6692500531673433
  - 1.5007704079151154
  - 1.46341210603714
  - 1.7175170719623567
  - 1.5918344557285309
  - 1.4654849529266358
  - 1.4382648527622224
  - 1.4708327114582063
  - 1.456691163778305
  - 1.4661838233470919
  - 1.504579645395279
  - 1.4889297842979432
  - 1.4529471397399902
  - 1.464486837387085
  - 1.4705795884132387
  - 1.4855393528938294
  - 1.5717776775360108
  - 1.4830213785171509
  - 1.4540699541568758
  - 1.478919392824173
  - 1.432271045446396
  - 1.4424047112464906
  - 1.5638606429100037
  - 2.288236713409424
  - 1.6555991649627686
  - 2.1887280106544496
  - 1.5393758594989777
  - 1.5108738601207734
  - 1.851496982574463
  validation_losses:
  - 0.47652682662010193
  - 0.4155658781528473
  - 0.40961888432502747
  - 0.6402913928031921
  - 0.40632104873657227
  - 0.41846975684165955
  - 0.42261189222335815
  - 0.4067525863647461
  - 0.4094724655151367
  - 0.41660231351852417
  - 0.40694159269332886
  - 0.42013248801231384
  - 0.4093567430973053
  - 0.4271000325679779
  - 0.39997580647468567
  - 0.45063304901123047
  - 0.42694175243377686
  - 0.41157782077789307
  - 0.40327945351600647
  - 0.39631444215774536
  - 0.41219648718833923
  - 0.47159457206726074
  - 0.41076159477233887
  - 0.39609336853027344
  - 0.4063757658004761
  - 0.405886709690094
  - 0.4105767011642456
  - 0.39200589060783386
  - 0.39515766501426697
  - 0.406840980052948
  - 0.39533671736717224
  - 0.39848411083221436
  - 0.39798399806022644
  - 0.4339984059333801
  - 0.48811209201812744
  - 0.44570645689964294
  - 0.47099968791007996
  - 0.39754265546798706
  - 0.4314686954021454
  - 0.40181612968444824
  - 0.39870017766952515
  - 0.4234040081501007
  - 0.3947661221027374
  - 0.6430478692054749
  - 0.5685619711875916
  - 0.4379032552242279
  - 0.49281811714172363
  - 0.421858549118042
  - 0.39770159125328064
  - 0.45480379462242126
  - 0.3892495334148407
  - 0.401719331741333
  - 0.3965628743171692
  - 0.3986343741416931
  - 0.3992673456668854
  - 0.4193958640098572
  - 0.4191509783267975
  - 0.4693223536014557
  - 0.3949068486690521
  - 0.39393168687820435
  - 0.5502768754959106
  - 0.5235220789909363
  - 0.39551225304603577
  - 0.5317238569259644
  - 0.432995080947876
  - 0.46851739287376404
  - 0.3954455852508545
  - 0.44908684492111206
  - 0.40267470479011536
  - 0.3941645920276642
  - 0.39560002088546753
  - 0.39919543266296387
  - 0.4027251899242401
  - 0.503730297088623
  - 0.3997500538825989
  - 0.4385467767715454
  - 0.40012475848197937
  - 0.4060232937335968
  - 0.44346752762794495
  - 0.3993421196937561
  - 0.3982059359550476
  - 0.5188789963722229
  - 0.39511343836784363
  - 0.45434248447418213
  - 0.39528143405914307
  - 0.40293312072753906
  - 0.395575612783432
  - 0.411945641040802
  - 0.9349187612533569
  - 0.45685330033302307
  - 0.43788909912109375
  - 0.40456339716911316
  - 0.410564124584198
  - 0.40372729301452637
  - 0.4083317816257477
loss_records_fold2:
  train_losses:
  - 1.5531395077705383
  - 1.5742275714874268
  - 1.5715702414512636
  - 1.8231749176979066
  - 1.5141266584396362
  - 1.643901550769806
  - 1.601654750108719
  - 1.552958679199219
  - 1.5355775356292725
  - 1.502899706363678
  - 1.5165593326091766
  validation_losses:
  - 0.39605388045310974
  - 0.3799802362918854
  - 0.41919782757759094
  - 1.0870226621627808
  - 0.38047149777412415
  - 0.38707664608955383
  - 0.38548707962036133
  - 0.38855618238449097
  - 0.38460251688957214
  - 0.38500353693962097
  - 0.38131603598594666
loss_records_fold3:
  train_losses:
  - 1.494645744562149
  - 1.5910235285758974
  - 1.604092538356781
  - 1.6486691474914552
  - 1.5688561260700227
  - 1.5524663507938385
  - 2.031069350242615
  - 2.2778726875782014
  - 1.649110895395279
  - 1.6947977006435395
  - 1.709628736972809
  - 1.7802152693271638
  - 1.9563667476177216
  - 1.5223233580589295
  - 1.5244145095348358
  - 1.5453894019126893
  - 1.462567886710167
  - 1.5285592436790467
  - 1.4838445305824282
  validation_losses:
  - 0.39485964179039
  - 0.4094793200492859
  - 0.41002875566482544
  - 0.39634960889816284
  - 0.3921293616294861
  - 1.2241363525390625
  - 0.4727828800678253
  - 0.6831012964248657
  - 0.4632565379142761
  - 0.39805638790130615
  - 2.45428466796875
  - 0.39303523302078247
  - 0.41149067878723145
  - 0.410550057888031
  - 0.39482998847961426
  - 0.40131667256355286
  - 0.40161997079849243
  - 0.3970339000225067
  - 0.3983737528324127
loss_records_fold4:
  train_losses:
  - 1.5144091963768007
  - 1.623940807580948
  - 1.5624893963336945
  - 1.5783431708812714
  - 1.5861434102058412
  - 1.6302893221378327
  - 1.4953387260437012
  - 1.4885056495666504
  - 1.508626574277878
  - 1.4924869775772096
  - 1.4827267646789553
  validation_losses:
  - 0.3908100128173828
  - 0.3901536464691162
  - 0.41440659761428833
  - 0.3971041142940521
  - 0.3931691348552704
  - 0.3921501040458679
  - 0.39334002137184143
  - 0.3940047025680542
  - 0.39108744263648987
  - 0.3929655849933624
  - 0.4000154435634613
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 90 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 95 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:20:07.470191'
