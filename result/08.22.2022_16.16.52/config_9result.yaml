config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.184678'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_9fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_9fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_9fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_9fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_9fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_9fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_9fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_9fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_9fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_9fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 36.07991876006127
  - 15.031292229890823
  - 7.830662721395493
  - 7.290054732561112
  - 6.032798165082932
  - 4.439882117509842
  - 4.086584880948067
  - 5.064447522163391
  - 4.816504254937172
  - 4.934066465497017
  - 10.16604271531105
  - 5.261840271949769
  - 3.975960224866867
  - 4.447917884588242
  - 5.767154330015183
  - 4.696040254831314
  - 6.434603446722031
  - 5.974257528781891
  - 4.874153330922127
  - 5.143438410758972
  - 4.80691337287426
  - 3.6152298629283908
  - 3.2766318857669834
  - 4.391659647226334
  - 3.685920774936676
  - 4.156224316358567
  - 8.023248487710953
  - 4.102774846553802
  - 3.079644870758057
  - 3.0519409745931627
  - 2.9488842725753788
  - 4.198702460527421
  - 3.17551703453064
  - 3.6921415209770205
  - 4.479166263341904
  - 3.0165348976850512
  - 7.225357386469842
  - 6.1239688992500305
  - 3.79668290913105
  - 3.0124696224927905
  - 3.2142143785953525
  - 3.408060795068741
  - 3.546715235710144
  - 3.775001493096352
  validation_losses:
  - 3.2488462924957275
  - 0.8725874423980713
  - 0.570630669593811
  - 1.0592782497406006
  - 0.9830397963523865
  - 0.391622930765152
  - 0.3765559196472168
  - 0.699405312538147
  - 0.38801485300064087
  - 0.6302109360694885
  - 0.46534407138824463
  - 0.39818447828292847
  - 0.49049919843673706
  - 0.37617403268814087
  - 0.37307947874069214
  - 0.38045766949653625
  - 1.2364604473114014
  - 0.41271209716796875
  - 0.39223501086235046
  - 0.44863009452819824
  - 0.40540775656700134
  - 0.37622714042663574
  - 0.6028863191604614
  - 0.4170604348182678
  - 1.1206659078598022
  - 0.49906209111213684
  - 0.3851558566093445
  - 0.39247649908065796
  - 0.43815895915031433
  - 0.4237738251686096
  - 0.39158129692077637
  - 0.37542563676834106
  - 0.4708998501300812
  - 0.37596091628074646
  - 0.40649521350860596
  - 0.3800562024116516
  - 0.4300565719604492
  - 0.44302594661712646
  - 0.4194163382053375
  - 0.4186016619205475
  - 0.38981756567955017
  - 0.3929886221885681
  - 0.3810581564903259
  - 0.38336437940597534
loss_records_fold1:
  train_losses:
  - 3.2565904021263123
  - 3.632030332088471
  - 3.1913854956626895
  - 3.1598302125930786
  - 3.397579842805863
  - 3.1810485422611237
  - 3.2426400750875475
  - 3.359227675199509
  - 2.8301994621753694
  - 2.8306658148765567
  - 2.895775702595711
  - 2.84824720621109
  - 3.6002850949764253
  - 3.400795513391495
  - 3.0769352048635485
  - 3.03023362159729
  - 3.3712179720401765
  - 3.101093846559525
  - 2.9456839591264727
  - 3.1157111287117005
  - 2.972767707705498
  - 2.862613239884377
  - 2.893985837697983
  - 2.988466051220894
  - 3.300248229503632
  - 3.05123370885849
  - 2.7760412812232973
  - 2.955903345346451
  - 2.750887590646744
  - 2.914171314239502
  - 2.924759638309479
  - 2.861811321973801
  - 3.2586704194545746
  - 3.14930916428566
  - 2.8716508507728578
  - 2.87241270840168
  - 3.2242192119359974
  - 3.4378721654415134
  - 3.0877644747495654
  - 2.8168666094541552
  - 2.848535877466202
  - 2.826629856228829
  - 2.880149939656258
  - 2.8669710367918015
  - 2.999195116758347
  - 2.8470391899347307
  - 2.881073307991028
  - 2.8809204876422885
  - 2.928183448314667
  - 2.8819689214229585
  - 2.88553763628006
  - 2.826064565777779
  - 2.8724440872669224
  - 3.29361732006073
  - 2.9344196677207948
  - 2.8218777716159824
  - 2.817828217148781
  - 2.84492364525795
  - 2.9495781004428867
  - 2.7988247841596605
  - 2.8539978176355363
  - 2.8406690895557407
  - 2.8234358847141268
  - 2.8122974753379824
  - 2.88827323615551
  - 3.018585363030434
  - 2.859436255693436
  validation_losses:
  - 0.4085632264614105
  - 0.44228482246398926
  - 0.4007870554924011
  - 0.4774101972579956
  - 0.43850815296173096
  - 0.3974232077598572
  - 0.4067799150943756
  - 0.39749935269355774
  - 0.41469746828079224
  - 0.400584876537323
  - 0.39615684747695923
  - 0.40811091661453247
  - 0.4166089594364166
  - 0.4779985249042511
  - 0.40268415212631226
  - 0.39640459418296814
  - 0.49723687767982483
  - 0.4025760591030121
  - 0.43750128149986267
  - 0.8242369890213013
  - 0.39848238229751587
  - 0.40961766242980957
  - 0.4349474608898163
  - 0.3995116055011749
  - 0.39679762721061707
  - 0.39804282784461975
  - 0.3978423774242401
  - 0.43674546480178833
  - 0.9558513760566711
  - 0.4412146210670471
  - 0.49775081872940063
  - 0.41024649143218994
  - 0.5573436617851257
  - 0.41869640350341797
  - 0.39706888794898987
  - 0.3971659541130066
  - 0.3993456959724426
  - 0.49388420581817627
  - 0.39648860692977905
  - 0.4015727639198303
  - 0.43126925826072693
  - 0.4000144600868225
  - 0.40576833486557007
  - 0.4805828332901001
  - 0.43312308192253113
  - 0.4063132405281067
  - 0.40208014845848083
  - 0.39635199308395386
  - 0.41669440269470215
  - 0.3963210880756378
  - 0.3966864347457886
  - 0.3954327404499054
  - 0.40630385279655457
  - 0.46717602014541626
  - 0.4146488904953003
  - 0.4075128436088562
  - 0.39589381217956543
  - 0.40599000453948975
  - 0.4050022065639496
  - 0.39724886417388916
  - 0.41473254561424255
  - 0.3967055380344391
  - 0.39688804745674133
  - 0.40346452593803406
  - 0.4048627018928528
  - 0.40437567234039307
  - 0.3974478840827942
loss_records_fold2:
  train_losses:
  - 3.0068924307823184
  - 3.096657830476761
  - 2.8195879369974137
  - 2.8181280553340913
  - 2.9427038431167603
  - 3.1101035833358766
  - 2.856020912528038
  - 2.915638267993927
  - 2.963273593783379
  - 2.863606232404709
  - 2.8520480483770374
  - 2.8697979688644413
  - 2.8494003206491474
  - 2.913898703455925
  - 2.862482073903084
  - 2.777036187052727
  - 2.903556263446808
  - 3.0310411989688877
  - 2.8948596209287647
  - 2.827379494905472
  - 2.7668075382709505
  - 2.867679136991501
  - 2.879480040073395
  - 2.8130875259637835
  - 2.82618787586689
  - 2.9017380297183992
  - 3.0453509926795963
  - 3.0020511478185656
  - 2.8564441144466404
  - 2.869365692138672
  - 3.0601881742477417
  - 3.119572377204895
  - 2.8886945217847826
  - 2.9590866029262544
  - 2.948156422376633
  - 3.088273674249649
  - 2.9686338871717455
  - 2.886393252015114
  - 2.9610544919967654
  - 2.8755088567733766
  - 2.9410275459289554
  - 2.895957863330841
  - 2.977426192164421
  - 3.646174329519272
  - 3.657725536823273
  - 3.449629691243172
  - 3.2446605414152145
  - 2.9704362958669663
  - 3.10243231356144
  - 3.072340884804726
  - 3.0223418056964877
  - 3.006560277938843
  - 3.1404712140560154
  - 3.0454390883445743
  - 2.9827663362026215
  - 3.0219378113746647
  - 2.971369153261185
  - 2.971335017681122
  - 2.8798605471849443
  - 2.9739421129226686
  - 3.07449049949646
  - 2.8878493249416355
  - 3.02910393178463
  - 2.9773147061467173
  - 2.978035938739777
  - 3.0435916006565096
  - 2.941772106289864
  - 2.9435927987098696
  - 3.103646376729012
  - 2.9145290166139604
  - 3.0061242222785953
  validation_losses:
  - 0.7515503764152527
  - 0.37991100549697876
  - 0.3768007457256317
  - 0.4052339196205139
  - 0.37557873129844666
  - 0.37251850962638855
  - 0.5878514051437378
  - 0.3731539249420166
  - 0.779410183429718
  - 0.44959449768066406
  - 0.46295827627182007
  - 0.38374730944633484
  - 0.37163910269737244
  - 0.3969394862651825
  - 0.3799898028373718
  - 0.37323495745658875
  - 0.3848535716533661
  - 0.44527706503868103
  - 0.43638551235198975
  - 0.3906119763851166
  - 0.3858943283557892
  - 0.42151349782943726
  - 0.3762882351875305
  - 0.378216028213501
  - 0.37699946761131287
  - 0.37474390864372253
  - 0.41908133029937744
  - 0.37097203731536865
  - 0.38689249753952026
  - 0.409326434135437
  - 0.4093340337276459
  - 0.37366488575935364
  - 0.3838239312171936
  - 0.42779141664505005
  - 0.398191899061203
  - 0.3874206244945526
  - 0.392034649848938
  - 0.42271384596824646
  - 0.47065791487693787
  - 0.38028982281684875
  - 0.5056650042533875
  - 0.395792156457901
  - 39.324440002441406
  - 0.4361335337162018
  - 0.39914798736572266
  - 0.42309507727622986
  - 0.4002130925655365
  - 0.4057902693748474
  - 0.385053426027298
  - 0.3845137357711792
  - 0.38805651664733887
  - 0.40985623002052307
  - 0.3823656737804413
  - 0.4011574387550354
  - 0.38737669587135315
  - 0.38467079401016235
  - 0.3827589452266693
  - 0.47789454460144043
  - 0.41491612792015076
  - 0.3840353190898895
  - 0.38576728105545044
  - 0.3770020306110382
  - 0.3814695179462433
  - 0.41750192642211914
  - 0.4356016516685486
  - 0.38332444429397583
  - 0.3860703408718109
  - 0.3854323625564575
  - 0.39502212405204773
  - 0.38241705298423767
  - 0.376101553440094
loss_records_fold3:
  train_losses:
  - 2.930457180738449
  - 2.8955403149127963
  - 3.074461516737938
  - 2.9936315000057223
  - 3.0052484035491944
  - 2.9243616223335267
  - 2.9162703394889835
  - 2.937565249204636
  - 2.9182621955871584
  - 2.9403020799160005
  - 3.0538476765155793
  - 2.9682418376207353
  - 2.953195267915726
  - 3.0146759778261187
  validation_losses:
  - 0.38405123353004456
  - 0.3875945806503296
  - 0.4062190651893616
  - 0.38608336448669434
  - 0.38782861828804016
  - 0.3982226848602295
  - 0.6941890716552734
  - 1.1050657033920288
  - 0.40648290514945984
  - 0.3865174651145935
  - 0.388773113489151
  - 0.38788989186286926
  - 0.38633158802986145
  - 0.39120036363601685
loss_records_fold4:
  train_losses:
  - 2.9464905589818957
  - 3.097480231523514
  - 2.8937336802482605
  - 2.9919445276260377
  - 2.887734752893448
  - 2.9071047961711884
  - 3.0323332369327547
  - 2.91594260931015
  - 3.0285836279392244
  - 2.9221245646476746
  - 2.8363429188728335
  - 3.054435542225838
  - 2.8857932627201084
  - 2.8618282794952394
  - 2.9640188574790955
  - 2.8892749041318897
  - 2.8661765277385713
  - 2.8757797062397006
  - 2.9337511122226716
  - 2.9889628231525425
  - 3.0266611576080322
  - 2.891577586531639
  - 2.8880106806755066
  - 2.879411363601685
  - 2.8572740852832794
  - 2.8955963879823687
  - 2.8233254194259647
  validation_losses:
  - 0.565925657749176
  - 0.39055103063583374
  - 0.39285188913345337
  - 0.3794446587562561
  - 0.38833776116371155
  - 0.4065743088722229
  - 0.3796272575855255
  - 0.4236784279346466
  - 0.3759153485298157
  - 0.41842973232269287
  - 0.4309138357639313
  - 0.40211644768714905
  - 0.3928881585597992
  - 0.37672194838523865
  - 0.38131701946258545
  - 0.3753563165664673
  - 0.43202054500579834
  - 0.37810182571411133
  - 0.38971245288848877
  - 0.41672176122665405
  - 0.5167049765586853
  - 0.4874311685562134
  - 0.4698810577392578
  - 0.41214245557785034
  - 0.3705664575099945
  - 0.37775424122810364
  - 0.37845492362976074
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 44 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 67 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 71 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:19:42.691856'
