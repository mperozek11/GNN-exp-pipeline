config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:49:05.344222'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_115fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_115fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_115fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_115fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_115fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_115fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_115fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_115fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_115fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_115fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.0441648721694947
  - 0.8106975674629212
  - 0.7972187340259552
  - 0.8026100754737855
  - 0.9516133308410645
  - 0.8047040224075318
  - 0.8237893402576447
  - 0.7549263060092927
  - 0.7700607657432557
  - 0.7579959154129029
  - 0.7754236936569214
  - 0.7736379206180573
  - 0.7686552822589875
  validation_losses:
  - 0.49510809779167175
  - 0.4062430262565613
  - 0.4090457558631897
  - 0.41661253571510315
  - 0.3888223171234131
  - 0.40866363048553467
  - 0.42355382442474365
  - 0.4003533720970154
  - 0.38901612162590027
  - 0.38686853647232056
  - 0.38992029428482056
  - 0.3835222125053406
  - 0.3845311403274536
loss_records_fold1:
  train_losses:
  - 0.8092088043689728
  - 0.7666016936302186
  - 0.7395938843488694
  - 0.7510741531848908
  - 0.8316081941127778
  - 0.7592718720436097
  - 0.8146689891815186
  - 0.8080000579357147
  - 0.8173675954341889
  - 0.7752127945423126
  - 0.8665202617645265
  - 0.7392167091369629
  - 0.7498053550720215
  validation_losses:
  - 0.3958353102207184
  - 0.38747501373291016
  - 0.3871336579322815
  - 0.38751593232154846
  - 0.39380332827568054
  - 0.3881553113460541
  - 0.3992953300476074
  - 0.38952040672302246
  - 0.38783782720565796
  - 0.38873520493507385
  - 0.38684555888175964
  - 0.38454633951187134
  - 0.392823189496994
loss_records_fold2:
  train_losses:
  - 0.7764298975467683
  - 0.7654601454734803
  - 0.8085879027843476
  - 0.8741786420345307
  - 0.78511500954628
  - 0.764382141828537
  - 0.7385152012109757
  - 0.7925418257713318
  - 0.7927514076232911
  - 0.7727538347244263
  - 0.7455545902252197
  - 0.7291790962219239
  - 0.7907454431056977
  - 0.7293619424104691
  - 0.7635315418243409
  - 0.7535946011543274
  - 0.7567454993724824
  - 0.7923037588596344
  - 0.8088595807552338
  - 0.7509512126445771
  - 0.7441441535949708
  - 0.7421848833560944
  - 0.7479610323905945
  - 0.7471252262592316
  - 0.7679395914077759
  - 0.7486482560634613
  - 0.7399541318416596
  validation_losses:
  - 0.3814166188240051
  - 0.4007793962955475
  - 0.4025186598300934
  - 0.39318785071372986
  - 0.3962523341178894
  - 0.3877749443054199
  - 0.38569143414497375
  - 0.390785276889801
  - 0.3880298137664795
  - 0.3966061472892761
  - 0.4229999780654907
  - 0.41344374418258667
  - 0.42300140857696533
  - 0.38499516248703003
  - 0.3859608471393585
  - 0.3859609067440033
  - 0.4008353352546692
  - 0.3905338644981384
  - 0.39752066135406494
  - 0.3854684829711914
  - 0.40060198307037354
  - 0.3955579102039337
  - 0.3967824876308441
  - 0.3843074142932892
  - 0.3894685208797455
  - 0.3859194219112396
  - 0.38697347044944763
loss_records_fold3:
  train_losses:
  - 0.7637593686580658
  - 0.7785252571105957
  - 0.7596269607543946
  - 0.8220062732696534
  - 0.7975589454174042
  - 0.7865552365779878
  - 0.8364763021469117
  - 0.7861879706382752
  - 0.7410409212112428
  - 0.8430714905261993
  - 0.7933061003684998
  - 0.7763298928737641
  - 0.7736833095550537
  - 0.7629779875278473
  - 0.7829858779907227
  - 0.7992697656154633
  - 0.7647580444812775
  - 0.7887325525283814
  - 0.767447316646576
  - 0.7807277977466583
  - 0.7415752857923508
  - 0.7398462772369385
  - 0.7408296167850494
  - 0.785012173652649
  - 0.7582326471805573
  - 0.7625205576419831
  - 0.8101609528064728
  - 0.8905362129211426
  - 0.780493313074112
  - 0.749376130104065
  - 0.7465065360069275
  - 0.7546495020389558
  - 0.8299932956695557
  - 0.766739046573639
  - 0.7695327281951905
  - 1.0078765094280244
  - 0.8193258821964264
  - 0.7825135648250581
  - 0.8024568676948548
  validation_losses:
  - 0.38221168518066406
  - 0.39101916551589966
  - 0.3727887272834778
  - 0.38166579604148865
  - 0.36884889006614685
  - 0.3743896782398224
  - 0.42492416501045227
  - 0.3726999759674072
  - 0.37095320224761963
  - 0.37936124205589294
  - 0.3695850074291229
  - 0.3840383291244507
  - 0.3843880593776703
  - 0.3665449023246765
  - 0.4079730212688446
  - 0.38543084263801575
  - 0.3774902820587158
  - 0.36714449524879456
  - 0.374102920293808
  - 0.39565831422805786
  - 0.37548932433128357
  - 0.37682801485061646
  - 0.3837883472442627
  - 0.37194573879241943
  - 0.36467915773391724
  - 0.40491998195648193
  - 0.3749292194843292
  - 0.3705950677394867
  - 0.38213658332824707
  - 0.37874874472618103
  - 0.3729703426361084
  - 0.37479621171951294
  - 0.3967595100402832
  - 0.3704673945903778
  - 0.37885427474975586
  - 0.37115558981895447
  - 0.37402355670928955
  - 0.3727302551269531
  - 0.3689461052417755
loss_records_fold4:
  train_losses:
  - 0.745149201154709
  - 0.7505316495895387
  - 0.7523104965686799
  - 0.7828203201293946
  - 0.772765588760376
  - 0.8093368351459503
  - 0.7698323547840119
  - 0.7312130093574525
  - 0.7433677017688751
  - 0.7635013401508332
  - 0.7394630610942841
  - 0.7637767732143402
  - 0.7869138360023499
  - 0.730463868379593
  - 0.7671560525894165
  - 0.8227045893669129
  - 0.7449732184410096
  - 0.7512988686561585
  - 0.7560475111007691
  - 0.75214963555336
  - 0.7828674256801605
  - 0.7446115553379059
  - 0.7351581394672394
  - 0.7289364755153657
  - 0.7168589532375336
  - 0.7241119980812073
  - 0.736880522966385
  - 0.7910629868507386
  - 0.7782558619976044
  - 0.7612935841083527
  - 0.7711109161376953
  - 0.7284695982933045
  - 0.7731906533241273
  - 0.767915827035904
  validation_losses:
  - 0.39658820629119873
  - 0.3927070200443268
  - 0.382591187953949
  - 0.4333917796611786
  - 0.3816244304180145
  - 0.3970175087451935
  - 0.38901686668395996
  - 0.39215871691703796
  - 0.40868958830833435
  - 0.3744891285896301
  - 0.4101620018482208
  - 0.38749754428863525
  - 0.37227123975753784
  - 0.3784368336200714
  - 0.38203778862953186
  - 0.4050219655036926
  - 0.557309091091156
  - 0.38577407598495483
  - 0.3850666284561157
  - 0.3753117322921753
  - 0.415831595659256
  - 0.37061455845832825
  - 0.39740386605262756
  - 0.4437074363231659
  - 0.37990233302116394
  - 0.5810964107513428
  - 0.4588986933231354
  - 0.5400848388671875
  - 0.46139049530029297
  - 0.38947540521621704
  - 0.38491299748420715
  - 0.39154574275016785
  - 0.3962351679801941
  - 0.3855188190937042
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:11:04.918980'
