config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 16:57:04.007189'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/31/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/31/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/31/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/31/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/31/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/31/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/31/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/31/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/31/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/31/fold_4_state_dict.pt
loss_records_fold2:
  train_losses:
  - 6.9085568487644196
  - 5.484191447496414
  - 5.530516117811203
  - 4.8666770458221436
  - 4.621922791004181
  - 5.161459565162659
  - 4.766706019639969
  - 4.876000210642815
  - 4.698862969875336
  - 4.465323567390442
  - 4.304124981164932
  - 4.662873148918152
  - 4.332622617483139
  - 4.556814461946487
  - 5.323259860277176
  - 5.053269982337952
  - 4.8891401290893555
  - 5.623331278562546
  - 5.1002069264650345
  - 6.543532341718674
  - 5.830730527639389
  - 4.938965886831284
  - 4.3312258422374725
  - 4.913614124059677
  - 4.595354288816452
  - 5.24932324886322
  - 5.137071669101715
  - 5.046851873397827
  - 4.442654490470886
  - 4.392278254032135
  - 4.743528515100479
  - 4.873631864786148
  - 4.647453844547272
  - 4.793922156095505
  - 4.509891033172607
  - 4.947869122028351
  - 5.4648153483867645
  - 7.803346812725067
  - 14.144852221012115
  - 7.787043452262878
  - 5.538914442062378
  - 7.86235249042511
  - 6.394836992025375
  - 5.155229181051254
  - 4.548277527093887
  - 5.099266350269318
  - 4.646725594997406
  - 5.60273876786232
  - 8.458674013614655
  - 7.318997919559479
  - 6.319473534822464
  - 4.727942943572998
  - 4.788836866617203
  - 4.647338569164276
  - 4.496809035539627
  - 5.086474180221558
  - 4.558465212583542
  - 5.108399868011475
  - 4.975920915603638
  - 4.429477155208588
  - 4.30271902680397
  - 4.472571223974228
  - 4.327348679304123
  - 4.807366102933884
  - 5.009304225444794
  - 5.156251609325409
  - 6.663510710000992
  - 6.985530495643616
  - 5.105382949113846
  - 4.6007179617881775
  - 7.55040118098259
  - 7.9916200041770935
  - 6.530440032482147
  - 7.129961550235748
  - 8.242033869028091
  - 5.082462415099144
  - 4.893062859773636
  - 4.634000152349472
  - 4.571380585432053
  - 4.353169918060303
  - 5.743273943662643
  - 7.462873339653015
  - 6.418904781341553
  - 10.12060421705246
  - 9.264704763889313
  - 6.9865952134132385
  - 6.604231223464012
  - 5.3205728232860565
  - 6.289918214082718
  - 7.600555062294006
  - 4.88525627553463
  - 6.197995811700821
  - 4.548911705613136
  - 4.612227976322174
  - 4.811187207698822
  - 6.9541913866996765
  - 6.2316365242004395
  - 4.63513520359993
  - 4.5265732407569885
  - 5.185823768377304
  validation_losses:
  - 0.4337238669395447
  - 0.5371206998825073
  - 16769576.0
  - 35850780.0
  - 60993180.0
  - 441138720.0
  - 49137580.0
  - 210981712.0
  - 635279488.0
  - 155002400.0
  - 1014283968.0
  - 0.44804510474205017
  - 0.43846848607063293
  - 0.5983385443687439
  - 0.6191371083259583
  - 0.4539513885974884
  - 0.8406104445457458
  - 0.4661411941051483
  - 0.7776662111282349
  - 0.7355507016181946
  - 0.4611361026763916
  - 0.4311741292476654
  - 0.44216814637184143
  - 0.4575265645980835
  - 0.5614235401153564
  - 0.5897319912910461
  - 0.5609028339385986
  - 0.4013745188713074
  - 0.42443352937698364
  - 0.45924460887908936
  - 0.47101014852523804
  - 0.45371589064598083
  - 0.46049362421035767
  - 0.40910467505455017
  - 0.46738284826278687
  - 0.48423829674720764
  - 0.7237303853034973
  - 0.6270691156387329
  - 0.6833544373512268
  - 0.5411774516105652
  - 0.7168312072753906
  - 0.5296441316604614
  - 0.6912546753883362
  - 0.41608357429504395
  - 0.5079482793807983
  - 0.4663498103618622
  - 0.44763293862342834
  - 0.6775442957878113
  - 0.9516897201538086
  - 0.7401629090309143
  - 0.4040350019931793
  - 0.4309769868850708
  - 0.45184165239334106
  - 0.442322313785553
  - 0.44449570775032043
  - 0.4755668342113495
  - 0.4173802435398102
  - 0.4284479320049286
  - 0.4115791320800781
  - 0.41222047805786133
  - 0.41173332929611206
  - 0.41710564494132996
  - 0.45355692505836487
  - 0.4358632266521454
  - 0.5320528149604797
  - 0.8061027526855469
  - 0.7630912661552429
  - 0.6044011116027832
  - 0.4547058045864105
  - 0.43736159801483154
  - 0.8065772652626038
  - 0.6355248689651489
  - 0.41248613595962524
  - 0.5580136775970459
  - 0.493853896856308
  - 0.48272067308425903
  - 0.4968743324279785
  - 0.4512491524219513
  - 0.39591455459594727
  - 0.5107017159461975
  - 0.721223771572113
  - 0.6190230846405029
  - 0.6814390420913696
  - 0.601307213306427
  - 0.7391206622123718
  - 0.4249316453933716
  - 0.4976855516433716
  - 0.5306704640388489
  - 0.4451696574687958
  - 0.4671976566314697
  - 0.5246069431304932
  - 0.4416976571083069
  - 0.4866136312484741
  - 0.4454539120197296
  - 0.5781494379043579
  - 0.6248670816421509
  - 0.4733990430831909
  - 0.3900812864303589
  - 0.43343663215637207
  - 1.0944385528564453
loss_records_fold3:
  train_losses:
  - 6.112619340419769
  - 8.280159890651703
  - 11.753184854984283
  - 7.387592613697052
  - 6.56165999174118
  - 4.996605694293976
  - 4.710261046886444
  - 4.398383438587189
  - 4.6433247327804565
  - 5.305546373128891
  - 5.446301490068436
  - 6.119611024856567
  - 5.523605182766914
  - 4.802874207496643
  - 5.130315542221069
  - 5.21279177069664
  - 4.72404283285141
  - 4.130464345216751
  - 4.519649565219879
  - 6.4214217364788055
  - 6.614416092634201
  - 5.60136142373085
  - 4.74196845293045
  - 4.750435560941696
  - 4.6153344810009
  - 4.614212483167648
  - 5.466580390930176
  - 10.631957650184631
  - 5.8654965460300446
  - 6.414434105157852
  - 7.424908518791199
  - 6.00645649433136
  - 4.826815068721771
  - 5.80160927772522
  - 5.232041627168655
  - 6.755745440721512
  - 5.135834097862244
  - 5.101017773151398
  - 4.890332758426666
  - 6.259912878274918
  - 4.778029888868332
  - 126.14926424622536
  - 66.25490152835846
  - 28.484226495027542
  - 14.008569151163101
  - 4.889154762029648
  - 5.373400628566742
  - 6.884351551532745
  - 10.247422099113464
  - 54.800112158060074
  - 8.509171277284622
  - 21.717116743326187
  - 19.20561695098877
  - 5.682537734508514
  - 7.336696267127991
  - 18.372835844755173
  - 10.197319746017456
  - 8.265566319227219
  - 4.995246887207031
  - 8.487757116556168
  - 5.586461573839188
  - 5.66365122795105
  - 5.352210342884064
  - 4.75560200214386
  - 5.426942139863968
  - 4.5187607407569885
  - 4.3984509110450745
  - 4.689325600862503
  - 4.479993939399719
  - 4.434251397848129
  - 11.978305608034134
  - 5.130625367164612
  - 4.541804015636444
  - 8.413411259651184
  - 6.0757357478141785
  - 14.815050691366196
  - 7.352373659610748
  - 6.891911804676056
  - 8.387776643037796
  - 6.1999671459198
  - 13.7991953343153
  - 8.891149520874023
  - 10.700334459543228
  - 12.872255742549896
  - 411.2827546298504
  - 7.40849843621254
  - 16.82973736524582
  - 7.293616026639938
  - 9.817581683397293
  - 5.898858428001404
  - 8.525957360863686
  - 6.925598680973053
  - 113.1323971748352
  - 13.355578601360321
  - 82.9214195907116
  - 17.78598117828369
  - 6.555188059806824
  - 9.665191531181335
  - 16.780614376068115
  - 8.171920239925385
  validation_losses:
  - 0.594851016998291
  - 0.9224702715873718
  - 0.7779685854911804
  - 0.49699079990386963
  - 0.46996134519577026
  - 0.4764505922794342
  - 0.4111248254776001
  - 0.4735017418861389
  - 2.130549907684326
  - 2.7923717498779297
  - 7.811326026916504
  - 6.303772449493408
  - 0.4430485963821411
  - 0.44864022731781006
  - 0.5880629420280457
  - 0.5176124572753906
  - 0.41171956062316895
  - 0.42216432094573975
  - 0.6241875290870667
  - 0.6827374696731567
  - 0.6063601970672607
  - 0.5014161467552185
  - 0.4751884341239929
  - 0.45913901925086975
  - 0.5177256464958191
  - 0.4438457787036896
  - 1.7684859037399292
  - 0.6746764779090881
  - 0.6847869157791138
  - 0.734237551689148
  - 0.8059716820716858
  - 0.631837010383606
  - 0.49359065294265747
  - 0.6629975438117981
  - 0.6364747285842896
  - 0.5885322690010071
  - 1.2695112228393555
  - 4.378076076507568
  - 1.4798455847256064e+16
  - 0.4869512617588043
  - 0.5343420505523682
  - 0.4146813750267029
  - 280.6850280761719
  - 1.1206241846084595
  - 0.5869710445404053
  - 0.500948965549469
  - 40.176368713378906
  - 0.4927102327346802
  - 0.4643385112285614
  - 0.5866393446922302
  - 0.5900688767433167
  - 0.6170079112052917
  - 3.347991704940796
  - 0.9418864846229553
  - 0.5629769563674927
  - 0.4215087890625
  - 0.41701623797416687
  - 0.44650933146476746
  - 0.570397138595581
  - 0.5573585033416748
  - 0.7643934488296509
  - 0.7361690402030945
  - 0.454199880361557
  - 0.5304099917411804
  - 0.45672541856765747
  - 0.45461583137512207
  - 0.45377451181411743
  - 0.4246332049369812
  - 0.4831087589263916
  - 0.41085904836654663
  - 0.4422973394393921
  - 0.4493950605392456
  - 0.5755625367164612
  - 0.5923908948898315
  - 0.46385082602500916
  - 0.4594985842704773
  - 0.5116639137268066
  - 0.4610782265663147
  - 0.5333179831504822
  - 0.46608445048332214
  - 0.4191493093967438
  - 0.5158492922782898
  - 0.8602033257484436
  - 0.7054770588874817
  - 0.4520622789859772
  - 0.6723347306251526
  - 0.5955560207366943
  - 0.5886646509170532
  - 0.5527644157409668
  - 0.4846060574054718
  - 0.48462796211242676
  - 0.64265376329422
  - 0.45982444286346436
  - 0.43108126521110535
  - 0.6026384234428406
  - 0.509081244468689
  - 0.43079063296318054
  - 0.9443204402923584
  - 1.0298268795013428
  - 0.7992867231369019
loss_records_fold4:
  train_losses:
  - 6.455060124397278
  - 5.673726290464401
  - 5.83603373169899
  - 4.995649665594101
  - 5.9884165823459625
  - 5.332075238227844
  - 4.736108630895615
  - 6.774918168783188
  - 7.090864539146423
  - 6.519139140844345
  - 6.456380158662796
  - 4.8828345239162445
  - 5.350303828716278
  - 6.478168845176697
  - 6.716966599225998
  - 8.018711231648922
  - 6.908195465803146
  - 5.620686322450638
  - 8.953491300344467
  - 8.54244577884674
  - 9.003115117549896
  - 6.052833259105682
  - 4.863316625356674
  - 4.213095724582672
  - 5.746775209903717
  - 5.469894081354141
  - 4.851522356271744
  - 6.386987626552582
  - 4.7088311314582825
  - 4.8436181247234344
  - 7.148783028125763
  - 4.607837557792664
  - 8.148648083209991
  - 5.381120979785919
  - 4.978548377752304
  - 6.45475172996521
  - 6.209598064422607
  - 6.761873960494995
  - 5.079694628715515
  - 6.154941022396088
  - 5.850313305854797
  - 10.784023821353912
  - 7.673383474349976
  - 6.851816713809967
  - 8.959569334983826
  - 5.930605947971344
  - 6.035708427429199
  - 6.65876430273056
  - 5.605707377195358
  - 5.557116478681564
  - 5.144797444343567
  - 4.510760068893433
  - 5.443437069654465
  - 4.813046365976334
  - 5.637932181358337
  - 5.268196403980255
  - 4.44087752699852
  - 4.199414402246475
  - 4.477217018604279
  - 5.239310473203659
  - 4.850413590669632
  - 4.3206193298101425
  - 5.068948179483414
  - 4.791356444358826
  - 4.64212292432785
  - 7.596802711486816
  - 4.979492872953415
  - 5.337066948413849
  - 5.349677488207817
  - 5.555121302604675
  - 5.71330913901329
  - 5.099948734045029
  - 67.59807997941971
  - 5.566856533288956
  - 4.751130163669586
  - 4.8563211262226105
  - 5.017038285732269
  - 4.773513555526733
  - 5.174107044935226
  - 6.5026533007621765
  - 6.199498683214188
  - 4.839814066886902
  - 5.332492113113403
  - 6.304315656423569
  - 5.076350197196007
  - 5.4592941999435425
  - 5.754968076944351
  - 5.921734064817429
  - 4.985604614019394
  - 4.726979672908783
  - 6.783948540687561
  - 5.220862507820129
  - 4.800850987434387
  - 5.647920191287994
  - 4.1654742658138275
  - 5.123623460531235
  - 6.803890645503998
  - 6.099205583333969
  - 5.053339779376984
  - 4.514377653598785
  validation_losses:
  - 0.6824914813041687
  - 0.5384804606437683
  - 0.5751315355300903
  - 0.8263473510742188
  - 0.623665452003479
  - 0.5048846006393433
  - 0.6098083257675171
  - 0.608619749546051
  - 0.700530469417572
  - 0.6185277104377747
  - 0.6390776038169861
  - 0.7000754475593567
  - 1.1487911939620972
  - 0.9318506121635437
  - 1.5809080600738525
  - 0.9160268306732178
  - 0.6193460822105408
  - 0.6008421182632446
  - 0.7986308336257935
  - 0.7040178775787354
  - 0.8051547408103943
  - 1.2662014961242676
  - 0.8943905234336853
  - 1.0252803564071655
  - 0.5843312740325928
  - 0.6351884603500366
  - 0.7455402612686157
  - 0.5216529965400696
  - 0.4600003659725189
  - 0.5696171522140503
  - 0.6781412363052368
  - 0.47655045986175537
  - 0.6390599012374878
  - 0.47944504022598267
  - 0.6955824494361877
  - 0.549035370349884
  - 1.0015469789505005
  - 0.5805363059043884
  - 0.44937628507614136
  - 0.69789057970047
  - 0.4959099292755127
  - 1.0771926641464233
  - 0.8823428153991699
  - 0.6575139164924622
  - 0.6622953414916992
  - 0.9835896492004395
  - 0.5304185748100281
  - 0.5631380081176758
  - 0.6153420805931091
  - 0.5822834968566895
  - 0.5599908828735352
  - 0.5261397957801819
  - 0.5158749222755432
  - 0.6911304593086243
  - 0.5189299583435059
  - 0.43717485666275024
  - 0.4434396028518677
  - 0.4190481901168823
  - 0.475119411945343
  - 0.5268115401268005
  - 0.4971654415130615
  - 0.5713214874267578
  - 0.5576736927032471
  - 0.5163668394088745
  - 0.5771906971931458
  - 0.6463355422019958
  - 0.5289772152900696
  - 0.6829292178153992
  - 0.7559785842895508
  - 0.5097997188568115
  - 0.49658241868019104
  - 0.5316219925880432
  - 0.5538504123687744
  - 0.5238462686538696
  - 0.9054856300354004
  - 0.4273189306259155
  - 0.47195467352867126
  - 0.4053916931152344
  - 0.7736353874206543
  - 0.5461542010307312
  - 0.4810684025287628
  - 0.4707481265068054
  - 0.5093244314193726
  - 0.49116355180740356
  - 0.5284910202026367
  - 0.4165209233760834
  - 0.5935360789299011
  - 0.42541033029556274
  - 0.4272634983062744
  - 1.2238051891326904
  - 0.5075193047523499
  - 0.4565174877643585
  - 0.7426163554191589
  - 0.4136323034763336
  - 0.4362700879573822
  - 0.4085848331451416
  - 0.6570906043052673
  - 0.478110134601593
  - 0.4236767888069153
  - 0.6469753980636597
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 96 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 71 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.7530017152658662, 0.8576329331046312, 0.5831903945111492, 0.7495711835334476,
    0.6237113402061856]'
  fold_eval_f1: '[0.1111111111111111, 0.0, 0.11636363636363636, 0.12048192771084337,
    0.12749003984063748]'
  mean_eval_accuracy: 0.713421513324256
  mean_f1_accuracy: 0.09508934300524566
  total_train_time: '0:03:32.579835'
