config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:16:57.268851'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_79fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_79fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_79fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_79fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_79fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_79fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_79fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_79fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_79fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_79fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 11.002649354934693
  - 7.901988124847413
  - 5.057750082015992
  - 4.096904903650284
  - 5.806175374984742
  - 2.066821449995041
  - 2.1474992275238036
  - 1.5429584473371507
  - 1.6203969478607179
  - 1.6818575859069824
  - 1.682416844367981
  - 2.0515120029449463
  - 1.247894310951233
  - 1.4303737878799438
  - 2.4855770945549014
  - 5.204582381248475
  - 10.05408548116684
  - 1.9329615235328674
  - 1.35595760345459
  - 1.917735993862152
  - 2.3623923480510713
  - 2.1761669516563416
  - 1.5871967613697053
  - 1.3268929898738862
  - 1.9406444132328033
  - 1.348707151412964
  - 1.9168892800807953
  - 1.5463904201984406
  - 1.1935058414936066
  - 0.9595196485519409
  - 1.2098661422729493
  - 0.9507053434848786
  - 0.8541150391101837
  - 2.496959149837494
  - 1.0370247125625611
  - 0.9742272019386292
  - 1.0810580134391785
  - 0.9331095814704895
  - 1.000039917230606
  - 1.0733452260494232
  - 1.2226534962654114
  - 0.8146571308374405
  - 1.0295628011226654
  - 1.3146705448627474
  - 1.414760708808899
  - 0.9676874279975891
  - 0.9535125613212586
  - 3.990014535188675
  - 1.4076495110988618
  - 1.4462272465229036
  - 1.113087409734726
  - 1.1857288718223573
  - 1.50032417178154
  - 0.9203519999980927
  - 1.1347220182418825
  - 1.0983952641487122
  - 1.4686139285564423
  - 2.025392872095108
  - 1.861150598526001
  - 0.9342918932437897
  - 1.7859634399414064
  - 6.3558952689170845
  - 4.854573744535447
  - 2.014561539888382
  - 1.2007112026214601
  - 7.78932004570961
  - 1.6419974803924562
  - 1.8482211530208588
  - 1.2994533419609071
  - 1.2253869712352754
  - 2.7394538044929506
  - 1.3064274847507478
  - 6.8998437404632575
  - 1.3087981998920442
  - 4.1407923161983495
  - 3.4598544895648957
  - 1.3706394016742707
  - 1.074756795167923
  - 0.8733709990978241
  - 1.017683756351471
  - 1.010903787612915
  - 1.234314376115799
  - 1.3804841578006746
  - 1.0155384778976442
  - 1.503031998872757
  - 1.3311898529529573
  - 9.22543113231659
  - 1.2777669966220857
  - 2.2945318937301638
  - 1.148106211423874
  - 1.0109430372714996
  - 0.9239885270595551
  - 1.0786092877388
  - 1.0635845184326171
  - 0.8421699613332749
  - 1.0248190701007844
  - 0.9567082703113556
  - 0.8922047197818757
  - 1.0766403555870057
  - 0.8157998502254487
  validation_losses:
  - 3.050072431564331
  - 4.1851935386657715
  - 5.360394477844238
  - 2.252028465270996
  - 2.0721659660339355
  - 0.8139421343803406
  - 1.460069179534912
  - 0.7879654765129089
  - 1.3024944067001343
  - 0.6542067527770996
  - 0.7424901723861694
  - 0.45102688670158386
  - 0.4997102916240692
  - 0.7369264960289001
  - 0.5120764374732971
  - 0.7096018195152283
  - 0.7052109837532043
  - 0.7196586728096008
  - 0.6359281539916992
  - 0.6582326889038086
  - 0.6713147163391113
  - 0.7143551707267761
  - 0.4957045316696167
  - 0.607541561126709
  - 0.7802029252052307
  - 0.656467616558075
  - 0.4824359714984894
  - 0.726154625415802
  - 0.44065478444099426
  - 0.4003145396709442
  - 0.4031868278980255
  - 0.38351136445999146
  - 0.4247448146343231
  - 0.3956005573272705
  - 0.4479173421859741
  - 0.48073896765708923
  - 0.5005736351013184
  - 0.4270622730255127
  - 0.40295645594596863
  - 0.43828144669532776
  - 0.3756231665611267
  - 0.4403339624404907
  - 0.41086089611053467
  - 0.9701568484306335
  - 0.518988311290741
  - 0.4041842818260193
  - 0.4134788513183594
  - 0.45919978618621826
  - 0.4670450985431671
  - 0.4279181659221649
  - 0.4293839931488037
  - 0.45377179980278015
  - 0.38948434591293335
  - 0.4034079909324646
  - 0.4112929403781891
  - 0.4103965759277344
  - 0.40228530764579773
  - 0.43643319606781006
  - 0.4347483515739441
  - 0.3999541699886322
  - 0.4420168101787567
  - 0.40865957736968994
  - 0.5900251865386963
  - 0.6188448667526245
  - 0.4269697666168213
  - 0.4711092412471771
  - 1.1026557683944702
  - 0.5656760334968567
  - 0.4747708737850189
  - 0.43421703577041626
  - 0.424800306558609
  - 0.44719254970550537
  - 0.41211333870887756
  - 0.5626699328422546
  - 0.6777912974357605
  - 0.6029506325721741
  - 0.39483657479286194
  - 0.4969414472579956
  - 0.41927263140678406
  - 0.43373483419418335
  - 0.4494244158267975
  - 0.5706174373626709
  - 0.42149293422698975
  - 0.42254436016082764
  - 0.4048892855644226
  - 0.42289432883262634
  - 0.41763240098953247
  - 0.4427306652069092
  - 0.4634247124195099
  - 0.4113868772983551
  - 0.40390634536743164
  - 0.44901609420776367
  - 0.41854575276374817
  - 0.435371994972229
  - 0.39801615476608276
  - 0.3887387812137604
  - 0.4073013365268707
  - 0.39780235290527344
  - 0.3930177390575409
  - 0.3892485499382019
loss_records_fold1:
  train_losses:
  - 0.9105101704597474
  - 0.83453808426857
  - 0.8776237547397614
  - 0.9362083137035371
  - 0.9188108205795289
  - 1.1803178369998932
  - 0.8952310502529145
  - 0.9224942803382874
  - 0.9195762395858765
  - 0.9956001222133637
  - 0.8094391494989396
  - 0.9033409595489502
  - 0.8916391253471375
  - 0.9261610627174378
  - 6.286554574966431
  - 1.3543037593364717
  - 0.924759602546692
  - 1.2181031733751297
  - 0.8652121841907502
  - 1.426317322254181
  - 0.9367635428905488
  validation_losses:
  - 0.40348872542381287
  - 0.4075525999069214
  - 0.40422365069389343
  - 0.4085201323032379
  - 0.4200378358364105
  - 0.4218287765979767
  - 0.48854565620422363
  - 0.4322851300239563
  - 0.6891157627105713
  - 0.5969357490539551
  - 0.49085676670074463
  - 0.40440833568573
  - 0.42071732878685
  - 0.4423045814037323
  - 0.4759300649166107
  - 0.4274766743183136
  - 0.42341816425323486
  - 0.4010476768016815
  - 0.4030201733112335
  - 0.4027118980884552
  - 0.4044118821620941
loss_records_fold2:
  train_losses:
  - 1.1201710343360902
  - 1.5741453111171724
  - 0.9706744134426117
  - 1.2791959345340729
  - 1.1664909958839418
  - 0.8632643043994904
  - 0.9117777466773987
  - 0.8835651516914368
  - 0.9217544317245484
  - 0.8900492072105408
  - 7.148880273103714
  validation_losses:
  - 0.4046896994113922
  - 0.4087810814380646
  - 0.3785305917263031
  - 0.3963707983493805
  - 0.3908598721027374
  - 0.3987259268760681
  - 0.38589709997177124
  - 0.38878726959228516
  - 0.3925873935222626
  - 0.39458534121513367
  - 0.40415114164352417
loss_records_fold3:
  train_losses:
  - 1.582044631242752
  - 1.083772999048233
  - 0.9424264073371887
  - 0.884487509727478
  - 0.9285021305084229
  - 0.9086421370506287
  - 0.868694931268692
  - 4.243133914470673
  - 1.1281156063079834
  - 1.1328563392162323
  - 0.8486690700054169
  - 2.42404659986496
  - 3.533441108465195
  - 1.5398014307022097
  - 1.2204937398433686
  - 1.0730430841445924
  - 0.9392284452915192
  - 0.8545756042003632
  - 0.8278535544872284
  - 0.8464598536491394
  - 0.8951302170753479
  - 1.042553895711899
  - 0.9941361427307129
  - 0.932022500038147
  - 0.9145658314228058
  - 1.05998512506485
  - 0.9282840967178345
  - 0.9098851680755615
  - 0.8572958409786224
  - 0.8709575295448304
  - 0.943967628479004
  - 2.0088825941085817
  - 0.9809803128242494
  - 0.9051247537136078
  - 0.8888706564903259
  - 1.2653363525867463
  - 0.8575751781463623
  - 1.3316664636135103
  - 0.8755046665668488
  - 1.0193407237529755
  - 0.9019372522830964
  - 0.8322614371776581
  - 2.8656002938747407
  - 1.155759471654892
  - 0.8732080936431885
  - 0.975898826122284
  - 0.9719004571437836
  validation_losses:
  - 0.39648520946502686
  - 0.399554580450058
  - 0.453559011220932
  - 0.4115672707557678
  - 0.39898937940597534
  - 0.4123491644859314
  - 0.4106537699699402
  - 0.4164502024650574
  - 0.6333755254745483
  - 0.39482957124710083
  - 0.3963742256164551
  - 0.39824599027633667
  - 0.4164595305919647
  - 0.4725085496902466
  - 0.4236445724964142
  - 0.4073919951915741
  - 0.3946016728878021
  - 0.4073886275291443
  - 0.4240942597389221
  - 0.41263630986213684
  - 0.3951842784881592
  - 0.40962132811546326
  - 0.4618806838989258
  - 0.4767477214336395
  - 0.42569828033447266
  - 0.4159301519393921
  - 0.4026799201965332
  - 0.3962627649307251
  - 0.4074644446372986
  - 0.41823360323905945
  - 0.40671879053115845
  - 0.39589208364486694
  - 0.44563204050064087
  - 0.41010016202926636
  - 0.42655226588249207
  - 0.427854984998703
  - 0.42270195484161377
  - 0.3995997905731201
  - 0.4022690951824188
  - 0.40353602170944214
  - 1.2699264287948608
  - 0.444973349571228
  - 0.4265332520008087
  - 0.4171704947948456
  - 0.40684056282043457
  - 0.41066983342170715
  - 0.41079163551330566
loss_records_fold4:
  train_losses:
  - 0.9303072512149811
  - 1.0198209643363954
  - 0.9999430418014527
  - 0.9366978824138642
  - 0.8497165739536285
  - 0.8250545471906663
  - 0.8797527849674225
  - 0.8947310447692871
  - 0.8341498017311096
  - 0.8274730861186982
  - 0.8560004949569703
  - 0.9384243428707123
  - 0.9117468535900116
  - 0.8890632152557374
  - 0.8644011139869691
  - 0.8400289595127106
  - 0.862709927558899
  - 0.8971787333488465
  - 0.8739429295063019
  - 0.8589234352111816
  - 0.8769492149353028
  - 0.893483430147171
  - 1.0948125064373018
  - 0.9998924821615219
  - 0.7961261838674546
  - 0.8488669097423553
  - 0.8992221415042878
  - 0.9520275354385377
  - 3.369488090276718
  - 1.4543541550636292
  - 1.0424521565437317
  - 1.5630115270614624
  - 0.897803395986557
  - 0.8896256148815156
  - 0.8516759634017945
  - 0.8739979267120361
  - 0.8499976098537445
  - 0.833462131023407
  - 0.7969075798988343
  - 0.8070023626089097
  - 0.901787942647934
  - 0.9496674954891206
  - 0.8995096325874329
  - 0.873652321100235
  - 0.8959398865699768
  - 0.7974025905132294
  - 0.8344528913497925
  - 0.8712522268295289
  - 0.8505793750286103
  - 0.8247184813022614
  - 0.8705840170383454
  - 0.858185487985611
  - 0.8556552112102509
  - 1.2285341680049897
  - 0.8918377101421356
  - 0.8083507239818574
  - 0.8472406029701234
  - 0.8786460220813752
  - 0.8168381810188294
  - 0.8016318500041962
  - 0.9345541596412659
  - 0.8348167717456818
  - 0.8934324443340302
  - 0.8134666860103608
  - 0.8954042851924897
  - 0.8200457513332368
  - 0.828059059381485
  - 0.8894281804561616
  - 1.0121371090412141
  - 0.8553297758102417
  - 0.875835543870926
  - 0.8860607922077179
  - 0.8341997265815735
  - 0.8023094892501832
  - 0.8580030858516694
  - 0.8620829880237579
  - 2.1863017618656158
  - 0.8757500410079957
  - 0.997924268245697
  - 0.868361508846283
  - 0.8471344769001008
  - 0.8376490354537964
  - 0.8350394129753114
  - 0.8457834482192994
  - 0.8402804374694824
  - 0.8638637721538545
  validation_losses:
  - 0.44841718673706055
  - 0.5047571063041687
  - 0.4785803258419037
  - 0.4810727536678314
  - 0.4554330110549927
  - 0.435639888048172
  - 0.42905986309051514
  - 0.42439186573028564
  - 0.4545215666294098
  - 0.45802202820777893
  - 0.3996357023715973
  - 0.47017306089401245
  - 0.4652046263217926
  - 0.4256059229373932
  - 0.43647217750549316
  - 0.4110027849674225
  - 0.41978007555007935
  - 0.47911810874938965
  - 0.44711703062057495
  - 0.417972594499588
  - 0.44198426604270935
  - 0.44729846715927124
  - 0.437669962644577
  - 0.4179735779762268
  - 0.42652034759521484
  - 0.4756752848625183
  - 0.4453851282596588
  - 0.4111497700214386
  - 0.5390298366546631
  - 0.46330690383911133
  - 0.4369724690914154
  - 0.48810192942619324
  - 0.42795440554618835
  - 0.43932098150253296
  - 0.4578119218349457
  - 0.4457750916481018
  - 0.43528470396995544
  - 0.40303269028663635
  - 0.4465039372444153
  - 0.4387664794921875
  - 0.4369947016239166
  - 0.4136873185634613
  - 0.4591306746006012
  - 0.40088433027267456
  - 0.44006773829460144
  - 0.39239197969436646
  - 0.4394198954105377
  - 0.41707277297973633
  - 0.4252171516418457
  - 0.41534197330474854
  - 0.42246708273887634
  - 0.44889628887176514
  - 0.45099937915802
  - 0.44345030188560486
  - 0.4198102056980133
  - 0.4071410596370697
  - 0.41839832067489624
  - 0.42822837829589844
  - 0.4296211898326874
  - 0.43234783411026
  - 0.41502732038497925
  - 0.43546760082244873
  - 0.43413734436035156
  - 0.40145307779312134
  - 0.42261916399002075
  - 0.42781171202659607
  - 0.4306214451789856
  - 0.4356231093406677
  - 0.4115881621837616
  - 0.40974369645118713
  - 0.4723421633243561
  - 0.4160189926624298
  - 0.40136095881462097
  - 0.3933631181716919
  - 0.4259052276611328
  - 0.40332886576652527
  - 0.4127315878868103
  - 0.4585472643375397
  - 0.4224996566772461
  - 0.44397974014282227
  - 0.44828811287879944
  - 0.4350101947784424
  - 0.44110107421875
  - 0.43220406770706177
  - 0.4101719558238983
  - 0.40302857756614685
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 47 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 86 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:21:57.711188'
