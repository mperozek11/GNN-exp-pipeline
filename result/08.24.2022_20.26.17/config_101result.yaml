config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:40:40.910329'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_101fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_101fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_101fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_101fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_101fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_101fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_101fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_101fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_101fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_101fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.8437541127204895
  - 3.363894134759903
  - 3.1857719123363495
  - 3.156756186485291
  - 3.3165477156639103
  - 3.239899748563767
  - 3.2056457936763767
  - 3.1295814990997317
  - 3.6014742791652683
  - 3.239009135961533
  - 3.165684551000595
  - 3.1201751708984378
  - 3.3439301162958146
  - 3.6226518690586094
  - 3.1822039246559144
  - 3.1025093734264377
  - 3.0596414983272555
  - 3.0185614556074145
  - 3.1956227958202366
  validation_losses:
  - 0.7738252878189087
  - 0.40776315331459045
  - 0.3996478021144867
  - 0.3994613289833069
  - 0.408437579870224
  - 0.4287840723991394
  - 0.4099925756454468
  - 0.39822885394096375
  - 0.43256911635398865
  - 0.39097505807876587
  - 0.41381123661994934
  - 0.40305787324905396
  - 0.41622602939605713
  - 0.4017336070537567
  - 0.3999692499637604
  - 0.3979758620262146
  - 0.3897934556007385
  - 0.39973413944244385
  - 0.4068854749202728
loss_records_fold1:
  train_losses:
  - 3.107170331478119
  - 3.4148987889289857
  - 3.064191755652428
  - 3.370633670687676
  - 3.4424289852380756
  - 3.1547350466251376
  - 2.942272531986237
  - 3.0884595274925233
  - 3.034626463055611
  - 3.1059264779090885
  - 3.1600139439105988
  - 3.163617303967476
  - 3.0403650939464573
  - 3.2504067659378055
  - 3.164255952835083
  - 3.0477208495140076
  - 3.0451312541961673
  - 3.018667906522751
  validation_losses:
  - 0.3915124535560608
  - 0.38644060492515564
  - 0.3944491446018219
  - 0.3991643488407135
  - 0.412387490272522
  - 0.3976203203201294
  - 0.4083433449268341
  - 0.39102792739868164
  - 0.4400315582752228
  - 0.39884909987449646
  - 0.3903515338897705
  - 0.4007686376571655
  - 0.39722031354904175
  - 0.4070625603199005
  - 0.398062527179718
  - 0.40726715326309204
  - 0.3947201371192932
  - 0.38790374994277954
loss_records_fold2:
  train_losses:
  - 3.026911675930023
  - 3.0923718452453617
  - 2.9800773054361347
  - 3.128964364528656
  - 3.0148139894008636
  - 3.03396800160408
  - 2.989343798160553
  - 3.023546922206879
  - 2.9838874816894534
  - 3.0701435804367065
  - 3.0614674389362335
  - 3.0221161127090457
  - 3.11926531791687
  - 3.089488768577576
  - 2.9921290040016175
  - 3.0642372548580172
  - 3.0058708727359775
  - 2.9696699380874634
  - 3.0173886716365814
  - 2.971025687456131
  - 3.0372471749782566
  - 3.0264342904090884
  - 2.9782681941986087
  - 2.9668213427066803
  - 3.0328488230705264
  - 3.0153119146823886
  - 3.029174041748047
  - 2.997666144371033
  - 2.96927125453949
  - 2.9187756329774857
  - 2.9943915009498596
  - 3.0034555673599246
  - 3.0140046358108523
  - 3.021612429618836
  - 2.973357078433037
  - 2.9771020114421844
  - 2.9681027233600616
  - 2.878713658452034
  - 3.010505771636963
  - 2.9730696976184845
  - 2.9477868139743806
  - 3.0062231481075288
  - 3.010273665189743
  - 2.9182384461164474
  - 3.0181861966848373
  - 3.0366058349609375
  - 3.0476150065660477
  - 3.003812146186829
  - 2.9733378767967227
  - 2.973007428646088
  - 2.941222667694092
  - 2.9054584562778474
  - 2.9320285558700565
  validation_losses:
  - 0.39578676223754883
  - 0.41754385828971863
  - 0.3927648663520813
  - 0.3999572694301605
  - 0.39007967710494995
  - 0.3930738568305969
  - 0.40611588954925537
  - 0.44604116678237915
  - 0.40221306681632996
  - 0.3919234573841095
  - 0.3941326439380646
  - 0.41002121567726135
  - 0.3850196897983551
  - 0.3895256519317627
  - 0.4140128493309021
  - 0.4133816957473755
  - 0.40715596079826355
  - 0.39946049451828003
  - 0.3825053870677948
  - 0.40936389565467834
  - 0.4092367887496948
  - 0.40842297673225403
  - 0.451487272977829
  - 0.40882164239883423
  - 0.39896219968795776
  - 0.4103774130344391
  - 0.43530938029289246
  - 0.4027611315250397
  - 0.42146119475364685
  - 0.391693115234375
  - 0.4529579281806946
  - 0.4147316515445709
  - 0.40614140033721924
  - 0.425950288772583
  - 0.41036099195480347
  - 0.4470757246017456
  - 0.4148784875869751
  - 0.40201491117477417
  - 0.39691147208213806
  - 0.40735968947410583
  - 0.40965744853019714
  - 0.45262300968170166
  - 0.458127498626709
  - 0.3996738791465759
  - 0.44897451996803284
  - 0.3897032141685486
  - 0.4140625596046448
  - 0.4170030951499939
  - 0.4150700569152832
  - 0.3907443583011627
  - 0.3933843672275543
  - 0.3942432999610901
  - 0.39271628856658936
loss_records_fold3:
  train_losses:
  - 3.0120021879673007
  - 3.015815752744675
  - 3.0269875168800358
  - 3.065921324491501
  - 2.945474696159363
  - 3.094552928209305
  - 3.0301438450813296
  - 3.0060209691524507
  - 2.9843042492866516
  - 3.11407704949379
  - 3.004783010482788
  - 2.9115289330482486
  - 2.9907925188541413
  - 3.065756815671921
  - 2.9821888327598574
  - 2.993944150209427
  - 2.971509310603142
  - 2.934765687584877
  - 2.9711838185787203
  - 2.9862571120262147
  - 2.969953027367592
  - 3.0072863519191744
  - 3.0213658154010776
  - 2.932743000984192
  - 3.0067875623703006
  - 3.03860225379467
  - 2.9594176381826403
  - 2.9652210772037506
  - 2.9604325115680696
  - 2.9860359847545626
  - 2.9582377672195435
  - 3.0075334906578064
  - 2.9656127929687504
  - 2.992743492126465
  - 2.9933951556682588
  - 2.963068422675133
  - 3.0102502167224885
  - 2.932969728112221
  - 2.955884373188019
  - 3.0090689659118652
  - 2.9596272945404056
  - 2.9295224189758304
  - 3.2204038202762604
  - 3.0430516719818117
  - 3.0635508537292484
  - 3.044575744867325
  - 3.0316993951797486
  - 2.898146778345108
  - 2.920344424247742
  - 3.019828009605408
  - 3.0141917169094086
  - 2.9733328163623813
  - 3.0145923614501955
  - 2.930658066272736
  - 3.0007862329483035
  - 3.026727789640427
  - 2.9851814031600954
  - 2.9521263241767883
  - 2.93508762717247
  - 2.9967802286148073
  - 2.9569427937269213
  - 2.9866753667593002
  - 2.9813614666461947
  - 2.951155507564545
  - 2.9152850389480593
  - 2.9705948352813722
  - 3.0637784898281097
  - 2.955498826503754
  - 2.9673479676246646
  - 2.9878439188003543
  - 3.025287675857544
  - 3.0918789088726046
  - 2.9223713755607608
  - 3.0546324014663697
  - 3.019382965564728
  - 3.021832817792893
  - 2.9563527643680576
  - 2.982736712694168
  - 2.8936523318290712
  - 2.9679662883281708
  - 2.958109325170517
  - 3.2679286241531376
  - 2.949633646011353
  - 2.969262254238129
  - 3.02556489109993
  - 2.9647812992334366
  - 2.9938793778419495
  - 2.922308033704758
  - 2.840904086828232
  - 2.952554374933243
  - 2.9987130939960482
  - 2.939781975746155
  - 3.0289864242076874
  - 2.8994615733623506
  - 2.989357137680054
  - 3.012637171149254
  - 2.958606719970703
  - 2.9069137632846833
  - 2.894535553455353
  - 2.904302912950516
  validation_losses:
  - 0.39164814352989197
  - 0.4162497818470001
  - 0.42915526032447815
  - 0.40501323342323303
  - 0.40689617395401
  - 0.4095834791660309
  - 0.6760084629058838
  - 0.7484664916992188
  - 0.4924173057079315
  - 1.5355589389801025
  - 0.42309659719467163
  - 0.7482767701148987
  - 0.5101098418235779
  - 0.38688382506370544
  - 0.39330652356147766
  - 1.6669925451278687
  - 0.5549998879432678
  - 0.6418234705924988
  - 0.39827901124954224
  - 3.2502405643463135
  - 0.5277625322341919
  - 0.4504040777683258
  - 0.6137222051620483
  - 0.41176316142082214
  - 0.43704953789711
  - 0.5226165056228638
  - 0.5005542635917664
  - 2.520817279815674
  - 0.408328115940094
  - 0.5308562517166138
  - 0.45359355211257935
  - 0.44672659039497375
  - 1.127842664718628
  - 0.4333861768245697
  - 0.4148312211036682
  - 0.46853941679000854
  - 0.4647049009799957
  - 0.8370345234870911
  - 2.6515281200408936
  - 0.45363810658454895
  - 14.406373977661133
  - 15.224540710449219
  - 0.39633995294570923
  - 0.3967372179031372
  - 0.4719080328941345
  - 0.5132406949996948
  - 0.47237128019332886
  - 0.5545874834060669
  - 0.48546093702316284
  - 0.40499767661094666
  - 0.4251023232936859
  - 0.5774552822113037
  - 0.5037521719932556
  - 0.43387264013290405
  - 0.41399237513542175
  - 0.5231228470802307
  - 0.40156736969947815
  - 1.260941743850708
  - 3.235055685043335
  - 0.4386933445930481
  - 0.6740488409996033
  - 0.5214992761611938
  - 0.4709146022796631
  - 0.4434302747249603
  - 0.5172814726829529
  - 0.6885082125663757
  - 0.42658722400665283
  - 0.6435556411743164
  - 0.7137966156005859
  - 0.8492823243141174
  - 0.8170439600944519
  - 0.44879770278930664
  - 0.45338883996009827
  - 0.43604034185409546
  - 0.49263709783554077
  - 0.43554165959358215
  - 0.6102349758148193
  - 0.47567105293273926
  - 0.5226830244064331
  - 0.4499196708202362
  - 21.43824005126953
  - 0.42499464750289917
  - 0.41861972212791443
  - 0.6336762309074402
  - 0.4432562589645386
  - 0.521385133266449
  - 0.445677250623703
  - 0.38445547223091125
  - 0.4232717454433441
  - 0.3879227638244629
  - 0.3954395651817322
  - 0.3912976086139679
  - 0.6656896471977234
  - 0.5261154174804688
  - 0.5952580571174622
  - 0.4112716317176819
  - 0.48038533329963684
  - 0.6372610926628113
  - 0.4279060363769531
  - 0.4600570499897003
loss_records_fold4:
  train_losses:
  - 2.952131575345993
  - 2.973160168528557
  - 2.8999377995729447
  - 3.0067502558231354
  - 2.938766932487488
  - 2.959631872177124
  - 3.005946916341782
  - 2.95211678147316
  - 2.9662049502134327
  - 2.9719966650009155
  - 2.9181917279958727
  - 2.9083683073520663
  - 2.994963413476944
  - 3.0009934306144714
  - 2.9076238662004474
  - 2.967126873135567
  - 2.9888456940650943
  - 2.9842807143926624
  - 2.8935039937496185
  - 2.9563957393169407
  - 2.9015839993953705
  - 2.868236941099167
  - 2.962085431814194
  - 2.9363643348217012
  - 2.9467320859432222
  - 2.894663614034653
  - 2.957897746562958
  - 2.9195499598979953
  - 2.976018387079239
  - 2.923541983962059
  - 2.947519373893738
  - 2.9410004675388337
  - 2.878170156478882
  - 3.0428875505924227
  - 2.988472950458527
  - 2.8790032625198365
  - 2.946331346035004
  - 2.967629086971283
  - 2.9806739866733554
  - 3.021658140420914
  - 2.9755071103572845
  - 3.0021641433238986
  - 2.9384846299886704
  - 2.9542093724012375
  - 2.923709619045258
  validation_losses:
  - 0.44736191630363464
  - 0.41872847080230713
  - 0.3984129726886749
  - 0.4760991930961609
  - 0.49081170558929443
  - 0.42039725184440613
  - 0.4156487286090851
  - 0.4262799620628357
  - 0.39657413959503174
  - 0.39397335052490234
  - 0.40434563159942627
  - 0.8095923662185669
  - 0.4170335531234741
  - 0.4302731156349182
  - 0.4850561320781708
  - 0.4376012980937958
  - 0.3889352083206177
  - 0.37316495180130005
  - 0.3917376399040222
  - 0.38899797201156616
  - 0.392964631319046
  - 0.40767842531204224
  - 0.387978732585907
  - 0.3692673444747925
  - 0.40837427973747253
  - 0.48757296800613403
  - 0.4239550232887268
  - 0.4520040452480316
  - 0.3813200294971466
  - 0.3906513750553131
  - 0.4168528914451599
  - 0.39863842725753784
  - 0.41478848457336426
  - 0.3717122972011566
  - 0.39703187346458435
  - 0.40762460231781006
  - 0.42028388381004333
  - 0.41885581612586975
  - 0.5724201202392578
  - 0.3925139904022217
  - 0.3824484050273895
  - 0.3778053820133209
  - 0.3794781267642975
  - 0.3768930733203888
  - 0.3835255205631256
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 53 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8507718696397941,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.02247191011235955, 0.0]'
  mean_eval_accuracy: 0.8565554396326618
  mean_f1_accuracy: 0.00449438202247191
  total_train_time: '0:22:57.592228'
