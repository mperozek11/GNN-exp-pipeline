config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 17:15:42.967054'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/36/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/36/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/36/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/36/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/36/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/36/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/36/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/36/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/36/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/36/fold_4_state_dict.pt
loss_records_fold3:
  train_losses:
  - 7.208092510700226
  - 7.400753021240234
  - 7.425015777349472
  - 7.313822120428085
  - 7.0563410595059395
  - 7.3838682770729065
  - 7.300138711929321
  - 7.453283309936523
  - 7.268079042434692
  - 7.2866190969944
  - 7.402200102806091
  - 7.044615566730499
  - 7.118355348706245
  - 7.171541154384613
  - 7.078374564647675
  - 7.324484050273895
  - 7.3552146553993225
  - 7.102266639471054
  - 7.084877103567123
  - 7.195157885551453
  - 7.221237540245056
  - 7.199968248605728
  - 7.171175867319107
  - 7.065346568822861
  - 7.232092499732971
  - 7.277865946292877
  - 7.314232498407364
  - 7.342529863119125
  - 7.248108506202698
  - 7.153431385755539
  - 7.13876810669899
  - 7.033409893512726
  - 7.015622019767761
  - 6.859642922878265
  - 7.090309947729111
  - 7.174732446670532
  - 7.229275643825531
  - 6.97782701253891
  - 7.132674664258957
  - 7.133276879787445
  - 7.098177522420883
  - 6.967824399471283
  - 7.0699092000722885
  - 6.9489704966545105
  - 6.989894598722458
  - 7.092712342739105
  - 7.000346973538399
  - 6.985706627368927
  - 7.142619282007217
  - 6.943533957004547
  - 7.031505435705185
  - 6.8927586525678635
  - 6.948919415473938
  - 6.932243138551712
  - 6.772361747920513
  - 6.953064918518066
  - 7.16715195775032
  - 6.979114055633545
  - 7.33223769068718
  - 7.377227872610092
  - 7.17694479227066
  - 7.425718903541565
  - 7.367577493190765
  - 7.171454638242722
  - 6.990689665079117
  - 7.170214384794235
  - 7.081995993852615
  - 7.005868911743164
  - 7.114207416772842
  - 6.979120656847954
  - 7.175776183605194
  - 7.2175072729587555
  - 6.914610683917999
  - 6.986152082681656
  - 7.236894935369492
  - 7.164355218410492
  - 6.981228440999985
  - 6.966144174337387
  - 7.29604035615921
  - 7.10515496134758
  - 6.929902642965317
  - 6.994735449552536
  - 6.965497583150864
  - 6.96546471118927
  - 6.8271417915821075
  - 6.926297336816788
  - 6.951416105031967
  - 6.904358729720116
  - 7.012232929468155
  - 7.071872651576996
  - 6.878945544362068
  - 6.920429527759552
  - 6.94225600361824
  - 6.7952752858400345
  - 6.891334682703018
  - 7.017826676368713
  - 6.921823680400848
  - 6.90002378821373
  - 7.04215307533741
  - 6.983794569969177
  validation_losses:
  - 0.3883342146873474
  - 0.3659873306751251
  - 0.3650873899459839
  - 0.3638758361339569
  - 0.38049769401550293
  - 0.4126119911670685
  - 0.38621577620506287
  - 0.3659667670726776
  - 0.8472899794578552
  - 0.3659244179725647
  - 0.37382611632347107
  - 0.5860284566879272
  - 0.6489827036857605
  - 0.7764301896095276
  - 0.6611228585243225
  - 0.3878503739833832
  - 0.7094706892967224
  - 0.9732517600059509
  - 0.43060871958732605
  - 0.4194466173648834
  - 1.2694913148880005
  - 0.5154411792755127
  - 0.5872409343719482
  - 0.5861616134643555
  - 0.4486388862133026
  - 0.45010852813720703
  - 0.6641766428947449
  - 0.7134313583374023
  - 0.7535844445228577
  - 0.4401222765445709
  - 0.5603196620941162
  - 0.5181934237480164
  - 0.42625364661216736
  - 0.6748294830322266
  - 1.0041260719299316
  - 0.727863073348999
  - 0.674977719783783
  - 1.2479795217514038
  - 1.0469248294830322
  - 1.1002804040908813
  - 0.7562962174415588
  - 0.47085660696029663
  - 1.2200641632080078
  - 0.4096314311027527
  - 0.48916906118392944
  - 0.37465786933898926
  - 0.3696270287036896
  - 0.9847217798233032
  - 1.005584716796875
  - 1.0035637617111206
  - 0.8318129777908325
  - 1.0926371812820435
  - 0.5704749822616577
  - 0.49518823623657227
  - 1.2079354524612427
  - 0.7540587186813354
  - 0.9776048064231873
  - 2.407019853591919
  - 0.40148475766181946
  - 0.3721299171447754
  - 0.40471911430358887
  - 0.46847298741340637
  - 0.750666618347168
  - 1.2604310512542725
  - 2.021665096282959
  - 1.362877607345581
  - 1.528347373008728
  - 0.9785869717597961
  - 1.089269757270813
  - 1.1445339918136597
  - 0.44016724824905396
  - 0.6484431028366089
  - 0.4636392295360565
  - 0.9289985299110413
  - 0.569905161857605
  - 0.43999189138412476
  - 0.42323142290115356
  - 0.6100991368293762
  - 0.5993975400924683
  - 0.6796573400497437
  - 0.5275314450263977
  - 0.49798575043678284
  - 0.687214732170105
  - 0.5716366767883301
  - 0.41892048716545105
  - 0.512982189655304
  - 0.7477274537086487
  - 0.39503878355026245
  - 0.9429935812950134
  - 1.1543755531311035
  - 0.5714952945709229
  - 1.3929378986358643
  - 0.7638642191886902
  - 1.0045979022979736
  - 1.875597596168518
  - 0.4166175127029419
  - 0.40607771277427673
  - 0.9778062701225281
  - 0.6059885621070862
  - 0.5385773777961731
loss_records_fold4:
  train_losses:
  - 7.033209413290024
  - 6.937949299812317
  - 6.873700723052025
  - 6.9567751586437225
  - 7.28322646021843
  - 7.0441544353961945
  - 6.865805745124817
  - 6.854253053665161
  - 7.032025992870331
  - 7.324463903903961
  - 7.3775801956653595
  - 7.096940785646439
  - 7.322902500629425
  - 7.057450830936432
  - 7.289202183485031
  - 6.899514228105545
  - 7.05168142914772
  - 6.9469218254089355
  - 6.847803205251694
  - 7.0032919049263
  - 7.132314920425415
  - 6.86949235200882
  - 7.073438763618469
  - 6.955020725727081
  - 7.088974893093109
  - 6.905328273773193
  - 6.963679760694504
  - 6.913952708244324
  - 6.905100494623184
  - 6.647479355335236
  - 6.777404636144638
  - 6.743918254971504
  - 6.834035187959671
  - 6.866338014602661
  - 6.649435728788376
  - 6.690086305141449
  - 6.883584767580032
  - 7.016607165336609
  - 6.961218476295471
  - 6.999633967876434
  - 6.864907801151276
  - 6.792985528707504
  - 6.880593508481979
  - 6.805391699075699
  - 6.73393651843071
  - 6.955445766448975
  - 6.911950320005417
  - 6.866622716188431
  - 6.9947375655174255
  - 7.007583856582642
  - 6.844464123249054
  - 6.8143521547317505
  - 6.747247278690338
  - 6.754901513457298
  - 6.850119054317474
  - 6.8567589819431305
  - 6.705442428588867
  - 6.81744709610939
  - 7.109697982668877
  - 6.951549738645554
  - 6.819774091243744
  - 6.739799797534943
  - 6.685504451394081
  - 6.845055729150772
  - 6.834674596786499
  - 7.057897239923477
  - 6.982214063405991
  - 7.172124534845352
  - 6.914316922426224
  - 6.808462977409363
  - 7.000701397657394
  - 7.341333568096161
  - 7.107117086648941
  - 6.965610653162003
  - 6.853179842233658
  - 7.136841967701912
  - 6.834916144609451
  - 6.794540703296661
  - 7.006626546382904
  - 6.876184403896332
  - 6.862679958343506
  - 6.754142343997955
  - 6.93786808848381
  - 6.808672726154327
  - 6.757348388433456
  - 6.687684893608093
  - 6.639055952429771
  - 6.750826448202133
  - 7.110843479633331
  - 6.852010190486908
  - 7.0250585079193115
  - 7.0494667291641235
  - 6.893590003252029
  - 6.647373855113983
  - 6.7492126524448395
  - 6.919569432735443
  - 7.013472408056259
  - 6.936613917350769
  - 6.721553236246109
  - 6.7612364292144775
  validation_losses:
  - 0.49812963604927063
  - 0.351803183555603
  - 0.4541197717189789
  - 0.4236127734184265
  - 0.4837023615837097
  - 0.48748958110809326
  - 0.5901739001274109
  - 0.34179550409317017
  - 0.4162575304508209
  - 0.39519304037094116
  - 0.3815385401248932
  - 0.3588530123233795
  - 0.5774611830711365
  - 0.7282161116600037
  - 0.5324504971504211
  - 0.3915513753890991
  - 0.3676697909832001
  - 0.3805806338787079
  - 0.38110819458961487
  - 0.5209428668022156
  - 0.5079571008682251
  - 0.5781368613243103
  - 0.3697875738143921
  - 0.3573196530342102
  - 0.3687126636505127
  - 0.6275389194488525
  - 0.47050052881240845
  - 0.3696698546409607
  - 0.45928776264190674
  - 0.3615080416202545
  - 0.47708645462989807
  - 0.641133189201355
  - 0.4951723515987396
  - 0.6126499176025391
  - 0.42395880818367004
  - 0.43957480788230896
  - 0.6357715725898743
  - 0.7601690292358398
  - 0.4906119108200073
  - 0.41921600699424744
  - 0.4064544141292572
  - 0.3972378969192505
  - 0.42277905344963074
  - 0.4133925437927246
  - 0.5113188028335571
  - 0.6141195893287659
  - 0.39838433265686035
  - 0.6482741236686707
  - 0.4223303198814392
  - 0.3894701600074768
  - 0.48202160000801086
  - 0.7533731460571289
  - 0.6043875813484192
  - 0.8981292843818665
  - 0.4232197701931
  - 0.4051249921321869
  - 0.9094294905662537
  - 0.5506137609481812
  - 0.4447154700756073
  - 0.7432511448860168
  - 0.43945470452308655
  - 0.626553475856781
  - 0.5502389669418335
  - 1.3867254257202148
  - 0.3710382878780365
  - 0.3792479932308197
  - 0.4089442491531372
  - 0.4041154384613037
  - 0.4601713716983795
  - 0.3500582277774811
  - 0.371214896440506
  - 0.3895758092403412
  - 0.48699313402175903
  - 0.5324484705924988
  - 0.44644370675086975
  - 0.5216457843780518
  - 0.40703919529914856
  - 0.5368368625640869
  - 0.5531377196311951
  - 0.40771758556365967
  - 0.5295875072479248
  - 0.4814108610153198
  - 0.5042026042938232
  - 0.5286420583724976
  - 0.5862788558006287
  - 0.5510058403015137
  - 0.48653167486190796
  - 0.6423882246017456
  - 0.5470137596130371
  - 0.5086846351623535
  - 0.5076578259468079
  - 0.3700319230556488
  - 0.48689916729927063
  - 0.5323375463485718
  - 0.5640296339988708
  - 0.535892903804779
  - 0.4086509048938751
  - 0.5193704962730408
  - 0.36141616106033325
  - 0.697805643081665
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8061749571183533,
    0.781786941580756]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.2206896551724138, 0.23030303030303031]'
  mean_eval_accuracy: 0.8325151927758425
  mean_f1_accuracy: 0.09019853709508882
  total_train_time: '0:03:21.925067'
