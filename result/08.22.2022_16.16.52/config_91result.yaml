config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:13:08.508678'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_91fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_91fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_91fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_91fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_91fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_91fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_91fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_91fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_91fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_91fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 15.042586779594423
  - 4.448346197605133
  - 3.772519886493683
  - 2.5629022359848026
  - 6.987164306640626
  - 1.9802646040916443
  - 2.758217966556549
  - 2.4712328761816025
  - 2.92310351729393
  - 1.685293734073639
  - 1.2573391020298006
  - 1.0067564845085144
  - 0.9220917105674744
  - 0.8805158555507661
  - 1.0149002969264984
  - 2.5984842777252197
  - 4.446697568893433
  - 1.534646987915039
  - 1.0635149657726288
  - 1.1733470499515535
  - 1.1527975082397461
  - 1.3108112156391145
  - 2.740922045707703
  - 1.5449097156524658
  - 1.1374981582164765
  - 1.028549075126648
  - 1.2220947265625002
  - 1.0200255632400512
  - 0.8889844298362732
  - 1.2080625295639038
  - 1.0091096460819244
  - 1.0544310152530671
  - 0.8754392385482789
  - 2.003429937362671
  - 0.9683905482292176
  - 1.164847904443741
  - 1.113842535018921
  - 0.9375334560871125
  - 1.1114739894866943
  - 0.9679562151432037
  - 1.7353269875049593
  - 0.9434755742549896
  - 0.7911604225635529
  - 0.8714605450630188
  - 0.8615110576152802
  - 0.7891490519046784
  - 0.8955207914113998
  - 2.068617618083954
  - 1.3437616884708405
  - 1.0129043936729432
  - 1.191921329498291
  - 1.0944408893585205
  - 2.0448686242103578
  - 0.8768005311489105
  - 1.984061908721924
  - 1.770162171125412
  - 1.5654421567916872
  - 3.496900147199631
  - 1.8346038401126863
  - 1.3500511169433596
  - 2.3011527836322787
  - 2.0802023291587832
  validation_losses:
  - 2.9049699306488037
  - 1.8077538013458252
  - 1.7451541423797607
  - 3.3140878677368164
  - 2.3907694816589355
  - 0.8484750390052795
  - 2.504082202911377
  - 1.6943289041519165
  - 0.8062376379966736
  - 0.8776959180831909
  - 0.7354181408882141
  - 0.3809502124786377
  - 0.40180057287216187
  - 0.6544615626335144
  - 0.40568944811820984
  - 2.3610236644744873
  - 0.49435195326805115
  - 0.5109854340553284
  - 0.4788818061351776
  - 0.4538797438144684
  - 0.5081726908683777
  - 0.5269538164138794
  - 0.4465215504169464
  - 0.9667474627494812
  - 0.49684056639671326
  - 0.3862117528915405
  - 0.7777193784713745
  - 0.6999974846839905
  - 0.5380645990371704
  - 0.6632066369056702
  - 0.42659932374954224
  - 0.46606361865997314
  - 0.4933711588382721
  - 0.38526132702827454
  - 0.4204244017601013
  - 0.4285077154636383
  - 0.4375278353691101
  - 0.38991227746009827
  - 0.4470936357975006
  - 0.46268999576568604
  - 0.4264771640300751
  - 0.3985763192176819
  - 0.3842569887638092
  - 0.3823273181915283
  - 0.4033305048942566
  - 0.38917553424835205
  - 0.37830233573913574
  - 0.5247830748558044
  - 0.4015193283557892
  - 0.40917477011680603
  - 0.5530880093574524
  - 1.0541584491729736
  - 0.4467533826828003
  - 0.7838642597198486
  - 0.5912293791770935
  - 0.7825222015380859
  - 0.6185246109962463
  - 0.5647191405296326
  - 0.5214048027992249
  - 0.4476596415042877
  - 0.42742490768432617
  - 0.42071986198425293
loss_records_fold1:
  train_losses:
  - 2.157072699069977
  - 1.2981231331825258
  - 1.3088371455669403
  - 1.221038889884949
  - 1.0618500590324402
  - 0.9337127834558487
  - 0.8635395467281342
  - 0.9941724181175232
  - 1.004564595222473
  - 0.9305569186806679
  - 0.8710323631763459
  - 0.935294508934021
  - 0.8029745280742646
  - 0.8733490765094758
  - 0.8166265726089478
  - 1.0402985692024231
  - 1.2622276723384858
  - 1.116834032535553
  - 0.8603591144084931
  - 0.9797288894653321
  - 3.5955633878707887
  - 1.414415502548218
  - 1.1101438164711
  - 0.9586837470531464
  - 0.8550864696502686
  - 0.8180875360965729
  - 0.8559728741645813
  - 0.876662403345108
  - 0.8413026869297028
  - 0.827190911769867
  - 0.8692507386207581
  - 1.1380486845970155
  - 0.8191885232925415
  - 0.8125096738338471
  - 0.8391333878040315
  - 0.7743194222450257
  - 1.0025611519813538
  - 0.9571827530860901
  - 0.901165509223938
  - 0.8369967997074128
  - 0.8306057870388032
  - 0.9279542744159699
  validation_losses:
  - 0.40658730268478394
  - 0.4097221791744232
  - 0.41868913173675537
  - 0.5795360207557678
  - 0.4037795662879944
  - 0.4040486514568329
  - 0.42220479249954224
  - 0.4202158451080322
  - 0.4078834354877472
  - 0.43350544571876526
  - 0.41713154315948486
  - 0.4035840630531311
  - 0.4317404329776764
  - 0.4036910831928253
  - 0.4128955006599426
  - 0.4245184659957886
  - 0.4354628622531891
  - 0.41145429015159607
  - 0.4286428689956665
  - 0.45890188217163086
  - 0.6288340091705322
  - 0.452429860830307
  - 0.4569208025932312
  - 0.49628493189811707
  - 0.4166565239429474
  - 0.43716952204704285
  - 0.40211886167526245
  - 0.41292688250541687
  - 0.39832746982574463
  - 0.399593710899353
  - 0.39855533838272095
  - 0.39892908930778503
  - 0.40901824831962585
  - 0.400943398475647
  - 0.40064194798469543
  - 0.41403520107269287
  - 0.4028680622577667
  - 0.4039371609687805
  - 0.40058380365371704
  - 0.4010031521320343
  - 0.4075768291950226
  - 0.41687363386154175
loss_records_fold2:
  train_losses:
  - 0.9423431634902955
  - 0.814998209476471
  - 0.8944755077362061
  - 0.8688004195690155
  - 1.2362015128135682
  - 0.8976476192474365
  - 0.8196880698204041
  - 0.8676733255386353
  - 0.8563514709472657
  - 0.7579910919070244
  - 0.788589906692505
  - 0.8190983712673188
  - 0.803236883878708
  - 1.0814036905765534
  - 0.7789849162101746
  - 0.8101378619670868
  - 0.9189892947673798
  - 1.1428792536258698
  - 0.889229029417038
  - 0.8844398736953736
  - 0.8312621533870698
  - 0.8352069377899171
  - 0.7855187058448792
  - 0.7895083487033845
  validation_losses:
  - 0.3992980718612671
  - 0.3807961046695709
  - 0.38753771781921387
  - 0.387568861246109
  - 0.3901832699775696
  - 4.537947177886963
  - 1.95854914188385
  - 3.545628786087036
  - 1.4209293127059937
  - 0.49794575572013855
  - 0.3799513876438141
  - 0.40730541944503784
  - 0.38795018196105957
  - 0.399292528629303
  - 0.7023912072181702
  - 0.4939999282360077
  - 0.43639707565307617
  - 1.1115144491195679
  - 0.38431069254875183
  - 0.38698479533195496
  - 0.3844151198863983
  - 0.38342297077178955
  - 0.3852103650569916
  - 0.3826889097690582
loss_records_fold3:
  train_losses:
  - 0.753159186244011
  - 0.7699644505977631
  - 2.6086570084095
  - 1.3235571563243866
  - 0.8550270915031434
  - 0.8445286005735397
  - 0.9381673634052277
  - 1.0909252494573594
  - 1.5304679811000825
  - 2.7331632733345033
  - 0.8402536779642106
  - 3.4660357892513276
  - 1.0959235310554505
  - 0.8205867886543274
  - 1.2164750456809998
  - 1.5032853305339815
  - 1.3027017652988435
  - 1.4001956343650819
  - 0.8010998070240021
  - 0.8301646232604981
  - 0.8472191452980042
  - 0.8146044671535493
  - 0.7908973693847656
  - 0.7851098537445069
  - 0.7766411364078523
  - 0.8826904833316803
  - 0.8368753075599671
  - 0.7812557637691498
  validation_losses:
  - 0.3926238715648651
  - 0.39322686195373535
  - 0.5435646176338196
  - 0.39359134435653687
  - 0.3895660638809204
  - 0.38821181654930115
  - 0.5628691911697388
  - 6.399018287658691
  - 0.4325065016746521
  - 0.39800626039505005
  - 0.45657554268836975
  - 0.44011300802230835
  - 0.40707969665527344
  - 0.39495933055877686
  - 0.39549949765205383
  - 0.3921283781528473
  - 0.4576858878135681
  - 0.41570571064949036
  - 0.3934018015861511
  - 0.3943897783756256
  - 0.39465004205703735
  - 0.40625205636024475
  - 0.39739179611206055
  - 0.3930562138557434
  - 0.39142850041389465
  - 0.39457428455352783
  - 0.3962900638580322
  - 0.40334951877593994
loss_records_fold4:
  train_losses:
  - 0.7870533227920533
  - 0.8088589549064636
  - 0.8649510025978089
  - 1.2526545584201814
  - 0.8212915539741517
  - 0.9250797808170319
  - 0.833953207731247
  - 0.8221762657165528
  - 0.7799195885658264
  - 0.7885247945785523
  - 0.7808802723884583
  - 0.7824656307697296
  validation_losses:
  - 0.40385702252388
  - 0.40138861536979675
  - 0.40474608540534973
  - 0.4049723744392395
  - 0.43178123235702515
  - 0.83599454164505
  - 0.5072484016418457
  - 0.4444114565849304
  - 0.4409215450286865
  - 0.43262454867362976
  - 0.43196359276771545
  - 0.4331505298614502
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 62 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 42 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
training_metrics:
  fold_eval_accs: '[0.8524871355060034, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8575845991523874
  mean_f1_accuracy: 0.0
  total_train_time: '0:13:49.845146'
