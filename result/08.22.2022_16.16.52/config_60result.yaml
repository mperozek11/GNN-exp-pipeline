config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:25:41.077109'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_60fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_60fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_60fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_60fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_60fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_60fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_60fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_60fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_60fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_60fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 92.96887107789517
  - 44.24921324253083
  - 26.8815359801054
  - 30.076941883563997
  - 28.58916197270155
  - 34.36085750758648
  - 18.923264503479004
  - 14.770134872198106
  - 26.500471144914627
  - 18.225410130620002
  - 17.44717315733433
  - 28.498574578762057
  - 15.992192536592484
  - 13.771069815754892
  - 13.428320518136026
  - 15.192125183343888
  - 19.166113549470904
  - 8.541289877891542
  - 8.285510501265525
  - 15.528689232468606
  - 16.511639319360256
  - 11.915772801637651
  - 13.065015894174577
  - 9.684420019388199
  - 7.125442406535149
  - 8.272872841358184
  - 8.058838778734208
  - 7.460059171915055
  - 8.320875683426857
  - 7.907230073213578
  - 6.663597041368485
  - 6.325624802708626
  - 7.956541830301285
  - 8.162767046689988
  - 6.664869666099548
  - 7.149962356686593
  - 6.417522495985032
  - 6.98824580013752
  - 7.182147458195686
  - 6.726469790935517
  - 6.471012625098229
  - 6.581538978219033
  - 6.519890651106834
  - 6.394266530871391
  - 6.578679665923119
  - 6.328774103522301
  - 6.689742374420167
  - 6.616383394598961
  - 6.747220301628113
  - 6.956090849637985
  - 6.435094678401947
  - 6.487591549754143
  - 6.519187185168267
  - 6.443551814556122
  - 6.598466077446938
  - 6.742321568727494
  - 6.561536258459092
  - 6.848055750131607
  - 6.315650045871735
  - 6.443957030773163
  - 6.472058498859406
  - 6.536733177304268
  - 6.7242065429687505
  - 6.325767105817795
  - 6.512578684091569
  - 6.515816098451615
  - 6.387050807476044
  - 6.554911130666733
  - 6.177586033940315
  - 6.314311203360558
  - 6.352875277400017
  - 6.464079731702805
  - 6.690893045067788
  - 6.48582370877266
  - 6.565368133783341
  - 6.706574815511704
  - 6.723345184326172
  - 6.443661120533943
  - 6.4492097258567815
  - 6.357391729950905
  - 6.453220370411874
  - 6.509641107916832
  - 6.448406133055688
  - 6.485596063733102
  - 6.934314492344857
  - 6.490168258547783
  - 6.327866771817208
  - 6.471020808815957
  - 6.237856549024582
  - 6.390668699145317
  - 6.359525620937347
  - 6.349162831902504
  - 6.58515399992466
  - 6.746807503700257
  - 6.754455718398095
  - 7.156943079829216
  - 7.26335281431675
  - 6.433127504587174
  - 6.259629854559899
  - 6.305810776352883
  validation_losses:
  - 1.6703413724899292
  - 0.8881453275680542
  - 0.7651207447052002
  - 0.41480156779289246
  - 0.5521767139434814
  - 0.5835373997688293
  - 0.5552939772605896
  - 0.4305119216442108
  - 0.532661497592926
  - 0.4271102249622345
  - 0.4727626442909241
  - 0.4787273108959198
  - 0.6634681820869446
  - 0.46881815791130066
  - 0.5750753283500671
  - 0.602938175201416
  - 0.425894170999527
  - 0.43427374958992004
  - 0.45322468876838684
  - 1.728890299797058
  - 0.4973263442516327
  - 0.4560670554637909
  - 0.42131128907203674
  - 0.45065778493881226
  - 0.4490509033203125
  - 0.42264264822006226
  - 0.3946821987628937
  - 0.43716275691986084
  - 0.5516210794448853
  - 0.4203880727291107
  - 0.4090331494808197
  - 0.43146705627441406
  - 0.47117486596107483
  - 0.40683603286743164
  - 0.4230763912200928
  - 0.40909960865974426
  - 0.40214207768440247
  - 0.4586353003978729
  - 0.3991014063358307
  - 0.4053763747215271
  - 0.4020496904850006
  - 0.4167345464229584
  - 0.4012543857097626
  - 0.3968813121318817
  - 0.4252738356590271
  - 0.4255799949169159
  - 0.408573180437088
  - 0.44921058416366577
  - 0.6461215615272522
  - 0.40739476680755615
  - 0.4009152948856354
  - 0.5556719303131104
  - 0.3996075689792633
  - 0.4041123390197754
  - 0.42484250664711
  - 0.41020500659942627
  - 0.42207443714141846
  - 0.4077812731266022
  - 0.42449110746383667
  - 0.4329737722873688
  - 0.46623700857162476
  - 0.40624040365219116
  - 1173781.0
  - 18286821376.0
  - 26691702.0
  - 16622566768640.0
  - 0.40973734855651855
  - 0.5035163164138794
  - 0.4111470878124237
  - 0.4205620586872101
  - 0.40213653445243835
  - 5355.0849609375
  - 0.41241076588630676
  - 0.44679945707321167
  - 0.40810078382492065
  - 0.4084416925907135
  - 0.40412619709968567
  - 0.41695502400398254
  - 0.4132183790206909
  - 0.42784515023231506
  - 0.5011006593704224
  - 1145803520.0
  - 85721874432.0
  - 0.41702720522880554
  - 0.42163971066474915
  - 0.4098566472530365
  - 0.40964365005493164
  - 193363520.0
  - 0.4083970785140991
  - 0.3976581394672394
  - 0.42839327454566956
  - 0.46741101145744324
  - 0.4583078622817993
  - 0.4189882278442383
  - 0.42847439646720886
  - 6.030909061431885
  - 3.9023208618164062
  - 274589073408.0
  - 0.40555816888809204
  - 0.4673375189304352
loss_records_fold1:
  train_losses:
  - 6.561234870553017
  - 6.4255644381046295
  - 6.3299692392349245
  - 6.490192294120789
  - 6.3978429436683655
  - 6.401514852046967
  - 6.453623041510582
  - 6.331065657734872
  - 6.558228904008866
  - 6.580303457379341
  - 6.467040351033211
  - 6.383799755573273
  - 6.21004518866539
  - 6.644158655405045
  - 6.375597304105759
  - 6.393066865205765
  - 6.301135689020157
  - 6.192419284582138
  - 6.677438300848007
  - 6.505047166347504
  - 6.478098285198212
  - 6.20468188226223
  - 6.926158758997918
  - 6.371697548031808
  - 6.415824681520462
  - 6.525582167506219
  - 6.377435278892517
  - 6.564773443341256
  - 6.431358259916306
  - 6.5521741062402725
  - 6.466620537638665
  - 6.316029632091523
  - 6.441882205009461
  - 6.496668842434883
  - 6.417984041571618
  - 6.236866655945779
  - 6.689063286781312
  - 6.531101894378662
  - 6.682772383093834
  - 6.5093073636293415
  - 6.301214048266411
  - 6.276632782816887
  - 6.52370606660843
  - 6.267543894052506
  - 6.404698061943055
  - 6.8953900963068016
  - 6.303659525513649
  - 6.444983887672425
  - 6.553723698854447
  - 6.4865115731954575
  - 6.652562552690506
  - 6.367464318871498
  - 6.586498022079468
  - 6.368096852302552
  - 6.2584118694067
  - 6.310325405001641
  - 6.432904773950577
  - 6.539436805248261
  - 6.701116579771043
  - 6.542601162195206
  - 6.518399849534035
  - 6.333621841669083
  - 6.461229792237282
  - 6.358574116230011
  - 6.3007870644330985
  - 6.475208151340485
  - 6.3759399414062505
  - 6.589103931188584
  - 6.5051695883274085
  - 6.696322906017304
  - 6.351622346043587
  - 6.755947974324227
  - 6.237422385811806
  - 6.57157872915268
  - 6.371121126413346
  - 6.557661473751068
  - 6.2635249435901645
  - 6.392877253890038
  - 6.338673326373101
  - 6.360126179456711
  - 6.281524550914765
  - 6.224912303686143
  - 6.176535296440125
  - 6.386291579902172
  - 6.496873742341996
  - 6.507414728403091
  - 6.2981217265129095
  - 6.373105478286743
  - 6.384741714596749
  - 6.3876154899597175
  - 6.162633892893791
  - 6.4482727468013765
  - 6.565815556049348
  - 6.5015900015831
  - 6.6534724056720735
  - 6.406161144375801
  - 6.382025974988938
  - 6.51708388030529
  - 6.5709906905889515
  - 6.216319811344147
  validation_losses:
  - 5582979.0
  - 50465054720.0
  - 0.48692822456359863
  - 0.40793508291244507
  - 0.44095808267593384
  - 0.43417003750801086
  - 0.43094581365585327
  - 0.5080438852310181
  - 0.5095744729042053
  - 0.4336169958114624
  - 1.4113341569900513
  - 0.41931483149528503
  - 0.41825807094573975
  - 0.44830092787742615
  - 0.4237879514694214
  - 0.4435780346393585
  - 0.42701631784439087
  - 0.4389601945877075
  - 0.4622286856174469
  - 0.4316973090171814
  - 0.41863858699798584
  - 0.4084426760673523
  - 242.23550415039062
  - 53500339814400.0
  - 0.4693615138530731
  - 0.43557488918304443
  - 0.4214480221271515
  - 0.4362938106060028
  - 0.42240914702415466
  - 0.569057285785675
  - 0.4147360622882843
  - 0.4284813702106476
  - 0.46625056862831116
  - 0.4208262264728546
  - 0.4780619144439697
  - 0.49368587136268616
  - 0.5110411047935486
  - 0.4559975266456604
  - 0.4301472008228302
  - 0.4457567632198334
  - 0.40109720826148987
  - 0.45283418893814087
  - 0.4578244984149933
  - 0.4831732511520386
  - 0.4995054006576538
  - 0.4375254809856415
  - 0.4281918406486511
  - 0.4288772940635681
  - 0.4278010427951813
  - 0.44322139024734497
  - 0.42610785365104675
  - 0.4368387460708618
  - 0.4053497612476349
  - 0.47115960717201233
  - 0.41731756925582886
  - 0.4360845386981964
  - 0.4503777027130127
  - 0.4268292188644409
  - 0.4587153196334839
  - 0.41965967416763306
  - 0.44748035073280334
  - 0.4143511652946472
  - 0.4582245349884033
  - 0.4369452893733978
  - 0.4356536865234375
  - 0.41354691982269287
  - 0.4531339108943939
  - 0.4716847240924835
  - 0.46679070591926575
  - 0.4324929416179657
  - 0.40595781803131104
  - 0.4266851544380188
  - 0.42698222398757935
  - 0.42029210925102234
  - 0.4099379777908325
  - 0.4829733073711395
  - 0.4267866611480713
  - 0.5161543488502502
  - 0.4189915060997009
  - 0.4213685095310211
  - 0.4570493698120117
  - 0.42928582429885864
  - 0.4700419008731842
  - 0.41005393862724304
  - 0.4373696744441986
  - 0.4177358150482178
  - 0.4195185601711273
  - 0.4274812638759613
  - 0.4395867884159088
  - 0.42303478717803955
  - 0.4232918620109558
  - 0.4325009882450104
  - 0.4379212558269501
  - 0.42215055227279663
  - 0.46563994884490967
  - 0.423501580953598
  - 0.41857030987739563
  - 0.4247991144657135
  - 0.5162250995635986
  - 0.43155980110168457
loss_records_fold2:
  train_losses:
  - 6.589257353544236
  - 6.396789509057999
  - 6.364710965752602
  - 6.434802573919296
  - 6.350878870487214
  - 6.63065277338028
  - 6.4153376728296285
  - 6.372375121712685
  - 6.385630378127098
  - 6.381911152601242
  - 6.948105925321579
  - 6.320519372820854
  - 6.425632479786874
  - 6.404111471772194
  - 6.663129788637161
  - 6.602077370882035
  - 6.341896504163742
  - 6.581530845165253
  - 6.534966534376145
  - 6.658831742405892
  - 6.53191853761673
  - 6.317802014946938
  - 6.379119341075421
  - 6.3702608197927475
  - 6.628114712238312
  - 6.559615537524223
  - 6.411082935333252
  - 6.300372180342674
  - 6.298638325929642
  - 6.4445173203945165
  - 6.298395521938801
  - 6.3473974615335464
  - 6.800387725234032
  - 6.2168894737958915
  - 6.282796657085419
  - 6.482388338446618
  - 6.472655496001244
  - 6.623576876521111
  - 6.56916815340519
  - 6.2684842407703405
  - 6.307662782073021
  - 6.5374575614929205
  - 6.641147756576538
  - 6.293395465612412
  - 6.524500393867493
  - 6.4768794000148775
  - 6.594817408919335
  - 6.430542317032814
  - 6.461301383376122
  - 6.37377514243126
  - 6.441448512673379
  - 6.38527733385563
  - 6.378266459703446
  - 6.215978106856347
  - 6.484307831525803
  - 6.601088783144951
  - 6.561090177297593
  - 6.490048843622208
  - 6.258780547976494
  - 6.581902837753296
  - 6.286021634936333
  - 6.478203311562538
  - 6.42151330113411
  - 6.822807413339615
  - 6.433300215005875
  - 6.473734468221664
  - 6.512185508012772
  - 6.426699167490006
  - 6.426218941807747
  - 6.44745559990406
  - 6.38974176645279
  - 6.528557541966439
  - 6.4417507529258735
  - 6.374386841058731
  - 6.509301626682282
  - 6.417090997099876
  - 6.488433295488358
  - 6.527434754371644
  - 6.356608971953392
  - 6.9109658926725395
  - 6.504191279411316
  - 6.521523316204548
  - 6.414486029744149
  - 6.416003662347794
  - 6.507735228538514
  - 6.754206517338753
  - 6.307862687110902
  - 6.449562814831734
  - 6.342390593886376
  - 6.453085923194886
  - 6.825771445035935
  - 6.941699582338334
  - 6.355925354361535
  - 6.317912432551385
  - 6.2227725267410285
  - 6.520446023344994
  - 6.510763016343117
  - 6.758124929666519
  - 6.392560529708863
  - 6.590530332922936
  validation_losses:
  - 0.4614551067352295
  - 0.4015752077102661
  - 0.3992137908935547
  - 0.40917855501174927
  - 0.499020516872406
  - 0.40995028614997864
  - 0.40836551785469055
  - 0.46445992588996887
  - 0.410331666469574
  - 0.5094266533851624
  - 0.396262526512146
  - 0.4292982220649719
  - 0.410106360912323
  - 0.3983985185623169
  - 0.4515726566314697
  - 0.4038895070552826
  - 0.41557225584983826
  - 0.4173332750797272
  - 0.403333842754364
  - 0.4774490296840668
  - 0.4477654993534088
  - 0.3980295956134796
  - 0.45203790068626404
  - 0.3988553285598755
  - 0.40352359414100647
  - 0.45884138345718384
  - 0.39251837134361267
  - 0.4347115755081177
  - 0.4002658724784851
  - 0.43946945667266846
  - 0.4226740300655365
  - 0.4362223744392395
  - 0.41267335414886475
  - 0.41445302963256836
  - 0.3962848484516144
  - 0.4207306504249573
  - 0.4164658188819885
  - 0.41255679726600647
  - 0.4029349684715271
  - 0.4211004376411438
  - 0.40232524275779724
  - 0.42188018560409546
  - 0.47815757989883423
  - 0.40996864438056946
  - 0.40326154232025146
  - 0.40469157695770264
  - 0.4755350649356842
  - 0.3988761305809021
  - 0.40367817878723145
  - 0.41418778896331787
  - 0.4471525251865387
  - 0.4000234007835388
  - 0.39752301573753357
  - 0.40803030133247375
  - 0.4006844758987427
  - 0.44041934609413147
  - 0.41714194416999817
  - 0.41724893450737
  - 0.396849125623703
  - 0.4096039831638336
  - 0.4196450710296631
  - 0.46769991517066956
  - 0.3976976275444031
  - 0.4090716242790222
  - 0.40235239267349243
  - 0.3916018009185791
  - 0.40724897384643555
  - 0.41785600781440735
  - 0.408027708530426
  - 0.4159316122531891
  - 0.4247720241546631
  - 0.4120223820209503
  - 0.3963097035884857
  - 0.4364282190799713
  - 0.3969919979572296
  - 0.395603209733963
  - 0.41491594910621643
  - 0.4009072780609131
  - 0.40594202280044556
  - 0.47727739810943604
  - 0.4108462929725647
  - 0.41663143038749695
  - 0.41650864481925964
  - 0.4222078323364258
  - 0.5530612468719482
  - 0.4122522175312042
  - 0.43928635120391846
  - 0.43723005056381226
  - 0.3994716703891754
  - 0.47130659222602844
  - 0.4321427047252655
  - 0.41900092363357544
  - 0.39362746477127075
  - 0.41181817650794983
  - 0.4189201593399048
  - 0.4166153073310852
  - 0.4782547950744629
  - 0.4211335778236389
  - 0.4008367657661438
  - 0.4081175923347473
loss_records_fold3:
  train_losses:
  - 6.610785290598869
  - 6.504614087939263
  - 6.283256751298905
  - 6.452467253804207
  - 6.496253934502602
  - 6.658006823062897
  - 6.454410815238953
  - 6.510360091924667
  - 6.482653045654297
  - 6.666021224856377
  - 6.523896411061287
  - 6.553006902337074
  - 6.588299039006234
  - 6.473278766870499
  - 6.323284447193146
  - 6.520551097393036
  - 6.405317115783692
  - 6.690104025602341
  - 6.700064206123352
  - 6.307308325171471
  - 6.356244286894799
  - 6.479030022025109
  - 6.291286885738373
  - 6.694114097952843
  - 6.358781701326371
  - 6.466289579868317
  - 6.396725726127625
  - 6.426418578624726
  - 6.48524004817009
  - 6.298063436150551
  - 6.637231197953224
  - 6.268625056743622
  - 6.363475126028061
  - 6.387898582220078
  - 6.381932899355888
  - 6.318213218450547
  - 6.367150929570198
  - 6.395802274346352
  - 6.420976686477662
  - 6.465858981013298
  - 6.325294899940491
  - 6.689542296528817
  - 6.5248302847146995
  - 6.4802664726972585
  - 6.292044669389725
  - 6.240020248293877
  - 6.338640986382962
  - 6.201377359032631
  - 6.535815712809563
  - 6.735713163018227
  - 6.481214073300362
  - 6.5331308305263525
  - 6.359698674082757
  - 6.36240416765213
  - 6.592699024081231
  - 6.386235305666924
  - 6.178118208050728
  - 6.472881704568863
  - 6.548899471759796
  - 6.438033872842789
  - 6.956623530387879
  - 6.581972274184228
  - 6.498079457879067
  - 6.550215956568718
  - 6.356028160452843
  - 6.39892558157444
  - 6.234147053956986
  - 6.616576179862022
  - 6.566136771440506
  - 6.367828983068467
  - 6.306123396754265
  - 6.351393669843674
  - 6.506118047237397
  - 6.560717876255513
  - 6.292987930774689
  - 6.347664335370064
  - 6.459642010927201
  - 6.405927658081055
  - 6.36064981520176
  validation_losses:
  - 0.4162169396877289
  - 0.42963913083076477
  - 0.411486953496933
  - 0.4150846600532532
  - 0.44395875930786133
  - 0.4902040362358093
  - 0.4213314652442932
  - 0.420786589384079
  - 0.4544469118118286
  - 0.4077889025211334
  - 0.4180854260921478
  - 0.40734317898750305
  - 0.4569045305252075
  - 0.4066546559333801
  - 0.43227076530456543
  - 0.3979054391384125
  - 0.4737909138202667
  - 0.40926089882850647
  - 0.46586111187934875
  - 0.4144684672355652
  - 0.4062778651714325
  - 0.44413524866104126
  - 0.4187667965888977
  - 0.41044408082962036
  - 0.4092041850090027
  - 0.439208060503006
  - 0.45197927951812744
  - 0.4028891324996948
  - 0.40265053510665894
  - 0.40741831064224243
  - 0.43718284368515015
  - 0.4228767156600952
  - 0.41530606150627136
  - 0.4190150201320648
  - 0.48331862688064575
  - 0.41540125012397766
  - 0.4250621795654297
  - 0.49899372458457947
  - 0.4204455018043518
  - 0.43238121271133423
  - 0.4200902581214905
  - 0.5430130958557129
  - 0.4146179258823395
  - 0.40170687437057495
  - 0.4182303249835968
  - 0.4277358651161194
  - 0.4914340376853943
  - 0.4023284912109375
  - 0.45478302240371704
  - 0.4502716362476349
  - 0.42445266246795654
  - 0.4102938175201416
  - 0.4231564402580261
  - 0.40721845626831055
  - 0.4207630157470703
  - 0.4066051244735718
  - 0.4021482765674591
  - 0.47084301710128784
  - 0.46846458315849304
  - 0.39804741740226746
  - 0.43091413378715515
  - 0.43566760420799255
  - 0.41437119245529175
  - 0.43714773654937744
  - 0.40889331698417664
  - 0.4356323778629303
  - 0.44097012281417847
  - 0.42489880323410034
  - 0.424179345369339
  - 0.4220600426197052
  - 0.40428435802459717
  - 0.4723728597164154
  - 0.4878460168838501
  - 0.41685011982917786
  - 0.41813886165618896
  - 0.4255095422267914
  - 0.4345572888851166
  - 0.4371054470539093
  - 0.43484294414520264
loss_records_fold4:
  train_losses:
  - 6.392810034751893
  - 6.587356382608414
  - 6.411929535865784
  - 6.348279061913491
  - 6.488766685128212
  - 6.983118835091592
  - 6.57721355855465
  - 6.886236265301704
  - 6.503268849849701
  - 6.55917803645134
  - 6.19944096505642
  - 6.39022240638733
  - 6.531625536084175
  - 6.381073474884033
  - 6.396453168988228
  - 6.901200988888741
  - 6.529680159687996
  - 6.576695811748505
  - 6.5032640397548676
  - 6.709396985173226
  - 6.483573830127717
  - 6.629626151919365
  validation_losses:
  - 0.4155898094177246
  - 0.40583959221839905
  - 0.40679094195365906
  - 0.4322315752506256
  - 0.3963701128959656
  - 0.4254763722419739
  - 0.41675233840942383
  - 0.4164763391017914
  - 0.42289939522743225
  - 0.40332919359207153
  - 0.42551758885383606
  - 0.4085538983345032
  - 0.4373270869255066
  - 0.4124032258987427
  - 0.40570738911628723
  - 0.4644865095615387
  - 0.43493616580963135
  - 0.4329180121421814
  - 0.4304805397987366
  - 0.4078906774520874
  - 0.4109322428703308
  - 0.4041532576084137
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 79 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:42:32.243119'
