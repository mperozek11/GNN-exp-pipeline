config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 08:33:20.302045'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/156/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/156/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/156/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/156/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/156/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/156/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/156/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/156/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/156/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/156/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 100.88830035924911
  - 36.14695620536804
  - 23.872973561286926
  - 26.47357702255249
  - 23.822667509317398
  - 18.27083131670952
  - 22.7403863966465
  - 19.32341380417347
  - 21.310813546180725
  - 13.039924204349518
  - 15.10265102982521
  - 18.728811591863632
  - 11.008820593357086
  - 12.3371641933918
  - 19.274385392665863
  - 44.48406130075455
  - 67.81760743260384
  - 30.632635861635208
  - 9.878630578517914
  - 9.21704214811325
  - 12.084649056196213
  - 10.688744276762009
  - 39.7269247174263
  - 31.643445789813995
  - 13.19506511092186
  - 8.994415700435638
  - 16.051972776651382
  - 18.53308755159378
  - 33.24998691678047
  - 11.97221353650093
  - 16.30717071890831
  - 10.672522246837616
  - 8.690186321735382
  - 32.98662006855011
  - 18.141097903251648
  - 10.162040680646896
  - 9.613057941198349
  - 9.008862316608429
  - 13.305779367685318
  - 10.336794555187225
  - 14.700224727392197
  - 9.65439322590828
  - 8.749275594949722
  - 14.9914271235466
  - 8.431785374879837
  - 9.75419855862856
  - 9.322498381137848
  - 18.617283299565315
  - 13.194364964962006
  - 10.81245094537735
  - 10.447485029697418
  - 25.234952241182327
  - 9.387385711073875
  - 8.659736633300781
  - 8.869890838861465
  - 9.207967609167099
  - 16.17952424287796
  - 20.8475079536438
  - 11.706980913877487
  - 11.297045290470123
  - 13.073985546827316
  - 15.06271231174469
  - 22.679229229688644
  - 17.948620408773422
  - 10.433700859546661
  - 9.907791584730148
  - 10.348246335983276
  - 10.917128294706345
  - 11.155324041843414
  - 11.212620466947556
  - 9.64789804816246
  - 8.6338851749897
  - 9.782021433115005
  - 11.218311816453934
  - 16.3460590839386
  - 10.593497961759567
  - 8.966025620698929
  - 12.396642327308655
  - 8.444162517786026
  - 9.09147983789444
  - 9.85997423529625
  - 8.18124035000801
  - 9.91734316945076
  - 10.854372382164001
  - 9.31828659772873
  - 8.399991601705551
  - 13.934865832328796
  - 10.800513446331024
  - 8.955724895000458
  - 9.136130303144455
  - 8.344633713364601
  - 8.829791069030762
  - 8.613343983888626
  - 8.529309749603271
  - 8.027481853961945
  - 8.719523191452026
  - 9.027312517166138
  - 9.161035984754562
  - 8.677121132612228
  - 8.303476631641388
  validation_losses:
  - 6.962527275085449
  - 0.9324082732200623
  - 0.6830424666404724
  - 1.1603186130523682
  - 0.7251531481742859
  - 0.6904619336128235
  - 0.5125572681427002
  - 0.7821751832962036
  - 0.6089376211166382
  - 0.8158535957336426
  - 0.7388893365859985
  - 0.5336368680000305
  - 0.41066670417785645
  - 0.46062615513801575
  - 0.47488629817962646
  - 0.5293991565704346
  - 0.6159557104110718
  - 0.44766607880592346
  - 0.403149276971817
  - 0.4128161668777466
  - 0.5661554336547852
  - 0.4661584794521332
  - 0.4856342375278473
  - 0.4236396551132202
  - 0.4538309574127197
  - 0.3948255479335785
  - 0.4087912440299988
  - 0.5242016315460205
  - 0.4059130549430847
  - 0.397154301404953
  - 0.5243257880210876
  - 0.3993312120437622
  - 0.45973241329193115
  - 0.40577125549316406
  - 0.4315625727176666
  - 0.38460639119148254
  - 0.4326265752315521
  - 0.4082675874233246
  - 0.43522703647613525
  - 0.41312164068222046
  - 0.41052886843681335
  - 0.42805054783821106
  - 0.40314197540283203
  - 0.4114960730075836
  - 0.4005182683467865
  - 0.8775614500045776
  - 0.39673763513565063
  - 0.4617665708065033
  - 0.421433687210083
  - 0.4152620732784271
  - 0.3993226885795593
  - 0.40294697880744934
  - 0.4171997606754303
  - 0.4231022596359253
  - 0.3897814154624939
  - 0.3976995348930359
  - 0.4030131697654724
  - 0.3996261656284332
  - 0.41555607318878174
  - 0.45924732089042664
  - 0.4648500382900238
  - 0.4148261845111847
  - 1.4331921339035034
  - 0.6007233262062073
  - 0.421906054019928
  - 0.45889729261398315
  - 0.43616124987602234
  - 0.47682633996009827
  - 0.569118857383728
  - 0.5243725180625916
  - 0.44061365723609924
  - 0.4242557883262634
  - 0.41457507014274597
  - 0.46377506852149963
  - 0.39830249547958374
  - 0.4261893033981323
  - 0.3982507288455963
  - 0.42872101068496704
  - 0.4794147312641144
  - 0.4112725853919983
  - 0.4209173321723938
  - 0.40377315878868103
  - 0.42528244853019714
  - 0.4448171555995941
  - 0.4475293457508087
  - 0.3986944556236267
  - 0.4733971655368805
  - 0.4101722836494446
  - 0.4218286871910095
  - 0.41265952587127686
  - 0.41362857818603516
  - 0.40869414806365967
  - 0.4049493372440338
  - 0.4287493824958801
  - 0.43592846393585205
  - 0.40641018748283386
  - 0.44051608443260193
  - 0.38626629114151
  - 0.4057334363460541
  - 0.41085508465766907
loss_records_fold1:
  train_losses:
  - 8.585649877786636
  - 8.368525475263596
  - 8.04294827580452
  - 8.572545140981674
  - 8.316179871559143
  - 9.05975404381752
  - 8.218410909175873
  - 8.850220501422882
  - 9.12630608677864
  - 8.000485181808472
  - 8.337859511375427
  - 8.421258300542831
  - 8.36138966679573
  - 8.206041902303696
  - 9.173779487609863
  - 8.187926471233368
  - 8.16628684103489
  - 7.948972910642624
  - 8.121034175157547
  - 8.358815640211105
  - 8.051543474197388
  - 8.209351986646652
  - 8.18017429113388
  - 7.934424102306366
  - 8.197853177785873
  - 8.609386950731277
  - 12.787681221961975
  - 9.182459264993668
  - 8.222159177064896
  - 8.155436754226685
  - 8.331983834505081
  - 8.254901736974716
  - 8.521785855293274
  - 8.211719512939453
  - 8.212854653596878
  - 9.01323676109314
  - 8.12303739786148
  - 7.863435357809067
  - 7.766550838947296
  - 8.359744548797607
  - 8.235649973154068
  - 8.385310918092728
  - 8.70320412516594
  - 8.309097975492477
  - 8.289417415857315
  - 8.163244366645813
  - 8.190193742513657
  - 8.44869577884674
  - 8.39204877614975
  - 8.042927145957947
  - 8.14646977186203
  - 8.483134180307388
  - 8.117028832435608
  - 8.157826274633408
  - 10.211594551801682
  - 8.818658888339996
  - 10.324355393648148
  - 14.909917503595352
  - 8.152701735496521
  - 8.306393206119537
  - 8.177117645740509
  - 8.638889253139496
  - 7.874953418970108
  - 8.074991047382355
  - 9.469987660646439
  - 9.85061290860176
  - 9.153230547904968
  - 9.280768007040024
  - 8.25734755396843
  - 8.929963171482086
  - 8.480870187282562
  - 8.072066456079483
  - 8.081498771905899
  - 8.136441081762314
  - 8.206879436969757
  - 11.306201964616776
  - 9.006983935832977
  - 8.144610852003098
  - 8.330800265073776
  - 8.782913982868195
  - 8.248130410909653
  - 8.196911811828613
  - 9.103088766336441
  - 7.930558234453201
  - 8.194349318742752
  - 8.83717930316925
  - 8.349980354309082
  - 9.100749403238297
  - 9.102528303861618
  - 8.258891880512238
  - 8.022847384214401
  - 8.174138873815536
  - 8.81796133518219
  - 7.934510380029678
  - 8.464201360940933
  - 7.905018091201782
  - 8.45981353521347
  - 7.792428642511368
  - 7.920325458049774
  - 7.771498069167137
  validation_losses:
  - 0.4208911657333374
  - 0.44025957584381104
  - 0.4443455934524536
  - 0.4566669762134552
  - 0.42716625332832336
  - 0.42333468794822693
  - 0.4396858513355255
  - 0.43190693855285645
  - 0.4501023590564728
  - 0.4217199683189392
  - 0.43043944239616394
  - 0.4036392271518707
  - 0.41775035858154297
  - 0.4218350648880005
  - 0.4235951006412506
  - 0.41040170192718506
  - 0.42284733057022095
  - 0.41365161538124084
  - 0.42342036962509155
  - 0.43920204043388367
  - 0.41488173604011536
  - 0.41451457142829895
  - 0.4039504826068878
  - 0.5611555576324463
  - 0.4278886616230011
  - 0.4204483926296234
  - 0.43364351987838745
  - 0.41745680570602417
  - 0.44088631868362427
  - 0.4122600257396698
  - 0.47156694531440735
  - 0.41462376713752747
  - 0.41604113578796387
  - 0.4144662320613861
  - 0.41259756684303284
  - 0.4207639992237091
  - 0.46007993817329407
  - 0.4083606004714966
  - 0.4017326235771179
  - 0.411565363407135
  - 0.4456576406955719
  - 0.43390604853630066
  - 0.4666863977909088
  - 0.40654319524765015
  - 0.4237237572669983
  - 0.4135250151157379
  - 0.43642690777778625
  - 0.405500590801239
  - 0.40682247281074524
  - 0.4152475595474243
  - 0.4112383723258972
  - 0.40874361991882324
  - 0.43693533539772034
  - 0.41498830914497375
  - 0.42685258388519287
  - 0.42973825335502625
  - 0.435086727142334
  - 0.40841734409332275
  - 0.42846742272377014
  - 0.414059042930603
  - 0.3949311375617981
  - 0.4070245027542114
  - 0.4471588134765625
  - 0.42564448714256287
  - 0.44159480929374695
  - 0.4750617742538452
  - 0.5156059265136719
  - 0.4041580259799957
  - 0.4362248182296753
  - 0.44829246401786804
  - 0.41169726848602295
  - 0.4238976538181305
  - 0.41051551699638367
  - 0.43266761302948
  - 0.4122842848300934
  - 0.40805310010910034
  - 0.4210703372955322
  - 0.4103330373764038
  - 0.41015151143074036
  - 0.435050904750824
  - 0.4469948410987854
  - 0.4074648916721344
  - 0.42523106932640076
  - 0.5165525674819946
  - 0.44220641255378723
  - 0.4010663628578186
  - 0.5045984983444214
  - 0.4408002495765686
  - 0.4426305890083313
  - 0.4126676321029663
  - 0.4581608772277832
  - 0.48008251190185547
  - 0.41674333810806274
  - 0.42443811893463135
  - 0.4075082838535309
  - 0.4086916148662567
  - 0.4041862487792969
  - 0.4271779954433441
  - 0.40028858184814453
  - 0.4078919589519501
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 32 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:03:58.587028'
