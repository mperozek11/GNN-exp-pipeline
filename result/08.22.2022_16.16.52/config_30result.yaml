config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:45:44.706516'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_30fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_30fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_30fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_30fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_30fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_30fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_30fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_30fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_30fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_30fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 39.918554115295414
  - 9.144321954250335
  - 6.363611817359924
  - 5.088173007965088
  - 3.366056436300278
  - 2.68444459438324
  - 2.5577349901199344
  - 3.3418069541454316
  - 2.4781036496162416
  - 2.528361910581589
  - 2.1682046949863434
  - 2.2217704534530642
  - 2.250637799501419
  - 9.927971893548966
  - 6.93882024884224
  - 3.988986158370972
  - 4.655591660737992
  - 2.7591132462024692
  - 2.465391403436661
  - 3.8091571569442753
  - 2.012203633785248
  - 4.503518146276474
  - 2.2275793671607973
  - 2.0913085758686067
  - 4.630775952339173
  - 4.303368020057678
  - 8.997116732597352
  - 4.660471111536026
  - 2.378245669603348
  - 2.1565347611904144
  - 2.383338361978531
  - 3.0061588048934937
  - 2.226994788646698
  - 2.436366152763367
  - 2.0822629749774935
  - 1.8385369956493378
  - 6.736713922023774
  - 5.935345351696014
  - 3.7622916221618654
  - 4.0006223022937775
  - 4.020261543989181
  - 2.9011993408203125
  - 6.342940723896027
  - 3.2517015635967255
  - 2.4743601441383363
  - 1.9494728535413743
  - 1.7318676531314852
  - 1.7464419037103653
  - 1.665474170446396
  - 2.2504034459590914
  - 1.911239504814148
  - 2.350627601146698
  - 2.029573941230774
  - 2.3002909660339355
  - 2.2186706185340883
  - 2.149864837527275
  - 2.1339316189289095
  - 2.8419877648353578
  - 1.8263273656368257
  - 1.838975763320923
  - 2.152930843830109
  - 2.202870208024979
  - 1.8418945729732514
  - 2.4241777062416077
  - 1.7958938658237458
  - 2.162159460783005
  - 1.9306961476802826
  - 1.9800567209720612
  - 1.9403197646141053
  - 1.6628524601459505
  - 1.8594989120960237
  - 1.7140546560287477
  - 1.9219777822494508
  - 2.9978063106536865
  - 2.1313463866710665
  - 1.8776974737644196
  - 1.7528215765953066
  - 1.624386602640152
  - 1.660098993778229
  - 1.8211710274219515
  - 1.7470330476760865
  - 1.6779874175786973
  - 1.8354301750659943
  - 1.8744327366352083
  - 1.6446290850639345
  - 1.8668823838233948
  - 1.8925239026546479
  - 1.6106839627027512
  - 1.7905111432075502
  - 1.8462420761585236
  - 1.739241397380829
  - 2.0042430818080903
  - 1.8507196247577669
  - 1.835565984249115
  - 2.053769940137863
  - 1.884410560131073
  - 1.654974138736725
  - 1.572861307859421
  - 1.7380725741386414
  - 1.7777782082557678
  validation_losses:
  - 3.3732428550720215
  - 1.8825314044952393
  - 2.5943400859832764
  - 0.6180680990219116
  - 1.2020748853683472
  - 0.4777645170688629
  - 0.5076789855957031
  - 0.5746463537216187
  - 0.41701045632362366
  - 0.47254058718681335
  - 0.40931767225265503
  - 0.3959249258041382
  - 0.7036260962486267
  - 0.5636128187179565
  - 0.42070522904396057
  - 0.4913277328014374
  - 0.45533451437950134
  - 0.4518645405769348
  - 0.44477754831314087
  - 0.5597055554389954
  - 0.486691415309906
  - 0.4559885263442993
  - 0.45785632729530334
  - 0.7301035523414612
  - 0.43165695667266846
  - 0.8565903306007385
  - 0.9348711967468262
  - 0.5445881485939026
  - 0.45624399185180664
  - 0.5220370292663574
  - 0.4846418797969818
  - 0.4760870635509491
  - 0.4628540873527527
  - 0.4009438157081604
  - 0.38889235258102417
  - 0.49400976300239563
  - 0.5207843780517578
  - 0.5265913605690002
  - 0.43990787863731384
  - 0.5194116830825806
  - 0.3923070430755615
  - 0.6020396947860718
  - 0.4591085612773895
  - 0.5405982732772827
  - 0.4199698567390442
  - 0.42427515983581543
  - 0.40703409910202026
  - 0.39427679777145386
  - 0.46891719102859497
  - 0.551649808883667
  - 0.41285088658332825
  - 0.6183356046676636
  - 0.4861028492450714
  - 0.4000885486602783
  - 0.7042349576950073
  - 0.4611108899116516
  - 0.41524848341941833
  - 0.43099892139434814
  - 0.43506762385368347
  - 0.38476690649986267
  - 0.6714550256729126
  - 0.41282370686531067
  - 0.38534900546073914
  - 0.3970915377140045
  - 0.4563294053077698
  - 0.4472355842590332
  - 0.4253056049346924
  - 0.4684966504573822
  - 0.42470842599868774
  - 0.3938354253768921
  - 0.4838349223136902
  - 0.4471587538719177
  - 0.47058725357055664
  - 0.4452577531337738
  - 0.4388975501060486
  - 0.4188362956047058
  - 0.3912849724292755
  - 0.40463367104530334
  - 0.3961164355278015
  - 0.5461381673812866
  - 0.3899770677089691
  - 0.38887766003608704
  - 0.4599795341491699
  - 0.4037018120288849
  - 0.41613975167274475
  - 0.3963490128517151
  - 0.4449705481529236
  - 0.3911110758781433
  - 0.4001238942146301
  - 0.41357672214508057
  - 0.3911546468734741
  - 0.41344332695007324
  - 0.41170984506607056
  - 0.4511171281337738
  - 0.49249958992004395
  - 0.4513002336025238
  - 0.3810614049434662
  - 0.389591246843338
  - 0.41701140999794006
  - 0.39223840832710266
loss_records_fold1:
  train_losses:
  - 1.7844648718833924
  - 1.6795126020908357
  - 1.7380232870578767
  - 1.6229960203170777
  - 1.709980595111847
  - 1.5843563437461854
  - 1.6423825323581696
  - 1.6220474898815156
  - 1.6835441172122956
  - 1.616364371776581
  - 1.6999665558338166
  - 1.9028997451066971
  - 1.9440394163131716
  - 1.6430996894836427
  - 1.8672908544540405
  - 1.8067320227622987
  - 1.693211740255356
  - 2.0711285531520844
  - 1.7307267725467683
  - 1.7988878548145295
  - 1.5870985388755798
  - 1.6456919729709627
  - 1.7135727405548096
  - 1.8895422399044037
  - 2.1798199355602264
  - 1.8908124387264253
  - 1.736112242937088
  - 1.7134772360324861
  - 2.77788280248642
  - 1.6713877201080323
  - 1.6325038552284241
  - 1.6748255491256714
  - 1.911086767911911
  - 1.8109402596950532
  - 1.6252943992614748
  - 1.8463178753852845
  - 1.5921613276004791
  - 1.6644170880317688
  - 1.698560243844986
  validation_losses:
  - 0.4073042869567871
  - 0.44843944907188416
  - 0.4373341202735901
  - 0.4109450578689575
  - 0.4072558581829071
  - 0.40179717540740967
  - 0.40405651926994324
  - 0.4238460958003998
  - 0.4334445297718048
  - 0.4050297141075134
  - 0.40390145778656006
  - 0.5607918500900269
  - 0.41634100675582886
  - 0.5876842141151428
  - 0.43070027232170105
  - 0.4450702667236328
  - 0.3960713744163513
  - 0.4277881681919098
  - 0.40164393186569214
  - 0.43602484464645386
  - 0.4064410626888275
  - 0.3981890380382538
  - 0.4190962314605713
  - 0.5442004203796387
  - 0.4034782350063324
  - 0.4001999795436859
  - 0.4145122170448303
  - 0.4060214161872864
  - 0.43821004033088684
  - 0.4346962869167328
  - 0.414423406124115
  - 0.396453857421875
  - 0.4170144498348236
  - 0.4167543947696686
  - 0.41167935729026794
  - 0.4200962483882904
  - 0.41899165511131287
  - 0.4154009222984314
  - 0.41515257954597473
loss_records_fold2:
  train_losses:
  - 1.6564442276954652
  - 1.659362244606018
  - 1.7080584347248078
  - 1.622365951538086
  - 1.7176689684391022
  - 1.6969698429107667
  - 1.6297442674636842
  - 1.7374152958393099
  - 2.1713179409503938
  - 3.3960610151290895
  - 1.963896667957306
  - 1.8566562175750734
  - 1.6827163934707643
  - 1.8453520953655245
  - 1.665858894586563
  - 1.8276385188102724
  - 1.8187892585992813
  - 2.0864070415496827
  - 1.9535515427589418
  - 1.8453401237726212
  validation_losses:
  - 0.4031144380569458
  - 0.43494224548339844
  - 0.39733120799064636
  - 0.3930521011352539
  - 0.4338330030441284
  - 0.3930584788322449
  - 0.3903079628944397
  - 0.4009139835834503
  - 1.2495980262756348
  - 0.4129452407360077
  - 0.4195350706577301
  - 0.3859821557998657
  - 0.3861941993236542
  - 0.4288143813610077
  - 0.4214363992214203
  - 0.4071982502937317
  - 0.38901931047439575
  - 0.39699235558509827
  - 0.390177458524704
  - 0.39399445056915283
loss_records_fold3:
  train_losses:
  - 1.6588214099407197
  - 1.6023894786834718
  - 1.8318579494953156
  - 2.037501448392868
  - 1.6774447917938233
  - 1.742251342535019
  - 1.627746820449829
  - 1.5646307677030564
  - 1.5724267721176148
  - 1.6038724184036255
  - 1.6262779414653779
  - 1.6378951370716095
  - 1.6269596993923188
  - 1.611319822072983
  - 1.6517677068710328
  - 1.6778897941112518
  - 1.885270255804062
  - 2.082729268074036
  - 2.011183249950409
  - 1.6753515779972077
  - 1.6247355282306672
  - 1.6450330674648286
  - 1.5967786252498628
  - 1.6203567445278169
  - 1.5353645056486132
  - 1.5911852896213532
  - 1.670164406299591
  - 2.2157998859882357
  - 1.854476696252823
  - 1.817255985736847
  - 1.6074207544326784
  - 1.6852653801441193
  - 1.6141291797161104
  - 1.5710790097713472
  - 1.6154170334339142
  - 1.6523180782794953
  - 1.626920795440674
  - 1.5749543130397798
  - 1.6837335884571076
  - 1.6411825239658357
  - 1.7337550640106203
  - 1.714717823266983
  - 1.590490382909775
  - 1.6159255743026735
  - 1.615088176727295
  - 1.7492984771728517
  - 1.6204238414764405
  - 1.6329348981380463
  - 1.6043692409992218
  - 1.677023696899414
  - 1.5839479982852938
  - 1.5775763094425201
  - 1.589617395401001
  - 1.590422695875168
  - 1.6107490897178651
  - 1.5670746088027956
  - 1.6138746321201325
  validation_losses:
  - 0.4017758369445801
  - 0.41219812631607056
  - 0.4338345527648926
  - 0.4277178645133972
  - 0.4372691810131073
  - 0.4444291293621063
  - 0.3873976469039917
  - 0.3961334824562073
  - 0.42913711071014404
  - 0.4033719003200531
  - 0.4026363790035248
  - 0.4408693015575409
  - 0.41345489025115967
  - 0.40880444645881653
  - 0.4497174620628357
  - 0.40508848428726196
  - 0.4000917971134186
  - 0.38156038522720337
  - 0.43070220947265625
  - 0.4185706079006195
  - 0.45147961378097534
  - 0.47457996010780334
  - 0.42975255846977234
  - 0.40472841262817383
  - 0.39638257026672363
  - 0.4073481857776642
  - 0.42037761211395264
  - 0.580255389213562
  - 0.48949888348579407
  - 0.4102006256580353
  - 0.39824607968330383
  - 0.41027069091796875
  - 0.4014318883419037
  - 0.3902551233768463
  - 0.38926056027412415
  - 0.40267282724380493
  - 0.3912283182144165
  - 0.4148881435394287
  - 0.39158502221107483
  - 0.4178365468978882
  - 0.42259839177131653
  - 0.40607208013534546
  - 0.3933681547641754
  - 0.43917351961135864
  - 0.4017481207847595
  - 0.4467751681804657
  - 0.4219778776168823
  - 0.4090708792209625
  - 0.4191630780696869
  - 0.397726833820343
  - 0.4151885211467743
  - 0.3991234302520752
  - 0.39170190691947937
  - 0.392927348613739
  - 0.40186041593551636
  - 0.41107749938964844
  - 0.4002986252307892
loss_records_fold4:
  train_losses:
  - 1.5842920243740082
  - 1.6536503285169601
  - 1.7371859073638918
  - 1.6833386957645418
  - 1.5972182512283326
  - 1.6068290680646897
  - 1.587209701538086
  - 1.6121269345283509
  - 1.5999956727027893
  - 1.62071852684021
  - 1.575294876098633
  - 1.6335864484310152
  - 1.6006296157836915
  - 1.5913898229599
  - 1.6004364609718325
  - 1.5484932243824006
  - 1.58665691614151
  - 1.5618527978658676
  - 1.5607216686010361
  - 1.530274397134781
  - 1.5985486626625063
  - 1.589591807126999
  - 1.6104696989059448
  - 1.5967593491077423
  - 1.5873840093612672
  - 1.614728033542633
  - 1.6058373272418978
  - 1.5820536315441132
  validation_losses:
  - 0.4357588291168213
  - 0.41612517833709717
  - 0.42806896567344666
  - 0.4023164212703705
  - 0.43480023741722107
  - 0.3946912884712219
  - 0.43578872084617615
  - 0.40571632981300354
  - 0.38849422335624695
  - 0.4048469662666321
  - 0.4097651243209839
  - 0.3876609802246094
  - 0.41499149799346924
  - 0.411834716796875
  - 0.39367011189460754
  - 0.4015381932258606
  - 0.38946330547332764
  - 0.39096739888191223
  - 0.40681788325309753
  - 0.41320696473121643
  - 0.38559746742248535
  - 0.41957056522369385
  - 0.3953493535518646
  - 0.38189294934272766
  - 0.38658666610717773
  - 0.3905789256095886
  - 0.3894602060317993
  - 0.384992390871048
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 57 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:20:30.355140'
