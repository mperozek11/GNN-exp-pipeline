config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 00:52:34.106754'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/98/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/98/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/98/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/98/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/98/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/98/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/98/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/98/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/98/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/98/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 14.94833080470562
  - 14.370447471737862
  - 14.294227093458176
  - 14.390863969922066
  - 14.383313447237015
  - 14.16220872104168
  - 14.198784619569778
  - 14.15781007707119
  - 14.381012558937073
  - 14.485547423362732
  - 14.615458399057388
  - 13.972243249416351
  - 14.203244984149933
  - 14.246449962258339
  - 14.175009995698929
  - 13.991909697651863
  - 14.369118988513947
  - 14.289836302399635
  - 14.04356050491333
  - 14.018113493919373
  - 14.02100245654583
  - 13.807945787906647
  - 14.177825331687927
  - 13.978081107139587
  - 14.179393231868744
  - 14.082326263189316
  - 13.875782415270805
  - 13.862573757767677
  - 13.705686926841736
  - 14.025350078940392
  - 13.807913035154343
  - 13.848408669233322
  - 13.930983945727348
  - 13.948671221733093
  - 14.103582292795181
  - 13.820829525589943
  - 14.275702252984047
  - 14.054449573159218
  - 13.971116349101067
  - 13.784448713064194
  - 13.79579445719719
  - 13.923232421278954
  - 13.894644290208817
  - 13.888672947883606
  - 13.612478777766228
  - 13.75132840871811
  - 13.608130410313606
  - 13.623810335993767
  - 13.776042640209198
  - 13.837803781032562
  - 13.692545235157013
  - 13.644289314746857
  - 13.705049574375153
  - 13.6964490711689
  - 13.685048967599869
  - 13.776968613266945
  - 13.683243095874786
  - 13.744729995727539
  - 13.773286581039429
  - 14.81714043021202
  - 14.353749513626099
  - 13.889154523611069
  - 14.102531656622887
  - 14.119598463177681
  - 13.528752893209457
  - 14.192228585481644
  - 14.087155133485794
  - 14.09129935503006
  - 14.074211671948433
  - 14.312518239021301
  - 13.728762716054916
  - 13.614758610725403
  - 13.84945224225521
  - 13.759103149175644
  - 13.71131081879139
  - 13.73243796825409
  - 13.503395527601242
  - 13.80982692539692
  - 13.945870876312256
  - 13.520943701267242
  - 13.623177006840706
  - 13.752752110362053
  - 13.874441012740135
  - 13.43577042222023
  - 13.583586499094963
  - 13.702917441725731
  - 13.599406123161316
  - 13.550959438085556
  - 13.699079692363739
  - 13.625853553414345
  - 13.671043947339058
  - 13.700859472155571
  - 13.73541009426117
  - 13.87685614824295
  - 13.905965000391006
  - 13.492981195449829
  - 13.681581363081932
  - 13.409170791506767
  - 13.576904147863388
  - 13.474980726838112
  validation_losses:
  - 0.4123213291168213
  - 0.4041226804256439
  - 0.3881787359714508
  - 0.42904236912727356
  - 0.44421178102493286
  - 0.4669215679168701
  - 0.39690101146698
  - 0.3995840847492218
  - 0.42442786693573
  - 0.3882969915866852
  - 0.3895830810070038
  - 0.4182451367378235
  - 0.5753432512283325
  - 0.40644222497940063
  - 0.4014301598072052
  - 0.3991139233112335
  - 0.638331949710846
  - 0.5670160055160522
  - 0.5804071426391602
  - 0.5419235229492188
  - 0.4737910032272339
  - 0.612821102142334
  - 0.4437849521636963
  - 0.4090993106365204
  - 0.4103887677192688
  - 0.3956683576107025
  - 0.40709763765335083
  - 0.727127730846405
  - 0.7568264603614807
  - 0.4857138991355896
  - 0.5705441832542419
  - 0.6193631887435913
  - 0.5304043889045715
  - 0.527373731136322
  - 0.7829992175102234
  - 0.7021586298942566
  - 0.4910367429256439
  - 0.39080706238746643
  - 0.403982549905777
  - 0.40200912952423096
  - 0.4527043104171753
  - 0.5401790738105774
  - 0.48631933331489563
  - 0.5155256390571594
  - 0.6514614224433899
  - 0.5281089544296265
  - 1.2638212442398071
  - 0.6875734925270081
  - 0.742550790309906
  - 0.6595836877822876
  - 0.7860897183418274
  - 0.6104528903961182
  - 1.1415024995803833
  - 0.5198931694030762
  - 0.5234068632125854
  - 0.5192914009094238
  - 0.5164366960525513
  - 0.577613890171051
  - 0.49423444271087646
  - 0.4619576930999756
  - 0.6204540133476257
  - 0.6958858966827393
  - 0.48530226945877075
  - 0.5630086064338684
  - 0.7127788662910461
  - 0.41434699296951294
  - 0.5333791375160217
  - 0.5470061898231506
  - 0.42726123332977295
  - 0.9352758526802063
  - 0.5686156749725342
  - 0.5653968453407288
  - 0.5562631487846375
  - 0.5181851387023926
  - 0.699010968208313
  - 0.6726716160774231
  - 0.9429004192352295
  - 0.6214680671691895
  - 0.6742269992828369
  - 0.5301274061203003
  - 0.73231041431427
  - 0.7545027136802673
  - 0.7819637656211853
  - 0.7532399296760559
  - 0.4098908603191376
  - 0.5188255310058594
  - 0.747207760810852
  - 0.7342528700828552
  - 0.4455645680427551
  - 1.6097691059112549
  - 1.368911623954773
  - 0.7159181237220764
  - 0.4146522581577301
  - 0.5580816864967346
  - 0.42736372351646423
  - 0.5825721621513367
  - 0.5479168891906738
  - 0.8683059811592102
  - 0.5266753435134888
  - 0.6466054916381836
loss_records_fold2:
  train_losses:
  - 13.330982282757759
  - 13.788200989365578
  - 13.566276162862778
  - 13.581750333309174
  - 13.985795497894287
  - 13.471090346574783
  - 13.515611320734024
  - 13.578266441822052
  - 13.655732333660126
  - 13.580334693193436
  - 13.708887547254562
  - 13.722689479589462
  - 13.420945048332214
  - 13.425271451473236
  - 13.397075295448303
  - 13.345777213573456
  - 13.453528046607971
  - 13.362247422337532
  - 13.244604408740997
  - 13.404671713709831
  - 13.160865873098373
  - 13.898277670145035
  - 13.379099011421204
  - 13.28701762855053
  - 13.60581287741661
  - 13.164296060800552
  - 13.315174028277397
  - 13.564713090658188
  - 13.407096713781357
  - 13.272753566503525
  - 13.086281910538673
  - 13.383914977312088
  - 13.558094814419746
  - 13.22527927160263
  - 13.183672457933426
  - 13.392494857311249
  - 13.456144705414772
  - 13.277575433254242
  - 13.165474995970726
  - 13.492017537355423
  - 13.197973996400833
  - 13.07454463839531
  - 13.071772992610931
  - 13.164424180984497
  - 13.504576861858368
  - 13.463794559240341
  - 12.955495789647102
  - 13.322178140282631
  - 13.522301599383354
  - 12.876444809138775
  - 13.087691828608513
  - 13.029077306389809
  - 13.109866067767143
  - 13.02071899175644
  - 13.359110981225967
  - 13.173337787389755
  - 13.168771803379059
  - 12.951082453131676
  - 13.239491939544678
  - 12.87338411808014
  - 13.789415493607521
  - 12.916622877120972
  - 13.065700814127922
  - 13.00933463871479
  - 12.882435292005539
  - 13.23110231757164
  - 12.956319645047188
  - 13.197256863117218
  - 13.180828139185905
  - 13.456039756536484
  - 13.068130195140839
  - 12.802564710378647
  - 13.142927169799805
  - 12.852192610502243
  - 12.821740239858627
  - 12.868210673332214
  - 13.009591147303581
  - 13.534408763051033
  - 13.269355043768883
  - 13.130157321691513
  - 13.296432331204414
  - 13.075136572122574
  - 12.857439517974854
  - 13.046969622373581
  - 13.254970356822014
  - 12.908232718706131
  - 13.151484921574593
  - 12.93388757109642
  - 13.014808744192123
  - 12.831286147236824
  - 12.75145411491394
  - 13.126958727836609
  - 12.734142944216728
  - 12.858396157622337
  - 12.46570137143135
  - 12.968967035412788
  - 12.636615499854088
  - 12.96951910853386
  - 12.928468227386475
  - 12.995801627635956
  validation_losses:
  - 0.5386704206466675
  - 0.5009336471557617
  - 0.6563410758972168
  - 0.4586521089076996
  - 0.5323906540870667
  - 0.43961212038993835
  - 0.5588693022727966
  - 0.4586852788925171
  - 0.5667541027069092
  - 0.5421158671379089
  - 0.5693795680999756
  - 0.5116044282913208
  - 0.5352672338485718
  - 0.9234653115272522
  - 0.5474544167518616
  - 0.6685307621955872
  - 0.5479043126106262
  - 0.5238795876502991
  - 0.6136671304702759
  - 0.531377911567688
  - 0.5186695456504822
  - 0.5640329122543335
  - 0.6462422609329224
  - 0.5166464447975159
  - 0.6173570156097412
  - 0.5915181636810303
  - 0.6167197823524475
  - 0.42358237504959106
  - 0.5046700239181519
  - 0.5057948231697083
  - 0.6577440500259399
  - 0.6328126192092896
  - 0.6221809387207031
  - 0.630312979221344
  - 0.632428765296936
  - 0.6955686211585999
  - 0.647561252117157
  - 1.0218570232391357
  - 0.7564761638641357
  - 0.6044067740440369
  - 0.678277313709259
  - 1.0321811437606812
  - 1.1830779314041138
  - 1.0321539640426636
  - 0.5520415902137756
  - 1.0345202684402466
  - 0.5958586931228638
  - 2.0550291538238525
  - 0.6459023356437683
  - 0.5984423756599426
  - 0.6778856515884399
  - 0.6702307462692261
  - 0.5931998491287231
  - 0.7570719122886658
  - 0.7080411314964294
  - 0.6083846688270569
  - 0.5390465259552002
  - 0.648036003112793
  - 0.807894229888916
  - 2.569167137145996
  - 2.0493123531341553
  - 0.9609163999557495
  - 1.328891396522522
  - 0.6618813872337341
  - 0.8532546758651733
  - 0.6026968955993652
  - 2.6767959594726562
  - 0.558258593082428
  - 0.5696071982383728
  - 0.605344831943512
  - 1.4342586994171143
  - 2.3888063430786133
  - 1.1416271924972534
  - 0.6328692436218262
  - 0.7022178769111633
  - 0.6950359344482422
  - 1.2011774778366089
  - 0.7130241394042969
  - 0.8846294283866882
  - 1.5233234167099
  - 0.7431694269180298
  - 0.5927935242652893
  - 0.5693386793136597
  - 0.6560583710670471
  - 0.6381996273994446
  - 0.8011452555656433
  - 0.818564772605896
  - 2.2016146183013916
  - 3.0046629905700684
  - 1.4759502410888672
  - 1.4918107986450195
  - 0.6690109372138977
  - 2.940434694290161
  - 0.6323826313018799
  - 0.7551394104957581
  - 4.94328498840332
  - 0.46488234400749207
  - 0.7724153399467468
  - 0.6488911509513855
  - 0.9109125733375549
loss_records_fold3:
  train_losses:
  - 13.218322649598122
  - 13.570974364876747
  - 12.977307289838791
  - 13.223636418581009
  - 13.565529853105545
  - 13.255344331264496
  - 13.31367202103138
  - 13.22298276424408
  - 13.30721889436245
  - 13.32088777422905
  - 13.313742756843567
  - 12.998755916953087
  - 13.252768442034721
  - 13.231603920459747
  - 13.202692106366158
  - 13.125791683793068
  - 13.039731055498123
  - 13.081553280353546
  - 13.069219335913658
  - 13.207760155200958
  - 13.494770735502243
  - 13.270767241716385
  - 13.262683868408203
  - 13.202301621437073
  - 13.04270824790001
  - 12.863884210586548
  - 12.944387137889862
  - 13.198454797267914
  - 12.900161877274513
  - 12.757488384842873
  - 13.07449471950531
  - 12.868546545505524
  - 13.058638244867325
  - 13.399835795164108
  - 13.162570640444756
  - 13.157214313745499
  - 13.328475296497345
  - 12.746934413909912
  - 13.121638536453247
  - 12.95061182975769
  - 13.009101465344429
  - 12.959649667143822
  - 13.256855547428131
  - 13.111042469739914
  - 12.73879623413086
  - 13.152991712093353
  - 13.078747048974037
  - 12.933738052845001
  - 13.210843592882156
  - 13.322831898927689
  - 12.911109238862991
  - 12.99153096973896
  - 12.882812723517418
  - 12.952277421951294
  - 13.094472780823708
  - 12.894535407423973
  - 12.918955683708191
  - 13.950898066163063
  - 13.533451467752457
  - 13.158513233065605
  - 12.9407849162817
  - 12.940130189061165
  - 12.960001200437546
  - 13.217347919940948
  - 13.149136692285538
  - 12.888661414384842
  - 13.049857839941978
  - 12.82016932964325
  - 12.7622060328722
  - 12.79315510392189
  - 12.65977494418621
  - 12.842487931251526
  - 12.950393632054329
  - 12.896623238921165
  - 12.96461372077465
  - 12.995286002755165
  - 12.897350683808327
  - 12.777017965912819
  - 12.928142175078392
  - 13.072592556476593
  - 13.109247550368309
  - 14.691396236419678
  - 13.9758922457695
  - 13.802846789360046
  - 13.896094858646393
  - 13.658826008439064
  - 13.34443399310112
  - 13.277100026607513
  - 13.457382783293724
  - 13.650707021355629
  - 13.681545451283455
  - 13.659400224685669
  - 13.399637088179588
  - 13.50905293226242
  - 13.32195021212101
  - 13.570301190018654
  - 13.163474202156067
  - 13.481433376669884
  - 13.25044983625412
  - 13.212901517748833
  validation_losses:
  - 0.6861463189125061
  - 0.8900107145309448
  - 0.6112645864486694
  - 0.9239531755447388
  - 0.5631595849990845
  - 0.849309504032135
  - 0.6643599271774292
  - 0.6839219927787781
  - 0.7147684693336487
  - 0.8142966628074646
  - 0.5948367118835449
  - 0.7790870666503906
  - 0.634269118309021
  - 1.6170146465301514
  - 0.5733452439308167
  - 0.5914297699928284
  - 0.5999305844306946
  - 0.583088219165802
  - 0.6081262826919556
  - 0.8217552900314331
  - 0.5834987759590149
  - 0.5120618939399719
  - 0.5223974585533142
  - 0.5419473648071289
  - 0.49815261363983154
  - 0.6331329941749573
  - 0.5940738320350647
  - 0.6016318798065186
  - 0.624431312084198
  - 0.535332441329956
  - 0.5336960554122925
  - 0.5815492868423462
  - 0.5318759679794312
  - 0.5506256818771362
  - 0.6001607775688171
  - 0.5710595846176147
  - 0.5754363536834717
  - 0.5142260789871216
  - 0.5542082190513611
  - 0.6681932806968689
  - 0.621083676815033
  - 1.0726145505905151
  - 1.3642692565917969
  - 0.8136746287345886
  - 0.5210319757461548
  - 0.642288863658905
  - 0.8366150856018066
  - 0.8224970698356628
  - 1.0616084337234497
  - 0.6098776459693909
  - 0.7911962866783142
  - 0.9471454620361328
  - 1.080483317375183
  - 0.9993945956230164
  - 0.8575088977813721
  - 0.7899563312530518
  - 0.6333122849464417
  - 0.5600863695144653
  - 0.6015925407409668
  - 0.662585437297821
  - 0.5724917650222778
  - 0.7037280201911926
  - 0.7441306710243225
  - 0.5877341032028198
  - 0.7114768028259277
  - 1.075940489768982
  - 0.6944051384925842
  - 0.7741596102714539
  - 0.821876585483551
  - 0.7062469124794006
  - 1.0335149765014648
  - 1.023997187614441
  - 1.4810577630996704
  - 0.9952627420425415
  - 2.3952596187591553
  - 2.1447887420654297
  - 0.7918277382850647
  - 0.6317751407623291
  - 0.5788500308990479
  - 0.6775213479995728
  - 0.8237568736076355
  - 0.4374617636203766
  - 0.4134994447231293
  - 0.4172913730144501
  - 0.37349259853363037
  - 0.3965291976928711
  - 0.41560110449790955
  - 0.40550917387008667
  - 0.41865041851997375
  - 0.4749937057495117
  - 0.4823998510837555
  - 0.41471001505851746
  - 0.45988568663597107
  - 0.45642247796058655
  - 0.45316436886787415
  - 0.4133599102497101
  - 0.4721788763999939
  - 0.4420594871044159
  - 0.4404968023300171
  - 0.47701552510261536
loss_records_fold4:
  train_losses:
  - 13.510632395744324
  - 13.547625616192818
  - 13.49155044555664
  - 13.465878739953041
  - 13.585042223334312
  - 13.715715125203133
  - 13.461368530988693
  - 13.016498386859894
  - 13.488943070173264
  - 13.376478061079979
  - 13.498259425163269
  - 13.455395117402077
  - 13.393338337540627
  - 13.41619299352169
  - 13.564477980136871
  - 13.177336141467094
  - 13.42763340473175
  - 13.244809821248055
  - 13.343886628746986
  - 13.2597014605999
  - 13.327666893601418
  - 13.211536526679993
  - 13.287326037883759
  - 13.191238299012184
  - 13.094911009073257
  - 13.03574986755848
  - 13.517128929495811
  - 13.398605778813362
  - 13.291286960244179
  - 13.334371700882912
  - 13.554563373327255
  - 13.516966879367828
  - 13.11854374408722
  - 13.17292696237564
  - 13.339118599891663
  - 13.153167515993118
  - 13.173016741871834
  - 13.431917011737823
  - 13.439274117350578
  - 13.465334072709084
  - 13.369806975126266
  - 13.299623191356659
  - 13.180584818124771
  - 13.15911115705967
  - 13.255595937371254
  - 13.178514957427979
  - 13.05679526925087
  - 13.049754604697227
  - 13.137950837612152
  - 13.334489986300468
  - 13.273514300584793
  - 13.102171570062637
  - 13.320950016379356
  - 13.156380534172058
  - 13.508104652166367
  - 13.22095313668251
  - 13.880703374743462
  - 13.533821627497673
  - 13.194467559456825
  - 13.233732730150223
  - 13.093822807073593
  - 13.33825869858265
  - 13.236825302243233
  - 13.375608205795288
  - 13.00885172188282
  - 12.981512635946274
  - 13.147681668400764
  - 13.061095282435417
  - 13.125192523002625
  - 13.103905335068703
  - 13.224155679345131
  - 12.867008373141289
  - 13.05366998910904
  - 13.072117641568184
  - 13.06817078590393
  - 13.072769314050674
  - 13.306811466813087
  - 13.136815041303635
  - 12.846454083919525
  - 13.098234593868256
  - 12.991865113377571
  - 12.992928966879845
  - 13.281469494104385
  - 12.853051275014877
  - 13.164957895874977
  - 12.79183979332447
  - 12.898767650127411
  - 13.355691760778427
  - 13.017743796110153
  - 13.168017417192459
  - 12.818367809057236
  - 12.85816103219986
  - 12.977855876088142
  - 14.724281623959541
  - 13.114250019192696
  - 13.14892229437828
  - 12.785652860999107
  - 12.912911608815193
  - 13.158851623535156
  - 13.347383677959442
  validation_losses:
  - 0.5375315546989441
  - 0.5089388489723206
  - 0.608076810836792
  - 0.49197331070899963
  - 0.4608839154243469
  - 0.36863046884536743
  - 0.40062379837036133
  - 0.40891098976135254
  - 0.5305229425430298
  - 0.4701485335826874
  - 0.5041758418083191
  - 0.4676652252674103
  - 0.3830861449241638
  - 0.456083208322525
  - 0.4388309419155121
  - 0.48444390296936035
  - 0.4336640238761902
  - 0.3884340524673462
  - 0.5400253534317017
  - 0.46040821075439453
  - 0.508953332901001
  - 0.5675426721572876
  - 0.4861356019973755
  - 0.44062694907188416
  - 0.445087194442749
  - 0.5711226463317871
  - 0.4680694043636322
  - 0.3862122893333435
  - 0.5166410803794861
  - 0.5121533870697021
  - 0.4457516074180603
  - 0.5080239772796631
  - 0.5139259696006775
  - 0.4627772569656372
  - 0.43253546953201294
  - 0.5302698016166687
  - 0.7438853979110718
  - 0.6068170666694641
  - 0.4559935927391052
  - 0.4691019654273987
  - 0.4770163893699646
  - 0.5316256284713745
  - 0.5222486853599548
  - 0.4104169011116028
  - 0.4795721471309662
  - 0.513251781463623
  - 0.4720648229122162
  - 0.5130029320716858
  - 0.41100984811782837
  - 0.5851067304611206
  - 0.46190550923347473
  - 0.4317779541015625
  - 0.4801890254020691
  - 0.426035076379776
  - 0.7889875173568726
  - 0.5958050489425659
  - 0.496443510055542
  - 0.4637787938117981
  - 0.45213836431503296
  - 0.5371412634849548
  - 0.5429790616035461
  - 0.4862886965274811
  - 0.5138506889343262
  - 0.40020060539245605
  - 0.5086641907691956
  - 0.46768084168434143
  - 0.5753787159919739
  - 0.5786948800086975
  - 0.6052126884460449
  - 0.47896867990493774
  - 0.5074876546859741
  - 0.45949363708496094
  - 0.6428519487380981
  - 0.5555211305618286
  - 0.43753352761268616
  - 0.5807307362556458
  - 0.49242761731147766
  - 0.7504431009292603
  - 0.4857636094093323
  - 0.5554693341255188
  - 0.5524017214775085
  - 0.7435990571975708
  - 0.7804721593856812
  - 0.5133141875267029
  - 0.5948161482810974
  - 0.529586911201477
  - 0.540016770362854
  - 0.46774691343307495
  - 0.5296614170074463
  - 0.5778383612632751
  - 0.5452485084533691
  - 0.5401572585105896
  - 0.7388030886650085
  - 0.5446019172668457
  - 0.5867772102355957
  - 0.5784270167350769
  - 0.49777165055274963
  - 0.4518284201622009
  - 0.48443886637687683
  - 0.46262791752815247
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8421955403087479, 0.8027444253859348, 0.79073756432247,
    0.8058419243986255]'
  fold_eval_f1: '[0.0, 0.11538461538461538, 0.30303030303030304, 0.24691358024691357,
    0.2097902097902098]'
  mean_eval_accuracy: 0.8198304775040819
  mean_f1_accuracy: 0.17502374169040835
  total_train_time: '0:11:11.702836'
