config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:40:42.300291'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_25fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_25fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_25fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_25fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_25fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_25fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_25fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_25fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_25fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_25fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 37.896902865171434
  - 9.747927406430245
  - 5.980509352684021
  - 6.2121207356452945
  - 6.216519325971603
  - 4.083613407611847
  - 6.041042101383209
  - 12.20104872584343
  - 6.8024001121521
  - 4.114046338200569
  - 6.038704439997673
  - 4.302774602174759
  - 3.554677999019623
  - 5.924367815256119
  - 5.235873448848725
  - 4.384810778498649
  - 5.484476640820503
  - 6.398616075515747
  - 5.867735952138901
  - 4.852202636003494
  - 3.7708803713321686
  - 4.209996739029885
  - 3.4914270222187045
  - 3.92489427626133
  - 3.204634517431259
  - 3.4747994601726533
  - 4.467029339075089
  - 4.537359189987183
  - 3.503736007213593
  - 3.100349667668343
  - 2.963348758220673
  - 4.103170147538186
  - 3.034236431121826
  - 3.8016877084970475
  - 3.6688272744417194
  - 3.123486712574959
  - 3.8740147709846497
  - 3.360833412408829
  - 3.8985374182462693
  - 3.8526863515377046
  - 3.478459060192108
  - 4.0824977517128
  - 3.4751973837614063
  - 5.14406775534153
  - 2.994140011072159
  - 2.914337232708931
  - 3.550451934337616
  - 3.1402313709259033
  - 2.9675663113594055
  - 3.2542785853147507
  - 3.6574639081954956
  - 3.5814522445201877
  - 3.1264055714011194
  - 3.671954667568207
  - 3.9960005640983582
  - 3.4624613642692568
  - 3.193923541903496
  - 3.41125807762146
  - 3.4265446960926056
  - 3.2655319005250933
  - 3.1449145674705505
  - 3.1054335236549377
  - 2.8475040048360825
  validation_losses:
  - 1.6723108291625977
  - 0.4680725634098053
  - 0.479503870010376
  - 0.5590655207633972
  - 0.42738062143325806
  - 0.42826980352401733
  - 0.5542421936988831
  - 1.8118549585342407
  - 0.5405133366584778
  - 0.4250190556049347
  - 0.4560001492500305
  - 0.4417288303375244
  - 0.49673065543174744
  - 0.3939928412437439
  - 0.3787105977535248
  - 0.39690452814102173
  - 0.4523068964481354
  - 0.40555310249328613
  - 0.3879658579826355
  - 0.3933711647987366
  - 0.402795672416687
  - 0.38128143548965454
  - 0.4634266495704651
  - 0.3751179575920105
  - 0.3736611008644104
  - 0.5249853730201721
  - 0.5842607021331787
  - 0.3820444941520691
  - 0.40535375475883484
  - 0.464166522026062
  - 0.3781014382839203
  - 0.3859727680683136
  - 0.44066953659057617
  - 0.37928125262260437
  - 0.4619217813014984
  - 0.40325871109962463
  - 0.43331968784332275
  - 0.38153958320617676
  - 0.3847874104976654
  - 0.4068727195262909
  - 0.3786560893058777
  - 0.3779204189777374
  - 0.414905846118927
  - 0.3799772262573242
  - 0.3959271013736725
  - 0.38480502367019653
  - 0.4242340922355652
  - 0.37996751070022583
  - 0.4672951102256775
  - 0.3978317677974701
  - 0.3908654749393463
  - 0.41020599007606506
  - 0.38675451278686523
  - 0.38122493028640747
  - 0.5786305665969849
  - 0.3837639391422272
  - 0.41218093037605286
  - 0.38812536001205444
  - 0.3904774487018585
  - 0.38276103138923645
  - 0.3861238956451416
  - 0.3896976113319397
  - 0.38494420051574707
loss_records_fold1:
  train_losses:
  - 3.419529438018799
  - 4.019200769066811
  - 3.273503488302231
  - 3.068341827392578
  - 3.0378820240497593
  - 3.1969874382019046
  - 2.891141223907471
  - 2.8331272304058075
  - 2.8487870305776597
  - 2.7786813974380493
  - 3.0289174497127536
  - 2.841422599554062
  - 2.9675289511680605
  - 2.948801773786545
  - 3.013026171922684
  - 2.8934857100248337
  - 2.8920448184013368
  - 2.9000045448541645
  - 3.152230340242386
  - 3.022048771381378
  - 2.862654346227646
  - 2.8510788887739182
  - 2.881519037485123
  - 2.9432108581066134
  - 2.911478891968727
  - 2.939737004041672
  - 2.8838024348020554
  - 2.9270957708358765
  - 2.9685898572206497
  - 2.881765788793564
  - 2.8779681742191316
  - 2.893709307909012
  - 2.826399341225624
  - 2.8859750628471375
  - 2.975402307510376
  - 2.8570100724697114
  - 2.8991063416004184
  - 2.8269742846488954
  validation_losses:
  - 0.4575924575328827
  - 0.39543023705482483
  - 0.4126731753349304
  - 0.44292202591896057
  - 0.39778804779052734
  - 0.41433191299438477
  - 0.39519959688186646
  - 0.41094860434532166
  - 0.3973749577999115
  - 0.3960925042629242
  - 0.39885592460632324
  - 0.40097516775131226
  - 0.4023796021938324
  - 0.4243103861808777
  - 0.39451879262924194
  - 0.4114457666873932
  - 0.3967435359954834
  - 0.43845677375793457
  - 0.46116480231285095
  - 0.4193964898586273
  - 0.4050990343093872
  - 0.4470098614692688
  - 0.39685043692588806
  - 0.40441304445266724
  - 0.6284290552139282
  - 0.44685447216033936
  - 0.40510353446006775
  - 0.3946917653083801
  - 0.4086068868637085
  - 0.4096701145172119
  - 0.41813164949417114
  - 0.44872942566871643
  - 0.41234147548675537
  - 0.39570239186286926
  - 0.39929670095443726
  - 0.4057306945323944
  - 0.40332362055778503
  - 0.39813709259033203
loss_records_fold2:
  train_losses:
  - 2.9164442479610444
  - 2.932683172821999
  - 2.970339918136597
  - 2.913428157567978
  - 3.153974696993828
  - 2.894361430406571
  - 2.853237345814705
  - 3.030096459388733
  - 2.8439803421497345
  - 2.950287139415741
  - 2.9365419492125513
  - 3.150669497251511
  - 2.9478046327829364
  - 2.9277917176485064
  - 3.0448679566383365
  - 3.094525483250618
  - 3.0537238240242006
  - 3.051816260814667
  - 3.019760835170746
  - 3.0197012662887577
  - 2.967112362384796
  - 2.98536371588707
  - 2.9584671676158907
  - 3.1149064838886265
  - 3.0233719646930695
  - 2.926017290353775
  - 3.0000758171081543
  validation_losses:
  - 0.4283277988433838
  - 0.388186514377594
  - 0.376382052898407
  - 0.37401077151298523
  - 0.3775988817214966
  - 0.38761574029922485
  - 0.3759608268737793
  - 0.3809971511363983
  - 0.3863811790943146
  - 0.4182838201522827
  - 0.5191633105278015
  - 0.4135950207710266
  - 0.3830048143863678
  - 0.4357968866825104
  - 0.38810449838638306
  - 0.3833385705947876
  - 0.43931886553764343
  - 0.9159220457077026
  - 0.4204641878604889
  - 0.3854835629463196
  - 0.4016573429107666
  - 0.3823772072792053
  - 0.38361313939094543
  - 0.3834382891654968
  - 0.39005792140960693
  - 0.3865543007850647
  - 0.3830198645591736
loss_records_fold3:
  train_losses:
  - 3.005852371454239
  - 2.9257491290569306
  - 2.943998256325722
  - 2.9937536597251895
  - 2.955443048477173
  - 2.9065072536468506
  - 2.9760333329439166
  - 2.996445941925049
  - 2.9735363066196445
  - 2.9379699140787126
  - 2.931986922025681
  - 2.943668794631958
  - 2.9207824856042865
  - 3.046969258785248
  - 2.936549258232117
  - 3.043430978059769
  - 2.979921728372574
  validation_losses:
  - 0.39884957671165466
  - 0.39526495337486267
  - 0.3986443877220154
  - 0.3979414403438568
  - 0.394375205039978
  - 0.4065239727497101
  - 0.40203046798706055
  - 0.39739206433296204
  - 0.4056394100189209
  - 0.3997844159603119
  - 0.42133593559265137
  - 0.39915570616722107
  - 0.3966614603996277
  - 0.4007066488265991
  - 0.3941076695919037
  - 0.3969201445579529
  - 0.39468011260032654
loss_records_fold4:
  train_losses:
  - 3.0724631518125536
  - 3.0311394035816193
  - 2.9962786912918093
  - 3.0684848695993425
  - 2.9224039733409883
  - 3.1054156482219697
  - 2.9980402410030367
  - 3.1604602545499803
  - 3.043411946296692
  - 2.9803733944892885
  - 2.96005551815033
  - 2.9670370638370516
  - 2.91444970369339
  - 2.9555853813886643
  validation_losses:
  - 0.41033869981765747
  - 0.3925391733646393
  - 0.40833306312561035
  - 0.39300668239593506
  - 0.3908574879169464
  - 0.42917707562446594
  - 0.39477622509002686
  - 0.41332390904426575
  - 0.3933059275150299
  - 0.3904349207878113
  - 0.3967691957950592
  - 0.39369648694992065
  - 0.3904860317707062
  - 0.3995814025402069
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 63 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 38 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:14:13.368444'
