config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 05:39:29.012196'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/132/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/132/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/132/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/132/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/132/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/132/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/132/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/132/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/132/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/132/fold_4_state_dict.pt
loss_records_fold2:
  train_losses:
  - 7.201457262039185
  - 7.035941332578659
  - 7.347706705331802
  - 7.144797325134277
  - 7.1297924518585205
  - 7.1306911408901215
  - 7.103740096092224
  - 6.9988797307014465
  - 6.894594222307205
  - 7.040388107299805
  - 6.955635130405426
  - 6.900157630443573
  - 7.022807389497757
  - 6.992324709892273
  - 7.113797202706337
  - 6.983224123716354
  - 6.955436497926712
  - 6.8363532572984695
  - 7.042941689491272
  - 6.957036703824997
  - 6.774345561861992
  - 7.252609074115753
  - 7.066322222352028
  - 7.050599098205566
  - 7.015635371208191
  - 7.040572226047516
  - 7.021522551774979
  - 7.019645065069199
  - 6.941550135612488
  - 7.070677801966667
  - 7.005863606929779
  - 6.831825941801071
  - 7.0315419137477875
  - 6.960789233446121
  - 7.0734817534685135
  - 6.861572444438934
  - 7.106802046298981
  - 7.089869916439056
  - 7.242754012346268
  - 6.938678205013275
  - 6.891172617673874
  - 7.0579584538936615
  - 6.962969332933426
  - 6.725875809788704
  - 6.760025963187218
  - 7.053655564785004
  - 7.166107028722763
  - 7.09377658367157
  - 6.909164726734161
  - 6.807660385966301
  - 6.969767868518829
  - 7.261926084756851
  - 6.884107291698456
  - 6.973988831043243
  - 7.072422951459885
  - 6.947301417589188
  - 6.736854329705238
  - 6.734963849186897
  - 6.7250480353832245
  - 6.940246999263763
  - 6.977108836174011
  - 6.879397034645081
  - 7.158414751291275
  - 7.250113427639008
  - 7.174887865781784
  - 6.862837553024292
  - 6.766775399446487
  - 6.7414330542087555
  - 6.722430765628815
  - 6.695861399173737
  - 6.87441685795784
  - 6.879220634698868
  - 6.723141193389893
  - 6.62995482981205
  - 6.692100495100021
  - 6.754278734326363
  - 7.082136243581772
  - 6.865296959877014
  - 6.817695915699005
  - 6.876151964068413
  - 6.796994462609291
  - 6.630984529852867
  - 6.854291707277298
  - 6.981329649686813
  - 7.0585967898368835
  - 6.854820117354393
  - 6.716675087809563
  - 6.708181083202362
  - 6.789265424013138
  - 6.691088378429413
  - 6.6009975373744965
  - 6.874891400337219
  - 6.802301436662674
  - 6.735601305961609
  - 6.735732316970825
  - 6.468719989061356
  - 6.7969235479831696
  - 6.600183308124542
  - 6.915840804576874
  - 6.753416270017624
  validation_losses:
  - 0.38046321272850037
  - 0.37996944785118103
  - 0.3790154755115509
  - 0.39241108298301697
  - 0.4868599772453308
  - 0.6120432615280151
  - 1.0495007038116455
  - 0.748263418674469
  - 1.0798802375793457
  - 1.8597732782363892
  - 0.47464871406555176
  - 0.752529501914978
  - 0.4627278447151184
  - 0.8606705069541931
  - 0.4817579984664917
  - 0.4369703233242035
  - 0.46846744418144226
  - 0.41058194637298584
  - 0.6534253358840942
  - 0.5156552791595459
  - 0.6553957462310791
  - 0.4675019383430481
  - 0.389230877161026
  - 0.4760596752166748
  - 0.4575905203819275
  - 0.44905272126197815
  - 0.38236039876937866
  - 0.39366278052330017
  - 0.37867075204849243
  - 0.4898512065410614
  - 0.4650934934616089
  - 0.4661894142627716
  - 0.5310311317443848
  - 0.8392912149429321
  - 0.6059570908546448
  - 0.3862076699733734
  - 0.9534175395965576
  - 0.9765148162841797
  - 0.9349691867828369
  - 0.5641303658485413
  - 0.5792415738105774
  - 0.4335891008377075
  - 0.7819562554359436
  - 1.4211922883987427
  - 0.8102256059646606
  - 1.0101544857025146
  - 0.3923976719379425
  - 0.42176467180252075
  - 0.45162007212638855
  - 0.4442448019981384
  - 0.6811767816543579
  - 0.8288432955741882
  - 0.6793884634971619
  - 0.6586908102035522
  - 0.3883551359176636
  - 0.379195898771286
  - 0.4776305854320526
  - 0.46693119406700134
  - 0.4754992127418518
  - 0.5503625273704529
  - 0.4699961543083191
  - 0.7111378908157349
  - 0.49766284227371216
  - 0.7650796175003052
  - 0.4815082550048828
  - 0.4903509020805359
  - 1.2789287567138672
  - 0.3802781403064728
  - 0.4280450642108917
  - 1.3492540121078491
  - 0.40458375215530396
  - 0.6609451174736023
  - 0.710880696773529
  - 1.9771603345870972
  - 0.6073287725448608
  - 0.5084699988365173
  - 0.38340502977371216
  - 0.3885115087032318
  - 0.428573340177536
  - 0.7663477063179016
  - 0.703594446182251
  - 0.8821887969970703
  - 1.1556050777435303
  - 0.38050442934036255
  - 0.415229469537735
  - 0.4591701030731201
  - 0.674426257610321
  - 0.4664973020553589
  - 0.4865964651107788
  - 0.6984317302703857
  - 0.5759791135787964
  - 0.485318660736084
  - 0.48611682653427124
  - 1.0785316228866577
  - 0.7601756453514099
  - 0.865095317363739
  - 0.4904349148273468
  - 0.48654884099960327
  - 0.8046656847000122
  - 0.4098803400993347
loss_records_fold3:
  train_losses:
  - 7.062873333692551
  - 6.919537603855133
  - 6.964625746011734
  - 6.732286512851715
  - 7.138807058334351
  - 6.913719564676285
  - 7.03690904378891
  - 6.895253390073776
  - 6.839020311832428
  - 6.946734726428986
  - 7.011855363845825
  - 6.663560792803764
  - 6.743023097515106
  - 6.896758496761322
  - 7.058520823717117
  - 6.983891546726227
  - 6.772646814584732
  - 6.868806943297386
  - 6.880894720554352
  - 6.794185996055603
  - 6.7865636348724365
  - 6.750190496444702
  - 7.018320739269257
  - 6.792119741439819
  - 6.734082341194153
  - 6.674296259880066
  - 6.618514776229858
  - 7.026476591825485
  - 6.950973838567734
  - 6.53991787135601
  - 6.6009631752967834
  - 6.588398814201355
  - 6.7604643404483795
  - 6.729620546102524
  - 6.877761542797089
  - 6.843858480453491
  - 6.779579967260361
  - 6.687727183103561
  - 6.624219059944153
  - 6.825464278459549
  - 6.565435081720352
  - 6.772130578756332
  - 6.564420983195305
  - 6.662330433726311
  - 6.68202131986618
  - 6.7765843868255615
  - 6.793341159820557
  - 6.619755417108536
  - 6.516670197248459
  - 6.685269758105278
  - 6.776694446802139
  - 6.659036457538605
  - 6.72644966840744
  - 6.68681688606739
  - 6.646023750305176
  - 6.53952431678772
  - 6.515411913394928
  - 6.592399924993515
  - 6.531521797180176
  - 6.55790975689888
  - 6.510097771883011
  - 6.488681778311729
  - 6.733154237270355
  - 6.596105128526688
  - 6.528232663869858
  - 6.56181663274765
  - 6.389115676283836
  - 6.3537082970142365
  - 6.9410392343997955
  - 6.4866539388895035
  - 6.58795753121376
  - 6.551847368478775
  - 6.258022844791412
  - 6.711672872304916
  - 6.576049834489822
  - 6.459589779376984
  - 6.463826954364777
  - 6.461258918046951
  - 6.51091456413269
  - 6.717919498682022
  - 6.5043333768844604
  - 6.7305110692977905
  - 6.296819195151329
  - 6.647862881422043
  - 6.60762058198452
  - 6.536351457238197
  - 6.337538838386536
  - 6.301986023783684
  - 6.546687886118889
  - 6.621227920055389
  - 6.829069346189499
  - 6.442965447902679
  - 6.521485388278961
  - 6.451599836349487
  - 6.849054843187332
  - 6.684126198291779
  - 6.588970437645912
  - 6.43360598385334
  - 6.516038566827774
  - 6.401735842227936
  validation_losses:
  - 0.5156453251838684
  - 0.48307088017463684
  - 0.5296990275382996
  - 0.4494408071041107
  - 0.6016813516616821
  - 0.6517256498336792
  - 0.4625574052333832
  - 0.5112115144729614
  - 0.5534014701843262
  - 0.733036994934082
  - 0.45785802602767944
  - 0.5385717153549194
  - 0.564375638961792
  - 0.42071130871772766
  - 0.7325798869132996
  - 0.4888070225715637
  - 0.5193089842796326
  - 0.5343475341796875
  - 0.7377210855484009
  - 0.5693016052246094
  - 0.45454803109169006
  - 0.44308650493621826
  - 0.523869514465332
  - 0.5223091244697571
  - 0.5893614292144775
  - 0.4927043616771698
  - 0.5859578251838684
  - 0.39532262086868286
  - 0.4140070378780365
  - 0.3868761658668518
  - 0.5944128036499023
  - 0.5803363919258118
  - 0.48337605595588684
  - 0.4718053936958313
  - 0.47675180435180664
  - 0.450237512588501
  - 0.4883526563644409
  - 0.4907909333705902
  - 0.5157046318054199
  - 0.5461243987083435
  - 0.48423436284065247
  - 0.5658525228500366
  - 0.5496900081634521
  - 0.6735504865646362
  - 0.4890729784965515
  - 0.4954321086406708
  - 0.46948614716529846
  - 0.5047951340675354
  - 0.43803563714027405
  - 0.45054855942726135
  - 0.45573899149894714
  - 0.4771236479282379
  - 0.5056107044219971
  - 0.47237086296081543
  - 0.5264108777046204
  - 0.49578744173049927
  - 0.5827593207359314
  - 0.5138870477676392
  - 0.5280551314353943
  - 0.47413188219070435
  - 0.5220373868942261
  - 0.5170477628707886
  - 0.5655220150947571
  - 0.6946917772293091
  - 1.1139591932296753
  - 0.5040498971939087
  - 0.7503368854522705
  - 0.6684155464172363
  - 0.4971925914287567
  - 0.4982757568359375
  - 0.42202311754226685
  - 0.8697701692581177
  - 0.42611122131347656
  - 0.48481518030166626
  - 0.5014634728431702
  - 0.594543993473053
  - 0.6262438893318176
  - 0.5205617547035217
  - 0.5599279999732971
  - 0.5539316534996033
  - 0.4641747772693634
  - 0.48626846075057983
  - 0.47678184509277344
  - 0.4654674232006073
  - 0.4711565673351288
  - 0.540901780128479
  - 0.5013132691383362
  - 0.5096026062965393
  - 0.5243769884109497
  - 0.8124307990074158
  - 0.601544201374054
  - 0.6187148690223694
  - 0.5745247602462769
  - 0.5291736721992493
  - 0.8589437007904053
  - 0.4327312707901001
  - 0.8481248617172241
  - 0.9598561525344849
  - 0.9820739030838013
  - 0.641676664352417
loss_records_fold4:
  train_losses:
  - 6.610912621021271
  - 6.4114159643650055
  - 6.682523414492607
  - 6.5411927700042725
  - 6.779154479503632
  - 6.575912803411484
  - 6.47799289226532
  - 6.503421664237976
  - 6.713837459683418
  - 6.652365058660507
  - 6.438356906175613
  - 6.553189337253571
  - 6.4304061233997345
  - 6.578583270311356
  - 6.366403713822365
  - 6.893877670168877
  - 6.324840933084488
  - 6.582608342170715
  - 6.327788025140762
  - 6.394485130906105
  - 6.225608050823212
  - 6.285807475447655
  - 6.313114523887634
  - 6.496359720826149
  - 6.44989675283432
  - 6.314115792512894
  - 6.1603506952524185
  - 6.4427193105220795
  - 6.287935048341751
  - 6.453097343444824
  - 6.456889271736145
  - 6.341459333896637
  - 6.290404826402664
  - 6.271990418434143
  - 6.379786282777786
  - 6.234049215912819
  - 6.538952559232712
  - 6.263124957680702
  - 6.426765859127045
  - 6.577650755643845
  - 6.438477009534836
  - 6.474288880825043
  - 6.223764672875404
  - 6.505219608545303
  - 6.3319219052791595
  - 6.283030480146408
  - 6.242858499288559
  - 6.307673335075378
  - 6.077954351902008
  - 6.622001647949219
  - 6.386671930551529
  - 6.555112510919571
  - 6.432149648666382
  - 6.063681274652481
  - 6.037736088037491
  - 6.376288294792175
  - 6.14747054874897
  - 6.3711130023002625
  - 6.269448548555374
  - 6.198155045509338
  - 6.350358262658119
  - 6.159512534737587
  - 6.165132969617844
  - 6.367481425404549
  - 5.9577068239450455
  - 6.2993468046188354
  - 6.196108981966972
  - 6.404479846358299
  - 6.1228525042533875
  - 6.376384720206261
  - 6.183555722236633
  - 6.263034671545029
  - 6.133796334266663
  - 6.271967902779579
  - 6.054733991622925
  - 6.098907917737961
  - 6.182052195072174
  - 6.298896759748459
  - 6.298201709985733
  - 6.312982141971588
  - 6.038521260023117
  - 5.939212799072266
  - 6.010050445795059
  - 6.034301698207855
  - 6.2181538343429565
  - 6.016375511884689
  - 6.046743273735046
  - 5.798097461462021
  - 5.981667250394821
  - 6.259670093655586
  - 6.150460377335548
  - 6.136830255389214
  - 5.870081141591072
  - 5.944067597389221
  - 5.9589827954769135
  - 6.302542984485626
  - 5.9510593712329865
  - 6.102695509791374
  - 5.998287931084633
  - 5.865745648741722
  validation_losses:
  - 0.44412946701049805
  - 0.4513899087905884
  - 0.5228808522224426
  - 0.46785104274749756
  - 0.484078586101532
  - 0.45633649826049805
  - 0.4379585087299347
  - 0.42445576190948486
  - 0.45148372650146484
  - 0.46634772419929504
  - 0.4453941881656647
  - 0.4676454961299896
  - 0.45807671546936035
  - 0.463506281375885
  - 0.4726090133190155
  - 0.42111659049987793
  - 0.4844744801521301
  - 0.4849928617477417
  - 0.4548129141330719
  - 0.45122241973876953
  - 0.3486021161079407
  - 0.4624771773815155
  - 0.48851412534713745
  - 0.4721406102180481
  - 0.4963298738002777
  - 0.4192717671394348
  - 0.49652299284935
  - 0.4937137961387634
  - 0.5021677017211914
  - 0.4842546284198761
  - 0.48153823614120483
  - 0.4793822765350342
  - 0.4985050559043884
  - 0.44315898418426514
  - 0.41655898094177246
  - 0.4868301451206207
  - 0.44018810987472534
  - 0.4841829836368561
  - 0.45351818203926086
  - 0.5198929905891418
  - 0.5181577205657959
  - 0.5665054321289062
  - 0.5140599012374878
  - 0.48635733127593994
  - 0.46688446402549744
  - 0.4907248616218567
  - 0.5179966688156128
  - 0.5749053359031677
  - 0.4475172758102417
  - 0.4159543216228485
  - 0.46790462732315063
  - 0.5127534866333008
  - 0.5314317941665649
  - 0.5038811564445496
  - 0.5064646005630493
  - 0.48713013529777527
  - 0.5082814693450928
  - 0.5879489779472351
  - 0.5748457908630371
  - 0.5357247591018677
  - 0.58052659034729
  - 0.5194016098976135
  - 0.5078964829444885
  - 0.5334843993186951
  - 0.5490182042121887
  - 0.5454082489013672
  - 0.5932496190071106
  - 0.612026035785675
  - 0.6742011904716492
  - 0.41491833329200745
  - 0.5562522411346436
  - 0.486608624458313
  - 0.5767509341239929
  - 0.5238455533981323
  - 0.5679984092712402
  - 0.5816193222999573
  - 0.6207758784294128
  - 0.6006458401679993
  - 0.525814414024353
  - 0.5875357389450073
  - 0.5389895439147949
  - 0.5783896446228027
  - 0.5406635999679565
  - 0.5169398188591003
  - 0.6065971255302429
  - 0.4091086983680725
  - 0.663988471031189
  - 0.6596063375473022
  - 0.633952796459198
  - 0.6184099912643433
  - 0.5833216309547424
  - 0.5270289182662964
  - 0.560761034488678
  - 0.5928474068641663
  - 0.5430662631988525
  - 0.5717183351516724
  - 0.5930002927780151
  - 0.5020584464073181
  - 0.5099678635597229
  - 0.6282169222831726
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 62 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8353344768439108, 0.8490566037735849, 0.8181818181818182,
    0.8144329896907216]'
  fold_eval_f1: '[0.0, 0.07692307692307693, 0.169811320754717, 0.20895522388059704,
    0.25]'
  mean_eval_accuracy: 0.8349277643189333
  mean_f1_accuracy: 0.1411379243116782
  total_train_time: '0:05:36.550145'
