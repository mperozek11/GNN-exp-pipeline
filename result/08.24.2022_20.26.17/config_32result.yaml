config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:04:07.612155'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_32fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_32fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_32fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_32fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_32fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_32fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_32fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_32fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_32fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_32fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.0931661337614065
  - 5.918839123845101
  - 6.067535701394082
  - 6.105311715602875
  - 5.8252855271101005
  - 5.950858011841774
  - 5.875441944599152
  - 5.7454253256320955
  - 5.83438678085804
  - 5.789343392848969
  - 5.68366311788559
  - 5.712339442968369
  - 5.733327326178551
  - 5.769546034932137
  - 5.6269246518611915
  - 5.94948665201664
  - 5.7998947471380236
  - 5.75534135401249
  - 5.645549264550209
  - 5.57959578037262
  - 5.615378358960152
  - 5.653697580099106
  - 5.603125280141831
  - 5.611268472671509
  - 5.597216019034386
  - 5.6699013173580175
  - 5.564048004150391
  - 5.6794709980487825
  - 5.661464747786522
  - 5.589922219514847
  - 5.560957828164101
  - 5.564787617325783
  - 5.539027324318886
  - 5.6299785614013675
  - 5.581496024131775
  - 5.618358528614045
  - 5.573621389269829
  - 5.575615474581719
  - 5.511107495427132
  - 5.519696414470673
  - 5.5210656970739365
  - 5.659535279870034
  - 5.566159602999687
  - 5.5490766108036045
  - 5.526757550239563
  - 5.552923697233201
  - 5.5328377991914754
  - 5.5714247018098835
  - 5.530421799421311
  - 5.500028550624847
  - 5.595900581777096
  - 5.52329695224762
  - 5.514768022298814
  - 5.548579317331314
  - 5.516476985812187
  - 5.645882940292359
  - 5.539791260659695
  - 5.490175676345825
  - 5.523686265945435
  - 5.450541365146638
  - 5.514104753732681
  - 5.618501806259156
  - 5.534596058726311
  - 5.529681280255318
  - 5.5637750148773195
  - 5.56071030497551
  validation_losses:
  - 0.3936823606491089
  - 0.5113011598587036
  - 0.4133341610431671
  - 0.4132011830806732
  - 0.42452502250671387
  - 0.4040583372116089
  - 0.40202540159225464
  - 0.38729700446128845
  - 0.40989333391189575
  - 0.3857993185520172
  - 0.3994494080543518
  - 0.4773520827293396
  - 0.3915348947048187
  - 0.3910440504550934
  - 0.38057664036750793
  - 1.0054261684417725
  - 0.3974534571170807
  - 0.3863499164581299
  - 0.39937523007392883
  - 0.3875771164894104
  - 0.39418816566467285
  - 0.38497862219810486
  - 0.3967161178588867
  - 0.3983714282512665
  - 0.39132294058799744
  - 0.410921186208725
  - 0.3889673352241516
  - 0.41334855556488037
  - 0.5650960206985474
  - 0.47027724981307983
  - 9.420570373535156
  - 4.295475482940674
  - 0.41206610202789307
  - 0.38591164350509644
  - 0.3878587782382965
  - 0.49241599440574646
  - 0.3877127468585968
  - 0.41949737071990967
  - 0.38594159483909607
  - 0.3893424868583679
  - 0.5577011704444885
  - 0.8676331043243408
  - 0.6753941774368286
  - 0.3928626775741577
  - 0.5947306156158447
  - 0.4609033763408661
  - 0.4599000811576843
  - 0.6800451278686523
  - 0.4367910921573639
  - 0.7217394709587097
  - 0.4013702869415283
  - 0.38352397084236145
  - 0.3942417800426483
  - 0.44389283657073975
  - 0.3895617723464966
  - 0.39379528164863586
  - 0.38601213693618774
  - 0.4302206337451935
  - 0.43822208046913147
  - 0.5212818384170532
  - 0.44804370403289795
  - 0.4218141734600067
  - 0.3960985243320465
  - 0.4020536541938782
  - 0.3833293616771698
  - 0.389595091342926
loss_records_fold1:
  train_losses:
  - 5.57516528069973
  - 5.484698894619942
  - 5.526159259676934
  - 5.456330579519272
  - 5.498524940013886
  - 5.527196383476258
  - 5.446862578392029
  - 5.52105220258236
  - 5.490785205364228
  - 5.452652311325074
  - 5.517610049247742
  - 5.659859442710877
  - 5.470241075754166
  - 5.509579384326935
  - 5.571063017845154
  - 5.835987854003907
  - 5.567809355258942
  - 5.539548352360725
  - 5.461922833323479
  - 5.4161923944950106
  - 5.475266334414482
  - 5.460658928751946
  - 5.529053157567978
  - 5.51500847041607
  - 5.511024874448776
  - 5.497892969846726
  - 5.439037045836449
  - 5.434165781736374
  - 5.522814503312111
  - 5.5057968437671665
  - 5.529462239146233
  - 5.471067976951599
  - 5.448589730262757
  - 5.391294831037522
  - 5.558176827430725
  - 5.437532830238343
  - 5.4790810525417335
  - 5.393469071388245
  - 5.442638537287713
  - 5.397860130667687
  - 5.408282819390298
  - 5.531838086247444
  - 5.427025589346886
  - 5.4964031130075455
  - 5.445893439650536
  - 5.480579710006714
  - 5.567279601097107
  - 5.52524788081646
  - 5.404602989554405
  - 5.368631795048714
  - 5.429524442553521
  - 5.411046001315118
  - 5.4926835089921955
  - 5.468112215399742
  - 5.4076065719127655
  - 5.438788595795632
  - 5.430671030282975
  - 5.485515046119691
  - 5.428769356012345
  - 5.427206444740296
  - 5.347812679409981
  - 5.429929983615875
  - 5.371874138712883
  - 5.3877308726310735
  - 5.846277591586113
  - 5.612438383698464
  - 5.548785147070885
  - 5.704301804304123
  - 5.625129163265228
  - 5.58368248641491
  - 5.5342134743928915
  - 5.632634612917901
  - 5.553804469108582
  - 5.5771999061107635
  - 5.562869471311569
  - 5.52565567791462
  - 5.547355273365975
  - 5.514233806729317
  - 5.51151414513588
  - 5.4873066574335105
  - 5.535516005754471
  - 5.542532807588578
  - 5.659709888696671
  - 5.534637925028801
  - 5.582645779848099
  - 5.575452443957329
  - 5.48826094865799
  - 5.516764250397682
  - 5.5649839550256734
  - 5.549859085679055
  - 5.5144012629985815
  - 5.571369215846062
  - 5.501946491003037
  - 5.536697837710381
  - 5.500256702303886
  - 5.495857971906663
  - 5.575460594892502
  - 5.798643252253533
  - 5.512944588065148
  - 5.483657541871072
  validation_losses:
  - 0.39941659569740295
  - 0.4715782403945923
  - 0.4672873020172119
  - 0.6565761566162109
  - 0.5313951969146729
  - 0.6073411107063293
  - 0.5016704797744751
  - 0.5680077075958252
  - 0.7819499373435974
  - 0.5116868615150452
  - 0.5888143181800842
  - 0.4354933202266693
  - 0.48417824506759644
  - 0.46880513429641724
  - 0.8642452955245972
  - 0.4836212396621704
  - 0.4353654384613037
  - 0.5340665578842163
  - 0.4825887680053711
  - 0.5181558132171631
  - 0.4559609889984131
  - 0.4724676012992859
  - 0.4632851183414459
  - 0.48894187808036804
  - 0.49373966455459595
  - 0.5431737303733826
  - 0.4905642569065094
  - 0.5351095795631409
  - 0.5341817140579224
  - 0.48259493708610535
  - 0.42642471194267273
  - 0.5201057195663452
  - 0.7807955741882324
  - 0.6516059041023254
  - 0.6712291836738586
  - 0.6074516177177429
  - 0.5694632530212402
  - 0.5781305432319641
  - 0.6268000602722168
  - 0.5337979197502136
  - 0.5893023014068604
  - 0.7024715542793274
  - 0.6173732280731201
  - 0.6767192482948303
  - 0.6628164649009705
  - 0.4946773946285248
  - 0.5270904898643494
  - 0.7044153213500977
  - 0.6127133965492249
  - 0.6880316138267517
  - 2.1777350902557373
  - 0.8219248056411743
  - 0.970848560333252
  - 2.3204567432403564
  - 1.1668105125427246
  - 0.8905411958694458
  - 0.585247278213501
  - 0.6586047410964966
  - 0.5938882231712341
  - 0.8802856802940369
  - 1.4608485698699951
  - 2.558411121368408
  - 1.8906902074813843
  - 1.5836704969406128
  - 0.4349260628223419
  - 0.40306541323661804
  - 0.4309964179992676
  - 0.3902249038219452
  - 0.41034379601478577
  - 0.3885652720928192
  - 0.4696248173713684
  - 0.39984330534935
  - 0.3955117464065552
  - 0.38483715057373047
  - 0.3891508877277374
  - 0.8299419283866882
  - 0.4120708703994751
  - 0.45670557022094727
  - 0.45891231298446655
  - 0.4494835138320923
  - 0.3951621651649475
  - 0.3859117031097412
  - 0.46917903423309326
  - 0.47795218229293823
  - 0.474873423576355
  - 0.3865094482898712
  - 0.41165465116500854
  - 0.4224892854690552
  - 0.4184984862804413
  - 0.43399879336357117
  - 0.4056170582771301
  - 0.5200788974761963
  - 0.4278958737850189
  - 0.4371197521686554
  - 0.4306980073451996
  - 0.38997122645378113
  - 0.39762750267982483
  - 0.5771793723106384
  - 0.38358739018440247
  - 0.38154956698417664
loss_records_fold2:
  train_losses:
  - 5.57829379439354
  - 5.512647932767869
  - 5.4733007699251175
  - 5.550346145033837
  - 5.5343695223331455
  - 5.515768599510193
  - 5.555074486136437
  - 5.548670938611031
  - 5.512762795388699
  - 5.526745173335076
  - 5.554079788923264
  - 5.541360983252526
  - 5.466614151000977
  - 5.522957137227059
  - 5.569354924559594
  validation_losses:
  - 0.533935010433197
  - 0.4018400311470032
  - 0.3893967866897583
  - 0.38933825492858887
  - 0.3857586979866028
  - 0.38233673572540283
  - 0.3815329670906067
  - 0.38084322214126587
  - 0.402663916349411
  - 0.3808349370956421
  - 0.38159701228141785
  - 0.3860318064689636
  - 0.38680046796798706
  - 0.389310747385025
  - 0.38143253326416016
loss_records_fold3:
  train_losses:
  - 5.6501257717609406
  - 5.601872533559799
  - 5.559725141525269
  - 5.586396798491478
  - 5.540328192710877
  - 5.5649635046720505
  - 5.537427607178689
  - 5.596842029690743
  - 5.537347856163979
  - 5.535662576556206
  - 5.539359095692635
  - 5.575565865635872
  - 5.553432855010033
  - 5.542161303758622
  - 5.515979525446892
  - 5.58782861828804
  - 5.544121184945107
  - 5.535544580221177
  - 5.543110096454621
  - 5.550223222374917
  - 5.52500456571579
  - 5.503619864583015
  - 5.484525606036186
  - 5.527026325464249
  - 5.53737345635891
  - 5.508313512802125
  - 5.420588475465775
  - 5.50244646370411
  - 5.507531002163887
  - 5.552475720643997
  - 5.568302190303803
  - 5.504386413097382
  - 5.500771871209145
  - 5.5279282391071325
  - 5.517182466387749
  - 5.484805369377137
  - 5.5146151959896095
  - 5.5318698436021805
  - 5.445460742712021
  - 5.510567978024483
  - 5.571594104170799
  - 5.507763978838921
  - 5.52994359433651
  - 5.567243239283562
  - 5.5126032859087
  - 5.470727977156639
  - 5.502270352840424
  - 5.547458684444428
  - 5.459779757261277
  - 5.441633263230324
  - 5.486297938227654
  - 5.489199307560921
  - 5.457554569840432
  - 5.418032243847847
  - 5.473728135228157
  - 5.464331406354905
  - 5.4869209915399555
  - 5.528286719322205
  - 5.443180486559868
  - 5.612663209438324
  - 5.5165245801210405
  - 5.422429591417313
  - 5.537792709469795
  - 5.519315406680107
  - 5.454215517640114
  - 5.559050896763802
  - 5.50678018629551
  - 5.455617883801461
  - 5.495928341150284
  - 5.5286682695150375
  - 5.548083201050758
  - 5.463735827803612
  - 5.473326605558396
  - 5.513393774628639
  - 5.565365061163902
  - 5.6267375528812416
  - 5.5391290396451955
  - 5.51605331003666
  - 5.447075304389
  - 5.406738460063934
  - 5.505251836776734
  - 5.427574360370636
  - 5.472227576375008
  - 5.468221771717072
  - 5.465404307842255
  - 5.487644046545029
  - 5.522653025388718
  - 5.462730243802071
  - 5.486427620053291
  - 5.469392731785774
  - 5.446169546246529
  - 5.464387395977974
  - 5.533995142579079
  - 5.522150811553002
  - 5.412391701340676
  - 5.42983122766018
  - 5.453252038359643
  - 5.481015676259995
  - 5.504261875152588
  - 5.485297188162804
  validation_losses:
  - 0.38156333565711975
  - 0.43364426493644714
  - 0.38838323950767517
  - 0.3885308802127838
  - 0.428585410118103
  - 0.42112013697624207
  - 0.3818359971046448
  - 0.3953811526298523
  - 0.3659689724445343
  - 0.43647950887680054
  - 0.4080124497413635
  - 0.5950056910514832
  - 0.4477454423904419
  - 0.3729743957519531
  - 0.37155887484550476
  - 0.5652197003364563
  - 1.130712628364563
  - 0.3735569715499878
  - 0.3726722002029419
  - 0.4183007776737213
  - 0.4771266579627991
  - 0.5616887211799622
  - 1.2367613315582275
  - 0.37278270721435547
  - 1.2477171421051025
  - 1.4487346410751343
  - 1.831391453742981
  - 0.6500778794288635
  - 0.9185948371887207
  - 0.37350988388061523
  - 0.629575252532959
  - 0.4029916524887085
  - 0.6572641730308533
  - 0.9127325415611267
  - 0.53648841381073
  - 0.3971623182296753
  - 0.8812844753265381
  - 1.4122503995895386
  - 1.0884513854980469
  - 0.44243910908699036
  - 0.3848213255405426
  - 0.39452916383743286
  - 0.4056565463542938
  - 0.5633530616760254
  - 0.39596906304359436
  - 0.4382115602493286
  - 0.5067370533943176
  - 0.4156290292739868
  - 0.41615045070648193
  - 0.604019820690155
  - 0.6443953514099121
  - 0.515950083732605
  - 0.3900775909423828
  - 7.313729763031006
  - 3.1507930755615234
  - 1.6341718435287476
  - 1.00376558303833
  - 0.4450816214084625
  - 0.6225314736366272
  - 0.4459071159362793
  - 0.6177982687950134
  - 0.5117252469062805
  - 0.671036422252655
  - 0.4286386966705322
  - 0.5034148693084717
  - 0.6506807208061218
  - 0.434831827878952
  - 0.5341823697090149
  - 0.44169196486473083
  - 0.41094282269477844
  - 0.5175023674964905
  - 0.9187329411506653
  - 0.5171425938606262
  - 0.6174396872520447
  - 0.8722743988037109
  - 2.4258668422698975
  - 2.145313262939453
  - 2.848830461502075
  - 0.869338870048523
  - 1.4120913743972778
  - 0.9539870619773865
  - 1.1285289525985718
  - 1.9316167831420898
  - 1.0720926523208618
  - 1.6816792488098145
  - 2.685814380645752
  - 2.164788007736206
  - 1.1630162000656128
  - 2.233813524246216
  - 1.4600485563278198
  - 2.8613553047180176
  - 2.627500295639038
  - 12.65754222869873
  - 3.8574106693267822
  - 1.2737836837768555
  - 1.7501697540283203
  - 3.583617687225342
  - 0.9149823188781738
  - 0.48465171456336975
  - 0.6748484373092651
loss_records_fold4:
  train_losses:
  - 5.526735535264016
  - 5.486969602108002
  - 5.446570813655853
  - 5.459579053521157
  - 5.401495882868767
  - 5.438982456922531
  - 5.4544125020504
  - 5.470193466544152
  - 5.479203444719315
  - 5.527685031294823
  - 5.451001504063607
  - 5.416370397806168
  - 5.4267599284648895
  - 5.451662597060204
  - 5.488359978795052
  - 5.485128971934319
  - 5.459557536244393
  - 5.4381887227296835
  - 5.436498323082924
  - 5.461202844977379
  - 5.4648668467998505
  - 5.460249510407448
  - 5.466500714421272
  - 5.5392515718936925
  - 5.400502759218217
  - 5.455527365207672
  - 5.4052953094244005
  - 5.562731292843819
  - 5.459751665592194
  - 5.49445443302393
  - 5.5110803812742235
  - 5.509150740504265
  - 5.429707282781601
  - 5.530656921863557
  - 5.394109302759171
  - 5.487816128134728
  - 5.441108050942422
  - 5.524365702271462
  - 5.547496044635773
  - 5.50209778547287
  - 5.472949561476708
  - 5.518665474653244
  - 5.426689922809601
  - 5.495244961977005
  - 5.470511463284493
  - 5.418038642406464
  - 5.5134657502174385
  - 5.535636928677559
  - 5.425815722346306
  - 5.46586484014988
  - 5.424029675126076
  - 5.424769219756127
  - 5.35959894657135
  - 5.468804827332497
  - 5.562195414304734
  - 5.480688136816025
  - 5.463963481783868
  - 5.42702331840992
  - 5.465868324041367
  - 5.446833083033562
  - 5.544677707552911
  - 5.533699814975262
  - 5.512144935131073
  - 5.479505816102028
  - 5.513594800233841
  - 5.472413441538811
  - 5.587594419717789
  - 5.559068804979325
  - 5.494034454226494
  - 5.457972073554993
  - 5.486091957986355
  - 5.450497731566429
  - 5.463435035943985
  - 5.432838103175164
  - 5.510803380608559
  - 5.450657939910889
  - 5.427175626158714
  - 5.501601344347001
  - 5.466466999053956
  - 5.437791740894318
  - 5.41654437482357
  - 5.449446779489517
  - 5.412138736248017
  - 5.517785611748696
  - 5.423560047149659
  - 5.403416243195534
  - 5.494141465425492
  - 5.484233039617539
  - 5.46292389035225
  - 5.4404460251331335
  - 5.429036208987236
  - 5.4227539002895355
  - 5.515505182743073
  - 5.45334624350071
  - 5.389266780018807
  - 5.397013247013092
  - 5.490026143193245
  - 5.4281736522912984
  - 5.3823457300663
  - 5.488013800978661
  validation_losses:
  - 0.37030014395713806
  - 0.4480403661727905
  - 0.4536062180995941
  - 0.4135259985923767
  - 0.4280523955821991
  - 0.5310582518577576
  - 0.5394622087478638
  - 0.4004417955875397
  - 0.5155284404754639
  - 0.4278848171234131
  - 0.46324414014816284
  - 0.464082270860672
  - 0.5758234858512878
  - 0.5598768591880798
  - 0.5677838325500488
  - 0.49289387464523315
  - 0.3641909658908844
  - 0.40328773856163025
  - 0.38376936316490173
  - 0.7094415426254272
  - 0.9424405097961426
  - 1.266098141670227
  - 2.273508310317993
  - 0.4094463884830475
  - 0.4255857765674591
  - 0.470462828874588
  - 0.639154314994812
  - 1.418503999710083
  - 2.3258697986602783
  - 1.0100274085998535
  - 0.6664533019065857
  - 0.5076911449432373
  - 0.5771097540855408
  - 0.6378276944160461
  - 0.6163984537124634
  - 0.7406342625617981
  - 0.7618993520736694
  - 0.8531677722930908
  - 0.6242713332176208
  - 0.554574728012085
  - 0.6461315155029297
  - 0.4244977831840515
  - 0.5400499701499939
  - 0.6698406338691711
  - 0.5544676184654236
  - 0.7266501188278198
  - 0.6556016802787781
  - 0.6027442812919617
  - 0.6369817852973938
  - 0.8323192596435547
  - 0.5675409436225891
  - 0.9381405711174011
  - 0.8116541504859924
  - 0.9791998267173767
  - 0.7021642923355103
  - 0.9137120246887207
  - 0.4167497158050537
  - 0.48332467675209045
  - 0.4859943389892578
  - 0.6409956812858582
  - 0.7817251086235046
  - 0.4934600293636322
  - 0.6152366995811462
  - 0.5923577547073364
  - 0.4242805540561676
  - 0.44844093918800354
  - 0.5946084856987
  - 0.5353201627731323
  - 0.7273290157318115
  - 0.4354398846626282
  - 0.3795877695083618
  - 0.4543258845806122
  - 0.47621166706085205
  - 0.4128822684288025
  - 0.4319881200790405
  - 0.48002535104751587
  - 0.4868961572647095
  - 0.4571542739868164
  - 0.4246583878993988
  - 0.47886747121810913
  - 0.5022071599960327
  - 0.5111644864082336
  - 0.6920552849769592
  - 0.7179598212242126
  - 0.530032753944397
  - 0.5037931203842163
  - 0.6101109981536865
  - 0.44321566820144653
  - 0.6826554536819458
  - 0.7662491202354431
  - 0.5693979859352112
  - 0.6250569224357605
  - 0.4993158280849457
  - 0.5513182282447815
  - 0.4883362650871277
  - 0.5196514129638672
  - 0.4018542170524597
  - 0.5000239610671997
  - 0.46509355306625366
  - 0.42746490240097046
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 66 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.855917667238422, 0.8593481989708405, 0.8524871355060034,
    0.8573883161512027]'
  fold_eval_f1: '[0.023255813953488372, 0.0, 0.0, 0.0851063829787234, 0.10752688172043011]'
  mean_eval_accuracy: 0.8562117970209782
  mean_f1_accuracy: 0.04317781573052838
  total_train_time: '0:40:41.568963'
