config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 19:00:20.075196'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_121fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_121fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_121fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_121fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_121fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_121fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_121fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_121fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_121fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_121fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 43.12797926664353
  - 27.673185095191002
  - 15.228898000717164
  - 11.692380726337433
  - 10.766799217462541
  - 10.626003855466843
  - 7.396577280759812
  - 9.319134682416916
  - 14.115015220642091
  - 12.365850877761842
  - 9.483278661966324
  - 9.099662029743195
  - 14.1704460978508
  - 14.113097268342973
  - 12.582305735349657
  - 6.000513434410095
  - 7.090714776515961
  - 6.749339127540589
  - 11.458168840408327
  - 6.578304821252823
  - 5.457004296779633
  - 9.03195334970951
  - 9.35943061709404
  - 5.614009368419648
  - 5.345291179418564
  - 5.066167107224465
  - 16.992918112874033
  - 8.007578369975091
  - 14.610483139753342
  - 5.536442482471466
  - 8.101902627944947
  - 5.661113601922989
  - 7.0306518316268924
  - 9.668186241388321
  - 4.614480692148208
  - 5.923537522554398
  - 5.153955399990082
  - 5.060071557760239
  - 4.073600199818611
  - 4.448940694332123
  - 4.345606833696365
  validation_losses:
  - 3.937084674835205
  - 1.3186745643615723
  - 2.003190755844116
  - 1.7717981338500977
  - 0.8966577649116516
  - 0.7566983103752136
  - 1.2349393367767334
  - 0.6481372714042664
  - 0.9008628129959106
  - 0.5836160778999329
  - 0.7922331690788269
  - 0.7709728479385376
  - 1.1543724536895752
  - 0.4247748553752899
  - 0.6073969602584839
  - 0.6011469960212708
  - 0.4988885521888733
  - 0.5050947666168213
  - 0.40384921431541443
  - 0.4126303791999817
  - 0.68073570728302
  - 0.3923090994358063
  - 0.5217676758766174
  - 0.39333394169807434
  - 0.4195120930671692
  - 0.48113369941711426
  - 0.41228505969047546
  - 0.4214538037776947
  - 0.4483698606491089
  - 0.46192464232444763
  - 0.42837223410606384
  - 0.4344455897808075
  - 0.4066939055919647
  - 0.4250982105731964
  - 0.43747395277023315
  - 0.4250056743621826
  - 0.40963661670684814
  - 0.4060910940170288
  - 0.38866126537323
  - 0.38607507944107056
  - 0.39241886138916016
loss_records_fold1:
  train_losses:
  - 3.830147182941437
  - 3.680366089940071
  - 5.419558453559876
  - 4.584524381160736
  - 6.037521344423294
  - 3.5398812115192415
  - 5.2061061561107635
  - 4.441961830854416
  - 3.7208301186561585
  - 4.213396072387695
  - 3.537437239289284
  - 4.673478335142136
  - 4.799076381325722
  - 3.45351327508688
  - 4.795410439372063
  - 5.994628244638443
  - 4.297235980629921
  - 4.573290896415711
  - 4.666099891066551
  - 3.4738624513149263
  - 5.593185311555863
  - 3.6979304015636445
  - 3.5498782187700275
  - 3.175351452827454
  - 3.6968468189239503
  - 3.413345378637314
  - 3.3549189269542694
  - 3.148729845881462
  - 4.1532964408397675
  - 4.07844769358635
  - 3.5957043379545213
  - 3.112646842002869
  - 3.201244106888771
  - 3.56500101685524
  - 3.472348487377167
  - 2.9730739146471024
  - 3.4075344145298008
  - 3.1239385634660723
  - 3.7358655482530594
  - 2.9477419316768647
  - 3.751835972070694
  - 3.710775023698807
  - 3.4028801321983337
  - 3.964182084798813
  - 3.3229150831699372
  - 2.926258194446564
  - 3.217797809839249
  - 3.185000622272492
  - 3.2972170531749727
  - 3.7007457971572877
  - 4.081226390600205
  - 4.400583830475807
  - 3.590559941530228
  - 3.747873800992966
  - 3.078517782688141
  - 3.1477514624595644
  - 3.313830387592316
  - 2.964993262290955
  - 2.9610462486743927
  - 2.9722043663263324
  - 3.332199689745903
  - 3.3130750149488453
  - 3.046183830499649
  - 2.9703196346759797
  - 3.451907390356064
  - 3.5385578125715256
  - 3.2613111913204196
  - 3.3470756828784944
  - 2.970031487941742
  - 3.1001903057098392
  - 3.1458896815776827
  - 3.029916152358055
  - 3.085282462835312
  - 2.82198705971241
  - 2.8905014932155613
  - 2.834934800863266
  - 2.8552223205566407
  - 3.0242729604244234
  - 3.0375657051801683
  - 3.094463559985161
  - 3.188187146186829
  - 2.9496583580970768
  - 2.975851917266846
  - 3.029533648490906
  - 2.9028464496135715
  - 2.936910700798035
  - 3.0332947224378586
  - 2.8960402697324756
  - 2.94718861579895
  - 3.0170964509248734
  - 2.8640763729810716
  - 2.8816637665033342
  - 2.9582251220941544
  - 3.509118449687958
  - 3.044355583190918
  - 3.2387139320373537
  - 2.9639956146478657
  - 3.1738038033246996
  - 3.1115345656871796
  - 2.997385758161545
  validation_losses:
  - 0.441326379776001
  - 0.3968387544155121
  - 0.40384209156036377
  - 0.4299236238002777
  - 0.4346677362918854
  - 0.43857911229133606
  - 0.4739936590194702
  - 0.41304683685302734
  - 0.40406113862991333
  - 0.44375866651535034
  - 0.4336487650871277
  - 0.7220237851142883
  - 0.39924007654190063
  - 0.42832180857658386
  - 0.5607739686965942
  - 0.4474794268608093
  - 0.4049719274044037
  - 0.5480117201805115
  - 0.41791296005249023
  - 0.3988496959209442
  - 0.4210127294063568
  - 0.4360618591308594
  - 0.3980596661567688
  - 0.4026399254798889
  - 0.41004061698913574
  - 0.39614763855934143
  - 0.39373862743377686
  - 0.4064893126487732
  - 0.39537322521209717
  - 0.3968689739704132
  - 0.3932901918888092
  - 0.39953166246414185
  - 0.4487512707710266
  - 0.40512678027153015
  - 0.39577943086624146
  - 0.4273064136505127
  - 0.4099922180175781
  - 0.41757872700691223
  - 0.40293943881988525
  - 0.44163715839385986
  - 0.560714066028595
  - 0.4306604564189911
  - 0.3942399024963379
  - 0.4433680772781372
  - 0.5399273633956909
  - 0.4491889774799347
  - 0.39941874146461487
  - 0.4406720995903015
  - 0.39519619941711426
  - 0.5145376920700073
  - 0.40041401982307434
  - 0.40237313508987427
  - 0.4407136142253876
  - 0.4520037770271301
  - 0.39959952235221863
  - 0.3978586196899414
  - 0.4026862680912018
  - 0.44908004999160767
  - 0.39930370450019836
  - 0.4555320739746094
  - 0.6071342825889587
  - 0.39921844005584717
  - 0.3968046307563782
  - 0.4049737751483917
  - 0.44582656025886536
  - 0.5299445390701294
  - 0.4156647026538849
  - 0.40762707591056824
  - 0.3987061083316803
  - 0.4411928951740265
  - 0.4230826199054718
  - 0.4162733256816864
  - 0.3994024693965912
  - 0.39604395627975464
  - 0.40684938430786133
  - 0.3956674635410309
  - 0.40287289023399353
  - 0.4567238390445709
  - 0.41048210859298706
  - 0.4077366292476654
  - 0.3979015052318573
  - 0.4442618191242218
  - 0.44536924362182617
  - 0.4774324595928192
  - 0.4012504518032074
  - 0.41230884194374084
  - 0.4209824502468109
  - 0.40518251061439514
  - 0.43018582463264465
  - 0.4118766188621521
  - 0.401134192943573
  - 0.4012290835380554
  - 0.4726882576942444
  - 0.41417834162712097
  - 0.4097647964954376
  - 0.421865314245224
  - 0.398980975151062
  - 0.4006296396255493
  - 0.43608570098876953
  - 0.4001215696334839
loss_records_fold2:
  train_losses:
  - 2.890570962429047
  - 2.890787288546562
  - 2.928980976343155
  - 3.0761783182621003
  - 3.1010350316762927
  - 3.2039318025112156
  - 3.2941903591156008
  - 3.05804306268692
  - 3.1181215941905975
  - 2.9865817159414294
  - 2.9123010575771335
  - 2.9957266747951508
  - 2.9245265275239944
  - 2.9540637642145158
  - 2.9423383653163913
  - 2.908079516887665
  - 2.9047658026218417
  - 2.8396807789802554
  - 2.9413170874118806
  - 3.1615614116191866
  - 3.0964611291885378
  - 3.091797244548798
  - 3.0209183737635614
  - 3.050740170478821
  - 3.0591793179512026
  - 2.951762038469315
  - 3.0047935754060746
  - 3.0545444548130036
  - 3.048154038190842
  - 3.017315423488617
  - 3.0408439666032794
  - 2.9594793915748596
  - 3.006692254543305
  - 3.0048402428627017
  - 3.0151786744594578
  - 3.016746699810028
  - 3.0676257193088534
  - 3.260370706021786
  - 3.0213070809841156
  - 2.9610136270523073
  - 3.006880307197571
  - 3.1457748383283617
  - 3.0346660733222963
  - 3.0182527363300324
  - 2.9387792199850082
  - 3.219952690601349
  - 4.842908626794816
  - 3.103973197937012
  - 3.128071439266205
  - 3.1195775270462036
  - 3.048318937420845
  - 3.012137153744698
  - 2.941964051127434
  - 2.980583906173706
  - 2.9785281836986544
  - 2.9744394510984424
  - 3.1160083889961245
  - 3.116571652889252
  - 2.9738550662994387
  - 2.9645000725984576
  - 2.9564371496438984
  - 2.943276470899582
  - 3.0187073409557343
  validation_losses:
  - 0.43803519010543823
  - 0.4842780828475952
  - 0.38049766421318054
  - 0.4803852140903473
  - 0.37585246562957764
  - 0.5154363512992859
  - 0.38476741313934326
  - 0.9863945245742798
  - 0.40746867656707764
  - 0.39323899149894714
  - 0.41880854964256287
  - 0.38204407691955566
  - 0.4166499674320221
  - 0.9398524165153503
  - 0.516900897026062
  - 0.8467469215393066
  - 0.8656408190727234
  - 0.5901345014572144
  - 0.9441458582878113
  - 0.5085922479629517
  - 0.38236820697784424
  - 0.5093969702720642
  - 0.38655325770378113
  - 0.41135188937187195
  - 0.3865610361099243
  - 0.3950328230857849
  - 0.383924663066864
  - 0.386457622051239
  - 0.3836483657360077
  - 0.579844057559967
  - 0.39549720287323
  - 0.6748075485229492
  - 0.4115856885910034
  - 0.38280752301216125
  - 0.4031749963760376
  - 0.3917834460735321
  - 0.3995538353919983
  - 0.39754921197891235
  - 0.6451579928398132
  - 133.3919677734375
  - 675.84521484375
  - 92.82207489013672
  - 1550.8031005859375
  - 269.3473815917969
  - 5685.70361328125
  - 1.449220061302185
  - 0.3845110535621643
  - 0.42165377736091614
  - 0.3839908540248871
  - 0.4076007306575775
  - 0.38122642040252686
  - 0.3966831564903259
  - 0.45383235812187195
  - 0.3816510736942291
  - 0.39835119247436523
  - 0.3913717567920685
  - 0.47274503111839294
  - 0.39485806226730347
  - 0.38501015305519104
  - 0.3840956389904022
  - 0.3831760585308075
  - 0.3853830397129059
  - 0.38420888781547546
loss_records_fold3:
  train_losses:
  - 2.943744033575058
  - 2.928326123952866
  - 3.1536970257759096
  - 2.950094774365425
  - 2.9608566522598267
  - 3.0901921808719637
  - 3.0539009094238283
  - 3.0034530431032183
  - 2.942891328036785
  - 3.1136368393898013
  - 2.9400354206562045
  - 2.989261430501938
  - 2.973323068022728
  - 3.025762575864792
  - 2.9572530031204227
  - 2.9574290335178377
  - 3.049496388435364
  - 3.046851003170014
  - 2.9658427834510803
  - 2.962070816755295
  - 2.9784117817878726
  - 3.024799156188965
  - 2.9965956568717957
  - 2.953306442499161
  - 2.975765818357468
  - 2.927461636066437
  - 2.999335724115372
  - 2.961776459217072
  validation_losses:
  - 0.39544156193733215
  - 0.4119890034198761
  - 0.3957756757736206
  - 0.4094371795654297
  - 0.4467788338661194
  - 0.4253615140914917
  - 0.3970036506652832
  - 0.4698110520839691
  - 0.4563298225402832
  - 0.3972507417201996
  - 0.41052916646003723
  - 0.42962178587913513
  - 0.447293758392334
  - 0.3982149362564087
  - 0.398551881313324
  - 0.40141865611076355
  - 0.3963460922241211
  - 0.41853028535842896
  - 0.42462867498397827
  - 0.39571893215179443
  - 0.3975999653339386
  - 0.4847589135169983
  - 0.40283340215682983
  - 0.39723214507102966
  - 0.4040500819683075
  - 0.41258102655410767
  - 0.4006488025188446
  - 0.4014917314052582
loss_records_fold4:
  train_losses:
  - 2.9449119538068773
  - 3.0089537769556047
  - 2.934235978126526
  - 3.159994432330132
  - 3.024700859189034
  - 3.0203482002019886
  - 3.009178426861763
  - 3.051523980498314
  - 3.6003540337085727
  - 3.11130348443985
  - 3.0453961282968525
  - 2.934977436065674
  - 3.0562170803546906
  - 3.0112546622753147
  - 2.999835714697838
  validation_losses:
  - 0.39540836215019226
  - 0.3899557590484619
  - 0.404746413230896
  - 0.3961937427520752
  - 0.4203158915042877
  - 0.40197837352752686
  - 0.41834649443626404
  - 0.40440574288368225
  - 0.6080493330955505
  - 0.3920609951019287
  - 0.39213892817497253
  - 0.3989720046520233
  - 0.38985708355903625
  - 0.38974231481552124
  - 0.39421170949935913
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 41 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 63 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:22:48.449966'
