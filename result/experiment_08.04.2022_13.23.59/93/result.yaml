config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 00:05:26.084831'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/93/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/93/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/93/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/93/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/93/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/93/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/93/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/93/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/93/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/93/fold_4_state_dict.pt
loss_records_fold2:
  train_losses:
  - 57.399985045194626
  - 26.17281149327755
  - 16.15076392889023
  - 33.002549171447754
  - 21.7619591653347
  - 42.42424553632736
  - 19.600933730602264
  - 16.890358358621597
  - 46.72103849053383
  - 19.38929769396782
  - 12.989397704601288
  - 25.39317473769188
  - 39.12320286035538
  - 11.787605553865433
  - 20.339447051286697
  - 16.252444177865982
  - 9.971508741378784
  - 78.81029438972473
  - 40.28722792863846
  - 11.90964064002037
  - 11.283863916993141
  - 9.084545016288757
  - 9.018521815538406
  - 13.58649131655693
  - 13.102200418710709
  - 10.549351841211319
  - 10.392670571804047
  - 16.247720420360565
  - 13.80855393409729
  - 10.245030105113983
  - 9.061430245637894
  - 12.212790697813034
  - 9.763583272695541
  - 10.07282429933548
  - 10.166240781545639
  - 8.959665805101395
  - 9.442105829715729
  - 13.233464300632477
  - 12.047488570213318
  - 10.432892620563507
  - 9.298047631978989
  - 9.088032841682434
  - 9.551496982574463
  - 10.097287774085999
  - 8.443410605192184
  - 9.597606211900711
  - 8.811003476381302
  - 9.560721099376678
  - 11.040088057518005
  - 16.750481843948364
  - 12.062807381153107
  - 12.716400295495987
  - 10.024466902017593
  - 11.446907699108124
  - 14.52826926112175
  - 14.18799215555191
  - 12.417603343725204
  - 12.537002593278885
  - 13.513097733259201
  - 11.568170219659805
  - 13.311978280544281
  - 10.951507985591888
  - 12.401219129562378
  - 8.380350857973099
  - 8.324528247117996
  - 8.307800516486168
  - 12.145859062671661
  - 9.44780370593071
  - 9.662289470434189
  - 10.297994911670685
  - 12.329094648361206
  - 12.213557779788971
  - 9.014902621507645
  - 12.190608620643616
  - 9.75052535533905
  - 8.870624005794525
  - 8.526166439056396
  - 14.150194525718689
  - 10.428804516792297
  - 13.263519287109375
  - 12.613212823867798
  - 9.261751025915146
  - 8.898791551589966
  - 9.209245771169662
  - 8.535832941532135
  - 8.781904339790344
  - 10.313212752342224
  - 10.900830686092377
  - 14.348291754722595
  - 11.659168601036072
  - 8.50472903251648
  - 10.268768012523651
  - 8.899784862995148
  - 11.987424403429031
  - 10.2542222738266
  - 9.391236811876297
  - 8.945741325616837
  - 8.823388934135437
  - 9.129721343517303
  - 9.554736495018005
  validation_losses:
  - 0.9521828293800354
  - 0.5676784515380859
  - 0.4313347637653351
  - 0.4014051556587219
  - 0.5211921334266663
  - 0.4806082546710968
  - 0.6754690408706665
  - 0.43463629484176636
  - 196.2351837158203
  - 2908.301025390625
  - 16.05133628845215
  - 1.4612351655960083
  - 0.8035764098167419
  - 0.41331547498703003
  - 0.6196743845939636
  - 0.488342821598053
  - 0.5112271308898926
  - 0.47553327679634094
  - 0.9229795336723328
  - 0.4751422703266144
  - 0.45336705446243286
  - 0.4587797224521637
  - 0.8349542021751404
  - 0.6280226707458496
  - 0.5796504616737366
  - 18601984.0
  - 1488416218808320.0
  - 6.542124094798225e+23
  - 2.1391890358262575e+26
  - 0.4211997389793396
  - 0.5783689618110657
  - 0.6378307938575745
  - 0.44247758388519287
  - 0.5309895277023315
  - 0.4637598395347595
  - 0.49566882848739624
  - 0.7255983352661133
  - 0.4757694602012634
  - 0.5543145537376404
  - 0.4245368540287018
  - 150321008.0
  - 1.0805131620646912e+16
  - 1.4988539132142711e+22
  - 2.6769102190106553e+24
  - 2.057248460300259e+25
  - 4.152804812849789e+25
  - 2.8142115257021343e+25
  - 4.433479402628909e+25
  - 4.4980697546658e+25
  - 2.5474999682653075e+25
  - 2.079532588309902e+25
  - 2.1075222942615435e+25
  - 6.518547923856051e+19
  - 4.312569757429178e+25
  - 4.36095602830312e+25
  - 2.6308584988913918e+25
  - 3.7543244731456402e+25
  - 2.396175331436844e+25
  - 2.355258147238347e+25
  - 2.514311508665093e+25
  - 1.2451989207004193e+25
  - 5.331400957027027e+25
  - 1.6720658451726516e+23
  - 6.796709852129893e+19
  - 6.457053908083364e+24
  - 7.904322613884614e+24
  - 1.1421129807933082e+25
  - 1.1216527746039533e+25
  - 6.3845750708394525e+19
  - 7.060140963419442e+19
  - 5.8399262685601464e+19
  - 6.141955075835796e+19
  - 6.048448208963214e+19
  - 6.044402006172998e+19
  - 5.8975868573439754e+19
  - 6.30925192726703e+19
  - 6.871916887274422e+19
  - 6.2639995471049785e+19
  - 6.21035129615323e+19
  - 5.781897123478687e+19
  - 6.384687221025486e+19
  - 6.463754421592811e+19
  - 6.794198567572052e+19
  - 5.673702100672971e+19
  - 6.39723264869841e+19
  - 7.0898840723647365e+19
  - 6.784431385880193e+19
  - 6.070373350430369e+19
  - 6.819171555271403e+19
  - 7.445751126201414e+19
  - 5.8647374081478885e+19
  - 7.165094186141824e+19
  - 6.189835728592883e+19
  - 6.033936414695175e+19
  - 6.149762927806959e+19
  - 6.277697702768463e+19
  - 7.044421025774803e+19
  - 5.891259827633101e+19
  - 6.598124418209363e+19
  - 6.787314745172872e+19
loss_records_fold4:
  train_losses:
  - 9.144691348075867
  - 10.040318220853806
  - 9.606154054403305
  - 12.116699606180191
  - 10.713978737592697
  - 11.230030983686447
  - 9.71833223104477
  - 10.114512383937836
  - 9.200497448444366
  - 8.9927619099617
  - 12.202349483966827
  - 8.75820106267929
  - 8.790160834789276
  - 9.19870612025261
  - 8.394895553588867
  - 8.813233882188797
  - 14.10332876443863
  - 12.300717681646347
  - 9.623034566640854
  - 8.469320207834244
  - 9.101302117109299
  - 9.003126829862595
  - 9.726104766130447
  - 9.64391577243805
  - 9.46401160955429
  - 13.1292225420475
  - 16.57458782196045
  - 14.390761226415634
  - 13.049468874931335
  - 10.66813188791275
  - 11.418894857168198
  - 10.99526360630989
  - 10.410843133926392
  - 13.056882619857788
  - 9.86708688735962
  - 9.889402568340302
  - 9.148232638835907
  - 9.692137956619263
  - 10.098962396383286
  - 11.635177999734879
  - 9.394417315721512
  - 12.877221047878265
  - 9.232857674360275
  - 9.121931701898575
  - 8.960790038108826
  - 8.904338389635086
  - 9.05751246213913
  - 9.148207157850266
  - 12.16445279121399
  - 10.683843195438385
  - 9.880789130926132
  - 8.855879873037338
  - 9.045832872390747
  - 8.449210524559021
  - 8.873995572328568
  - 9.11718887090683
  - 9.222731471061707
  - 11.909059286117554
  - 10.03782120347023
  - 10.464586913585663
  - 14.427127987146378
  - 21.3530752658844
  - 14.628774464130402
  - 12.9822738468647
  - 9.691531091928482
  - 9.790347248315811
  - 10.385071039199829
  - 13.39533431828022
  - 11.607529670000076
  - 10.21973192691803
  - 11.382157385349274
  - 10.015739381313324
  - 13.745812803506851
  - 9.593470931053162
  - 16.46334543824196
  - 9.413931041955948
  - 8.805745393037796
  - 9.630479872226715
  - 9.503534823656082
  - 8.716709822416306
  - 13.977952033281326
  - 11.824803054332733
  - 11.398761302232742
  - 10.177254766225815
  - 16.819775223731995
  - 15.81938749551773
  - 14.251352429389954
  - 17.249426245689392
  - 20.52299627661705
  - 10.613598495721817
  - 8.989000707864761
  - 13.713506519794464
  - 15.85523048043251
  - 12.584978580474854
  - 13.606542974710464
  - 14.491709381341934
  - 12.01121997833252
  - 10.684469997882843
  - 9.314321130514145
  - 9.210619315505028
  validation_losses:
  - 4.9285630705291756e+19
  - 5.402964074909873e+19
  - 5.400485775700866e+19
  - 4.698754145207714e+19
  - 4.253639732544458e+19
  - 4.796050368954263e+19
  - 5.026825105289563e+19
  - 5.280128395073343e+19
  - 4.458611569803762e+19
  - 4.270514597202913e+19
  - 4.740878194886417e+19
  - 4.9495690202755105e+19
  - 5.691329910894127e+19
  - 5.349103838116289e+19
  - 3.6861831008632177e+19
  - 4.296564666492833e+19
  - 5.005155930129354e+19
  - 4.032936962524237e+19
  - 5.282141380961475e+19
  - 5.2118188162721776e+19
  - 5.30517526995408e+19
  - 5.1107389531121254e+19
  - 4.685675674297644e+19
  - 4.667472159788184e+19
  - 4.6431118199678304e+19
  - 4.88063535867442e+19
  - 4.8587357258770285e+19
  - 4.740529869602737e+19
  - 4.4912600282742915e+19
  - 4.950462703326567e+19
  - 4.399311829085245e+19
  - 3.84684110108594e+19
  - 4.758400012186655e+19
  - 4.937961256118754e+19
  - 4.636183577298888e+19
  - 4.712147516247979e+19
  - 4.886555129278366e+19
  - 4.200808638634472e+19
  - 4.145755651627077e+19
  - 5.204200959910294e+19
  - 4.76917522613886e+19
  - 5.123011701901361e+19
  - 4.863459667634605e+19
  - 4.556741223364166e+19
  - 3.966522941769358e+19
  - 4.302277728910757e+19
  - 4.8738425758380196e+19
  - 5.592567818245426e+19
  - 4.804253165502123e+19
  - 5.367309991453655e+19
  - 6.070968406123322e+19
  - 4.5487372185186075e+19
  - 5.255446997857678e+19
  - 4.404903945224113e+19
  - 4.854499967282184e+19
  - 5.4609215318332015e+19
  - 5.093660458900206e+19
  - 4.144454269664441e+19
  - 4.793276960824361e+19
  - 5.457358234549905e+19
  - 5.012899570621455e+19
  - 5.268943283186303e+19
  - 4.776983957719325e+19
  - 5.2756459060692255e+19
  - 4.75117358196426e+19
  - 5.378355245461642e+19
  - 4.420831030859425e+19
  - 5.9291639516844e+19
  - 4.951638301158985e+19
  - 5.194591668088183e+19
  - 5.4700048172925846e+19
  - 4.9774908981605564e+19
  - 5.055979315807021e+19
  - 4.5959489284013556e+19
  - 4.832034745703465e+19
  - 5.4036277401283985e+19
  - 5.263448803679981e+19
  - 5.777595833990827e+19
  - 5.640264192853698e+19
  - 4.7492863802063454e+19
  - 6.021016273459505e+19
  - 4.948994195596509e+19
  - 4.762193327302482e+19
  - 4.842262402865037e+19
  - 4.135587368093404e+19
  - 5.3326511859229e+19
  - 4.796758894247202e+19
  - 4.489867166944225e+19
  - 4.03225306629176e+19
  - 5.16710255797983e+19
  - 3.8180677613963444e+19
  - 5.128842631965783e+19
  - 5.74034174121387e+19
  - 3.7581285447151124e+19
  - 5.467652742018446e+19
  - 4.4250184109426475e+19
  - 5.215872055936811e+19
  - 4.645406280832673e+19
  - 3.866188107688287e+19
  - 4.957964891065208e+19
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 82 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 55 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 97 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:04:37.334989'
