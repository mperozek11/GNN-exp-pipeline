config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:25:20.826901'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_43fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_43fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_43fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_43fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_43fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_43fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_43fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_43fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_43fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_43fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 18.45230016708374
  - 8.002581548690797
  - 11.274100732803346
  - 2.7644026041030885
  - 2.253799295425415
  - 2.0879494547843933
  - 2.009213864803314
  - 2.773614764213562
  - 3.013332162797451
  - 1.4451120257377625
  - 3.9516063928604126
  - 2.2637785911560058
  - 1.905817985534668
  - 1.1357896625995636
  - 1.0029513955116272
  - 0.8570300817489624
  - 0.9232655704021454
  - 0.8820106327533722
  - 0.8195771276950836
  - 0.813103199005127
  - 0.8839071571826935
  - 0.8584550619125366
  - 1.2359016239643097
  - 4.1554349899292
  - 1.340170818567276
  - 1.0313558816909791
  - 0.9243123561143876
  - 1.01924329996109
  - 0.9785994529724121
  - 1.0959194839000703
  - 1.3781192541122438
  - 1.4671223998069765
  - 1.1332689285278321
  validation_losses:
  - 4.504007816314697
  - 3.598480463027954
  - 2.898874044418335
  - 11.915183067321777
  - 5.171050071716309
  - 1.48897385597229
  - 3.9084787368774414
  - 0.6394205689430237
  - 1.1119801998138428
  - 0.5974894165992737
  - 0.7151833176612854
  - 0.9157842993736267
  - 0.6328062415122986
  - 0.45192110538482666
  - 0.3971373438835144
  - 0.6320366263389587
  - 0.5217880010604858
  - 0.4431631565093994
  - 0.3936830461025238
  - 0.3787078559398651
  - 0.391612708568573
  - 0.38073620200157166
  - 0.784359335899353
  - 0.482769250869751
  - 0.5805560350418091
  - 0.40480151772499084
  - 0.6264135241508484
  - 0.5191696882247925
  - 0.4793468415737152
  - 0.48176679015159607
  - 0.44518348574638367
  - 0.39643993973731995
  - 0.39764735102653503
loss_records_fold1:
  train_losses:
  - 1.0443789899349214
  - 0.9364539444446565
  - 1.0135178327560426
  - 0.8432901442050934
  - 1.0131655216217041
  - 0.9089823544025422
  - 1.3566885113716127
  - 1.1523875951766969
  - 0.93369979262352
  - 0.8702913463115692
  - 0.8177920103073121
  - 3.5270513892173767
  - 1.7786686420440674
  - 1.1638547599315643
  - 1.2066843509674072
  - 1.2791478097438813
  - 6.377529299259186
  - 2.34424786567688
  - 1.0290737211704255
  - 1.2902920097112656
  - 0.8992626518011093
  - 1.0022410213947297
  - 1.094597327709198
  - 0.9399220764636994
  validation_losses:
  - 0.5175340175628662
  - 0.4843043386936188
  - 0.47010838985443115
  - 0.5425164103507996
  - 0.39499005675315857
  - 0.6394058465957642
  - 0.9557964205741882
  - 0.5901408791542053
  - 0.39621010422706604
  - 0.5422708988189697
  - 0.397739440202713
  - 0.5288723707199097
  - 0.45450326800346375
  - 0.4211948812007904
  - 0.457619309425354
  - 0.4304295778274536
  - 0.5619802474975586
  - 0.6075909733772278
  - 0.4409456253051758
  - 0.3940163254737854
  - 0.3952638804912567
  - 0.3938845694065094
  - 0.3940616548061371
  - 0.38678884506225586
loss_records_fold2:
  train_losses:
  - 6.515557932853699
  - 0.8937398254871369
  - 1.2140304386615755
  - 0.8661764562129974
  - 1.3919772863388062
  - 1.8511869072914124
  - 3.8963888049125672
  - 1.2810219526290894
  - 0.8871661841869355
  - 0.9481823086738587
  - 1.051784121990204
  - 0.9215972006320954
  - 2.154063004255295
  - 1.838447093963623
  - 2.8374546051025393
  - 2.013827049732208
  - 2.195210373401642
  - 2.6902288496494293
  - 1.2876344561576845
  - 1.2964176356792452
  - 1.1470048010349274
  - 2.83402510881424
  - 1.3550860166549683
  - 1.0832180500030517
  - 2.949566048383713
  - 0.855620551109314
  - 2.0719882905483247
  - 0.9484646081924439
  - 0.9599880456924439
  - 0.8592854797840119
  - 0.8436644613742829
  - 0.9166411697864533
  - 0.84393190741539
  - 1.0412874042987823
  - 0.8414503276348114
  - 1.291662460565567
  - 1.0364729642868042
  - 0.871704638004303
  - 0.8684109032154084
  - 0.8729827284812928
  - 0.8060769319534302
  - 0.9776006221771241
  - 0.8299406588077546
  - 0.793318384885788
  - 0.8128546953201294
  validation_losses:
  - 0.43606823682785034
  - 0.4128522276878357
  - 0.38570037484169006
  - 0.39238476753234863
  - 0.49231085181236267
  - 0.5681151747703552
  - 0.44332432746887207
  - 0.45630544424057007
  - 0.3824022710323334
  - 0.3922939598560333
  - 0.3886512517929077
  - 0.39245739579200745
  - 0.40942296385765076
  - 0.8736339807510376
  - 1.3229069709777832
  - 1.064220905303955
  - 0.7424927353858948
  - 0.6885543465614319
  - 0.6171167492866516
  - 0.43354013562202454
  - 0.48476624488830566
  - 0.83218914270401
  - 0.4219713807106018
  - 0.4313892126083374
  - 0.38485780358314514
  - 0.4057633876800537
  - 0.4278686046600342
  - 0.5928356647491455
  - 0.38646405935287476
  - 0.3888546824455261
  - 0.3780812919139862
  - 0.3930955231189728
  - 0.38322770595550537
  - 0.3955521881580353
  - 0.3795923590660095
  - 0.37924322485923767
  - 0.41616737842559814
  - 0.39446520805358887
  - 0.4218067526817322
  - 0.38905295729637146
  - 0.3867661952972412
  - 0.38515540957450867
  - 0.38999465107917786
  - 0.38150960206985474
  - 0.3826865553855896
loss_records_fold3:
  train_losses:
  - 0.7890801429748535
  - 0.793464344739914
  - 0.7690235912799835
  - 0.9878959059715271
  - 0.856032943725586
  - 5.0805865108966834
  - 1.2313174188137055
  - 0.8811530590057374
  - 0.9125722527503968
  - 3.2840689897537234
  - 2.4343017101287843
  - 1.0264344274997712
  - 0.9094561576843262
  - 0.9055371940135957
  - 0.8140601187944413
  - 2.8018831610679626
  - 1.004783183336258
  - 0.822176080942154
  - 0.8065719962120057
  - 0.8397897303104401
  - 0.8873070180416107
  - 0.9153901934623718
  - 0.8260694921016694
  - 0.8509379327297211
  - 0.8472517728805542
  - 0.830415278673172
  - 0.8553852260112763
  - 0.8431065410375596
  - 0.8949025273323059
  - 0.7978130102157593
  - 0.8494217157363892
  - 0.8236539006233216
  - 0.8263085782527924
  - 0.8281128525733948
  - 0.7807309925556183
  - 0.8006162405014039
  - 0.7996647596359253
  validation_losses:
  - 0.3968554139137268
  - 0.397255539894104
  - 0.39379051327705383
  - 0.39528071880340576
  - 0.39073285460472107
  - 0.3950343132019043
  - 0.39562156796455383
  - 0.3896741569042206
  - 0.390018492937088
  - 0.39114877581596375
  - 1.171875238418579
  - 0.6343541145324707
  - 0.5434063673019409
  - 0.4196928143501282
  - 0.40621453523635864
  - 0.4265631437301636
  - 0.38585788011550903
  - 0.38718271255493164
  - 0.39639317989349365
  - 0.4675939679145813
  - 0.40623724460601807
  - 0.3892247974872589
  - 0.398892879486084
  - 0.39311787486076355
  - 0.41362690925598145
  - 0.3896849751472473
  - 0.496418297290802
  - 0.3893473446369171
  - 0.39481958746910095
  - 0.3882918655872345
  - 0.4223219156265259
  - 0.3998638391494751
  - 0.40841910243034363
  - 0.3991352617740631
  - 0.3973403573036194
  - 0.40139660239219666
  - 0.3947075605392456
loss_records_fold4:
  train_losses:
  - 0.7900345146656037
  - 0.7921624422073364
  - 1.5780279040336609
  - 0.8938390910625458
  - 1.019807243347168
  - 0.8661108016967773
  - 0.783937880396843
  - 0.7493343830108643
  - 0.8477752149105072
  - 0.8173420906066895
  - 0.8133888363838196
  - 0.801474928855896
  - 0.7730428040027619
  - 0.7874327898025513
  - 0.7928212702274323
  - 0.760123109817505
  validation_losses:
  - 0.3929792642593384
  - 0.3955295979976654
  - 0.49269455671310425
  - 0.5117729306221008
  - 0.4097347557544708
  - 0.4342993497848511
  - 0.40594807267189026
  - 0.3940518796443939
  - 0.3980984389781952
  - 0.44964599609375
  - 0.4143253266811371
  - 0.4092000424861908
  - 0.39708828926086426
  - 0.40047693252563477
  - 0.3959945738315582
  - 0.39831292629241943
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 33 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 37 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:13:14.384636'
