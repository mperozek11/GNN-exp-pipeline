config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:20:55.623503'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_81fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_81fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_81fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_81fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_81fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_81fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_81fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_81fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_81fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_81fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.134414941072464
  - 2.977406829595566
  - 2.9888499975204468
  - 3.0035730600357056
  - 3.2030405402183533
  - 2.8964310348033906
  - 2.8671832978725433
  - 2.833730524778366
  - 2.923751637339592
  - 2.9424807846546175
  - 2.9578721582889558
  - 2.934266665577889
  - 3.047016897797585
  - 2.8565329849720005
  - 2.8883252978324894
  - 2.9874023437500004
  - 2.8723037391901016
  - 2.861372083425522
  - 2.874160975217819
  - 2.807389754056931
  - 2.888971680402756
  - 2.8534445226192475
  - 2.8659032076597217
  - 2.8585127234458927
  - 2.85734099149704
  - 2.8479806035757065
  - 2.8575013458728793
  - 2.8371922582387925
  - 2.8526944100856784
  validation_losses:
  - 0.4073024094104767
  - 0.40723171830177307
  - 0.40201467275619507
  - 0.3888559341430664
  - 0.39963212609291077
  - 0.401119589805603
  - 0.39868882298469543
  - 0.3864397704601288
  - 0.3886212110519409
  - 0.4666270911693573
  - 0.39140263199806213
  - 0.40643826127052307
  - 0.3935655951499939
  - 0.3873777985572815
  - 0.40466323494911194
  - 0.3899749219417572
  - 0.390298455953598
  - 0.4185772240161896
  - 0.3901176154613495
  - 0.38808363676071167
  - 0.387305349111557
  - 0.3932201564311981
  - 0.4068945646286011
  - 0.3885076642036438
  - 0.39150017499923706
  - 0.38962289690971375
  - 0.39582520723342896
  - 0.3965205252170563
  - 0.38990727066993713
loss_records_fold1:
  train_losses:
  - 2.8644683331251146
  - 2.8636973738670353
  - 2.8152592480182648
  - 2.837701189517975
  - 2.8740127772092823
  - 2.8139828145504
  - 2.8490730017423633
  - 2.8226456463336946
  - 2.8125598192214967
  - 2.817566215991974
  - 2.8362490743398667
  validation_losses:
  - 0.39299675822257996
  - 0.3895595371723175
  - 0.3843652606010437
  - 0.39850127696990967
  - 0.3884143829345703
  - 0.3961319327354431
  - 0.39304351806640625
  - 0.38440921902656555
  - 0.3870525360107422
  - 0.3945091664791107
  - 0.3900045156478882
loss_records_fold2:
  train_losses:
  - 2.848710721731186
  - 2.8337546646595
  - 2.8043939590454103
  - 2.7934691190719607
  - 2.8070640921592713
  - 2.8023637473583225
  - 2.819019848108292
  - 2.8120798349380496
  - 2.791011953353882
  - 2.835301861166954
  - 2.850664469599724
  - 2.8018881261348727
  - 2.802957880496979
  validation_losses:
  - 0.3818666636943817
  - 0.3900095820426941
  - 0.40027785301208496
  - 0.38444390892982483
  - 0.39107394218444824
  - 0.3832504451274872
  - 0.3943151831626892
  - 0.38750386238098145
  - 0.38429614901542664
  - 0.3868529200553894
  - 0.38316625356674194
  - 0.3876650631427765
  - 0.3836264908313751
loss_records_fold3:
  train_losses:
  - 2.868226793408394
  - 2.8405775249004366
  - 2.8318818241357806
  - 2.840166556835175
  - 2.8332980155944827
  - 2.830453243851662
  - 2.824344420433045
  - 2.8311157256364825
  - 2.827963972091675
  - 2.8219546854496005
  - 2.801367843151093
  validation_losses:
  - 0.39611247181892395
  - 0.39916250109672546
  - 0.3773387670516968
  - 0.36510902643203735
  - 0.3671817481517792
  - 0.3707207143306732
  - 0.36965787410736084
  - 0.3719107508659363
  - 0.3686795234680176
  - 0.367479532957077
  - 0.37337562441825867
loss_records_fold4:
  train_losses:
  - 2.8355963826179504
  - 2.850930425524712
  - 2.803642320632935
  - 2.867654502391815
  - 2.8245270490646366
  - 2.818578279018402
  - 2.7954757034778597
  - 2.8223434746265412
  - 2.7928401648998262
  - 2.782307180762291
  - 2.8145599871873856
  validation_losses:
  - 0.37804874777793884
  - 0.40378716588020325
  - 0.3752921223640442
  - 0.37476488947868347
  - 0.37508973479270935
  - 0.37477797269821167
  - 0.37653759121894836
  - 0.3754212260246277
  - 0.3829312324523926
  - 0.3784937858581543
  - 0.37432757019996643
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8576329331046312,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.023809523809523808]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0047619047619047615
  total_train_time: '0:07:01.486041'
