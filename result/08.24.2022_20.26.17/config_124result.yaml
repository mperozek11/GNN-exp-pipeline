config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:21:35.261278'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_124fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_124fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_124fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_124fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_124fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_124fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_124fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_124fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_124fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_124fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 104.9915331453085
  - 66.23389025330543
  - 54.277548065781595
  - 57.801094834506515
  - 36.53205291032791
  - 26.053570139408112
  - 43.83470131158829
  - 48.475610950589186
  - 41.72535996437073
  - 37.841684538125996
  - 13.832321989536286
  - 25.21678062975407
  - 18.166780191659928
  - 16.659831002354622
  - 19.152815487980842
  - 9.897742629051208
  - 14.497337168455125
  - 11.71804593205452
  - 11.490178990364075
  - 9.255176204442979
  - 8.595045858621598
  - 10.379978361725808
  - 7.982755199074745
  - 8.834166729450226
  - 7.510978055000305
  - 7.621676482260227
  - 7.48748616874218
  - 7.371501943469048
  - 7.173265072703362
  - 7.046070471405983
  - 7.186264175176621
  - 8.754920065402985
  - 8.335812804102899
  - 9.314556592702866
  - 7.328819638490677
  - 7.180389827489853
  - 6.513311940431596
  - 6.4303053438663484
  - 6.525929638743401
  - 7.663475048542023
  - 6.746532163023949
  - 6.571897634863854
  - 6.600573626160622
  - 6.379511716961861
  - 6.59323280453682
  - 6.30196475982666
  - 6.489266565442086
  - 6.269813397526741
  - 6.583095729351044
  - 7.879182648658753
  - 7.728743731975555
  - 6.942815911769867
  - 7.137626504898072
  - 6.541276933252812
  - 6.842913994193077
  - 7.024577161669732
  - 6.258050629496575
  - 6.684755367040634
  - 8.217032274603843
  - 10.273464739322662
  - 11.131254652142525
  - 9.345448997616769
  - 10.58659366965294
  - 9.10581391453743
  - 7.0288084834814075
  - 6.9303698480129245
  - 7.1541958630085
  - 6.456377205252648
  - 6.4760625749826435
  - 6.381513696908951
  - 6.432779532670975
  - 6.7207294523715975
  - 6.695905643701554
  - 6.670949286222458
  - 6.545292481780052
  - 6.6147860348224645
  - 6.447075313329697
  - 6.613144946098328
  - 6.623639222979546
  - 6.346710786223412
  - 6.675646111369133
  - 6.845367351174355
  validation_losses:
  - 0.9621273279190063
  - 0.8266009092330933
  - 1.1160186529159546
  - 0.6512821316719055
  - 0.5636325478553772
  - 0.7375249266624451
  - 0.9638289213180542
  - 0.517389178276062
  - 0.9092029333114624
  - 0.42533010244369507
  - 1.7941664457321167
  - 0.5697385668754578
  - 0.5819069743156433
  - 0.5646172165870667
  - 0.4838770031929016
  - 0.4092658460140228
  - 0.4075143039226532
  - 0.492551326751709
  - 0.4288695454597473
  - 0.41527059674263
  - 0.43672940135002136
  - 0.45370572805404663
  - 0.44833171367645264
  - 0.39456161856651306
  - 0.4243854284286499
  - 0.4045669436454773
  - 0.4940832555294037
  - 0.5153829455375671
  - 0.41903212666511536
  - 0.40051624178886414
  - 0.4060811996459961
  - 2.3265249729156494
  - 0.40178292989730835
  - 0.46992653608322144
  - 0.5108555555343628
  - 0.39540255069732666
  - 0.4027053117752075
  - 0.4244539439678192
  - 0.4824787974357605
  - 0.403768926858902
  - 0.4336088299751282
  - 0.4124596416950226
  - 0.43875908851623535
  - 0.40611109137535095
  - 0.4045427739620209
  - 0.4331206977367401
  - 0.39841514825820923
  - 0.41922318935394287
  - 0.4686965048313141
  - 0.43356063961982727
  - 0.4027947187423706
  - 0.41116881370544434
  - 0.41037803888320923
  - 0.4153347611427307
  - 0.578270435333252
  - 0.41475170850753784
  - 0.43015602231025696
  - 0.4231669008731842
  - 0.5488302111625671
  - 0.42803075909614563
  - 0.6573238372802734
  - 0.4109434485435486
  - 0.4539133310317993
  - 0.414610892534256
  - 0.4316992163658142
  - 0.4104088246822357
  - 0.4231409430503845
  - 0.4094720482826233
  - 0.41731032729148865
  - 0.401557594537735
  - 0.4274296164512634
  - 0.5790572166442871
  - 0.41471347212791443
  - 0.41558006405830383
  - 0.4142170548439026
  - 0.4963354468345642
  - 0.4414622187614441
  - 0.44415128231048584
  - 0.4333507716655731
  - 0.4165186882019043
  - 0.4034292995929718
  - 0.40738868713378906
loss_records_fold1:
  train_losses:
  - 6.465751686692238
  - 6.609270530939103
  - 6.335518398880959
  - 6.616736894845963
  - 6.4232949972152715
  - 7.785193991661072
  - 9.20232515335083
  - 6.160204389691353
  - 6.435482847690583
  - 7.34255108833313
  - 6.594913867115975
  - 6.679058450460435
  - 6.751514407992364
  - 7.243287906050682
  - 6.718221104145051
  - 6.568532878160477
  - 6.594365361332894
  - 6.299740701913834
  - 6.529206469655037
  - 6.190204331278801
  - 6.662495751678944
  - 6.477240172028542
  - 6.467334377765656
  - 6.478369960188866
  - 6.736798065900803
  - 6.27670086324215
  - 6.833001831173897
  - 6.386948969960213
  validation_losses:
  - 0.4414746165275574
  - 0.4163719415664673
  - 0.41422730684280396
  - 19.956623077392578
  - 2.7379114627838135
  - 0.5189035534858704
  - 0.4119775891304016
  - 0.41935965418815613
  - 31571.736328125
  - 0.46961769461631775
  - 0.40705880522727966
  - 0.4135039150714874
  - 0.4415118396282196
  - 0.4683969020843506
  - 0.42968472838401794
  - 0.40882769227027893
  - 0.422385036945343
  - 0.45877543091773987
  - 0.41356146335601807
  - 0.4225446283817291
  - 0.418130099773407
  - 0.42876380681991577
  - 0.43442007899284363
  - 0.42201849818229675
  - 0.4249785840511322
  - 0.4190188944339752
  - 0.42646801471710205
  - 0.42672955989837646
loss_records_fold2:
  train_losses:
  - 6.399366572499275
  - 6.939573508501053
  - 6.26003592312336
  - 6.325157123804093
  - 6.46367801129818
  - 6.343789365887642
  - 6.598635751008988
  - 6.240894424915314
  - 6.681093519926072
  - 7.1648430049419405
  - 6.73892649114132
  - 6.520394518971443
  - 6.39873737692833
  - 6.570671790838242
  - 6.758427867293358
  - 6.6648889809846885
  - 6.693468427658082
  - 6.796511819958687
  - 7.305555212497712
  - 9.114312887191772
  - 6.457872146368027
  - 8.089136216044427
  - 8.599537461996078
  - 7.663451090455055
  - 6.723085513710976
  - 6.5379645258188255
  - 6.688314807415009
  - 6.595707413554192
  - 6.395801454782486
  - 6.830138075351716
  - 6.463590717315674
  - 6.714096254110337
  - 6.459215125441552
  - 6.395119202136994
  - 6.799393281340599
  - 6.693197309970856
  - 6.484673392772675
  - 6.772954788804054
  - 6.645582324266434
  - 6.52541248947382
  - 6.54853906929493
  - 6.569798284769059
  - 6.525808593630791
  - 6.883164155483247
  - 6.426144155859948
  - 6.308772841095925
  - 6.61989418566227
  - 6.6521564245224
  - 6.539141936600209
  - 6.66961399614811
  - 6.585469970107079
  - 6.324612018465996
  - 6.542402505874634
  - 6.399220070242882
  - 6.579635101556779
  - 6.62982392013073
  - 6.43258153796196
  - 6.423873940110207
  - 6.601452270150185
  - 6.504410228133202
  - 6.577959474921227
  - 6.302506095170975
  - 6.402889233827591
  - 6.535573643445969
  - 6.715071967244149
  - 6.750253054499627
  - 6.667891961336136
  - 6.5356630265712745
  - 6.613021537661552
  - 6.437407371401787
  - 6.4690753579139715
  - 6.351434147357941
  - 6.368248787522316
  - 6.409718716144562
  - 6.210795912146569
  - 6.596227827668191
  - 6.707931277155876
  - 6.38989042043686
  - 6.919641882181168
  - 6.5004282921552665
  - 6.537715691328049
  - 6.291217434406281
  - 6.512394237518311
  - 6.495822539925576
  - 6.58760823905468
  - 6.55787308216095
  - 6.546619212627411
  - 6.323333334922791
  - 6.374536827206612
  - 6.508840745687485
  - 6.474344149231911
  - 6.64299102127552
  - 6.972916641831398
  - 6.724276524782181
  - 6.2428103476762775
  - 6.501520857214928
  - 6.320067450404167
  - 6.441207325458527
  - 6.336185157299042
  - 6.789305338263512
  validation_losses:
  - 0.4374435544013977
  - 0.4911530613899231
  - 0.42030152678489685
  - 0.4055488407611847
  - 0.41106942296028137
  - 0.397744745016098
  - 0.4042545258998871
  - 1.4439713954925537
  - 20.779584884643555
  - 0.4457266330718994
  - 144.23577880859375
  - 68.13481140136719
  - 1562.170654296875
  - 0.44545772671699524
  - 819631.875
  - 3190904.75
  - 0.6622025370597839
  - 959012.0625
  - 54.43046951293945
  - 158.84912109375
  - 0.41840726137161255
  - 0.40766584873199463
  - 0.4696286916732788
  - 2.14996600151062
  - 0.4532760679721832
  - 0.44828230142593384
  - 0.3981659710407257
  - 0.3972555100917816
  - 0.4756128489971161
  - 0.4036657214164734
  - 0.4125042259693146
  - 0.4000687599182129
  - 0.41473254561424255
  - 0.4016629457473755
  - 0.3995002806186676
  - 0.39825135469436646
  - 0.43742698431015015
  - 0.4087032079696655
  - 0.42592382431030273
  - 0.3973081111907959
  - 0.39497581124305725
  - 0.40534013509750366
  - 0.43104660511016846
  - 0.4188252091407776
  - 0.4104914665222168
  - 0.40587806701660156
  - 0.44991108775138855
  - 0.4072810709476471
  - 0.4149401783943176
  - 0.401398241519928
  - 0.39908483624458313
  - 0.39503344893455505
  - 0.40722858905792236
  - 0.4088263213634491
  - 0.3968828022480011
  - 0.4202028214931488
  - 0.41172918677330017
  - 0.4068630039691925
  - 0.4524310231208801
  - 0.4741836190223694
  - 0.396415650844574
  - 0.40496206283569336
  - 0.41878947615623474
  - 0.42137548327445984
  - 0.40407514572143555
  - 0.4809271991252899
  - 0.3963683843612671
  - 0.4114186763763428
  - 0.4105275571346283
  - 0.42118361592292786
  - 0.4325679838657379
  - 0.40632301568984985
  - 0.4376455247402191
  - 0.3961885869503021
  - 0.39790621399879456
  - 0.4024699926376343
  - 0.40520530939102173
  - 0.4096280038356781
  - 429141376.0
  - 0.4024403393268585
  - 0.4030943810939789
  - 0.41557833552360535
  - 0.405185341835022
  - 0.42700669169425964
  - 0.4517451226711273
  - 0.4149872064590454
  - 0.40282928943634033
  - 0.4218991696834564
  - 0.3911820650100708
  - 0.41736704111099243
  - 0.4099031090736389
  - 0.4508998394012451
  - 0.42736107110977173
  - 0.4056890606880188
  - 0.41074928641319275
  - 0.44873517751693726
  - 0.4163011908531189
  - 0.4029989540576935
  - 0.4115231931209564
  - 0.45794421434402466
loss_records_fold3:
  train_losses:
  - 6.1945387661457065
  - 6.328008690476418
  - 6.639860954880715
  - 6.367175135016442
  - 6.207539284229279
  - 6.332925790548325
  - 6.4171503543853765
  - 6.56849485039711
  - 6.565726578235626
  - 6.772144439816476
  - 6.4065826594829565
  - 6.219006302952767
  - 6.311015367507935
  - 6.588146424293519
  - 6.531931406259537
  - 6.643848189711571
  - 6.222349092364311
  - 6.473892939090729
  - 6.3286906540393835
  - 6.173392698168755
  - 6.431983187794685
  - 6.651492211222649
  - 6.334532958269119
  - 6.370010739564896
  - 6.5494362920522695
  - 6.4592626154422765
  - 6.451757389307023
  - 6.789665386080742
  - 6.287653332948685
  - 6.487735423445702
  - 6.503978407382966
  - 6.616762199997902
  - 6.4665524661540985
  - 6.273821473121643
  - 6.4712900757789615
  - 6.409096547961235
  - 6.3904799908399585
  - 6.622859480977059
  - 6.8043253749609
  - 6.46317894756794
  - 6.348109686374665
  - 6.388628450036049
  - 6.548950400948525
  - 6.547713547945023
  - 6.6072146028280265
  - 6.368723472952843
  - 6.402217763662339
  - 6.441511085629464
  - 6.386193621158601
  - 6.3367707401514055
  - 6.3850026607513435
  - 6.515351390838624
  - 6.436814445257188
  - 6.447125512361527
  - 6.317126074433327
  - 6.632720848917962
  - 6.383026504516602
  - 6.447119271755219
  - 6.3481098324060445
  - 6.316059087216854
  - 6.405527389049531
  - 6.397709372639657
  - 6.280860960483551
  - 6.461673685908318
  validation_losses:
  - 0.41249316930770874
  - 0.4365161955356598
  - 0.45736491680145264
  - 0.414638876914978
  - 0.4775153696537018
  - 0.4130791425704956
  - 0.485636442899704
  - 0.42508161067962646
  - 0.40860646963119507
  - 0.439135879278183
  - 0.4083171486854553
  - 0.40445196628570557
  - 0.41630807518959045
  - 0.4246460795402527
  - 0.48671799898147583
  - 0.44317010045051575
  - 0.42062637209892273
  - 0.4381026029586792
  - 0.40328025817871094
  - 0.40763095021247864
  - 0.4399511516094208
  - 0.41809961199760437
  - 0.4137815237045288
  - 0.4104554057121277
  - 0.4130163788795471
  - 0.43780219554901123
  - 0.49460652470588684
  - 0.41195645928382874
  - 0.41369348764419556
  - 0.4747897684574127
  - 0.40318670868873596
  - 0.41365981101989746
  - 0.41536352038383484
  - 0.42729291319847107
  - 0.43008264899253845
  - 0.43390342593193054
  - 0.41312870383262634
  - 0.45044198632240295
  - 0.4243241548538208
  - 0.41651028394699097
  - 0.4145169258117676
  - 0.44184213876724243
  - 0.4311041831970215
  - 0.42162778973579407
  - 0.4042412042617798
  - 0.4149113893508911
  - 0.4086861312389374
  - 0.42019325494766235
  - 0.4196402132511139
  - 0.42156124114990234
  - 0.4753205180168152
  - 0.41041865944862366
  - 0.4127464294433594
  - 0.4152394235134125
  - 0.41536644101142883
  - 0.40499788522720337
  - 0.44214725494384766
  - 0.45745161175727844
  - 0.435065895318985
  - 0.43141642212867737
  - 0.4312542676925659
  - 0.43270954489707947
  - 0.4330042004585266
  - 0.40469813346862793
loss_records_fold4:
  train_losses:
  - 6.497823664546013
  - 6.795818847417832
  - 6.401349747180939
  - 6.6995630949735645
  - 6.180116486549378
  - 6.381881320476532
  - 6.61912186741829
  - 6.404457998275757
  - 6.3108667790889745
  - 6.4560915499925615
  - 6.518496412038804
  - 6.502188442647458
  - 6.355738154053689
  - 6.504953601956368
  - 6.524620009958745
  - 6.270715749263764
  - 6.587738654017449
  - 6.355421793460846
  - 6.829987388849259
  - 6.4564417690038685
  - 6.374560484290123
  - 6.67351128757
  - 6.322120344638825
  - 6.525726354122162
  - 6.448438256978989
  - 6.679183998703957
  - 6.501803612709046
  - 6.439474588632584
  - 6.463291814923287
  - 6.478957349061966
  - 6.272938925027848
  - 6.616282904148102
  - 6.566754728555679
  - 6.563203939795494
  - 6.662823414802552
  - 6.51604894399643
  - 6.435175579786301
  - 6.254037684202195
  - 6.728227046132088
  - 6.510004007816315
  - 6.575631803274155
  - 6.376668456196786
  - 6.4450434148311615
  - 6.591646391153336
  - 6.4507931768894196
  - 6.331561037898064
  - 6.338897567987442
  - 6.441860374808312
  - 6.166125309467316
  - 6.670519730448723
  - 6.4508789598941805
  - 6.54774418771267
  - 6.576210978627206
  - 6.362499770522118
  - 6.589870274066925
  - 6.398098829388619
  - 6.619458171725274
  - 6.3288985967636116
  - 6.426868960261345
  - 6.638624489307404
  - 6.355500873923302
  - 6.494832226634026
  - 6.6284575998783115
  - 6.631077921390534
  - 6.402392926812173
  - 6.633236476778984
  - 6.558032858371735
  - 6.701475621759892
  - 6.53599965274334
  - 6.5075883090496065
  - 6.561061981320382
  - 6.459813246130944
  - 6.318114364147187
  - 6.400143682956696
  - 6.565573409199715
  - 6.29977111518383
  - 6.418133035302162
  - 6.90417859852314
  - 6.397197401523591
  - 6.577686640620232
  - 6.42351277768612
  - 6.329339841008187
  - 6.24755634367466
  - 6.227665185928345
  - 6.42621777355671
  - 6.399044007062912
  - 6.738771837949753
  - 6.6739902883768085
  - 6.472729209065438
  - 6.300737935304642
  - 6.485930275917053
  - 6.411756777763367
  - 6.658082628250122
  - 6.405990052223206
  - 6.656996044516564
  - 6.649829965829849
  - 6.385552018880844
  - 6.420224535465241
  - 6.352611160278321
  - 6.4005161255598075
  validation_losses:
  - 0.43523603677749634
  - 0.43711912631988525
  - 0.4638049006462097
  - 0.41661643981933594
  - 0.4344189763069153
  - 0.4072667360305786
  - 0.4364112317562103
  - 0.4003404378890991
  - 0.40508532524108887
  - 0.4467177391052246
  - 0.41330966353416443
  - 0.41259074211120605
  - 0.4022672772407532
  - 0.4287569522857666
  - 0.39769116044044495
  - 0.40932872891426086
  - 0.41868484020233154
  - 0.4009239971637726
  - 0.40195634961128235
  - 0.4406736195087433
  - 0.4547561705112457
  - 0.4001135528087616
  - 0.42033350467681885
  - 0.44853031635284424
  - 0.4034956693649292
  - 0.40421199798583984
  - 0.4487391710281372
  - 0.42374110221862793
  - 0.40573766827583313
  - 0.4114288091659546
  - 0.4104849398136139
  - 0.45848798751831055
  - 0.46975719928741455
  - 0.4330952763557434
  - 0.40762975811958313
  - 0.4167444705963135
  - 0.40108129382133484
  - 0.39586707949638367
  - 0.5187833309173584
  - 0.48915520310401917
  - 0.4300805628299713
  - 0.41535693407058716
  - 0.4487121105194092
  - 0.39834311604499817
  - 0.41210511326789856
  - 0.40993356704711914
  - 0.40847358107566833
  - 0.4189642071723938
  - 0.4296610355377197
  - 0.4242105484008789
  - 0.40472617745399475
  - 0.4073875844478607
  - 0.4189373552799225
  - 0.43041858077049255
  - 0.4206372797489166
  - 0.4218461811542511
  - 0.46666914224624634
  - 0.4044376313686371
  - 0.40245580673217773
  - 0.43650540709495544
  - 0.42378929257392883
  - 0.4552461802959442
  - 0.5329640507698059
  - 0.42998644709587097
  - 0.40607544779777527
  - 0.4098813235759735
  - 0.4089166224002838
  - 0.45926377177238464
  - 0.44015392661094666
  - 0.4147222936153412
  - 0.4748876094818115
  - 0.4008474051952362
  - 0.4062730371952057
  - 0.4083530306816101
  - 0.4210486114025116
  - 0.5023175477981567
  - 0.4416327178478241
  - 0.4206291735172272
  - 0.4092852473258972
  - 0.4123043119907379
  - 0.4312606155872345
  - 0.40065574645996094
  - 0.4100646376609802
  - 0.4124417304992676
  - 0.4262559711933136
  - 0.40396222472190857
  - 0.405992329120636
  - 0.4044402539730072
  - 0.4178124964237213
  - 0.4111466109752655
  - 0.4120335578918457
  - 0.43823903799057007
  - 0.4267781674861908
  - 0.4044090807437897
  - 0.43390050530433655
  - 0.4299751818180084
  - 0.4542798101902008
  - 0.4046087861061096
  - 0.4162832200527191
  - 0.4109857976436615
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 82 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 64 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:26:02.612300'
