config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 19:01:12.337598'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/55/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/55/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/55/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/55/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/55/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/55/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/55/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/55/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/55/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/55/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 4.249442547559738
  - 4.17240971326828
  - 4.180750519037247
  - 5.019557595252991
  - 5.50996521115303
  - 4.340997874736786
  - 3.870463877916336
  - 4.5789448618888855
  - 4.6842943131923676
  - 4.38223859667778
  - 4.069570153951645
  - 3.8829499185085297
  - 3.865231901407242
  - 3.9210575222969055
  - 3.9546698331832886
  - 5.025484889745712
  - 3.911562740802765
  - 3.798470377922058
  - 4.390706270933151
  - 4.111257314682007
  - 3.9399293810129166
  - 4.173852890729904
  - 4.906944900751114
  - 6.1394262611866
  - 4.963038727641106
  - 4.423906534910202
  - 4.028274565935135
  - 4.181307598948479
  - 4.256194710731506
  - 4.072677820920944
  - 4.153898924589157
  - 3.8197203278541565
  - 3.9208487272262573
  - 4.335322916507721
  - 4.21309694647789
  - 4.083422511816025
  - 3.793385088443756
  - 3.7803248167037964
  - 4.232180267572403
  - 5.202764928340912
  - 4.994268596172333
  - 3.9434681981801987
  - 5.540926486253738
  - 5.6973618268966675
  - 3.9752129316329956
  - 4.0217829048633575
  - 4.257344365119934
  - 4.230337679386139
  - 4.292485594749451
  - 3.9555927217006683
  - 3.845294713973999
  - 3.944195955991745
  - 4.163292646408081
  - 3.9071366488933563
  - 4.163677483797073
  - 3.825316086411476
  - 4.733642637729645
  - 4.086576908826828
  - 3.9720241725444794
  - 4.489971965551376
  - 6.037908166646957
  - 4.723479211330414
  - 4.483464077115059
  - 6.281698048114777
  - 11.231415927410126
  - 10.504801750183105
  - 9.289673328399658
  - 6.1515540182590485
  - 5.849780976772308
  - 11.172951817512512
  - 8.488583952188492
  - 7.881708920001984
  - 6.93932244181633
  - 6.018495738506317
  - 5.351803153753281
  - 5.031872510910034
  - 4.332126677036285
  - 3.976449429988861
  - 4.023296654224396
  - 4.444414287805557
  - 5.0406555235385895
  - 4.67055544257164
  - 4.326670289039612
  - 4.11043581366539
  - 3.9725497663021088
  - 4.112963974475861
  - 4.066366493701935
  - 3.952612727880478
  - 4.342900484800339
  - 4.613101035356522
  - 4.489880442619324
  - 4.76764777302742
  - 4.714636355638504
  - 4.22840541601181
  - 3.93011274933815
  - 4.285629779100418
  - 5.3083440363407135
  - 5.934489697217941
  - 7.091137111186981
  - 8.104260854423046
  validation_losses:
  - 0.47179144620895386
  - 0.4550609588623047
  - 0.4387873411178589
  - 0.682948112487793
  - 0.5663267970085144
  - 0.42653292417526245
  - 0.4767141044139862
  - 0.4825769364833832
  - 0.507707953453064
  - 0.41727176308631897
  - 0.411418080329895
  - 0.40186506509780884
  - 0.42017942667007446
  - 0.40348970890045166
  - 0.40295934677124023
  - 0.4565325677394867
  - 0.4198620617389679
  - 0.4559132754802704
  - 0.4033372402191162
  - 0.43026822805404663
  - 0.43112513422966003
  - 0.48641514778137207
  - 0.45566943287849426
  - 0.5220632553100586
  - 0.5546881556510925
  - 0.42382532358169556
  - 0.4049874544143677
  - 0.40516045689582825
  - 0.41821959614753723
  - 0.41079431772232056
  - 0.40615740418434143
  - 0.4063261151313782
  - 0.494401752948761
  - 0.4221244752407074
  - 0.4587253928184509
  - 0.4473634660243988
  - 0.4295770823955536
  - 0.48945528268814087
  - 0.41654202342033386
  - 0.48746681213378906
  - 0.43130338191986084
  - 0.44197651743888855
  - 0.43990594148635864
  - 0.417314350605011
  - 1.731162190437317
  - 3.487013816833496
  - 1.754931092262268
  - 0.4324665367603302
  - 1.0602085590362549
  - 0.4624936878681183
  - 0.45026105642318726
  - 0.47705185413360596
  - 0.4083039462566376
  - 0.4319780468940735
  - 89777.46875
  - 8154498.0
  - 267282.71875
  - 2959988.5
  - 28005242.0
  - 233272048.0
  - 1732135808.0
  - 10940892160.0
  - 49411076096.0
  - 58974261248.0
  - 236896763904.0
  - 1453580484608.0
  - 6017697447936.0
  - 17519875194880.0
  - 17286741098496.0
  - 1.3230383396148682
  - 0.6260043382644653
  - 0.6178153157234192
  - 0.4339628219604492
  - 0.4630466401576996
  - 0.7677323818206787
  - 0.4383297562599182
  - 0.41057005524635315
  - 0.4436884820461273
  - 0.44909465312957764
  - 0.4096618592739105
  - 0.5910587310791016
  - 0.4174676239490509
  - 0.43089595437049866
  - 0.4319066107273102
  - 0.43258535861968994
  - 0.42766496539115906
  - 0.4361110031604767
  - 0.434160977602005
  - 0.5193156003952026
  - 0.523865818977356
  - 0.5501397252082825
  - 0.5188416242599487
  - 0.41508549451828003
  - 0.4179292321205139
  - 0.4487125873565674
  - 0.45740726590156555
  - 0.4216456711292267
  - 0.8435217142105103
  - 1.2419426441192627
  - 1.1751104593276978
loss_records_fold4:
  train_losses:
  - 6.521921753883362
  - 4.3386745154857635
  - 4.065710708498955
  - 4.46287077665329
  - 26.442265272140503
  - 4.48785400390625
  - 13.362706542015076
  - 18.240125745534897
  - 5.624388873577118
  - 5.569606065750122
  - 7.355037361383438
  - 5.445270776748657
  - 4.990557432174683
  - 4.633295953273773
  - 4.296996474266052
  - 4.179693758487701
  - 4.005909234285355
  - 4.091084837913513
  - 4.024858146905899
  - 4.116438999772072
  - 3.999845117330551
  - 4.117667853832245
  - 4.378260016441345
  - 5.45656681060791
  - 4.0092756152153015
  - 3.9239524453878403
  - 21.553620100021362
  - 3.969597578048706
  - 4.89978700876236
  - 6.01874178647995
  - 11.777729868888855
  - 4.005227401852608
  - 4.004105925559998
  - 3.941978633403778
  - 3.899824172258377
  - 4.116379201412201
  - 4.377843216061592
  - 4.61344701051712
  - 3.959538698196411
  - 4.522256582975388
  - 5.2322066724300385
  - 8.047002762556076
  - 7.774328052997589
  - 6.27640038728714
  - 29.004491358995438
  - 6.464788377285004
  - 5.377654820680618
  - 6.045313537120819
  - 8.464085698127747
  - 5.576625317335129
  - 4.773879259824753
  - 5.076075658202171
  - 4.8710635006427765
  - 4.267646998167038
  - 4.157100945711136
  - 5.386825531721115
  - 4.424964904785156
  - 3.9945636093616486
  - 4.044018924236298
  - 3.962456524372101
  - 3.7674504220485687
  - 4.212435781955719
  - 13.26735234260559
  - 4.926690548658371
  - 4.5185796320438385
  - 4.508122056722641
  - 4.205361396074295
  - 8.351179212331772
  - 3.952832281589508
  - 12.8720061480999
  - 6.32049560546875
  - 10.587717711925507
  - 10.32777738571167
  - 9.155394732952118
  - 5.556924372911453
  - 4.621886968612671
  - 4.242436632514
  - 4.310755670070648
  - 4.595511049032211
  - 3.880332827568054
  - 4.212193340063095
  - 3.9532150477170944
  - 4.250969797372818
  - 4.053864777088165
  - 4.1477062702178955
  - 4.227118939161301
  - 4.317467272281647
  - 5.801171094179153
  - 5.424129694700241
  - 4.958339542150497
  - 4.337312623858452
  - 4.279466688632965
  - 5.147095143795013
  - 4.895190536975861
  - 6.379774481058121
  - 6.028160125017166
  - 6.548802763223648
  - 6.9797444343566895
  - 5.460201278328896
  - 5.4361472725868225
  validation_losses:
  - 3.3209211826324463
  - 1.7588119506835938
  - 1.675695538520813
  - 1.4150172472000122
  - 1.2309497594833374
  - 0.7575674057006836
  - 6.894673824310303
  - 22.386518478393555
  - 6.851061820983887
  - 0.7684345245361328
  - 0.6211410760879517
  - 0.7103145718574524
  - 0.7345845699310303
  - 0.6792108416557312
  - 0.6425061821937561
  - 0.6444781422615051
  - 0.6633054614067078
  - 0.4429110586643219
  - 0.4905773103237152
  - 0.5715938210487366
  - 0.6035012006759644
  - 0.6424157023429871
  - 0.4439617693424225
  - 0.507419764995575
  - 0.4553890824317932
  - 0.45413222908973694
  - 0.46573197841644287
  - 540.0072631835938
  - 140.44467163085938
  - 10.519420623779297
  - 1.0592252016067505
  - 0.6273677945137024
  - 0.6378976106643677
  - 0.5570384860038757
  - 0.5915430784225464
  - 0.6347202062606812
  - 0.6091626286506653
  - 0.5375285148620605
  - 0.5485652089118958
  - 0.5918092727661133
  - 0.5827677845954895
  - 0.6116797924041748
  - 0.7021716237068176
  - 0.6621044278144836
  - 0.45620906352996826
  - 277.3396911621094
  - 0.6356961727142334
  - 0.8684481978416443
  - 0.8029375076293945
  - 0.4470253586769104
  - 0.4636184871196747
  - 0.5728041529655457
  - 0.42699551582336426
  - 0.45009568333625793
  - 0.6485385298728943
  - 0.5587562322616577
  - 0.4752419590950012
  - 0.4485185444355011
  - 0.40183040499687195
  - 0.43445271253585815
  - 0.4863634705543518
  - 0.45406606793403625
  - 0.5466499924659729
  - 1.0556330680847168
  - 1.0407867431640625
  - 0.9213652610778809
  - 0.9099298715591431
  - 0.9603554606437683
  - 0.9298691749572754
  - 0.9329242706298828
  - 16.548524856567383
  - 9.124397277832031
  - 0.7940652966499329
  - 0.6956057548522949
  - 0.4550405442714691
  - 0.596036434173584
  - 0.4979526996612549
  - 0.41587939858436584
  - 0.42494457960128784
  - 0.4323342442512512
  - 0.45679140090942383
  - 0.4478198289871216
  - 0.409954696893692
  - 0.43112337589263916
  - 424746.40625
  - 6188291.0
  - 482407872.0
  - 5478969856.0
  - 24544012288.0
  - 53298454528.0
  - 68867866624.0
  - 116162789376.0
  - 138012098560.0
  - 140201607168.0
  - 141574340608.0
  - 123821391872.0
  - 144814997504.0
  - 141248053248.0
  - 156896460800.0
  - 136254521344.0
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 76 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.140893470790378]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.24698795180722893]'
  mean_eval_accuracy: 0.7149711469882643
  mean_f1_accuracy: 0.04939759036144579
  total_train_time: '0:02:35.405353'
