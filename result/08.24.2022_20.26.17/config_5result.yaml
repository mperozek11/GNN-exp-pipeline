config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.843498'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_5fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_5fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_5fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_5fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_5fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_5fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_5fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_5fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_5fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_5fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.735962361097336
  - 3.2474266409873964
  - 3.158613067865372
  - 3.1029302030801773
  - 3.266251501441002
  - 3.0777351379394533
  - 3.1605448842048647
  - 3.2284286975860597
  - 3.134006786346436
  - 3.0801466286182406
  - 3.125071269273758
  - 3.0568695187568666
  - 3.0268807053565983
  - 3.197100353240967
  - 3.1517066180706026
  - 3.1041630506515503
  - 3.0586162924766542
  - 3.1427445352077488
  - 3.1683520257472995
  validation_losses:
  - 0.4782696068286896
  - 0.4097864329814911
  - 0.4060274362564087
  - 0.4176347553730011
  - 0.4322059750556946
  - 0.39212730526924133
  - 0.4093572497367859
  - 0.4010384678840637
  - 0.3904794752597809
  - 0.42209118604660034
  - 0.3900051712989807
  - 0.400687038898468
  - 0.6851887106895447
  - 0.39402252435684204
  - 0.39447686076164246
  - 0.4010201692581177
  - 0.39670485258102417
  - 0.3880373537540436
  - 0.3897412121295929
loss_records_fold1:
  train_losses:
  - 2.959512022137642
  - 3.1515773713588717
  - 3.0904052376747133
  - 3.129050961136818
  - 3.043133401870728
  - 3.0767582416534425
  - 3.035925704240799
  - 2.9636601686477664
  - 3.0217660009860996
  - 2.9859405517578126
  - 3.0854535877704623
  - 3.0107886373996737
  - 3.0051000118255615
  - 3.0127961546182633
  - 3.114962261915207
  - 2.9360893785953524
  - 2.936640936136246
  - 3.0591084599494938
  - 3.0111847996711734
  - 3.0436009526252747
  - 3.0208073556423187
  - 2.9779775589704514
  - 3.023017305135727
  - 3.0212787926197056
  - 3.0349965155124665
  - 2.995193934440613
  - 3.0020429611206056
  - 3.0182168722152714
  - 2.949914175271988
  - 3.0046391636133194
  - 3.003760033845902
  - 3.007641464471817
  - 3.012331348657608
  - 2.8988251745700837
  - 3.0008430123329166
  - 3.0198215663433077
  - 3.0430814504623416
  - 3.0256620168685915
  - 3.012546825408936
  - 2.96237605214119
  - 3.1405005097389225
  - 3.114285254478455
  - 3.0447096168994907
  - 2.9565247088670734
  - 2.982516658306122
  - 3.0591709554195408
  - 3.0302772998809817
  - 2.9476164460182193
  - 3.091858619451523
  - 3.0231936097145082
  - 3.016124129295349
  - 3.002352786064148
  - 2.9541416585445406
  - 3.04049910902977
  - 3.057691326737404
  - 2.9695916295051576
  - 2.9682240784168243
  - 2.962043756246567
  - 2.9683151662349703
  - 2.9629137217998505
  - 2.889907765388489
  - 2.991998714208603
  - 3.0109737515449524
  - 2.990637269616127
  - 2.9451415777206424
  - 2.931865668296814
  - 2.9104529291391374
  - 2.981212443113327
  - 3.0173342525959015
  - 3.018175995349884
  - 2.9109588444232943
  - 2.9957007229328156
  - 2.9554290711879734
  - 2.964076340198517
  - 2.932503187656403
  - 3.0596586108207706
  - 2.9650067389011383
  - 2.8845705807209017
  - 2.9932542592287064
  - 2.9367932826280594
  - 2.9159975826740268
  - 2.896510809659958
  - 2.9532589972019196
  - 2.9351724922657016
  - 2.935562488436699
  - 3.0275379449129107
  - 3.0770868360996246
  - 2.959160214662552
  - 2.892879727482796
  - 2.9685413181781772
  - 2.9587704598903657
  - 3.01453292965889
  - 3.0429623067379
  - 2.9499362766742707
  - 2.934177413582802
  - 2.9529246866703036
  - 2.935045295953751
  - 2.8739097207784656
  - 2.9709890663623812
  - 2.9778235018253327
  validation_losses:
  - 0.41012951731681824
  - 0.3934031128883362
  - 0.39834144711494446
  - 0.39722147583961487
  - 0.4047376811504364
  - 0.4329483211040497
  - 0.4015883803367615
  - 0.386592298746109
  - 0.3940616548061371
  - 0.37933143973350525
  - 0.39122137427330017
  - 0.4961897134780884
  - 0.40263769030570984
  - 0.392006516456604
  - 0.41424560546875
  - 0.815333366394043
  - 0.7355314493179321
  - 0.404065877199173
  - 0.39756572246551514
  - 0.386381059885025
  - 0.40312933921813965
  - 0.3901682198047638
  - 0.4321688711643219
  - 0.40498653054237366
  - 0.4053956866264343
  - 0.38934797048568726
  - 0.3923341929912567
  - 0.8168513178825378
  - 1.1386138200759888
  - 0.44595447182655334
  - 0.4917925000190735
  - 1.1537119150161743
  - 1.2846734523773193
  - 1.1927350759506226
  - 0.40497809648513794
  - 0.3981887698173523
  - 0.42287716269493103
  - 0.40565910935401917
  - 0.9126834273338318
  - 1.7826621532440186
  - 0.3905267119407654
  - 0.41318896412849426
  - 0.4055498242378235
  - 0.3995358943939209
  - 0.532126247882843
  - 0.4355955123901367
  - 0.612705647945404
  - 0.492715060710907
  - 0.5803675651550293
  - 0.43802565336227417
  - 0.5794225931167603
  - 0.43400177359580994
  - 0.45076820254325867
  - 0.4936996400356293
  - 0.5281039476394653
  - 0.6279622912406921
  - 0.738150954246521
  - 0.7685143947601318
  - 0.7598266005516052
  - 0.5741968154907227
  - 0.7903386354446411
  - 0.7294045686721802
  - 0.698066771030426
  - 0.6554437875747681
  - 1.3938424587249756
  - 0.5710954070091248
  - 0.9134746789932251
  - 1.3573473691940308
  - 1.0372174978256226
  - 0.4545409083366394
  - 0.6367743015289307
  - 0.5793802738189697
  - 0.7006229162216187
  - 0.7904880046844482
  - 1.371910572052002
  - 0.8359406590461731
  - 0.8076775074005127
  - 1.4957348108291626
  - 0.9425819516181946
  - 0.5682196021080017
  - 1.0044010877609253
  - 0.6400740146636963
  - 0.5003874897956848
  - 1.603639841079712
  - 0.5109835267066956
  - 0.42624542117118835
  - 0.6480685472488403
  - 0.5132758617401123
  - 0.6010143160820007
  - 1.006492257118225
  - 0.607109785079956
  - 0.5357169508934021
  - 0.5951235890388489
  - 0.6448206305503845
  - 0.6016935706138611
  - 0.43996915221214294
  - 0.743101954460144
  - 0.8381713032722473
  - 1.0534355640411377
  - 0.5792132019996643
loss_records_fold2:
  train_losses:
  - 2.949069517850876
  - 2.924363774061203
  - 3.0161450028419496
  - 2.9724937677383423
  - 2.882812896370888
  - 2.9588709831237794
  - 2.999645060300827
  - 2.9112043142318726
  - 2.9600264728069305
  - 2.9170140564441684
  - 2.9210824072360992
  - 2.9981445729732514
  - 2.8868027701973915
  - 2.932195353507996
  - 2.9370017409324647
  - 3.001085287332535
  - 3.00292027592659
  - 2.8946953892707827
  - 2.9685334444046023
  - 2.9522624611854553
  - 2.9926124274730683
  - 2.938634949922562
  - 2.954606541991234
  - 2.898474931716919
  - 2.9435373127460482
  - 2.9131140768527986
  - 2.980557763576508
  - 2.945374393463135
  - 2.9876381337642672
  - 2.9973921060562136
  - 2.9430988371372226
  - 2.889689338207245
  - 2.8863903462886813
  - 2.8781594276428226
  - 2.9536857903003693
  - 2.930769068002701
  - 2.939586064219475
  - 2.9126980483531955
  - 2.9440993130207063
  - 2.925292855501175
  - 2.9055466055870056
  - 2.8823659479618073
  - 2.938815027475357
  - 2.929020130634308
  - 2.954756808280945
  - 2.9529918849468233
  - 2.875738587975502
  - 2.889821258187294
  - 2.9690113365650177
  - 2.890383052825928
  - 2.909203225374222
  - 2.8762972712516786
  - 2.8895668387413025
  - 2.8767426520586015
  - 3.010632634162903
  - 3.0303183913230898
  - 2.9232399404048923
  - 2.94311044216156
  - 2.956226795911789
  - 3.011637616157532
  - 2.948093217611313
  - 2.950083050131798
  - 2.9268740832805635
  - 2.971220684051514
  - 2.970663291215897
  - 2.958447453379631
  - 2.9650224924087527
  - 2.89146418273449
  - 2.9010160088539125
  - 2.8993830263614657
  - 2.8777785301208496
  - 2.9664138853549957
  - 2.863099229335785
  - 2.9202114731073383
  - 2.9555033802986146
  - 2.8493866533041
  - 2.8961956709623338
  - 2.8617306113243104
  - 3.0110678136348725
  - 3.1080771028995517
  - 2.8970915108919146
  - 2.967755982279778
  - 2.963660913705826
  - 2.9945047557353974
  - 2.934656536579132
  - 2.9838157117366793
  - 2.9176862239837646
  - 2.951477211713791
  - 2.936184909939766
  - 2.871382385492325
  - 2.9351339101791383
  - 2.8123007059097294
  - 2.8168794929981233
  - 2.971640995144844
  - 2.9025994181633
  - 2.9398522078990936
  - 3.0010174572467805
  - 2.874199479818344
  - 2.982077753543854
  - 2.8991777896881104
  validation_losses:
  - 0.7238174080848694
  - 1.2496235370635986
  - 0.42327043414115906
  - 0.3830648362636566
  - 0.794363796710968
  - 1.454590916633606
  - 1.3050105571746826
  - 0.9170401692390442
  - 1.238067865371704
  - 0.8025155663490295
  - 0.7035559415817261
  - 0.3856819272041321
  - 0.42864790558815
  - 0.982865571975708
  - 0.38314464688301086
  - 0.4250340461730957
  - 0.4244857132434845
  - 0.44711005687713623
  - 0.4135047197341919
  - 0.4498833417892456
  - 0.45710495114326477
  - 0.45027458667755127
  - 0.7565355896949768
  - 0.6895576119422913
  - 0.607033908367157
  - 0.4771590828895569
  - 0.5226143002510071
  - 0.4514836072921753
  - 0.5939925909042358
  - 0.5947161912918091
  - 0.47008246183395386
  - 0.39002126455307007
  - 0.9368569850921631
  - 1.6369218826293945
  - 0.6758794784545898
  - 0.485764741897583
  - 0.5249723792076111
  - 0.8029678463935852
  - 0.43607091903686523
  - 0.7718703150749207
  - 0.9953600764274597
  - 0.7887181043624878
  - 1.151591420173645
  - 0.5705940127372742
  - 1.3332862854003906
  - 0.48734384775161743
  - 0.4304695725440979
  - 0.6516045331954956
  - 0.4601970911026001
  - 0.44716838002204895
  - 1.3667742013931274
  - 1.3836740255355835
  - 4.3605146408081055
  - 0.631999671459198
  - 0.41470181941986084
  - 0.3965802788734436
  - 0.39728763699531555
  - 0.39803722500801086
  - 0.4149916172027588
  - 0.3883613646030426
  - 0.39551037549972534
  - 0.4329392910003662
  - 0.4153249263763428
  - 0.4243876039981842
  - 0.3872683346271515
  - 0.483344167470932
  - 0.466300904750824
  - 0.473314106464386
  - 0.49329283833503723
  - 0.6200539469718933
  - 0.6408863663673401
  - 1.1299266815185547
  - 0.6000747680664062
  - 0.5358112454414368
  - 1.3418283462524414
  - 0.5223708152770996
  - 5.323398113250732
  - 0.6955533623695374
  - 0.7401681542396545
  - 0.4385814368724823
  - 0.5782787203788757
  - 0.9856623411178589
  - 0.8057296872138977
  - 1.0507789850234985
  - 1.2700718641281128
  - 3.1180548667907715
  - 1.253164291381836
  - 1.1847795248031616
  - 2.7192132472991943
  - 2.8849284648895264
  - 1.3033106327056885
  - 2.3627593517303467
  - 3.9500346183776855
  - 2.45291805267334
  - 4.0550360679626465
  - 4.726881980895996
  - 1.4923818111419678
  - 2.4069366455078125
  - 0.7212978005409241
  - 0.6592299342155457
loss_records_fold3:
  train_losses:
  - 2.9205265223979953
  - 2.93892323076725
  - 2.8633631438016893
  - 2.9515343427658083
  - 2.9055722773075106
  - 2.9412075579166412
  - 2.940501439571381
  - 2.8140907019376757
  - 2.95040225982666
  - 2.973557284474373
  - 3.009447282552719
  - 2.961142671108246
  - 2.928467571735382
  - 2.9831933915615085
  - 2.959094274044037
  - 2.902278423309326
  - 3.087857246398926
  - 2.963896006345749
  - 2.914662325382233
  - 2.8902284264564515
  - 3.006811738014221
  - 3.0073330342769626
  - 2.961118119955063
  - 2.986501628160477
  - 2.9329339206218723
  - 2.843747353553772
  - 2.9087306529283525
  - 2.8892096638679505
  - 2.9313748091459275
  - 2.9104612082242967
  - 2.9501691311597824
  - 2.8402909338474274
  - 2.9095942437648774
  - 2.8723401486873628
  - 3.0132326543331147
  - 2.852650094032288
  - 2.9385253071784976
  - 2.9463526189327243
  - 2.8905254840850834
  - 2.916465938091278
  - 2.8760925233364105
  - 2.9925682961940767
  - 2.9494600236415867
  - 2.889076691865921
  - 2.8929051011800766
  - 2.9802663385868073
  - 2.894596606492996
  - 2.978388684988022
  - 2.8926330864429475
  - 2.9309415459632877
  - 2.918596124649048
  - 2.9620673775672914
  - 2.966460180282593
  - 2.9555632770061493
  - 2.9425471603870395
  - 3.0000682175159454
  - 2.891066548228264
  - 2.9470820605754855
  - 2.8934867084026337
  - 2.88662565946579
  - 2.848953661322594
  - 2.954819691181183
  - 2.907762584090233
  - 2.963675254583359
  - 2.897048568725586
  - 2.910450130701065
  - 2.8333082795143127
  - 2.8180478870868684
  - 2.825812241435051
  - 2.924311262369156
  - 2.9402142763137817
  - 2.887402492761612
  - 2.9062398433685304
  - 2.899614375829697
  - 2.8847616612911224
  - 2.9450560808181763
  - 2.975669258832932
  - 3.0353066831827165
  - 2.9538426995277405
  - 2.9350310742855075
  - 2.855764165520668
  - 2.9566716313362122
  - 2.8610914140939716
  - 2.8773491919040683
  - 2.9881615817546847
  - 2.9977606505155565
  - 2.9966093063354493
  - 2.9479773938655853
  - 2.9715563833713534
  - 2.892776393890381
  - 2.9311263978481295
  - 2.9217175900936128
  - 2.8928094029426576
  - 2.8364167600870136
  - 2.8762127548456196
  - 2.8691997647285463
  - 3.03457787334919
  - 2.8351537823677067
  - 2.9472698271274567
  - 2.8932008475065234
  validation_losses:
  - 1.1413166522979736
  - 1.501615047454834
  - 2.470184803009033
  - 2.2539076805114746
  - 3.535353899002075
  - 3.1676816940307617
  - 4.050319671630859
  - 5.169337272644043
  - 7.034980773925781
  - 5.253211975097656
  - 1.7711610794067383
  - 0.4099476635456085
  - 1.0095863342285156
  - 1.3803348541259766
  - 1.2025444507598877
  - 0.6956600546836853
  - 0.829649806022644
  - 0.8536946773529053
  - 1.1970428228378296
  - 0.6289305090904236
  - 0.4571632146835327
  - 0.3955186903476715
  - 0.9008774757385254
  - 1.4624089002609253
  - 0.660279393196106
  - 0.7117279171943665
  - 0.7604542970657349
  - 0.6805461049079895
  - 1.237088680267334
  - 2.374789237976074
  - 0.5778146982192993
  - 2.014021158218384
  - 0.9559288620948792
  - 0.42378315329551697
  - 0.4784102737903595
  - 0.46248653531074524
  - 0.5041452646255493
  - 0.534485399723053
  - 0.42855727672576904
  - 20.58329963684082
  - 0.7531978487968445
  - 0.3878123164176941
  - 0.6785333752632141
  - 0.6383968591690063
  - 1.246410608291626
  - 0.4933079779148102
  - 2.015360116958618
  - 1.421252727508545
  - 2.254823684692383
  - 0.533980667591095
  - 1.5563851594924927
  - 1.6322338581085205
  - 0.377316951751709
  - 0.38731467723846436
  - 0.4185184836387634
  - 1.2456096410751343
  - 1.1985660791397095
  - 2.900820255279541
  - 2.8511550426483154
  - 3.0547924041748047
  - 2.725719690322876
  - 0.6917111277580261
  - 1.977431297302246
  - 0.39057600498199463
  - 3.1406781673431396
  - 0.6656025052070618
  - 0.6135584115982056
  - 1.088192343711853
  - 14.396902084350586
  - 1.2508662939071655
  - 3.4826929569244385
  - 0.41845449805259705
  - 0.8982952237129211
  - 2.106377601623535
  - 0.4945751130580902
  - 1.0177533626556396
  - 0.3798461854457855
  - 0.5853481292724609
  - 1.9606930017471313
  - 2.0017995834350586
  - 1.5809247493743896
  - 2.7600111961364746
  - 2.643963575363159
  - 2.8390920162200928
  - 0.47264161705970764
  - 0.442029744386673
  - 0.43610191345214844
  - 0.36884579062461853
  - 1.432647705078125
  - 0.4281946122646332
  - 2.4992713928222656
  - 0.5680648684501648
  - 0.4423049986362457
  - 4.8243513107299805
  - 0.6980432868003845
  - 2.1621081829071045
  - 1.3464385271072388
  - 1.1043437719345093
  - 0.6855886578559875
  - 0.7011193633079529
loss_records_fold4:
  train_losses:
  - 2.8442617774009706
  - 2.8998917520046237
  - 2.969062525033951
  - 2.9542794615030292
  - 3.012974828481674
  - 2.8364866167306904
  - 2.8624660551548007
  - 2.889184165000916
  - 2.9217956960201263
  - 2.870754185318947
  - 2.8625517934560776
  - 2.886452239751816
  - 3.087601584196091
  - 2.9794998824596406
  - 2.922300171852112
  - 2.997216355800629
  - 2.9503330081701282
  - 2.9329263210296634
  - 2.9225578635931018
  - 2.9102007836103443
  - 2.9046299278736116
  - 2.8788039773702625
  - 2.9335908710956575
  - 2.8752140283584597
  - 2.8898146927356723
  - 2.8221805512905123
  - 2.8258209943771364
  - 2.9217297732830048
  - 2.829537743330002
  - 2.8608742594718937
  - 2.921800869703293
  - 2.9257573127746586
  - 2.842742630839348
  - 2.8921714633703233
  - 2.8162106752395633
  - 2.836658787727356
  - 2.942075988650322
  - 2.949691301584244
  - 2.849320870637894
  - 2.9618072748184208
  - 2.868182495236397
  - 3.075309616327286
  - 2.988098305463791
  - 2.9311191171407702
  - 2.8542848348617555
  - 2.9723906695842746
  - 2.874293971061707
  - 2.894063311815262
  - 2.8879492670297626
  - 2.891181981563568
  - 2.8716319024562837
  - 2.852973508834839
  - 2.879787945747376
  - 2.9111566454172135
  - 2.949979239702225
  - 2.889280164241791
  - 2.826286092400551
  - 2.787376177310944
  - 2.8874573051929477
  - 2.909219264984131
  - 2.8564855694770817
  - 2.808310240507126
  - 2.9118773281574253
  - 2.9135238111019137
  - 2.79837229847908
  - 2.853497558832169
  - 3.066785150766373
  - 2.8363271325826647
  - 2.8921768903732303
  - 2.931568479537964
  - 2.8592887938022615
  - 2.8637023687362673
  - 2.896781846880913
  - 2.791717156767845
  - 2.903716498613358
  - 2.8015488713979724
  - 2.8764183580875398
  - 2.8404534459114075
  - 2.9021000385284426
  - 2.8244340986013414
  - 2.7862602889537813
  - 2.82964586019516
  - 2.863293927907944
  - 2.818839904665947
  - 2.840064209699631
  - 2.891383683681488
  - 2.935838678479195
  - 2.8766240179538727
  - 2.873082882165909
  - 2.869192159175873
  - 2.8442590951919557
  - 2.827113682031632
  - 2.9305527806282043
  - 2.8282653212547304
  - 2.8263793706893923
  - 2.8212741374969483
  - 2.8067232161760334
  - 2.8763410568237306
  - 2.852115458250046
  - 2.9036531627178195
  validation_losses:
  - 0.7306765913963318
  - 0.7422198057174683
  - 0.7055544853210449
  - 0.3728472888469696
  - 0.5307290554046631
  - 0.38678449392318726
  - 0.7807572484016418
  - 0.43926095962524414
  - 0.8251630067825317
  - 0.410203754901886
  - 1.1621755361557007
  - 0.617769181728363
  - 0.3807919919490814
  - 0.4043869078159332
  - 0.4347754418849945
  - 0.4199725389480591
  - 0.3791598677635193
  - 0.37949490547180176
  - 0.3824080526828766
  - 0.380888432264328
  - 0.4237813353538513
  - 0.42456942796707153
  - 0.4309234917163849
  - 0.3633887469768524
  - 0.4559292793273926
  - 0.36988547444343567
  - 0.47486400604248047
  - 0.5307879447937012
  - 0.45491406321525574
  - 0.4021175503730774
  - 0.4854637384414673
  - 0.4191589951515198
  - 0.47231143712997437
  - 0.41822633147239685
  - 0.6449456214904785
  - 0.4183596670627594
  - 0.6003630757331848
  - 0.46454381942749023
  - 0.6657248139381409
  - 0.8276780247688293
  - 1.408855676651001
  - 0.4054092466831207
  - 0.46248021721839905
  - 0.42134174704551697
  - 0.4268377423286438
  - 0.4004204273223877
  - 0.9754288792610168
  - 0.43705838918685913
  - 0.40944164991378784
  - 0.4285241663455963
  - 0.43416017293930054
  - 0.41450056433677673
  - 0.4155564606189728
  - 0.44182252883911133
  - 0.4395357668399811
  - 0.4238344132900238
  - 0.38988691568374634
  - 0.4428621828556061
  - 0.4320831298828125
  - 0.4848046600818634
  - 0.45254191756248474
  - 0.4882194399833679
  - 0.4966271221637726
  - 0.4447934031486511
  - 0.44060325622558594
  - 0.39407265186309814
  - 0.3572442829608917
  - 0.4525887072086334
  - 0.5109574198722839
  - 0.5812022686004639
  - 0.4329024851322174
  - 0.49457305669784546
  - 0.43087413907051086
  - 0.5113033652305603
  - 0.6269527077674866
  - 0.4321858584880829
  - 0.5055181980133057
  - 0.512852132320404
  - 0.5998945236206055
  - 0.48037442564964294
  - 0.5561668276786804
  - 0.7270586490631104
  - 0.5509101748466492
  - 1.0350618362426758
  - 1.3126994371414185
  - 0.45665454864501953
  - 0.4894331693649292
  - 0.6845718622207642
  - 0.47372525930404663
  - 0.46804022789001465
  - 0.8483301997184753
  - 2.1512608528137207
  - 0.5894360542297363
  - 0.6026493906974792
  - 0.5358583927154541
  - 0.9414952993392944
  - 1.752243995666504
  - 0.5287413001060486
  - 0.7836408615112305
  - 1.2946332693099976
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8490566037735849, 0.8490566037735849, 0.8593481989708405,
    0.8127147766323024]'
  fold_eval_f1: '[0.0, 0.022222222222222223, 0.06382978723404255, 0.023809523809523808,
    0.2043795620437956]'
  mean_eval_accuracy: 0.8455618232509888
  mean_f1_accuracy: 0.06284821906191683
  total_train_time: '0:37:23.476397'
