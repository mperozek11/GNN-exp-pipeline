config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:15:13.141967'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_77fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_77fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_77fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_77fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_77fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_77fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_77fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_77fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_77fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_77fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 33.35158087015152
  - 13.72875977754593
  - 7.569497454166413
  - 15.964270114898682
  - 19.402478456497192
  - 6.78285009264946
  - 8.607825803756715
  - 5.57344241887331
  - 11.857731199264528
  - 15.191812098026276
  - 13.14569097161293
  - 10.15313700437546
  - 7.075256860256196
  - 8.172831070423127
  - 10.688325646519662
  - 18.043791425228118
  - 7.581663781404496
  - 6.375994068384171
  - 4.501823222637177
  - 3.7084450483322144
  - 6.052476596832276
  - 3.528926652669907
  - 5.604030114412308
  - 4.165525364875793
  - 5.264367920160294
  - 4.034408614039421
  - 5.395403891801834
  - 5.292193901538849
  - 5.647116231918336
  - 3.613110661506653
  - 3.738131433725357
  - 3.499862360954285
  - 4.5657458186149595
  - 4.647714853286743
  - 3.93717443048954
  - 3.3369774222373962
  - 6.666839891672135
  - 3.3702029764652255
  - 3.5483814001083376
  - 4.8496340155601505
  - 5.989121240377426
  - 3.644645455479622
  - 3.1796424746513368
  validation_losses:
  - 2.7345001697540283
  - 0.7103824615478516
  - 0.6889874935150146
  - 1.183323621749878
  - 0.43232232332229614
  - 0.729652464389801
  - 0.3896196484565735
  - 0.444619357585907
  - 0.41224002838134766
  - 0.5648001432418823
  - 0.42866867780685425
  - 0.6358605027198792
  - 0.43404296040534973
  - 0.39981314539909363
  - 1.0158311128616333
  - 0.44320186972618103
  - 0.3975888788700104
  - 0.4123741388320923
  - 0.4127233028411865
  - 0.44100069999694824
  - 0.4101313650608063
  - 0.40061378479003906
  - 0.5341417789459229
  - 0.38685789704322815
  - 1.206272840499878
  - 0.40007540583610535
  - 0.4627659022808075
  - 0.43561461567878723
  - 0.42877981066703796
  - 0.3899899125099182
  - 0.39812713861465454
  - 0.3900284469127655
  - 0.4164610505104065
  - 0.3932224214076996
  - 0.43271932005882263
  - 0.4038732647895813
  - 0.41942012310028076
  - 0.40636131167411804
  - 0.39949241280555725
  - 0.4008065462112427
  - 0.40340161323547363
  - 0.385334312915802
  - 0.3855646848678589
loss_records_fold1:
  train_losses:
  - 4.098455625772476
  - 4.176879045367241
  - 3.4101555049419403
  - 3.726876521110535
  - 3.3475554615259173
  - 3.1155340284109116
  - 4.305505853891373
  - 3.10622093975544
  - 3.1499313056468967
  - 3.543754729628563
  - 4.5684976249933245
  - 3.666632688045502
  - 3.378844952583313
  - 3.4077908396720886
  - 3.358237081766129
  - 3.651826995611191
  - 3.229634314775467
  - 3.148408842086792
  - 3.060969400405884
  - 3.4390018701553347
  - 3.0792144119739535
  - 3.5243389427661898
  - 3.305752605199814
  - 3.1130343973636627
  - 3.44246740937233
  - 3.38482660651207
  - 3.25090674161911
  - 3.1157791018486023
  - 3.282529056072235
  - 3.1861405313014988
  - 3.0641992807388307
  - 3.022051513195038
  - 3.069102916121483
  - 3.121971994638443
  - 3.0754520773887637
  - 3.2276674568653108
  - 3.159691506624222
  - 3.1920973777771
  - 3.2461261302232742
  - 3.36768873333931
  - 3.185793155431748
  - 3.1755861043930054
  - 3.207626408338547
  - 3.270035070180893
  - 3.294722539186478
  - 3.4291582703590393
  - 3.5273574173450473
  - 3.100789847970009
  - 3.2760686814785007
  - 3.523223251104355
  - 3.2964220464229586
  - 3.845969167351723
  - 3.2285991311073303
  - 3.204230868816376
  - 3.4223579645156863
  - 3.35789110660553
  - 3.2275642275810243
  - 3.3243860542774204
  - 3.1479809224605564
  - 3.188338959217072
  - 3.2978133827447893
  - 3.1400211811065675
  - 3.201155656576157
  - 3.1793685615062715
  - 3.224453729391098
  - 6.261053180694581
  - 4.0765873193740845
  - 4.9797967910766605
  - 3.5754980325698855
  - 3.580963909626007
  - 3.2228376746177676
  - 3.656561899185181
  - 3.5503121465444565
  - 3.784942787885666
  - 3.8009751796722413
  - 3.22634229362011
  - 3.2056356728076936
  validation_losses:
  - 0.42751020193099976
  - 0.4666050374507904
  - 0.4120809733867645
  - 0.4763364791870117
  - 0.4115980863571167
  - 0.4035128951072693
  - 0.4290323555469513
  - 0.45786428451538086
  - 0.41654086112976074
  - 0.46591633558273315
  - 0.4895910620689392
  - 0.4529435932636261
  - 0.4152586758136749
  - 0.4008047580718994
  - 0.6421083807945251
  - 0.41354525089263916
  - 0.460089772939682
  - 0.4301658570766449
  - 0.4006882607936859
  - 0.4647565484046936
  - 0.4495512545108795
  - 0.40230634808540344
  - 0.4137677550315857
  - 0.4087893068790436
  - 0.40498974919319153
  - 0.4118974506855011
  - 0.45030441880226135
  - 0.40660926699638367
  - 0.45352259278297424
  - 0.5218464136123657
  - 0.421184241771698
  - 0.47329267859458923
  - 0.4081732928752899
  - 0.4356345236301422
  - 0.40024447441101074
  - 0.4411122798919678
  - 0.45333534479141235
  - 0.4025777578353882
  - 0.41996923089027405
  - 0.42605894804000854
  - 0.5030928254127502
  - 0.4390300214290619
  - 0.42668020725250244
  - 0.4313211739063263
  - 0.43259719014167786
  - 0.5676669478416443
  - 0.4214988946914673
  - 0.4305720627307892
  - 0.520463228225708
  - 0.42098554968833923
  - 0.4183874726295471
  - 0.4520176351070404
  - 0.4309559464454651
  - 0.4235793948173523
  - 0.4679125249385834
  - 0.442765474319458
  - 0.40717557072639465
  - 0.42571017146110535
  - 40.21895217895508
  - 1811.1549072265625
  - 268742.625
  - 2004095.625
  - 375951.09375
  - 2018095.75
  - 0.7002485394477844
  - 0.5510199069976807
  - 0.4060278534889221
  - 0.518704891204834
  - 0.43136730790138245
  - 0.41924381256103516
  - 0.49599963426589966
  - 0.4796120822429657
  - 0.4305782914161682
  - 0.4131911098957062
  - 0.41033485531806946
  - 0.41550782322883606
  - 0.40657302737236023
loss_records_fold2:
  train_losses:
  - 3.483570504188538
  - 3.4709493398666385
  - 3.3372918307781223
  - 3.3225613057613375
  - 3.307301527261734
  - 3.401258879899979
  - 3.2941443145275118
  - 3.1387466847896577
  - 3.1395577251911164
  - 3.1913069367408755
  - 3.311305916309357
  - 3.375741857290268
  - 3.467873340845108
  - 3.3733321607112887
  - 3.256224036216736
  - 3.207750850915909
  - 3.261265489459038
  - 3.927864408493042
  - 3.833237421512604
  - 3.643259966373444
  - 3.4659483402967455
  - 3.270799043774605
  - 3.1783522605896
  - 3.160458078980446
  - 3.1751217246055603
  - 3.1355329483747485
  - 3.2548106610774994
  - 3.1542718291282656
  - 3.2169527232646944
  - 3.2890995740890503
  - 3.179410070180893
  - 3.2495566546916965
  - 3.208315217494965
  - 3.255854696035385
  - 3.2734912514686587
  - 3.1577480286359787
  - 3.1759536266326904
  - 3.2666119933128357
  - 3.3733104050159457
  - 3.188586395978928
  - 3.153218185901642
  - 3.1840243577957157
  - 3.2494707226753237
  - 3.21193020939827
  - 3.165772938728333
  - 3.177303966879845
  - 3.2189848959445957
  - 3.1619831860065464
  - 3.182120752334595
  - 3.2384797990322114
  - 3.2050251811742783
  - 3.18077210187912
  - 3.5129968971014023
  - 3.28519788980484
  - 3.253811487555504
  - 3.2813162326812746
  - 3.262133300304413
  - 3.2537227213382724
  - 3.182875156402588
  - 3.177692407369614
  - 3.2170105755329135
  - 3.3003689765930178
  - 3.2566761791706087
  - 3.1643799573183062
  - 3.2195012390613558
  - 3.1904307514429093
  - 4.068081480264664
  - 3.3681503236293793
  - 3.3394317567348484
  - 3.33525242805481
  - 3.456650930643082
  - 3.159730473160744
  - 3.191768625378609
  - 3.2094482988119126
  - 3.267080762982369
  - 3.240980088710785
  - 3.2949450820684434
  - 3.183029055595398
  - 3.199385303258896
  - 3.2636347800493244
  - 3.229143309593201
  - 3.371158349514008
  - 3.2806214034557346
  - 3.1971840202808384
  - 3.228691512346268
  - 3.1510169208049774
  - 3.273925739526749
  - 3.320508748292923
  - 3.3801380306482316
  - 3.1690563142299655
  - 3.3017019093036652
  - 3.373773843050003
  - 3.329159289598465
  - 3.206879633665085
  validation_losses:
  - 0.4355744421482086
  - 0.3856569826602936
  - 0.3884522020816803
  - 0.4045470058917999
  - 0.4679381251335144
  - 143.1857147216797
  - 3499.794677734375
  - 4744.30908203125
  - 115.17961120605469
  - 530.0592041015625
  - 67.04257202148438
  - 517.8218383789062
  - 537.7684326171875
  - 57.47490310668945
  - 1543.8463134765625
  - 6880.44873046875
  - 0.42417627573013306
  - 0.45219576358795166
  - 0.41909393668174744
  - 0.4233075976371765
  - 0.4105070233345032
  - 0.414441853761673
  - 0.39831864833831787
  - 0.8199866414070129
  - 26.456024169921875
  - 512.117919921875
  - 1069.871337890625
  - 761.8965454101562
  - 21944.564453125
  - 14265.5419921875
  - 3007.82470703125
  - 46.88069534301758
  - 3229.006103515625
  - 7135.4541015625
  - 1731.6668701171875
  - 305.107421875
  - 550.25244140625
  - 1196.2152099609375
  - 64.86015319824219
  - 8608.4052734375
  - 341.4108581542969
  - 21652.15625
  - 873.2669067382812
  - 6079.2841796875
  - 5385.0009765625
  - 6282.03466796875
  - 2243.01123046875
  - 840.6119384765625
  - 0.46987712383270264
  - 393.5986022949219
  - 73.68081665039062
  - 181.19146728515625
  - 97.8838882446289
  - 495.04248046875
  - 403.6679992675781
  - 22.994537353515625
  - 0.40715858340263367
  - 0.3905925750732422
  - 0.40653204917907715
  - 0.39984893798828125
  - 0.40843480825424194
  - 0.3997001349925995
  - 0.4086263179779053
  - 0.4231320023536682
  - 0.41149577498435974
  - 1.0880881547927856
  - 0.4671078324317932
  - 0.39235568046569824
  - 0.40590378642082214
  - 0.39890897274017334
  - 0.40510693192481995
  - 0.398065447807312
  - 0.41402679681777954
  - 0.40507248044013977
  - 0.42605704069137573
  - 0.3973146378993988
  - 0.42136284708976746
  - 0.39700862765312195
  - 0.4023415744304657
  - 0.396480917930603
  - 0.41592535376548767
  - 0.4056154191493988
  - 0.4286980926990509
  - 0.404224693775177
  - 0.4063335657119751
  - 0.4256478250026703
  - 0.40219342708587646
  - 0.43511414527893066
  - 0.4207777678966522
  - 0.41101470589637756
  - 0.4045272767543793
  - 0.39902615547180176
  - 0.39956268668174744
  - 0.3977370262145996
loss_records_fold3:
  train_losses:
  - 3.314416337013245
  - 3.207641208171845
  - 3.189314764738083
  - 3.3844579219818116
  - 3.373426985740662
  - 3.1972264766693117
  - 3.3001501649618152
  - 3.1882905185222628
  - 3.4446715593338015
  - 3.2500771939754487
  - 3.5168725311756135
  - 4.056519544124604
  - 3.970397812128067
  - 3.9005298972129823
  - 3.447339797019959
  - 3.2174000769853595
  - 3.341094768047333
  - 3.1699500381946564
  - 4.013272795081138
  - 4.805609601736069
  - 3.3925853013992313
  - 3.2253006339073185
  - 3.9166865527629855
  - 3.531966018676758
  - 3.706158608198166
  - 3.4202071607112887
  - 3.8520502001047134
  - 3.211914283037186
  - 3.299049657583237
  - 3.3555140614509584
  - 3.1761047124862674
  - 3.38714896440506
  - 3.2752874076366427
  - 3.165008133649826
  - 3.178186142444611
  - 3.2972406625747683
  - 3.19921914935112
  - 3.220017594099045
  validation_losses:
  - 0.42174771428108215
  - 0.4275127351284027
  - 0.44307243824005127
  - 0.40922802686691284
  - 27.149343490600586
  - 0.4121895730495453
  - 0.5193131566047668
  - 0.4258364737033844
  - 0.41524145007133484
  - 343.51104736328125
  - 0.6407445669174194
  - 0.3957110345363617
  - 0.4398060142993927
  - 0.4056071937084198
  - 0.4049360454082489
  - 0.41518256068229675
  - 0.42675307393074036
  - 0.3985321521759033
  - 0.4250906705856323
  - 0.4154263138771057
  - 0.4097955524921417
  - 0.4198783338069916
  - 0.5454766154289246
  - 0.46665436029434204
  - 0.4092579185962677
  - 0.443880170583725
  - 0.43340134620666504
  - 0.4003295600414276
  - 0.4182490110397339
  - 0.4290412366390228
  - 0.40813323855400085
  - 0.4656508266925812
  - 0.42985770106315613
  - 0.4159512519836426
  - 0.41988518834114075
  - 0.4152553677558899
  - 0.419705331325531
  - 0.41231247782707214
loss_records_fold4:
  train_losses:
  - 3.22184574007988
  - 3.255055004358292
  - 3.290982806682587
  - 3.2655635654926303
  - 3.3953349709510805
  - 3.27246670126915
  - 3.2065049529075624
  - 3.141052722930908
  - 3.213171249628067
  - 3.296611422300339
  - 3.222278648614884
  - 3.2818104028701782
  - 3.1722514390945435
  - 3.1999492943286896
  - 3.1947720468044283
  - 3.2421692311763763
  - 3.3104747951030733
  - 3.3554214656353
  - 3.1410999536514286
  - 3.2417184978723528
  - 3.415396600961685
  validation_losses:
  - 0.42525938153266907
  - 0.40590792894363403
  - 0.42633071541786194
  - 0.4237439036369324
  - 0.4062471091747284
  - 0.40744349360466003
  - 0.429390549659729
  - 0.4095011055469513
  - 207.1685333251953
  - 2342.817138671875
  - 433.9970703125
  - 0.4257863461971283
  - 0.4021241366863251
  - 0.40225839614868164
  - 0.43015897274017334
  - 0.4087057113647461
  - 0.40938758850097656
  - 0.4064078629016876
  - 0.3965618312358856
  - 0.4060969352722168
  - 0.4023786187171936
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 43 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 77 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 94 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 38 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:25:08.535452'
