config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 18:00:55.655281'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/48/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/48/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/48/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/48/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/48/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/48/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/48/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/48/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/48/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/48/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 271.83946996182203
  - 93.10063739866018
  - 99.72347621619701
  - 99.32702840864658
  - 87.7829994559288
  - 81.40677350759506
  - 70.36450891196728
  - 68.6607648730278
  - 49.205497197806835
  - 53.39117572084069
  - 85.14486744999886
  - 41.8880063444376
  - 40.679257184267044
  - 31.59428107738495
  - 40.77948421239853
  - 33.946023762226105
  - 38.41222496330738
  - 42.88517718762159
  - 48.22494129836559
  - 35.967683628201485
  - 36.99015199393034
  - 33.89012202620506
  - 38.76406620442867
  - 38.00425073504448
  - 30.752897530794144
  - 36.61018067598343
  - 31.749961778521538
  - 34.96683871746063
  - 30.12810879945755
  - 35.287924617528915
  - 28.812379270792007
  - 32.56178820878267
  - 31.458754166960716
  - 30.366724357008934
  - 29.349097684025764
  - 30.557972595095634
  - 29.28666725754738
  - 29.884867265820503
  - 29.344944283366203
  - 29.275689393281937
  - 29.176193982362747
  - 28.476548671722412
  - 28.962194085121155
  - 29.129164323210716
  - 28.964216724038124
  - 28.372114568948746
  - 29.894714012742043
  - 29.698869913816452
  - 28.915196865797043
  - 28.696245342493057
  - 29.02352400124073
  - 29.79569247364998
  - 28.488303370773792
  - 29.20605754852295
  - 28.885230913758278
  - 29.55671653151512
  - 29.27364093065262
  - 28.886189460754395
  - 29.424621403217316
  - 30.030156508088112
  - 32.54690362513065
  - 30.49317315220833
  - 29.21102437376976
  - 29.678064674139023
  - 29.796714797616005
  - 29.37588456273079
  - 29.557627499103546
  - 29.684522584080696
  - 30.22337356209755
  - 31.011452451348305
  - 30.760409377515316
  - 30.52598288655281
  - 29.84756714105606
  - 30.501518353819847
  - 29.58392959833145
  - 30.10957942903042
  - 30.30665583908558
  - 42.04067678004503
  - 30.984615355730057
  - 30.13102126121521
  - 29.738014847040176
  - 30.20345041155815
  - 29.48381046205759
  - 29.647037237882614
  - 29.442443065345287
  - 29.59393735229969
  - 29.81585666537285
  - 29.653673067688942
  - 29.62590952217579
  - 34.56271889805794
  - 38.289216205477715
  - 29.558132737874985
  - 29.87903132289648
  - 31.37596508860588
  - 29.54966439306736
  - 29.23186892271042
  - 29.890881031751633
  - 31.551227062940598
  - 31.287125192582607
  - 30.75668068230152
  validation_losses:
  - 2.5924034118652344
  - 0.57586669921875
  - 0.4304965138435364
  - 0.5681827664375305
  - 1.834610104560852
  - 1.8171236515045166
  - 0.980569064617157
  - 0.707545816898346
  - 0.3935394287109375
  - 0.7595676183700562
  - 0.40465208888053894
  - 0.37688010931015015
  - 0.38857877254486084
  - 0.3986428678035736
  - 0.3845857083797455
  - 0.381672203540802
  - 0.4064406156539917
  - 0.3999790549278259
  - 0.39880016446113586
  - 0.43288877606391907
  - 0.5037002563476562
  - 0.45629799365997314
  - 0.397239089012146
  - 0.38940438628196716
  - 0.4050315022468567
  - 0.38108909130096436
  - 0.3826969861984253
  - 0.38583242893218994
  - 0.4009750783443451
  - 0.3855038583278656
  - 0.3988741338253021
  - 0.4686223268508911
  - 0.43328383564949036
  - 0.395799845457077
  - 0.39833182096481323
  - 0.4839860796928406
  - 0.44345423579216003
  - 0.39762139320373535
  - 0.4323737919330597
  - 0.44272106885910034
  - 0.3868494927883148
  - 0.7849488258361816
  - 0.7529761791229248
  - 0.44106030464172363
  - 1.0130994319915771
  - 0.5216224789619446
  - 0.4066180884838104
  - 0.38392165303230286
  - 0.5878238677978516
  - 0.39881205558776855
  - 0.42271679639816284
  - 0.5019493103027344
  - 0.38606882095336914
  - 0.5328205823898315
  - 0.6936171650886536
  - 0.4395744204521179
  - 0.49769675731658936
  - 0.4645489454269409
  - 0.5000928044319153
  - 0.3833495080471039
  - 0.4095025658607483
  - 0.38985538482666016
  - 0.4112863540649414
  - 0.4482485353946686
  - 0.38977062702178955
  - 0.506656289100647
  - 0.40584149956703186
  - 0.586013913154602
  - 0.4253832697868347
  - 0.4957779049873352
  - 0.5365956425666809
  - 0.4537650942802429
  - 0.5806474685668945
  - 0.39496856927871704
  - 0.39174574613571167
  - 0.393745094537735
  - 0.500697135925293
  - 0.4174101650714874
  - 0.4067970812320709
  - 0.38886991143226624
  - 0.4502856731414795
  - 0.42021819949150085
  - 0.45041266083717346
  - 0.3939737379550934
  - 0.3904862403869629
  - 0.38970765471458435
  - 0.41226160526275635
  - 0.456238329410553
  - 0.49511003494262695
  - 0.4346892833709717
  - 0.4434369206428528
  - 22786.501953125
  - 87149.125
  - 0.7902471423149109
  - 195967.1875
  - 1506.53955078125
  - 13502243.0
  - 0.4033769369125366
  - 0.7347944974899292
  - 1.8317056894302368
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 57 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 10 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 57 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 60 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:11:03.637295'
