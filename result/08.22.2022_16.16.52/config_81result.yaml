config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:59:28.110616'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_81fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_81fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_81fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_81fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_81fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_81fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_81fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_81fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_81fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_81fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.157156923413277
  - 3.000406634807587
  - 3.0143930256366733
  - 3.0306200683116913
  - 3.120290607213974
  - 2.956252557039261
  - 2.9761085212230682
  - 2.9065569013357164
  - 2.971219313144684
  - 2.9089726090431216
  - 2.964704316854477
  - 2.916890960931778
  - 3.1095430314540864
  - 2.8774337828159333
  - 3.0076189398765565
  - 3.0191555589437487
  - 2.8957029283046722
  - 2.86639521420002
  - 2.857467284798622
  - 2.805021193623543
  - 2.8903233736753466
  validation_losses:
  - 0.40969911217689514
  - 0.4122597873210907
  - 0.3944547176361084
  - 0.3974233567714691
  - 0.4169456362724304
  - 0.40211236476898193
  - 0.4051879644393921
  - 0.3874387741088867
  - 0.387167364358902
  - 0.4571181535720825
  - 0.3886765241622925
  - 0.4065885841846466
  - 0.38850051164627075
  - 0.3900304138660431
  - 0.41147810220718384
  - 0.38606876134872437
  - 0.3859887421131134
  - 0.39161574840545654
  - 0.38635358214378357
  - 0.38475295901298523
  - 0.3832946717739105
loss_records_fold1:
  train_losses:
  - 2.8666170179843906
  - 2.863019967079163
  - 2.80093210041523
  - 2.8166592299938205
  - 2.798614889383316
  - 2.8498291939496996
  - 2.858900186419487
  - 2.804363262653351
  - 2.849264365434647
  - 2.8499147534370426
  - 2.7953335255384446
  validation_losses:
  - 0.4198719263076782
  - 0.3969917595386505
  - 0.3914356827735901
  - 0.40385040640830994
  - 0.3858111500740051
  - 0.3851388990879059
  - 0.3932831287384033
  - 0.3935418426990509
  - 0.40111044049263
  - 0.39314717054367065
  - 0.38475072383880615
loss_records_fold2:
  train_losses:
  - 2.8288204789161684
  - 2.8256562501192093
  - 2.8003820836544038
  - 2.795292663574219
  - 2.8676898926496506
  - 2.8347735136747363
  - 2.8440492004156113
  - 2.829294103384018
  - 2.826094198226929
  - 2.8296341180801394
  - 2.805069348216057
  - 2.795037028193474
  - 2.7989078909158707
  - 2.7962438434362413
  - 2.8143848836421967
  - 2.8033152997493747
  - 2.7754449725151065
  - 2.8400790333747867
  - 2.8402751445770265
  - 2.7998508125543595
  - 2.7995037615299228
  - 2.804735237360001
  validation_losses:
  - 0.3900022804737091
  - 0.38615554571151733
  - 0.3879585564136505
  - 0.3870079815387726
  - 0.38345807790756226
  - 0.3835369348526001
  - 0.4012128412723541
  - 0.3823734223842621
  - 0.3839057385921478
  - 0.3957129716873169
  - 0.3979911506175995
  - 0.3832472562789917
  - 0.3831333518028259
  - 0.39795300364494324
  - 0.3963029682636261
  - 0.4194740653038025
  - 0.3934711515903473
  - 0.38839590549468994
  - 0.38262808322906494
  - 0.3853243291378021
  - 0.38146674633026123
  - 0.3898124396800995
loss_records_fold3:
  train_losses:
  - 2.79667364358902
  - 2.81620916724205
  - 2.8232298851013184
  - 2.8914143800735475
  - 2.8426885068416596
  - 2.8229791164398197
  - 2.83533847630024
  - 2.813865926861763
  - 2.798493164777756
  - 2.781194305419922
  - 2.797531044483185
  - 2.8171590238809587
  - 2.823512372374535
  - 2.837315607070923
  - 2.8366402566432956
  - 2.858476108312607
  - 2.8552074134349823
  - 2.830378639698029
  - 2.811547148227692
  - 2.750785267353058
  - 2.808538067340851
  - 2.79332289993763
  - 2.7711633771657946
  - 2.844225355982781
  - 2.822205784916878
  - 2.7563559263944626
  - 2.753615486621857
  - 2.697889837622643
  - 2.8339492291212083
  - 2.7880860388278963
  - 2.775408035516739
  - 2.746784543991089
  - 2.7890059471130373
  - 2.7858238160610203
  - 2.788550853729248
  - 2.7565279692411426
  - 2.7559027582407
  - 2.7386779963970187
  - 2.7518271028995516
  - 2.7207898944616318
  - 2.7233461380004886
  - 2.7057584822177887
  - 2.7466000467538834
  - 2.7123985260725023
  - 2.755567929148674
  - 2.6924333930015565
  - 2.7479147136211397
  - 2.7588304817676548
  - 2.767221981287003
  - 2.7536450743675234
  - 2.782590401172638
  - 2.7120000362396244
  - 2.6819098711013796
  - 2.692681592702866
  - 2.7035145044326785
  - 2.7193835735321046
  - 2.802859103679657
  - 2.7768577545881272
  - 2.7044634461402897
  - 2.716757780313492
  - 2.735488769412041
  - 2.7027927696704865
  - 2.72211306989193
  - 2.6948573142290115
  - 2.7187250524759294
  - 2.723235946893692
  - 2.7301047772169116
  - 2.6648643165826797
  - 2.6781285583972934
  - 2.706329169869423
  - 2.6993881642818454
  - 2.7040620267391207
  - 2.701146921515465
  - 2.7631289422512055
  - 2.681987011432648
  validation_losses:
  - 0.39591559767723083
  - 0.3933740556240082
  - 0.36592790484428406
  - 0.495160847902298
  - 0.3694517910480499
  - 0.3686603605747223
  - 0.37090954184532166
  - 0.3733866512775421
  - 0.3708206117153168
  - 0.3843030631542206
  - 0.37072697281837463
  - 0.3992144763469696
  - 0.4417707026004791
  - 0.4206956624984741
  - 0.46794649958610535
  - 0.38808417320251465
  - 0.37448054552078247
  - 0.3850806653499603
  - 0.5622498989105225
  - 0.3777364492416382
  - 0.4307803213596344
  - 0.9237930774688721
  - 0.3868829309940338
  - 0.41238412261009216
  - 0.6023072004318237
  - 0.3688727915287018
  - 0.6417691111564636
  - 0.4258174002170563
  - 0.4416857361793518
  - 0.37377312779426575
  - 0.4419659972190857
  - 0.4254811704158783
  - 0.44764038920402527
  - 0.3803214728832245
  - 0.4098566770553589
  - 0.4232877790927887
  - 0.44768863916397095
  - 0.4074918031692505
  - 0.4445042908191681
  - 0.7030847072601318
  - 0.46003809571266174
  - 0.49194660782814026
  - 0.4610362946987152
  - 0.4490889310836792
  - 0.447971373796463
  - 0.5316591262817383
  - 0.4571400284767151
  - 0.5186828970909119
  - 0.4371415674686432
  - 0.37484484910964966
  - 0.3918008804321289
  - 0.40926241874694824
  - 0.5775747895240784
  - 0.45676881074905396
  - 0.5128986239433289
  - 0.5299316644668579
  - 0.41193240880966187
  - 0.4343699514865875
  - 0.38052311539649963
  - 0.4462619125843048
  - 0.466487318277359
  - 0.4523259103298187
  - 0.4572700560092926
  - 0.47195419669151306
  - 0.46903562545776367
  - 0.45771950483322144
  - 0.4551592767238617
  - 0.4411313533782959
  - 0.6408817172050476
  - 0.5345490574836731
  - 0.5035927891731262
  - 0.46805575489997864
  - 0.4669763147830963
  - 0.4463542401790619
  - 0.3959544897079468
loss_records_fold4:
  train_losses:
  - 2.7601892918348314
  - 2.7043492197990417
  - 2.7213308691978457
  - 2.705930304527283
  - 2.7046603858470917
  - 2.7277544736862183
  - 2.7351281225681308
  - 2.7130678415298464
  - 2.71328267455101
  - 2.768154898285866
  - 2.7091228246688845
  - 2.7695427358150484
  - 2.7051272958517076
  - 2.748001325130463
  - 2.6585406541824343
  - 2.6915435731410984
  - 2.6954152792692185
  - 2.684725481271744
  - 2.727015769481659
  - 2.8318124353885654
  - 2.803247517347336
  - 2.7631683826446536
  - 2.7585838019847873
  - 2.6967933207750323
  - 2.691646635532379
  - 2.7275755554437637
  - 2.7466988205909733
  - 2.7202950298786166
  - 2.7005211234092714
  - 2.6977649688720704
  - 2.6808447360992433
  - 2.7232088625431063
  - 2.697198870778084
  - 2.6719934701919557
  - 2.720830771327019
  - 2.692330086231232
  - 2.648755016922951
  - 2.699532222747803
  - 2.668775805830956
  - 2.744009080529213
  - 2.7357326954603196
  - 2.7456644773483276
  - 2.7279207646846775
  - 2.6924928575754166
  - 2.6836728632450106
  - 2.6710324466228488
  - 2.7074053943157197
  - 2.6627315461635592
  - 2.6623005867004395
  - 2.683428964018822
  - 2.7052746295928958
  - 2.7039454996585848
  - 2.6867303937673572
  - 2.855478543043137
  - 2.706137776374817
  - 2.6866488397121433
  - 2.7125159144401554
  - 2.700865858793259
  - 2.6894371986389163
  - 2.6806494235992435
  - 2.6611709356307984
  - 2.686547076702118
  - 2.6940186232328416
  - 2.666837200522423
  - 2.691654545068741
  - 2.6761862695217133
  - 2.660802859067917
  - 2.7044092893600467
  - 2.717234101891518
  - 2.6485705226659775
  - 2.652683621644974
  - 2.6699121028184893
  - 2.6793745011091232
  - 2.671126300096512
  - 2.655382424592972
  - 2.6778826117515564
  - 2.706413224339485
  - 2.6814710050821304
  - 2.6490252494812014
  - 2.667524087429047
  - 2.655886834859848
  - 2.6650802195072174
  - 2.638170909881592
  - 2.6606410056352616
  - 2.6653460025787354
  - 2.634469023346901
  - 2.6320142298936844
  - 2.6311030983924866
  - 2.6822332173585894
  - 2.666547599434853
  - 2.6792901396751407
  - 2.6852347910404206
  - 2.6743057638406755
  - 2.6454405546188355
  - 2.636553943157196
  - 2.634399670362473
  - 2.6308027416467668
  - 2.621901321411133
  - 2.651787078380585
  - 2.6405587613582613
  validation_losses:
  - 0.4506678581237793
  - 0.45566442608833313
  - 0.5578924417495728
  - 0.41969895362854004
  - 0.45058244466781616
  - 0.4867393970489502
  - 0.555260956287384
  - 0.4968356192111969
  - 0.39253556728363037
  - 0.44685453176498413
  - 0.4131108522415161
  - 0.46838584542274475
  - 0.5852322578430176
  - 0.47050797939300537
  - 0.5957948565483093
  - 0.4066624641418457
  - 0.4706169664859772
  - 0.5623092651367188
  - 0.5567247271537781
  - 0.3990132510662079
  - 0.4227333068847656
  - 0.4427095055580139
  - 0.4667436182498932
  - 0.48076707124710083
  - 0.48971840739250183
  - 0.44036057591438293
  - 0.3759092092514038
  - 0.4608624577522278
  - 0.44471001625061035
  - 0.5151691436767578
  - 0.5923629403114319
  - 0.4217061698436737
  - 0.4829280972480774
  - 0.3850245773792267
  - 0.40800294280052185
  - 0.49501338601112366
  - 0.46826839447021484
  - 0.49104544520378113
  - 0.6060387492179871
  - 0.48697900772094727
  - 0.42691829800605774
  - 0.5356348156929016
  - 0.5367770195007324
  - 0.5897693634033203
  - 0.5427989959716797
  - 0.5033345818519592
  - 0.6003274321556091
  - 0.4930165112018585
  - 0.556308388710022
  - 0.495374858379364
  - 0.4704473614692688
  - 0.44786664843559265
  - 0.640230655670166
  - 0.7066966891288757
  - 0.4714701473712921
  - 0.5439601540565491
  - 0.4792221486568451
  - 0.42527785897254944
  - 0.5686859488487244
  - 0.43170490860939026
  - 0.5491885542869568
  - 0.4756850600242615
  - 0.37502849102020264
  - 0.4859304130077362
  - 0.45477619767189026
  - 0.5717096328735352
  - 0.5435711741447449
  - 0.6262560486793518
  - 0.6080805063247681
  - 0.4698778986930847
  - 0.5041118264198303
  - 0.6093741655349731
  - 0.5368227958679199
  - 0.6376302242279053
  - 0.5575355887413025
  - 0.4727764427661896
  - 0.5752305388450623
  - 0.6734989881515503
  - 0.5860075354576111
  - 0.5063802599906921
  - 0.7933380603790283
  - 0.5619024634361267
  - 0.4237179160118103
  - 0.5870873332023621
  - 0.6007868051528931
  - 0.7020485997200012
  - 0.3708913326263428
  - 0.747254490852356
  - 0.6665155291557312
  - 0.5300722122192383
  - 0.5696540474891663
  - 0.4311976432800293
  - 0.6595513224601746
  - 0.7135507464408875
  - 0.5423651933670044
  - 0.5546863079071045
  - 0.6813302040100098
  - 0.5724328756332397
  - 0.6654172539710999
  - 0.5077567100524902
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 75 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8439108061749572,
    0.7955326460481099]'
  fold_eval_f1: '[0.0, 0.0, 0.046511627906976744, 0.20869565217391303, 0.24203821656050956]'
  mean_eval_accuracy: 0.8428115034806339
  mean_f1_accuracy: 0.09944909932827986
  total_train_time: '0:20:50.700929'
