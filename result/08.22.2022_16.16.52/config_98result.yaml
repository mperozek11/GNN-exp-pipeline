config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:23:28.779649'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_98fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_98fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_98fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_98fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_98fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_98fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_98fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_98fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_98fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_98fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.753368890285492
  - 1.560988312959671
  - 1.5599709808826447
  - 1.5038442373275758
  - 1.6453912734985352
  - 1.5636298894882203
  - 1.5702424287796022
  - 1.4978586554527284
  - 1.4732262134552003
  - 1.4447803974151612
  - 1.4794410020112991
  - 1.4695048570632936
  - 1.4460875511169435
  - 1.5817233741283419
  - 1.5273665368556977
  - 1.4817246079444886
  - 1.4640915989875793
  validation_losses:
  - 0.43346330523490906
  - 0.5397482514381409
  - 0.40649834275245667
  - 0.39227914810180664
  - 0.41664478182792664
  - 0.7668051719665527
  - 0.5010380744934082
  - 0.38726910948753357
  - 0.40832075476646423
  - 0.4116860032081604
  - 0.4302634298801422
  - 0.3930445611476898
  - 0.3953382670879364
  - 0.40531617403030396
  - 0.3878156244754791
  - 0.3922600746154785
  - 0.38877925276756287
loss_records_fold1:
  train_losses:
  - 1.5649037837982178
  - 1.4819695174694063
  - 1.5382257759571076
  - 1.466446876525879
  - 1.518557071685791
  - 1.4677780747413636
  - 1.5127228558063508
  - 1.4300307512283326
  - 1.429767978191376
  - 1.4525677502155305
  - 1.4610696971416475
  - 1.4836679041385652
  - 1.4976282238960268
  - 1.462433385848999
  - 1.424220257997513
  - 1.4429425597190857
  - 1.4502485632896425
  - 1.4512971162796022
  - 1.4609045147895814
  - 1.469598811864853
  - 1.4611676752567293
  - 1.450360780954361
  - 1.4794799506664278
  - 1.4340289652347566
  - 1.4168194890022279
  - 1.4935183703899384
  - 1.4688872396945953
  validation_losses:
  - 0.4005206525325775
  - 0.39762550592422485
  - 0.4012840986251831
  - 0.40109366178512573
  - 0.39093679189682007
  - 0.3854439854621887
  - 0.3875117897987366
  - 0.39390885829925537
  - 0.4113818407058716
  - 0.3922020494937897
  - 0.4215948283672333
  - 0.40341097116470337
  - 0.4031578302383423
  - 0.39551037549972534
  - 0.3916398286819458
  - 0.39032071828842163
  - 0.4221135973930359
  - 0.393932044506073
  - 0.3877817392349243
  - 0.3865474760532379
  - 0.42729923129081726
  - 0.3914695382118225
  - 0.3874935209751129
  - 0.38799038529396057
  - 0.3913549482822418
  - 0.3898687958717346
  - 0.3870857357978821
loss_records_fold2:
  train_losses:
  - 1.4838770508766175
  - 1.4393910467624664
  - 1.4272977352142335
  - 1.440463799238205
  - 1.4600058972835541
  - 1.4478923857212067
  - 1.4541416287422182
  - 1.4441982924938204
  - 1.4271076738834383
  - 1.4223896086215975
  - 1.4854661881923676
  validation_losses:
  - 0.40781527757644653
  - 0.38399559259414673
  - 0.38514837622642517
  - 0.38435637950897217
  - 0.38677048683166504
  - 0.380947470664978
  - 0.38251766562461853
  - 0.3854370713233948
  - 0.3866724967956543
  - 0.39425167441368103
  - 0.4033043086528778
loss_records_fold3:
  train_losses:
  - 1.4497137248516083
  - 1.5114249706268312
  - 1.4905331015586853
  - 1.484460783004761
  - 1.4776665508747102
  - 1.4558206319808962
  - 1.450789487361908
  - 1.475037181377411
  - 1.4597912669181825
  - 1.439571911096573
  - 1.45062415599823
  - 1.4831989765167237
  - 1.5145283162593843
  - 1.4962732613086702
  - 1.459008949995041
  - 1.4573185265064241
  - 1.4497680962085724
  - 1.4449719786643982
  - 1.4357790499925613
  - 1.447509390115738
  - 1.4264593839645388
  - 1.42111337184906
  - 1.4089941442012788
  - 1.426112997531891
  - 1.4599776566028595
  - 1.4694168031215669
  - 1.473958569765091
  - 1.4223135828971865
  - 1.4180457890033722
  - 1.446500724554062
  - 1.462847250699997
  - 1.4876908183097841
  - 1.4382594287395478
  - 1.4596358299255372
  - 1.468604749441147
  - 1.5027380526065828
  - 1.455498057603836
  - 1.5064406335353853
  - 1.433679109811783
  - 1.408740156888962
  - 1.4743982613086701
  - 1.4207309365272522
  - 1.4138204395771028
  - 1.4604566276073456
  - 1.4635823726654054
  - 1.407401791214943
  - 1.423517429828644
  - 1.475420141220093
  - 1.4862452864646913
  - 1.4193486630916596
  - 1.4104642629623414
  - 1.395899385213852
  - 1.4073740124702454
  - 1.4161430656909944
  - 1.4539516478776933
  - 1.3978846400976181
  - 1.4378842771053315
  - 1.4557878017425538
  - 1.4258889138698578
  - 1.4414288043975831
  - 1.4897974848747255
  - 1.4133278906345368
  - 1.4270617425441743
  - 1.4293739765882494
  - 1.4043928146362306
  - 1.3985957682132721
  - 1.4298179030418396
  - 1.4036677062511445
  - 1.412191092967987
  - 1.4130008578300477
  - 1.4414541244506838
  - 1.4156021535396577
  - 1.4222584307193757
  - 1.4679297536611557
  - 1.4161524295806887
  - 1.384711539745331
  - 1.3903027147054674
  - 1.4276647686958315
  - 1.422308158874512
  - 1.4500113904476166
  - 1.4098226726055145
  - 1.4096991539001467
  - 1.414113312959671
  - 1.418693631887436
  - 1.441778713464737
  - 1.4243328750133515
  - 1.3915978252887726
  - 1.3954584896564484
  - 1.396588945388794
  - 1.3861534833908082
  - 1.4317210137844087
  - 1.4055152535438538
  - 1.4128435969352724
  - 1.412086182832718
  - 1.3991072356700898
  - 1.426674222946167
  - 1.4273551166057588
  - 1.4531367003917695
  - 1.4056514382362366
  - 1.4217737913131714
  validation_losses:
  - 0.42706018686294556
  - 0.4029388129711151
  - 0.3954664170742035
  - 0.3779498338699341
  - 0.3933703601360321
  - 0.3748253285884857
  - 0.47523632645606995
  - 1.6428437232971191
  - 0.39192694425582886
  - 0.4321730136871338
  - 0.3858952522277832
  - 0.41285720467567444
  - 0.37876659631729126
  - 0.37823188304901123
  - 0.38279393315315247
  - 0.40158334374427795
  - 0.3812080919742584
  - 0.3885786831378937
  - 0.40860268473625183
  - 0.3983398377895355
  - 0.3772001564502716
  - 0.6537020206451416
  - 0.45573917031288147
  - 0.44481757283210754
  - 0.4947788417339325
  - 0.43649032711982727
  - 0.48310354351997375
  - 0.6963904500007629
  - 0.41983017325401306
  - 0.37516772747039795
  - 0.518538236618042
  - 0.6791144013404846
  - 0.584976315498352
  - 0.43768736720085144
  - 0.5273975133895874
  - 0.37468475103378296
  - 0.3772977292537689
  - 0.46560606360435486
  - 0.667700469493866
  - 0.5451441407203674
  - 0.6042755842208862
  - 0.4567241072654724
  - 0.6541401147842407
  - 0.5770344734191895
  - 0.37821850180625916
  - 0.5641140341758728
  - 0.9422338604927063
  - 132.7482147216797
  - 0.3744833469390869
  - 0.8337813019752502
  - 6.851680278778076
  - 2.8809187412261963
  - 0.415714293718338
  - 0.4175609350204468
  - 1.5678693056106567
  - 1.820590615272522
  - 0.7611591815948486
  - 0.48756706714630127
  - 1.4828288555145264
  - 0.4175785779953003
  - 0.761388897895813
  - 0.5334188938140869
  - 0.5813707113265991
  - 1.4023710489273071
  - 0.8687047362327576
  - 1.5750670433044434
  - 1.218916654586792
  - 1.6436666250228882
  - 0.43723806738853455
  - 1.288223385810852
  - 0.487465500831604
  - 0.8698232173919678
  - 1.275177240371704
  - 0.38419198989868164
  - 0.40418586134910583
  - 0.737632155418396
  - 0.5054194927215576
  - 0.5106891393661499
  - 0.5737037062644958
  - 0.7131127119064331
  - 0.5236999988555908
  - 0.45047810673713684
  - 1.0514588356018066
  - 0.5493345856666565
  - 0.5342506766319275
  - 0.6685757040977478
  - 0.9037520885467529
  - 1.3655683994293213
  - 0.8187963962554932
  - 0.5805565714836121
  - 1.425086259841919
  - 0.47199195623397827
  - 0.45945048332214355
  - 4.748862266540527
  - 1.5486232042312622
  - 2.022616147994995
  - 0.6865371465682983
  - 0.4294356107711792
  - 0.4434235692024231
  - 0.8666607141494751
loss_records_fold4:
  train_losses:
  - 1.4122571051120758
  - 1.4510194659233093
  - 1.403743416070938
  - 1.415752798318863
  - 1.4096893191337587
  - 1.4091381430625916
  - 1.3960394859313965
  - 1.4359532237052919
  - 1.4242349863052368
  - 1.3965486466884613
  - 1.394838732481003
  - 1.4105996251106263
  - 1.4092590510845184
  - 1.4242816746234894
  - 1.397558254003525
  - 1.4182833194732667
  - 1.4389982283115388
  - 1.402744323015213
  - 1.361740952730179
  - 1.425768154859543
  - 1.4300736367702485
  - 1.4064510822296143
  - 1.4425146877765656
  - 1.4658827245235444
  - 1.4358951330184937
  - 1.373618268966675
  - 1.4210234224796296
  - 1.380007976293564
  - 1.3980090141296388
  - 1.4018503189086915
  - 1.388276743888855
  - 1.371469110250473
  - 1.3717638313770295
  - 1.4305705308914185
  - 1.4133978128433229
  - 1.4052518665790559
  - 1.410501664876938
  - 1.406626754999161
  - 1.403801703453064
  - 1.3818722546100617
  - 1.410527876019478
  - 1.389447668194771
  - 1.3667668879032135
  - 1.437131863832474
  - 1.4845093488693237
  - 1.4216543167829514
  - 1.4087833970785142
  - 1.4153793156147003
  - 1.4687846004962921
  - 1.4277961790561677
  - 1.4216271817684174
  - 1.4156633079051972
  - 1.4636866807937623
  - 1.3638750523328782
  - 1.4424135327339174
  - 1.4028921931982041
  - 1.4342903077602387
  - 1.4364938914775849
  - 1.4145708411931992
  - 1.3801334470510485
  - 1.406265026330948
  - 1.4079766213893892
  - 1.430394318699837
  - 1.3876134276390077
  - 1.3994934141635895
  - 1.395301276445389
  - 1.384537571668625
  - 1.4219566226005556
  - 1.4040094673633576
  - 1.3871005773544312
  - 1.3706702589988708
  - 1.3854738593101503
  - 1.4075532108545303
  - 1.438586312532425
  - 1.4133938908576966
  - 1.393984591960907
  - 1.4149860978126527
  - 1.3944534480571749
  - 1.3693630486726762
  - 1.3754551708698273
  - 1.37468980550766
  - 1.412271136045456
  - 1.4027475595474244
  - 1.3965237140655518
  - 1.4178277790546419
  - 1.3989394128322603
  - 1.4122266709804536
  - 1.4219315826892853
  - 1.4219191133975984
  - 1.3672548502683641
  - 1.396780574321747
  - 1.372957879304886
  - 1.3873702585697174
  - 1.3745256543159485
  - 1.3983822166919708
  - 1.4165809750556946
  - 1.3747264683246614
  - 1.3710434317588807
  - 1.3392057597637177
  - 1.3686191380023958
  validation_losses:
  - 0.3791746497154236
  - 0.39926889538764954
  - 0.3988954424858093
  - 0.35863885283470154
  - 0.38293102383613586
  - 0.3808160722255707
  - 0.40904664993286133
  - 0.35853734612464905
  - 0.39374077320098877
  - 0.38486331701278687
  - 0.44397178292274475
  - 0.4307701587677002
  - 0.4401332139968872
  - 0.3878535032272339
  - 0.3726322650909424
  - 0.4078642725944519
  - 0.3989109992980957
  - 0.3709679841995239
  - 0.3584941625595093
  - 0.3620947301387787
  - 0.40077725052833557
  - 0.48590371012687683
  - 0.37204670906066895
  - 0.365458220243454
  - 0.35939401388168335
  - 0.39955830574035645
  - 0.3911801278591156
  - 0.43214479088783264
  - 0.4193474352359772
  - 0.38316041231155396
  - 0.37792670726776123
  - 0.3569263219833374
  - 1.34255850315094
  - 0.8720982074737549
  - 1.232472538948059
  - 3.897291421890259
  - 1.7567118406295776
  - 0.36217305064201355
  - 0.3823615610599518
  - 0.35708001255989075
  - 4.8112616539001465
  - 8.398743629455566
  - 1.3963735103607178
  - 0.41005024313926697
  - 0.3871577978134155
  - 0.38362500071525574
  - 0.3807595372200012
  - 0.43714773654937744
  - 0.3708442449569702
  - 0.38482460379600525
  - 0.3797067105770111
  - 0.3719337284564972
  - 0.383113294839859
  - 0.3628229796886444
  - 0.3613840937614441
  - 0.3724084794521332
  - 1.4452250003814697
  - 0.3713602125644684
  - 0.3634325861930847
  - 0.376644104719162
  - 0.3745068609714508
  - 0.37602147459983826
  - 0.8858083486557007
  - 0.7153074741363525
  - 0.3745579719543457
  - 0.38701295852661133
  - 0.3936174511909485
  - 0.3714960217475891
  - 0.3759351372718811
  - 0.37686190009117126
  - 3.709817886352539
  - 0.39390793442726135
  - 0.39592835307121277
  - 0.3946237564086914
  - 0.3761662542819977
  - 0.3815653324127197
  - 0.47098737955093384
  - 0.3724695146083832
  - 0.36785969138145447
  - 0.36844784021377563
  - 0.36823979020118713
  - 0.383670836687088
  - 0.36391377449035645
  - 0.3725740313529968
  - 0.38795000314712524
  - 0.39109498262405396
  - 0.45239368081092834
  - 0.38736477494239807
  - 0.3904588222503662
  - 0.3965451121330261
  - 0.5335791110992432
  - 0.7187455296516418
  - 0.9709984660148621
  - 0.36423853039741516
  - 0.36356258392333984
  - 0.36629146337509155
  - 0.41184449195861816
  - 0.4620358645915985
  - 0.4518882632255554
  - 0.5350840091705322
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8353344768439108,
    0.8487972508591065]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.15789473684210525, 0.18518518518518517]'
  mean_eval_accuracy: 0.8517491585766239
  mean_f1_accuracy: 0.06861598440545809
  total_train_time: '0:22:42.469230'
