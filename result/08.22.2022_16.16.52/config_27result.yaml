config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:43:42.476755'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_27fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_27fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_27fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_27fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_27fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_27fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_27fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_27fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_27fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_27fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 18.574158477783204
  - 7.753675436973572
  - 2.840746110677719
  - 2.4663018584251404
  - 1.8424833595752717
  - 1.9570346117019655
  - 2.0021091222763063
  - 2.465539515018463
  - 1.617912542819977
  - 1.4237265527248384
  - 2.5097281813621524
  - 1.2245960354804994
  - 1.3000473618507387
  - 3.2284352779388428
  - 1.8679193019866944
  - 1.5192770898342134
  - 1.4416503012180328
  - 1.2679864823818208
  - 1.0644280910491943
  - 1.0258521020412446
  - 0.8306889653205872
  - 0.8330414354801179
  - 0.8414291262626649
  - 0.9468812167644501
  - 2.0914014279842377
  - 1.461816704273224
  - 2.211193782091141
  - 1.8254520058631898
  - 2.591681969165802
  - 1.4026268601417542
  - 1.3694433569908142
  - 1.294579839706421
  - 1.1734711170196535
  - 1.193083131313324
  - 0.9931528568267822
  - 0.9802633106708527
  - 4.148097056150436
  - 1.584477162361145
  - 1.1909199833869935
  - 7.50666361451149
  - 2.1837905943393707
  - 1.5364603102207184
  - 3.1934282660484317
  - 3.732941281795502
  - 1.9392223119735719
  - 0.935026615858078
  - 1.111985194683075
  - 1.0402804732322692
  - 1.0899911105632782
  - 1.0621064096689226
  - 1.0567487359046936
  - 1.2237073302268984
  - 0.7962126314640046
  - 0.8160508692264558
  - 0.9358543694019318
  - 0.8236293613910676
  - 0.7945861339569092
  - 0.795942059159279
  - 1.1774997353553773
  - 0.903576734662056
  - 0.9160166084766388
  - 0.8536835968494416
  - 1.0379399538040162
  - 1.0876389265060424
  - 1.1386267364025116
  - 1.1529959082603456
  - 1.0039781570434572
  - 0.9057797014713288
  - 1.3143329620361328
  - 1.0411784052848816
  - 0.9585903406143189
  - 0.9731831371784211
  - 0.9717026948928833
  - 1.3891348719596863
  - 0.9185474157333374
  - 0.9321321964263917
  - 0.774698355793953
  - 0.8042208194732666
  - 0.815245795249939
  - 0.8104112088680268
  validation_losses:
  - 4.817377090454102
  - 1.5622408390045166
  - 2.6904501914978027
  - 1.2172439098358154
  - 1.1270711421966553
  - 0.8456854820251465
  - 3.0363972187042236
  - 1.1164785623550415
  - 0.5084517598152161
  - 0.5591585040092468
  - 0.7265862822532654
  - 0.7221627831459045
  - 0.5149411559104919
  - 0.5714504718780518
  - 0.507888913154602
  - 0.7393186688423157
  - 0.4790297746658325
  - 0.4461812973022461
  - 0.43074464797973633
  - 0.44636496901512146
  - 0.7367045283317566
  - 0.517810583114624
  - 0.48598966002464294
  - 0.4198324978351593
  - 0.6630206108093262
  - 0.6688796877861023
  - 2.026503801345825
  - 0.6669529676437378
  - 0.7244874238967896
  - 1.033600091934204
  - 0.5713716745376587
  - 0.4324721097946167
  - 0.48658430576324463
  - 0.41862940788269043
  - 0.40050971508026123
  - 0.43781864643096924
  - 0.5842806696891785
  - 0.45649367570877075
  - 0.5964791178703308
  - 1.3964780569076538
  - 0.6003910303115845
  - 0.5624199509620667
  - 0.5110087394714355
  - 0.44672009348869324
  - 0.4259612560272217
  - 0.46120312809944153
  - 0.4529356360435486
  - 0.5314793586730957
  - 0.48556095361709595
  - 0.4822500944137573
  - 0.4185381829738617
  - 0.41942325234413147
  - 0.5367618799209595
  - 0.386320561170578
  - 0.40235620737075806
  - 0.46365541219711304
  - 0.4086473286151886
  - 0.3950282335281372
  - 0.4468410015106201
  - 0.39176180958747864
  - 0.4391861855983734
  - 0.4376934766769409
  - 0.40120306611061096
  - 0.39927592873573303
  - 0.6140614748001099
  - 0.4603099524974823
  - 0.3864955008029938
  - 0.5201371312141418
  - 0.6548337936401367
  - 0.42207810282707214
  - 0.4563765823841095
  - 0.4367372691631317
  - 0.4013150632381439
  - 0.49060243368148804
  - 0.4935491979122162
  - 0.3844502568244934
  - 0.3832593858242035
  - 0.3805733621120453
  - 0.37829872965812683
  - 0.3826449513435364
loss_records_fold1:
  train_losses:
  - 0.8384487271308899
  - 1.0153584003448486
  - 0.785077303647995
  - 0.740803536772728
  - 0.7859909415245057
  - 0.7790414571762085
  - 0.8091121375560761
  - 1.0745667755603792
  - 1.0045437753200532
  - 0.9308383882045747
  - 1.08429154753685
  - 0.8904499471187592
  - 0.8674409806728364
  - 0.8203584849834442
  - 0.7970329105854035
  - 0.7818014979362489
  - 0.766419130563736
  - 0.8087793529033661
  - 0.7676357090473176
  - 0.9981590449810028
  - 0.8959015369415284
  - 0.9583297729492188
  - 1.1599530756473542
  - 0.9303776144981385
  - 1.2470572352409364
  - 1.0728077590465546
  - 0.988218891620636
  - 0.9755619227886201
  - 0.8099756479263306
  - 0.7852240979671479
  - 0.8245239913463593
  - 0.9511760413646698
  - 1.876788640022278
  - 1.0809520542621613
  - 4.451052218675613
  - 1.1288274168968202
  - 1.0436883449554444
  - 1.2345136523246767
  - 1.4072613179683686
  - 0.9252943336963654
  - 0.8538413494825363
  - 0.9047397434711457
  - 1.5736101031303407
  - 0.7915041267871857
  - 1.0096651017665863
  - 0.8931027352809906
  - 1.0443713247776032
  - 1.0196282684803009
  - 0.9906093060970307
  - 0.8029753267765045
  - 1.3307122111320497
  - 2.0814967155456543
  - 1.4174152731895449
  - 1.380947858095169
  - 1.2037014186382295
  - 1.142516940832138
  - 1.0441738545894623
  - 1.0551052927970888
  - 1.5730658590793611
  - 1.087991851568222
  - 0.8771723985671998
  - 0.8489582896232606
  - 0.8161579310894013
  - 0.7952794164419175
  - 0.8893733322620392
  - 0.8398397743701935
  - 0.8043322443962098
  - 5.766819846630097
  - 0.9865415990352631
  - 0.855208370089531
  - 0.9465478241443634
  - 2.555823785066605
  - 1.3572949469089508
  - 1.045755821466446
  - 1.1496068596839906
  - 1.6114208042621614
  - 4.478155905008316
  - 1.5353762090206147
  - 0.9680307090282441
  - 0.8182986676692963
  - 0.7683088243007661
  - 0.8018013298511506
  - 0.8294422149658204
  - 0.792153000831604
  validation_losses:
  - 0.4101129174232483
  - 0.4033167064189911
  - 0.40202051401138306
  - 0.3935595750808716
  - 0.4180915355682373
  - 0.395610511302948
  - 0.3977139890193939
  - 0.40345004200935364
  - 0.4288673996925354
  - 0.42353153228759766
  - 0.4135138988494873
  - 0.39740294218063354
  - 0.4108259081840515
  - 0.3982974886894226
  - 0.39581042528152466
  - 0.40128830075263977
  - 0.3943804204463959
  - 0.4475206732749939
  - 0.40450799465179443
  - 0.4260920286178589
  - 0.4562990367412567
  - 0.428280234336853
  - 0.4126283526420593
  - 0.48765799403190613
  - 0.43077415227890015
  - 0.4583543539047241
  - 0.43916189670562744
  - 0.4096662104129791
  - 1.6958459615707397
  - 0.4300738275051117
  - 0.4044087529182434
  - 0.4006546139717102
  - 0.4504745304584503
  - 0.4527001678943634
  - 0.5282608270645142
  - 0.697642982006073
  - 0.4466671049594879
  - 0.5578480362892151
  - 0.4190175235271454
  - 0.4286026656627655
  - 0.4111434519290924
  - 0.40865352749824524
  - 0.396607369184494
  - 0.8548516035079956
  - 0.4035857915878296
  - 0.43629035353660583
  - 0.4263223707675934
  - 0.4339398443698883
  - 0.40592432022094727
  - 0.43134602904319763
  - 0.40737131237983704
  - 0.4092276990413666
  - 0.46395692229270935
  - 0.42512762546539307
  - 0.4044906198978424
  - 0.40556517243385315
  - 0.42201587557792664
  - 0.4060567319393158
  - 0.4665409028530121
  - 0.5988851189613342
  - 0.42082449793815613
  - 0.39998942613601685
  - 0.39723482728004456
  - 0.41302675008773804
  - 0.40312427282333374
  - 0.42996665835380554
  - 0.3991376757621765
  - 0.4188290536403656
  - 0.46184539794921875
  - 0.4028722941875458
  - 0.450801283121109
  - 0.40880894660949707
  - 0.40550145506858826
  - 0.4434965252876282
  - 0.4546182155609131
  - 0.40120792388916016
  - 0.4130309224128723
  - 0.5906679034233093
  - 0.4168049395084381
  - 0.39965754747390747
  - 0.39937397837638855
  - 0.40072357654571533
  - 0.4004575312137604
  - 0.40672439336776733
loss_records_fold2:
  train_losses:
  - 0.8173006296157838
  - 0.7670643746852875
  - 2.0104678690433504
  - 0.7962613821029664
  - 0.8156934499740601
  - 0.841852217912674
  - 0.8309330523014069
  - 0.8153469741344452
  - 1.3299662113189699
  - 1.1886956065893173
  - 0.8750286139547825
  - 1.0732576727867127
  - 1.225078856945038
  - 1.086948186159134
  - 0.9942302763462068
  - 2.5695724487304688
  - 1.2198263108730316
  - 0.9330689251422882
  - 2.478309947252274
  - 0.8724691331386567
  - 1.2499106526374817
  - 0.7674146324396134
  - 0.8445359349250794
  - 0.8713067710399628
  - 0.8192218840122223
  - 0.8131373405456543
  - 0.8084186255931854
  - 0.7803649961948396
  - 0.755884662270546
  - 0.8225592195987702
  - 0.8053299546241761
  - 0.8506267488002778
  - 0.768004333972931
  - 0.8111618757247925
  validation_losses:
  - 0.38009345531463623
  - 0.37928158044815063
  - 0.3935575485229492
  - 0.3841169476509094
  - 0.388034462928772
  - 0.3856678605079651
  - 0.4203249514102936
  - 0.39140060544013977
  - 0.3840383291244507
  - 0.3896077871322632
  - 0.38737523555755615
  - 0.39206060767173767
  - 0.41643816232681274
  - 0.41076189279556274
  - 0.3974074423313141
  - 0.380405068397522
  - 0.383004367351532
  - 0.38514968752861023
  - 0.42615002393722534
  - 0.3891906142234802
  - 0.3949526846408844
  - 0.402919203042984
  - 0.46254846453666687
  - 0.3787907660007477
  - 0.3792496919631958
  - 0.3825320303440094
  - 0.38084352016448975
  - 0.391714483499527
  - 0.38090330362319946
  - 0.38878726959228516
  - 0.3794812858104706
  - 0.38283491134643555
  - 0.3815802037715912
  - 0.3859415054321289
loss_records_fold3:
  train_losses:
  - 0.8269080460071564
  - 0.8045655250549317
  - 0.7954859375953675
  - 0.785559356212616
  - 0.7860262095928192
  - 0.7941806972026826
  - 0.7844085156917573
  - 0.7872878432273865
  - 0.7843348443508149
  - 0.7792738378047943
  - 0.9321086943149567
  - 0.8235290110111237
  - 0.7969648838043213
  - 0.7771292835474015
  - 0.8285272121429443
  - 0.804166865348816
  - 0.9223704814910889
  - 0.9186918437480927
  - 0.856691861152649
  - 0.9867136895656586
  - 0.8061215043067933
  - 0.8614748656749726
  - 0.8837887823581696
  validation_losses:
  - 0.3910832405090332
  - 0.39273518323898315
  - 0.39655885100364685
  - 0.39156022667884827
  - 0.3938511908054352
  - 0.40774983167648315
  - 0.39119085669517517
  - 0.39894258975982666
  - 0.3926014304161072
  - 0.3882121741771698
  - 0.38875287771224976
  - 0.4055309593677521
  - 0.3960621953010559
  - 0.3944494426250458
  - 0.3868710398674011
  - 0.38818925619125366
  - 0.3990150988101959
  - 0.3886245787143707
  - 0.38525131344795227
  - 0.38632407784461975
  - 0.3854883909225464
  - 0.3855498135089874
  - 0.38527318835258484
loss_records_fold4:
  train_losses:
  - 0.8048820614814759
  - 0.9154471576213837
  - 0.812510895729065
  - 0.7936303973197938
  - 0.7678138554096222
  - 0.7491715252399445
  - 0.8070205450057983
  - 0.8076646447181702
  - 0.8451113820075989
  - 0.8072316944599152
  - 0.8246877908706666
  - 0.8135279834270478
  - 0.7462928742170334
  - 0.758291694521904
  - 0.7698102772235871
  - 0.7989398181438446
  - 0.8500357806682587
  - 0.8867675364017487
  - 0.7874867141246796
  - 0.7768801748752594
  - 0.7827626943588257
  - 0.7903369128704072
  validation_losses:
  - 0.4045361876487732
  - 0.45477044582366943
  - 0.3992635905742645
  - 0.40242621302604675
  - 0.40383556485176086
  - 0.4041478633880615
  - 0.4392610490322113
  - 0.42116019129753113
  - 0.39692366123199463
  - 0.39379438757896423
  - 0.39382049441337585
  - 0.4045369327068329
  - 0.4264830946922302
  - 0.422310471534729
  - 0.447780966758728
  - 0.4847933351993561
  - 0.43263763189315796
  - 0.4266420304775238
  - 0.414781779050827
  - 0.4131452441215515
  - 0.4027888774871826
  - 0.4032007157802582
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 80 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 84 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:19:36.903165'
