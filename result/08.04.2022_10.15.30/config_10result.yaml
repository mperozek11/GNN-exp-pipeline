config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 10:15:54.142089'
fold_0_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_10fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_10fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_10fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_10fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_10fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_10fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_10fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_10fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_10fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_10fold_4_state_dict.pt
loss_records:
  train_losses:
  - 3.4959110468626022
  - 3.9281646609306335
  - 3.725108504295349
  - 3.861310452222824
  - 3.6663959622383118
  - 3.5946538150310516
  - 3.669944614171982
  - 3.6505607664585114
  - 3.6950094401836395
  - 3.6048038005828857
  - 3.688085585832596
  - 3.672576755285263
  - 4.341606110334396
  - 3.791911393404007
  - 3.655718147754669
  - 3.639848053455353
  - 3.610733389854431
  - 3.5475464463233948
  - 3.987354874610901
  - 3.777117282152176
  - 3.671490639448166
  - 3.753885895013809
  - 3.597164809703827
  - 3.6875001788139343
  - 3.6666158735752106
  - 3.5506147742271423
  - 3.6985554099082947
  - 3.76862633228302
  - 3.9073058366775513
  - 3.6760644018650055
  - 3.7554851174354553
  - 3.6647904813289642
  - 3.7442581355571747
  - 3.660602957010269
  - 3.9279765188694
  - 3.7944403290748596
  - 3.6211501359939575
  - 3.70238134264946
  - 3.7285885512828827
  - 3.8644959628582
  - 3.5949456095695496
  - 3.5903814136981964
  - 3.670475870370865
  - 3.6628392338752747
  - 3.717011958360672
  - 3.8159239888191223
  - 3.7096689343452454
  - 3.621980369091034
  - 3.826717436313629
  - 3.662874698638916
  - 3.694093346595764
  - 3.61485555768013
  - 3.9374479949474335
  - 3.6269510090351105
  - 3.8948822617530823
  - 3.797689527273178
  - 4.1584492623806
  - 3.843206435441971
  - 3.6570063531398773
  - 3.829873025417328
  - 3.590480476617813
  - 3.768579810857773
  - 3.82854688167572
  - 3.611913174390793
  - 3.5862145125865936
  - 3.9887759685516357
  - 3.7753139436244965
  - 3.811077654361725
  - 3.635219693183899
  - 3.5390224009752274
  - 3.713940382003784
  - 3.6684031784534454
  - 3.5824213325977325
  - 3.8417613208293915
  - 3.678589850664139
  - 3.667792171239853
  - 3.8703673183918
  - 3.5638149082660675
  - 3.5386079847812653
  - 3.6998854875564575
  - 3.829731285572052
  - 3.4804700762033463
  - 3.7456955909729004
  - 4.2060545682907104
  - 3.777707487344742
  - 3.9683795869350433
  - 3.905533105134964
  - 3.841410905122757
  - 3.9663796722888947
  - 3.7430033683776855
  - 3.560489609837532
  - 3.69499334692955
  - 3.703228324651718
  - 3.781194508075714
  - 3.669096916913986
  - 3.65329572558403
  - 3.693226933479309
  - 3.600724160671234
  - 3.6736637949943542
  - 3.9520045816898346
  validation_losses:
  - 0.4397333860397339
  - 0.3731435537338257
  - 0.36336082220077515
  - 0.3956981301307678
  - 0.37349000573158264
  - 0.45123690366744995
  - 0.37519821524620056
  - 0.5281432867050171
  - 0.4315967559814453
  - 0.4320971965789795
  - 0.47558271884918213
  - 0.4327395260334015
  - 0.405877023935318
  - 0.36873480677604675
  - 0.37017348408699036
  - 0.36634552478790283
  - 0.3938364088535309
  - 0.4035162925720215
  - 0.48064345121383667
  - 0.4136548936367035
  - 0.4483589828014374
  - 0.4344507157802582
  - 0.46915486454963684
  - 0.5041928291320801
  - 0.43856343626976013
  - 0.4631040394306183
  - 0.5061383247375488
  - 0.4449636936187744
  - 0.3795524835586548
  - 0.39560580253601074
  - 4.5131611824035645
  - 2.36279559135437
  - 3.7057909965515137
  - 1.4169597625732422
  - 0.39437416195869446
  - 0.38904958963394165
  - 0.3844969868659973
  - 0.39895445108413696
  - 0.8121280074119568
  - 1.0989971160888672
  - 0.4607134759426117
  - 0.6078667640686035
  - 1.099589228630066
  - 0.8987146615982056
  - 0.9592505097389221
  - 0.4200465679168701
  - 0.3978337049484253
  - 0.6975106000900269
  - 0.5578492283821106
  - 0.4862888753414154
  - 0.8445833921432495
  - 1.8068996667861938
  - 0.7846224904060364
  - 0.9732354283332825
  - 1.5010814666748047
  - 0.7935112714767456
  - 0.3921656608581543
  - 0.5102571845054626
  - 0.8364118933677673
  - 0.8152689337730408
  - 0.9926397800445557
  - 1.1481127738952637
  - 0.9223983883857727
  - 1.878483772277832
  - 0.8543607592582703
  - 0.40499597787857056
  - 0.4129049479961395
  - 0.7615697979927063
  - 1.0268020629882812
  - 1.7697100639343262
  - 1.0545061826705933
  - 0.5879456996917725
  - 0.7347725629806519
  - 0.49731844663619995
  - 0.5761284232139587
  - 1.0870202779769897
  - 0.4757881164550781
  - 0.6431185603141785
  - 0.8691873550415039
  - 0.9757853150367737
  - 0.9896156787872314
  - 0.41559353470802307
  - 0.4100516140460968
  - 0.3656434118747711
  - 0.3810878098011017
  - 0.73882657289505
  - 0.4908062219619751
  - 0.3723081946372986
  - 0.3693826496601105
  - 0.377017080783844
  - 0.37961113452911377
  - 0.424304336309433
  - 0.6901586651802063
  - 0.5326618552207947
  - 0.6416379809379578
  - 1.3746378421783447
  - 0.7727113962173462
  - 0.8997697234153748
  - 0.9857400059700012
  - 0.9758786559104919
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 9 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8353344768439108, 0.8524871355060034,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.15789473684210525, 0.27118644067796616, 0.046511627906976744]'
  mean_eval_accuracy: 0.8524388015537596
  mean_f1_accuracy: 0.09511856108540964
  total_train_time: '0:10:01.956075'
