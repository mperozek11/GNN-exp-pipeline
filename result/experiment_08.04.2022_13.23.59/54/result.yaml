config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 18:58:55.198728'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/54/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/54/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/54/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/54/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/54/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/54/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/54/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/54/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/54/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/54/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 101.1809892654419
  - 37.99429714679718
  - 10.526778936386108
  - 14.927375733852386
  - 10.2216337621212
  - 13.502836436033249
  - 10.157443761825562
  - 13.063550174236298
  - 9.290459722280502
  - 7.91682767868042
  - 10.57188069820404
  - 7.263133883476257
  - 7.968238085508347
  - 15.292696624994278
  - 8.487852215766907
  - 7.747052580118179
  - 11.615257799625397
  - 7.606716573238373
  - 7.200606614351273
  - 5.232847213745117
  - 5.0159547328948975
  - 4.9830202758312225
  - 4.907053351402283
  - 4.743643015623093
  - 10.501559793949127
  - 5.948881417512894
  - 13.861461013555527
  - 7.365165084600449
  - 6.3458835780620575
  - 5.461306631565094
  - 8.047450244426727
  - 6.5645657777786255
  - 5.875485360622406
  - 5.159962862730026
  - 5.13703316450119
  - 4.558963268995285
  - 19.918203830718994
  - 8.524356663227081
  - 5.626077890396118
  - 17.199927866458893
  - 13.06654566526413
  - 6.915771782398224
  - 16.547426223754883
  - 15.889571875333786
  - 9.639061480760574
  - 4.7743232399225235
  - 5.720635622739792
  - 5.91846177726984
  - 5.560506701469421
  - 6.301253467798233
  - 5.713625580072403
  - 8.459716111421585
  - 4.337861716747284
  - 6.32177597284317
  - 6.1444604098796844
  - 4.454561740159988
  - 4.021917104721069
  - 4.069500654935837
  - 5.109067499637604
  - 4.2973853796720505
  - 4.965441673994064
  - 4.186361312866211
  - 4.538243770599365
  - 4.48331555724144
  - 5.042114436626434
  - 5.608421593904495
  - 5.478725731372833
  - 4.901044011116028
  - 5.793567061424255
  - 6.792406231164932
  - 5.731927990913391
  - 5.168601214885712
  - 5.679970920085907
  - 5.871483623981476
  - 4.1144576370716095
  - 3.9674889147281647
  - 4.0992060005664825
  - 5.106094151735306
  - 3.993137925863266
  - 4.399647772312164
  - 3.934569537639618
  - 3.879327431321144
  - 4.308863937854767
  - 4.2720233500003815
  - 3.858234256505966
  - 5.040269523859024
  - 8.348291337490082
  - 4.930897563695908
  - 6.891349494457245
  - 4.765559434890747
  - 4.589059889316559
  - 4.725038051605225
  - 4.1028008460998535
  - 4.352169007062912
  - 4.237663358449936
  - 4.228755295276642
  - 3.923869103193283
  - 3.891336977481842
  - 4.974488437175751
  - 4.843237280845642
  validation_losses:
  - 5.620556354522705
  - 1.6657330989837646
  - 0.7186475396156311
  - 1.9895254373550415
  - 1.3101974725723267
  - 2.9242992401123047
  - 2.5649607181549072
  - 2.321962594985962
  - 0.711853563785553
  - 0.6204274892807007
  - 0.7275530695915222
  - 0.9406920671463013
  - 0.5358538627624512
  - 0.6691005825996399
  - 0.5440362691879272
  - 0.6659175753593445
  - 0.653546154499054
  - 0.7335065603256226
  - 0.4268031120300293
  - 0.46620458364486694
  - 0.4188102185726166
  - 0.7437891364097595
  - 0.39743098616600037
  - 0.401534765958786
  - 0.4153921902179718
  - 0.4886946976184845
  - 0.8705465793609619
  - 0.4894438683986664
  - 0.46978509426116943
  - 0.5290580987930298
  - 0.47996625304222107
  - 0.4450899362564087
  - 0.4980061650276184
  - 0.44168102741241455
  - 0.3991280496120453
  - 0.43714967370033264
  - 0.4507719576358795
  - 0.44299066066741943
  - 0.4911922216415405
  - 0.9870674014091492
  - 0.5325239300727844
  - 0.5091465711593628
  - 0.47116509079933167
  - 0.3990432024002075
  - 0.4362362027168274
  - 0.4967710077762604
  - 0.48826828598976135
  - 0.5249065160751343
  - 0.6857197880744934
  - 0.5095471739768982
  - 0.42670056223869324
  - 0.4243454933166504
  - 0.4719957709312439
  - 0.5931984186172485
  - 0.3829106092453003
  - 0.39383506774902344
  - 0.3840002417564392
  - 0.3835262060165405
  - 0.39508309960365295
  - 0.4880213141441345
  - 0.4082498550415039
  - 0.3848530948162079
  - 0.41675037145614624
  - 0.41138020157814026
  - 0.4494588077068329
  - 0.6191262006759644
  - 0.41175732016563416
  - 0.592848539352417
  - 0.5237165689468384
  - 0.7266351580619812
  - 0.4197172522544861
  - 0.40968945622444153
  - 0.4663954973220825
  - 0.4745272994041443
  - 0.4258837401866913
  - 0.3906196057796478
  - 0.4442635178565979
  - 0.3853420615196228
  - 0.38880014419555664
  - 0.39267978072166443
  - 0.38666194677352905
  - 0.3874979615211487
  - 0.46716174483299255
  - 0.3874722123146057
  - 0.3882628083229065
  - 0.5302955508232117
  - 0.41931918263435364
  - 0.5798953771591187
  - 0.5199444890022278
  - 0.4141589105129242
  - 0.5014002919197083
  - 0.4030430018901825
  - 0.4089808762073517
  - 0.40575021505355835
  - 0.44295018911361694
  - 0.39190980792045593
  - 0.39150944352149963
  - 0.3871954083442688
  - 0.3879217207431793
  - 0.4005233347415924
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 71 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 58 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:02:17.004657'
