config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:32:51.645664'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_92fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_92fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_92fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_92fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_92fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_92fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_92fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_92fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_92fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_92fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 72.62393113672734
  - 43.77933217734099
  - 49.40165823698044
  - 35.85150229334831
  - 36.0842264816165
  - 23.28675771355629
  - 17.412467896938324
  - 27.52690913975239
  - 21.842159309983256
  - 20.860961735248566
  - 11.297648477554322
  - 10.938737526535988
  - 11.14958678483963
  - 12.744920244812967
  validation_losses:
  - 2.6406655311584473
  - 1.9902960062026978
  - 0.5759116411209106
  - 1.059693694114685
  - 0.6592826247215271
  - 1.7837843894958496
  - 0.6222928762435913
  - 0.7264727354049683
  - 0.5150725841522217
  - 0.4631896913051605
  - 0.43730607628822327
  - 0.4073924124240875
  - 0.4170401990413666
  - 0.401138573884964
loss_records_fold1:
  train_losses:
  - 11.917423295974732
  - 7.886594650149346
  - 8.187219899892808
  - 7.045865085721016
  - 6.76274583041668
  - 7.16839651465416
  - 7.839138028025627
  - 6.866741880774498
  - 7.161111211776734
  - 8.10719084739685
  - 8.493756222724915
  - 8.559595003724098
  - 8.903940334916115
  - 6.9870133936405185
  - 6.971231302618981
  - 7.173896107077599
  - 6.732397729158402
  - 6.666702598333359
  - 6.497601264715195
  - 6.506597542762757
  - 6.536981058120728
  - 6.576665008068085
  - 6.465956813097001
  - 6.475063773989678
  - 6.595914968848229
  - 6.349603772163391
  - 6.660760509967805
  - 6.321674966812134
  - 6.736052480340004
  - 6.314092627167702
  - 6.689802795648575
  - 7.184115540981293
  - 8.841679319739342
  - 7.3401973962783815
  - 6.492637705802918
  - 6.399144846200944
  - 7.092956143617631
  - 7.065362733602524
  - 8.182334697246551
  - 7.470413154363633
  - 6.374171707034112
  - 6.3191479563713076
  - 6.346835950016976
  - 6.4747337102890015
  - 7.0001922935247425
  - 6.540300151705742
  - 6.2371098637580875
  validation_losses:
  - 0.41381481289863586
  - 0.47870564460754395
  - 0.42021337151527405
  - 0.4034065306186676
  - 0.4086921215057373
  - 0.4190908372402191
  - 0.41823893785476685
  - 0.4368171989917755
  - 0.40529897809028625
  - 0.4350332021713257
  - 0.4233643412590027
  - 0.44782280921936035
  - 0.48650720715522766
  - 0.41131332516670227
  - 0.4230681359767914
  - 0.4265211820602417
  - 0.4184357523918152
  - 0.4100048840045929
  - 0.40443116426467896
  - 0.41965237259864807
  - 0.4172205924987793
  - 0.4156709611415863
  - 0.4305752217769623
  - 0.49319329857826233
  - 0.44667139649391174
  - 0.45541566610336304
  - 0.40906578302383423
  - 0.4412548840045929
  - 0.453790545463562
  - 0.45128333568573
  - 0.516467809677124
  - 0.4284355044364929
  - 0.5406110882759094
  - 0.6379859447479248
  - 0.41521909832954407
  - 0.44553449749946594
  - 0.4096374213695526
  - 0.43271133303642273
  - 0.4147733151912689
  - 0.43184399604797363
  - 0.5141996145248413
  - 0.4508858323097229
  - 0.43235117197036743
  - 0.4201788902282715
  - 0.4188103675842285
  - 0.4277758300304413
  - 0.422086238861084
loss_records_fold2:
  train_losses:
  - 6.452820348739625
  - 6.872656393051148
  - 6.62224133014679
  - 6.5150710344314575
  - 6.797681963443757
  - 6.733860406279565
  - 6.562330347299576
  - 6.949958524107934
  - 6.553763109445573
  - 6.724898785352707
  - 6.8747191607952125
  - 7.005050611495972
  - 6.653516161441804
  - 6.852821210026741
  - 6.486678212881088
  - 6.3478430807590485
  - 7.21364366710186
  - 6.645572915673256
  - 7.178735625743866
  - 6.526101595163346
  - 6.644138815999032
  - 6.795172798633576
  - 6.342251837253571
  - 6.477878788113594
  - 6.813825803995133
  - 6.587637975811958
  - 6.682658895850182
  - 6.485076126456261
  - 6.767195299267769
  - 6.766096538305283
  - 6.534334555268288
  - 6.395740500092507
  - 6.606214237213135
  - 6.506293749809266
  - 6.482844439148903
  validation_losses:
  - 0.7644641399383545
  - 0.38967958092689514
  - 0.4810936152935028
  - 0.4051227867603302
  - 0.40201234817504883
  - 0.44873619079589844
  - 0.3968499004840851
  - 0.4395603835582733
  - 0.40081527829170227
  - 0.49446454644203186
  - 0.4322926700115204
  - 0.40949591994285583
  - 0.39822566509246826
  - 0.44021114706993103
  - 0.4268760681152344
  - 0.4064978063106537
  - 0.396903932094574
  - 0.4101237952709198
  - 0.41729915142059326
  - 0.39304661750793457
  - 0.4513995349407196
  - 113.02928161621094
  - 11383.0908203125
  - 0.40281885862350464
  - 0.4541393220424652
  - 665771.875
  - 0.4680366516113281
  - 0.4708513915538788
  - 3.263023853302002
  - 0.4158877730369568
  - 0.41936102509498596
  - 0.41121917963027954
  - 0.4025019109249115
  - 0.3967527747154236
  - 0.40654730796813965
loss_records_fold3:
  train_losses:
  - 6.293060392141342
  - 6.268957877159119
  - 6.3951045185327535
  - 6.572959297895432
  - 6.862351283431053
  - 6.61313263475895
  - 6.343069866299629
  - 6.661666503548623
  - 6.629617449641228
  - 6.433684152364731
  - 8.546160726249218
  - 10.539099690318109
  - 7.640713694691659
  - 8.024510300159454
  - 7.931205722689629
  - 7.027622544765473
  - 6.48656599521637
  - 7.237775766849518
  - 7.6273448169231415
  - 7.7858433634042745
  - 6.97839698791504
  - 7.082937279343605
  - 7.05651545226574
  - 6.711901068687439
  - 6.744870522618294
  - 6.764172345399857
  - 6.830267438292504
  - 6.959764939546585
  - 7.1684864372015005
  - 6.681181201338768
  - 6.971609222888947
  - 9.45157515704632
  - 7.070215597748756
  - 6.79852703511715
  - 6.903505846858025
  - 7.615063202381134
  - 6.600390461087227
  - 6.564477646350861
  - 6.661800765991211
  - 6.545395562052727
  - 6.511241611838341
  - 7.005517187714577
  - 6.316713550686837
  - 6.412898156046868
  - 7.403416562080384
  - 6.717571368813515
  - 6.635961323976517
  - 6.799215239286423
  - 6.85739986896515
  - 6.67334578037262
  - 6.286535179615021
  - 7.206058442592621
  - 7.361348122358322
  - 6.7326944231987005
  - 6.926683434844017
  - 6.87166541069746
  - 6.474680697917939
  - 6.512018871307373
  - 6.996162462234498
  - 7.170049011707306
  - 6.713222083449364
  - 7.2853061169385915
  - 6.821896970272064
  - 7.139778232574463
  - 6.642007377743721
  - 6.709217393398285
  - 6.92568733394146
  - 6.611191168427467
  - 6.405786317586899
  - 6.489693236351013
  - 6.414516347646714
  - 6.550027930736542
  - 6.523945391178131
  - 6.443199634552002
  - 6.547062480449677
  - 6.547103294730187
  - 6.583982810378075
  - 6.7569916367530825
  - 8.721965655684471
  - 7.724907764792443
  - 7.50348057448864
  - 6.794564992189407
  - 6.754290521144867
  - 6.497978141903878
  - 6.346196395158768
  - 6.468394711613655
  - 6.3878755450248725
  - 6.567392903566361
  - 6.419053322076798
  - 6.489002612233162
  - 6.515394285321236
  - 6.5041848689317705
  - 6.784461218118668
  - 6.356237760186196
  - 6.802125149965287
  - 6.730401268601418
  - 6.5116495013237
  - 6.579106670618057
  - 6.843124128878117
  - 6.492937135696412
  validation_losses:
  - 0.41245758533477783
  - 0.44588005542755127
  - 0.43406057357788086
  - 2377937152.0
  - 0.4234568476676941
  - 319.923095703125
  - 1.0515371561050415
  - 1245716.5
  - 62415.64453125
  - 8954792.0
  - 0.8631308078765869
  - 0.4665882885456085
  - 0.5630117654800415
  - 35.83179473876953
  - 30797.564453125
  - 71015.75
  - 108045.1640625
  - 20929.58203125
  - 9101.93359375
  - 54735.10546875
  - 20142.416015625
  - 41371.6171875
  - 32774.57421875
  - 28957.73046875
  - 15600.4365234375
  - 56060.578125
  - 2745.443115234375
  - 80594.9921875
  - 13183.9833984375
  - 3312.09033203125
  - 38796.65234375
  - 0.47156402468681335
  - 0.4108722507953644
  - 0.41401025652885437
  - 2.087120771408081
  - 103.82821655273438
  - 7596.69140625
  - 28258.099609375
  - 38.45183181762695
  - 1238.1778564453125
  - 19556.48828125
  - 25589.318359375
  - 15188.044921875
  - 30883.564453125
  - 3912.515625
  - 16876.923828125
  - 37072.24609375
  - 18472.67578125
  - 15226.666015625
  - 10532.5419921875
  - 25632.6171875
  - 22962.033203125
  - 8400.185546875
  - 12528.95703125
  - 26464.96875
  - 6774.92529296875
  - 9549.3076171875
  - 20070.341796875
  - 24447.16796875
  - 16634.615234375
  - 33830.7265625
  - 17177.859375
  - 491.3528747558594
  - 12367.212890625
  - 9674.97265625
  - 57718.19140625
  - 3781.0458984375
  - 3468.72412109375
  - 5432.0322265625
  - 2785.0244140625
  - 395.036865234375
  - 50436.69140625
  - 4636.27197265625
  - 3640.22705078125
  - 32879.9296875
  - 11435.5712890625
  - 24822.8515625
  - 19772.12109375
  - 0.41484448313713074
  - 0.46173784136772156
  - 0.4039265811443329
  - 0.5256462693214417
  - 0.4033331573009491
  - 0.433668315410614
  - 0.41627636551856995
  - 0.40711021423339844
  - 0.4080515503883362
  - 0.4288766384124756
  - 0.42001304030418396
  - 0.39706772565841675
  - 0.40122276544570923
  - 0.45450150966644287
  - 1.0608081817626953
  - 2.899202585220337
  - 120043.65625
  - 0.4335279166698456
  - 126121.703125
  - 0.4218846559524536
  - 15676.203125
  - 4526377.0
loss_records_fold4:
  train_losses:
  - 6.319599282741547
  - 6.296304956078529
  - 6.362323224544525
  - 6.696984893083573
  - 6.3251964330673225
  - 6.5268392547965055
  - 6.568287339806557
  - 6.876558569073677
  - 6.390485221147538
  - 6.4737041890621185
  - 9.041424176096916
  - 8.117833453416825
  - 6.719156095385552
  - 6.418784195184708
  - 7.708429658412934
  - 6.498391476273537
  - 6.522323885560036
  - 6.627260547876358
  - 9.205906203389167
  - 7.259280705451966
  - 7.4448785275220875
  - 7.498425173759461
  - 6.882668851315976
  - 6.601447346806527
  - 6.549196046590805
  - 6.290450683236123
  - 6.637379157543183
  - 6.61130690574646
  - 6.6181291684508325
  - 6.497319087386131
  - 6.434890994429589
  - 6.773195415735245
  - 6.386032515764237
  - 6.429490461945534
  - 6.371425473690033
  - 6.533165761828423
  - 6.390581348538399
  - 6.392282095551491
  - 6.635633209347725
  - 6.531730285286904
  - 6.379726997017861
  - 6.798614859580994
  - 6.673446536064148
  - 7.802105882763863
  - 6.534598055481911
  - 6.555058708786965
  - 6.538456732034684
  - 6.359731155633927
  - 6.3538717359304435
  - 6.504537454247475
  - 6.337060976028443
  - 6.302008259296418
  validation_losses:
  - 12991.5087890625
  - 2931492.0
  - 3993013.75
  - 3425727.5
  - 903265.9375
  - 8132868.5
  - 2458899.75
  - 2988736.5
  - 120.74699401855469
  - 3535499.5
  - 0.40715324878692627
  - 0.4126063287258148
  - 0.4267042875289917
  - 1.271120548248291
  - 0.4008338451385498
  - 0.3987491726875305
  - 0.41185134649276733
  - 0.4385189116001129
  - 0.4445665180683136
  - 0.44342440366744995
  - 0.41532421112060547
  - 0.40902993083000183
  - 0.47268572449684143
  - 0.39382100105285645
  - 0.45251497626304626
  - 0.4825107157230377
  - 0.42807140946388245
  - 0.40905871987342834
  - 0.4203367829322815
  - 0.4295545816421509
  - 0.42265796661376953
  - 204.96592712402344
  - 0.44069820642471313
  - 0.4263613522052765
  - 0.39716070890426636
  - 0.4225819408893585
  - 0.4050745964050293
  - 0.43076273798942566
  - 0.4268028736114502
  - 0.4289399981498718
  - 0.4377000629901886
  - 0.43490228056907654
  - 0.44624781608581543
  - 0.4059877395629883
  - 0.42362189292907715
  - 0.4561958909034729
  - 0.41399070620536804
  - 0.4078432023525238
  - 0.40947800874710083
  - 0.40646618604660034
  - 0.40372133255004883
  - 0.403768926858902
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 47 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 35 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 52 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8524871355060034,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8568984928059038
  mean_f1_accuracy: 0.0
  total_train_time: '0:25:56.266598'
