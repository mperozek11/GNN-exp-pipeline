config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.209526'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_13fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_13fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_13fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_13fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_13fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_13fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_13fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_13fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_13fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_13fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 49.92098331451416
  - 12.634829431772232
  - 13.031643015146257
  - 6.339683863520623
  - 7.787499511241913
  - 5.168384754657746
  - 4.837683230638504
  - 9.188781499862671
  - 7.858759489655495
  - 4.610740864276886
  - 6.310306966304779
  - 4.904519945383072
  - 5.959654128551484
  - 4.725404989719391
  - 6.786429852247238
  - 5.11421662569046
  - 7.427333635091782
  - 5.475291702151299
  - 6.016378480196
  - 8.58095150589943
  - 5.500836476683617
  - 4.412205743789673
  - 4.542495000362396
  - 5.5830825746059425
  - 7.887758052349091
  - 7.279356509447098
  - 7.776844972372055
  - 6.169119489192963
  - 3.6931531667709354
  - 3.4226840376853946
  - 3.5059447050094605
  - 5.783867353200913
  - 3.9460463643074037
  - 4.467739543318749
  - 3.677928858995438
  - 3.4101183474063874
  - 5.183368915319443
  - 5.11301326751709
  - 4.7113109707832335
  - 3.3790951609611515
  - 3.403016406297684
  - 3.3520466893911363
  - 3.951812297105789
  - 4.759788602590561
  - 3.7741749048233033
  - 3.6565979540348055
  validation_losses:
  - 4.077361583709717
  - 2.568638801574707
  - 0.6125615835189819
  - 0.42448437213897705
  - 0.5312927961349487
  - 0.4933014214038849
  - 0.41797080636024475
  - 0.4388943314552307
  - 0.4863186180591583
  - 0.4462394416332245
  - 0.7593260407447815
  - 0.4884776771068573
  - 0.41263243556022644
  - 0.46469396352767944
  - 0.5103887319564819
  - 0.41807976365089417
  - 0.5500249862670898
  - 0.48950111865997314
  - 0.4383825659751892
  - 0.6816676259040833
  - 0.41909080743789673
  - 0.42687198519706726
  - 0.44854992628097534
  - 0.4054035544395447
  - 0.42862457036972046
  - 0.4701942205429077
  - 0.48687097430229187
  - 0.4261171519756317
  - 0.5045758485794067
  - 0.42024436593055725
  - 0.40502530336380005
  - 0.41322168707847595
  - 0.45436498522758484
  - 0.4631040096282959
  - 0.41412851214408875
  - 0.5805326700210571
  - 0.4174114763736725
  - 0.4320102632045746
  - 0.41279369592666626
  - 0.4235678017139435
  - 0.4061683416366577
  - 0.39800477027893066
  - 0.39988911151885986
  - 0.4067109525203705
  - 0.40149375796318054
  - 0.3925621211528778
loss_records_fold1:
  train_losses:
  - 3.4215819716453555
  - 3.75342053771019
  - 3.468825078010559
  - 3.4305853962898256
  - 3.6407154977321627
  - 3.4873725116252903
  - 3.1901633083820347
  - 3.4850912332534794
  - 3.9235262215137485
  - 3.298433464765549
  - 4.5273599386215215
  - 4.638029712438583
  - 3.3535096883773807
  - 3.6260388970375064
  - 4.022838521003723
  - 3.5430817484855655
  - 3.1774346977472305
  - 3.4921669363975525
  - 3.7604328572750094
  - 3.135985237360001
  - 3.219703793525696
  - 3.2954167008399966
  - 3.3599401593208316
  - 3.1217288315296177
  - 3.1016152799129486
  - 3.1232199311256412
  - 3.0723658829927447
  - 3.3063280165195468
  - 3.1290250003337863
  - 3.296706342697144
  - 3.0980959564447406
  - 3.1346178472042086
  - 3.1676146864891055
  - 3.199828338623047
  - 3.2238916218280793
  - 3.3863868802785877
  - 3.3072671413421633
  - 3.0828234493732456
  - 3.099091872572899
  - 3.085429832339287
  - 3.1573013722896577
  - 3.2135193556547166
  - 3.248312467336655
  - 2.9743616938591004
  - 3.149168473482132
  - 3.182660529017449
  - 3.1086411952972415
  - 3.0844573557376864
  - 3.1823617696762088
  - 3.093416780233383
  - 3.174367368221283
  - 3.110311132669449
  - 3.107354259490967
  - 3.187879687547684
  - 3.301293444633484
  - 3.472676467895508
  - 3.4579847514629365
  - 3.1711284399032595
  - 3.5598359346389774
  - 3.338955330848694
  - 3.135596272349358
  - 3.088120448589325
  - 3.1361776113510134
  - 3.149332278966904
  - 3.1644950568675996
  - 3.3362943410873416
  - 3.6157088518142704
  - 3.096937856078148
  - 3.200211763381958
  - 3.097665542364121
  - 3.0606265664100647
  - 3.250001007318497
  - 3.376813638210297
  - 3.1990827023983
  - 2.983993723988533
  - 3.121013706922531
  - 3.4836089551448826
  - 3.3347127079963688
  - 3.1320300459861756
  - 3.260332852602005
  - 3.076400685310364
  - 3.3490272700786594
  - 3.255764436721802
  - 3.2661163687705996
  - 3.0635807991027835
  - 3.2092689990997316
  - 3.3113550394773483
  - 3.0350723385810854
  - 3.256696966290474
  - 3.124371248483658
  - 3.2164288967847825
  - 3.2497840851545337
  - 3.1397426187992097
  - 3.2574076414108277
  - 3.112992870807648
  - 3.052550804615021
  - 3.064371734857559
  - 3.092644914984703
  - 3.2094217360019686
  - 3.2408123075962068
  validation_losses:
  - 0.41528019309043884
  - 0.42347991466522217
  - 0.4293084740638733
  - 0.6032976508140564
  - 0.4627305567264557
  - 0.4068111479282379
  - 0.410727858543396
  - 0.5948060154914856
  - 0.49102962017059326
  - 0.411966472864151
  - 0.41136232018470764
  - 0.4092738628387451
  - 0.588645875453949
  - 0.4170266389846802
  - 0.4571034610271454
  - 0.40965840220451355
  - 0.3958125710487366
  - 0.40359723567962646
  - 0.41063249111175537
  - 0.42923831939697266
  - 0.4757867455482483
  - 0.4039735794067383
  - 0.40158557891845703
  - 0.4079708158969879
  - 0.42911186814308167
  - 0.45061954855918884
  - 0.4087004065513611
  - 0.3997972309589386
  - 0.4020429253578186
  - 0.4116855263710022
  - 0.4512377381324768
  - 0.43268901109695435
  - 0.40452098846435547
  - 0.4656983017921448
  - 0.41287511587142944
  - 0.4425869584083557
  - 0.43446779251098633
  - 0.4096076190471649
  - 0.49675217270851135
  - 0.44415974617004395
  - 0.4199143350124359
  - 0.46258264780044556
  - 0.3995923399925232
  - 0.4327883720397949
  - 0.39611905813217163
  - 0.45945408940315247
  - 0.42305341362953186
  - 0.40957558155059814
  - 0.4403507113456726
  - 0.4097473919391632
  - 0.4016796350479126
  - 0.42267778515815735
  - 3.2308173179626465
  - 0.43783408403396606
  - 0.42416083812713623
  - 0.607683002948761
  - 0.3983193039894104
  - 0.5637388229370117
  - 0.40959632396698
  - 0.47761985659599304
  - 0.41080886125564575
  - 0.4616457521915436
  - 0.40736114978790283
  - 0.41675931215286255
  - 0.49515751004219055
  - 0.4019932448863983
  - 0.5131648778915405
  - 0.39033961296081543
  - 0.44785431027412415
  - 0.4079342186450958
  - 0.40424326062202454
  - 0.45389124751091003
  - 0.4364190697669983
  - 0.4397416114807129
  - 0.4965672791004181
  - 48.050636291503906
  - 44.31576919555664
  - 0.4243839979171753
  - 2807.560546875
  - 3188.565185546875
  - 0.4908328950405121
  - 0.5162016153335571
  - 0.587794840335846
  - 0.4060985743999481
  - 0.42373934388160706
  - 0.4207186996936798
  - 0.39814627170562744
  - 0.40356168150901794
  - 0.42219966650009155
  - 0.4175533950328827
  - 2.6370739936828613
  - 0.41958874464035034
  - 0.4290759563446045
  - 57.8924674987793
  - 0.4145829379558563
  - 0.4119662344455719
  - 0.410177618265152
  - 0.577890932559967
  - 0.43346577882766724
  - 456905.5625
loss_records_fold2:
  train_losses:
  - 3.2163131833076477
  - 3.9878259897232056
  - 4.443419724702835
  - 3.813035723567009
  - 3.433138793706894
  - 3.3021332383155824
  - 3.29738849401474
  - 3.362216478586197
  - 3.477247565984726
  - 3.230993002653122
  - 3.2908384025096895
  - 3.346748009324074
  - 3.125861191749573
  - 3.31333686709404
  - 3.2818809747695923
  - 3.56285494863987
  - 3.3060950756073
  - 3.6943348944187164
  - 3.319540560245514
  - 3.5039794564247133
  - 3.340445646643639
  - 3.253351938724518
  - 3.3501332938671116
  - 3.3078811407089237
  - 3.322805768251419
  - 3.3409398168325426
  - 3.197818696498871
  - 3.239777368307114
  - 3.426298412680626
  - 3.439361107349396
  - 3.2043568551540376
  - 3.371376609802246
  - 3.481284236907959
  - 3.608270823955536
  - 3.375886905193329
  - 3.5595336198806766
  - 3.6382949113845826
  - 3.252779459953308
  - 3.412789160013199
  - 3.5396405488252642
  - 3.298175829648972
  - 3.2431128114461902
  - 3.366009491682053
  - 3.3004461169242862
  - 3.389586859941483
  - 3.168247932195664
  - 3.243019586801529
  - 3.2713885784149173
  - 3.1037482380867005
  - 3.3404749572277073
  - 3.204488927125931
  - 3.3117946684360504
  - 3.3037022411823274
  - 3.183815121650696
  - 3.3184908956289294
  - 3.180508124828339
  - 3.352382266521454
  - 3.2964216828346253
  - 3.32947695851326
  - 3.2479932069778443
  - 3.297845417261124
  - 3.290080088376999
  - 3.1587926089763645
  - 3.3250565230846405
  - 3.2353848814964294
  - 3.1729348808526994
  - 3.270517745614052
  - 3.2479055315256122
  - 3.238049453496933
  - 3.2288475871086124
  - 3.2183160543441773
  - 3.318431466817856
  - 3.2567726969718933
  - 3.3327929198741915
  - 3.240806841850281
  - 3.418665009737015
  - 3.2920360743999484
  - 3.1653225004673007
  - 3.2202323138713838
  - 3.231962335109711
  - 3.2934005916118623
  - 3.2130900382995606
  - 3.3459196329116825
  - 3.2684696912765503
  - 3.3444156765937807
  - 3.2771075606346134
  - 3.3279050409793856
  - 3.4266376942396164
  - 3.2449989795684817
  validation_losses:
  - 673979456.0
  - 0.4796295166015625
  - 0.41783636808395386
  - 0.39885443449020386
  - 0.39552217721939087
  - 0.39791667461395264
  - 0.40215003490448
  - 0.38716888427734375
  - 0.405852347612381
  - 0.39920878410339355
  - 0.4017237722873688
  - 0.39633816480636597
  - 0.4186992347240448
  - 0.4153616428375244
  - 0.3913370668888092
  - 0.392721027135849
  - 0.39640870690345764
  - 0.3935382664203644
  - 0.5388543009757996
  - 0.43816471099853516
  - 0.40171411633491516
  - 0.405121773481369
  - 0.3807785212993622
  - 0.40557172894477844
  - 0.4011137783527374
  - 0.40409064292907715
  - 0.39454928040504456
  - 0.43872663378715515
  - 0.40825915336608887
  - 0.4149875342845917
  - 13.91836166381836
  - 0.9556902050971985
  - 34.26072311401367
  - 0.40763747692108154
  - 0.4131777882575989
  - 0.4451121389865875
  - 0.39374786615371704
  - 0.5029758214950562
  - 0.4403645694255829
  - 0.4441045820713043
  - 0.4750765264034271
  - 0.3956960141658783
  - 0.48325109481811523
  - 0.4498176574707031
  - 0.40582114458084106
  - 0.3966587483882904
  - 0.3992278277873993
  - 0.39438682794570923
  - 0.4093363881111145
  - 0.4056256413459778
  - 0.4015631675720215
  - 0.40973782539367676
  - 0.4161757230758667
  - 0.5040872097015381
  - 0.41481441259384155
  - 0.3952162563800812
  - 0.4049453139305115
  - 0.49163109064102173
  - 0.4512733221054077
  - 0.4078388214111328
  - 0.40953049063682556
  - 0.39518165588378906
  - 0.39184659719467163
  - 0.4152580499649048
  - 0.40764811635017395
  - 0.4766387343406677
  - 0.4028184115886688
  - 0.41021913290023804
  - 0.40218880772590637
  - 0.4340801537036896
  - 0.39826953411102295
  - 0.4115298390388489
  - 0.4400917589664459
  - 0.4064411222934723
  - 0.44802233576774597
  - 0.39088329672813416
  - 0.4031546115875244
  - 0.4123149514198303
  - 0.4120140075683594
  - 0.45753371715545654
  - 0.42566239833831787
  - 0.40717312693595886
  - 0.4313047230243683
  - 0.4068406820297241
  - 0.3956296741962433
  - 0.40541544556617737
  - 0.4099898040294647
  - 0.39545199275016785
  - 0.40418997406959534
loss_records_fold3:
  train_losses:
  - 3.4263932764530183
  - 4.782529366016388
  - 3.7824602961540226
  - 3.252180141210556
  - 3.534549042582512
  - 4.202152487635613
  - 3.860318768024445
  - 3.719610196352005
  - 3.480758249759674
  - 3.115881392359734
  - 3.1627076208591465
  - 3.2172374308109286
  - 3.233925172686577
  - 3.2552415490150453
  - 3.441005823016167
  - 3.326403957605362
  - 3.426948291063309
  - 3.189448803663254
  - 3.58441116809845
  - 3.1038792669773105
  - 3.256204950809479
  - 3.249377101659775
  - 3.2782511800527576
  - 3.447605419158936
  - 3.1690715849399567
  - 3.6251516819000247
  - 3.200890946388245
  - 3.256107296049595
  - 3.2899922996759416
  - 3.4139344394207
  - 3.1180343210697177
  - 3.245293617248535
  - 3.177887070178986
  - 3.2490752696990968
  - 3.23863240480423
  - 3.3344023883342744
  - 3.1467700719833376
  - 3.1799206137657166
  - 3.3690504610538485
  - 3.238122946023941
  - 3.454303589463234
  - 3.4038256049156193
  - 3.3810916543006897
  - 3.3630187988281253
  - 3.4044548094272615
  - 3.4524800121784214
  - 3.277231252193451
  - 3.375894689559937
  - 3.2509859025478365
  - 3.449823147058487
  - 3.1930949985980988
  - 3.1549827575683596
  - 3.1864801704883576
  - 3.2315662741661075
  - 3.1875603020191194
  - 3.2982760250568393
  - 3.5150017619133
  - 3.470774322748184
  - 3.209379494190216
  - 3.2994545221328737
  - 3.3038776457309726
  - 3.2972408294677735
  - 3.231949955224991
  - 3.443950366973877
  validation_losses:
  - 93.88798522949219
  - 0.40971437096595764
  - 0.42336568236351013
  - 0.42878392338752747
  - 0.4407871663570404
  - 0.4776659607887268
  - 0.39735642075538635
  - 0.41897842288017273
  - 0.42683160305023193
  - 0.42172661423683167
  - 0.4152921438217163
  - 0.44321584701538086
  - 0.4142020344734192
  - 0.42708662152290344
  - 4.247060298919678
  - 44.742916107177734
  - 13.323299407958984
  - 0.40607720613479614
  - 0.4181509017944336
  - 0.4119825065135956
  - 0.4298606514930725
  - 1.0723432302474976
  - 0.7934244871139526
  - 0.41566675901412964
  - 0.41780635714530945
  - 0.830791711807251
  - 0.4384039044380188
  - 2.410994529724121
  - 0.3925497531890869
  - 0.416229784488678
  - 0.8939040899276733
  - 10.210261344909668
  - 0.40562155842781067
  - 0.40705928206443787
  - 0.414073646068573
  - 0.4182315766811371
  - 12.991836547851562
  - 11.871567726135254
  - 7.750001907348633
  - 1.3861932754516602
  - 0.4086988568305969
  - 0.40659523010253906
  - 0.4546334743499756
  - 0.4932391941547394
  - 0.49162301421165466
  - 0.44463783502578735
  - 0.4138435423374176
  - 0.4398219585418701
  - 0.42050132155418396
  - 0.4346824884414673
  - 3.6595511436462402
  - 0.4022277891635895
  - 0.41553980112075806
  - 5.645288944244385
  - 0.4173491895198822
  - 0.4625815451145172
  - 0.4709557592868805
  - 1.5508390665054321
  - 0.410305380821228
  - 0.41887181997299194
  - 0.42549988627433777
  - 0.41952452063560486
  - 0.4072534441947937
  - 0.4166068434715271
loss_records_fold4:
  train_losses:
  - 3.30329784154892
  - 3.332237827777863
  - 3.1954589873552326
  - 3.2974500179290773
  - 3.247759133577347
  - 3.2882392704486847
  - 3.220198339223862
  - 3.258114528656006
  - 3.2974031418561935
  - 3.219031232595444
  - 3.335138177871704
  - 3.2900095880031586
  - 3.1924057602882385
  - 3.244032210111618
  - 3.3751962006092073
  - 3.3011037170886994
  - 3.2420919418334964
  - 3.178871622681618
  - 3.456138122081757
  - 3.289050090312958
  - 3.179082489013672
  - 3.314803057909012
  - 3.216063749790192
  - 3.1936093986034395
  - 3.330713909864426
  - 3.356912925839424
  - 3.639311426877976
  - 3.30975404381752
  - 3.279142898321152
  - 3.3474605560302737
  - 3.2218840003013614
  - 3.2511238873004915
  - 3.2429478943347934
  - 3.2533679008483887
  - 3.2683437108993534
  - 3.6603605031967166
  - 3.3962161004543305
  - 3.2214991569519045
  - 3.1634810745716098
  - 3.1868453323841095
  - 3.2238596916198734
  - 3.113510930538178
  - 3.2736359238624573
  - 3.1895127028226855
  - 3.3040971696376804
  - 3.090078389644623
  - 3.1300697416067127
  - 3.2529639542102817
  - 3.282420712709427
  - 3.244634521007538
  - 3.18707315325737
  - 3.2248823553323747
  - 3.192063367366791
  - 3.217504668235779
  - 3.2285459578037266
  - 3.210412383079529
  - 3.2585937291383744
  - 3.285725194215775
  - 3.184836405515671
  - 3.242841762304306
  - 3.1910337567329408
  - 3.2651302218437195
  - 3.205707007646561
  - 3.1560490876436234
  - 3.1891912102699282
  - 3.3187109470367435
  - 3.229048562049866
  - 3.2715225219726562
  - 3.3020316809415817
  - 3.221792268753052
  - 3.181699639558792
  - 3.2260498464107514
  - 3.1750223577022556
  - 3.1679521381855014
  - 3.250855910778046
  - 3.1838575303554535
  - 3.1206411480903626
  - 3.155031788349152
  - 3.224381047487259
  - 3.179110139608383
  - 3.215832859277725
  - 3.1960762381553653
  - 3.178902912139893
  - 3.2173619449138644
  - 3.1620598375797275
  - 3.0872664988040928
  - 3.1638972580432894
  - 3.1401343643665314
  - 3.4743159592151645
  - 3.3069881021976473
  - 3.1355052530765537
  - 3.18850377202034
  - 3.2630622267723086
  - 3.180409660935402
  - 3.213445955514908
  - 3.1702336311340336
  - 3.1533360660076144
  - 3.26440492272377
  - 3.1951345741748813
  - 3.2151094615459446
  validation_losses:
  - 35.0670166015625
  - 12.657753944396973
  - 13.38048267364502
  - 13.825018882751465
  - 5.979339122772217
  - 40.91796875
  - 11.838217735290527
  - 25.38772201538086
  - 46.59990310668945
  - 5.370402812957764
  - 6.750135898590088
  - 20.953643798828125
  - 23.898433685302734
  - 18.114343643188477
  - 6.177717208862305
  - 10.515894889831543
  - 29.819286346435547
  - 8.836885452270508
  - 20.468870162963867
  - 5.219749927520752
  - 2.5816564559936523
  - 23.338071823120117
  - 3.2130444049835205
  - 28.206302642822266
  - 14.469856262207031
  - 6.473708629608154
  - 5.356841087341309
  - 5.811002731323242
  - 10.872613906860352
  - 49.8841438293457
  - 12.00523853302002
  - 13.978415489196777
  - 20.181447982788086
  - 183.69265747070312
  - 14.18718433380127
  - 14.973550796508789
  - 51.73288345336914
  - 22.70282554626465
  - 22.42156982421875
  - 181.1922607421875
  - 68.63565826416016
  - 26.829837799072266
  - 31.012651443481445
  - 36.638065338134766
  - 62.54094314575195
  - 34.39348220825195
  - 25.37066078186035
  - 22.336746215820312
  - 41.22074890136719
  - 65.80976104736328
  - 23.87801742553711
  - 50.058990478515625
  - 28.4135799407959
  - 44.396060943603516
  - 24.86907386779785
  - 15.81760311126709
  - 10.118921279907227
  - 37.401729583740234
  - 33.81142807006836
  - 61.2335090637207
  - 37.07526779174805
  - 67.45377349853516
  - 57.448421478271484
  - 64.41065216064453
  - 17.794078826904297
  - 68.00193786621094
  - 147.9036407470703
  - 56.32430648803711
  - 245.54945373535156
  - 14.269006729125977
  - 52.174076080322266
  - 29.285324096679688
  - 44.93218231201172
  - 24.508548736572266
  - 83.8936538696289
  - 41.884578704833984
  - 45.60724639892578
  - 18.669092178344727
  - 88.21856689453125
  - 122.55818176269531
  - 59.459869384765625
  - 67.43830108642578
  - 43.17242431640625
  - 32.104644775390625
  - 81.70426940917969
  - 12.381635665893555
  - 51.69770050048828
  - 61.83049011230469
  - 288.7560729980469
  - 54.795166015625
  - 24.038253784179688
  - 101.4622573852539
  - 91.38574981689453
  - 96.20706176757812
  - 47.786991119384766
  - 59.89857864379883
  - 25.869600296020508
  - 7.503997802734375
  - 36.19705581665039
  - 46.23226547241211
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 46 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 89 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 64 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.1423670668953688, 0.8593481989708405, 0.8593481989708405,
    0.8556701030927835]'
  fold_eval_f1: '[0.0, 0.2492492492492493, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.7148733002068929
  mean_f1_accuracy: 0.04984984984984986
  total_train_time: '0:35:21.950527'
