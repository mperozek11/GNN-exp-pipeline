config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 05:10:42.792201'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/129/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/129/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/129/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/129/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/129/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/129/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/129/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/129/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/129/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/129/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 29.990670576691628
  - 31.128727436065674
  - 30.161041289567947
  - 30.02263978123665
  - 30.200535133481026
  - 30.041526317596436
  - 30.079689413309097
  - 30.209875643253326
  - 30.25429154932499
  - 30.30890467762947
  - 29.87732309103012
  - 30.52556736767292
  - 31.24051959812641
  - 30.09046234190464
  - 30.307800859212875
  - 30.1286251693964
  - 30.285518318414688
  - 30.506212636828423
  - 30.475990906357765
  - 30.904013864696026
  - 30.019432827830315
  - 30.09804902970791
  - 30.051185742020607
  - 30.379782527685165
  - 30.567353025078773
  - 31.599095791578293
  - 31.399790436029434
  - 31.583098113536835
  - 31.634947642683983
  - 31.56679207086563
  - 30.16192227602005
  - 30.01439420878887
  - 30.22962425649166
  - 30.053847432136536
  - 30.414790391921997
  - 30.891819536685944
  - 30.453857377171516
  - 30.90783090889454
  - 30.03675578534603
  - 30.253618150949478
  - 30.39670406281948
  - 30.300486087799072
  - 30.45386852324009
  - 30.122364535927773
  - 30.31787896156311
  - 30.533723533153534
  - 30.331387400627136
  - 30.311874955892563
  - 30.415254525840282
  - 32.15949469804764
  - 30.533916488289833
  - 30.04167665541172
  - 30.34578527510166
  - 30.468134254217148
  - 34.67900034785271
  - 57.241181060671806
  - 29.569413110613823
  - 28.99413277953863
  - 29.889311514794827
  - 29.330513894557953
  - 28.857324689626694
  - 28.94955511391163
  - 29.516569331288338
  - 28.56083309650421
  - 30.0577684789896
  - 28.608210384845734
  - 30.710977345705032
  - 30.135097980499268
  - 29.92059974372387
  - 30.685677975416183
  - 30.21702168881893
  - 30.663484819233418
  - 30.161454766988754
  - 30.047376736998558
  - 30.291182413697243
  - 30.15667626261711
  - 30.3643187135458
  - 30.15618757903576
  - 30.452385410666466
  - 30.25506153702736
  - 30.025424361228943
  - 30.04913952946663
  - 30.792777597904205
  - 30.716334015130997
  - 31.50833284854889
  - 31.28073960542679
  - 30.230002969503403
  - 31.014592170715332
  - 30.254163041710854
  - 30.15306808054447
  - 30.379796519875526
  - 30.523903489112854
  - 30.21777532994747
  - 30.200879335403442
  - 30.26289100944996
  - 30.391787439584732
  - 30.179059371352196
  - 30.19771781563759
  - 31.457864478230476
  - 30.142247825860977
  validation_losses:
  - 0.4102547764778137
  - 0.40662166476249695
  - 0.4071165919303894
  - 0.4087132513523102
  - 0.4086211621761322
  - 0.4068865478038788
  - 0.40642082691192627
  - 0.41930505633354187
  - 0.4080415368080139
  - 0.41121408343315125
  - 0.4054790437221527
  - 0.427785187959671
  - 0.4166634678840637
  - 0.41076600551605225
  - 0.40848106145858765
  - 0.4073263108730316
  - 0.42414236068725586
  - 0.41593989729881287
  - 0.42021864652633667
  - 0.432726114988327
  - 0.4299938380718231
  - 0.42252856492996216
  - 0.41213685274124146
  - 0.41629770398139954
  - 0.4871971309185028
  - 1.6419925689697266
  - 0.4052513539791107
  - 0.412112832069397
  - 0.4445061683654785
  - 0.414414644241333
  - 0.4129716157913208
  - 0.40937545895576477
  - 1.0889221711593472e+16
  - 0.40971070528030396
  - 0.41492071747779846
  - 5166.21533203125
  - 0.4106099009513855
  - 510740129644544.0
  - 8.365443523385754e+16
  - 5424999.5
  - 4849266.0
  - 9.717993149405594e+16
  - 1.1779401357826458e+17
  - 1.097384502272983e+17
  - 1.0282064089291162e+17
  - 1.1289811163807744e+17
  - 1.1842179176805171e+17
  - 1.0764468803023667e+17
  - 1.0876348406117171e+17
  - 1.072911864519721e+17
  - 1.0623039824933683e+17
  - 1.1674062989924762e+17
  - 1.0759015912544666e+17
  - 1.081471648441303e+17
  - 850.4515380859375
  - 183653.40625
  - 32956326.0
  - 192705936.0
  - 4970570752.0
  - 3630422528.0
  - 2871703552.0
  - 1923313664.0
  - 845917888.0
  - 5145072640.0
  - 9003825152.0
  - 252204810240.0
  - 9590755387506688.0
  - 9.39548835011625e+17
  - 2.5782630678649635e+19
  - 2.5041038673017045e+19
  - 2.960104764493896e+19
  - 3.205383818418166e+19
  - 2.1401270356008763e+19
  - 3.3592395800044896e+19
  - 2.1921075473156145e+19
  - 2.803157835917243e+19
  - 2.671285270012022e+19
  - 1.7390657657672565e+19
  - 3.1344859892452164e+19
  - 2.4702668367615492e+19
  - 1.478183102528566e+19
  - 3.3095588068104077e+19
  - 2.9447938451747897e+19
  - 2.7181332614483018e+19
  - 1.5190938411260183e+19
  - 3.215821262398318e+19
  - 1.87598827862769e+19
  - 3.422872056243421e+19
  - 2.771208886744305e+19
  - 1.6428162770903499e+19
  - 2.439156375155953e+19
  - 1.4332229725571777e+19
  - 2.897184991692089e+19
  - 2.8695353528860803e+19
  - 2.381676106279079e+19
  - 1.8069296025545212e+19
  - 2.3404338649258525e+19
  - 1.876539133953206e+19
  - 2.6649742931709133e+19
  - 3.505421630038239e+19
loss_records_fold2:
  train_losses:
  - 30.95416997373104
  - 43.11002527177334
  - 34.15851294994354
  - 32.12662486732006
  - 30.216286912560463
  - 31.260726794600487
  - 30.745214879512787
  - 30.934769421815872
  - 30.057856172323227
  - 30.860041171312332
  - 34.52912351489067
  - 31.45500583946705
  - 31.21870620548725
  - 30.717815577983856
  - 30.127049647271633
  - 31.229101434350014
  - 30.80881790816784
  - 30.398987635970116
  - 31.044179618358612
  - 30.412906304001808
  - 30.5156514570117
  - 30.59368123114109
  - 30.486470595002174
  - 30.26662653684616
  - 30.183440700173378
  - 30.52832494676113
  - 30.494568422436714
  - 30.493472829461098
  - 30.790588423609734
  - 30.15279123187065
  - 30.87567639350891
  - 30.251687720417976
  - 32.45860335230827
  - 30.843565717339516
  - 30.05981920659542
  - 30.77290117740631
  - 30.067227140069008
  - 30.658247232437134
  - 30.30263751745224
  - 30.141911447048187
  - 30.32991373538971
  - 29.77227419614792
  - 30.626289058476686
  - 31.10240836441517
  - 30.657548516988754
  - 30.399786815047264
  - 30.478742003440857
  - 29.98775552213192
  - 30.93416666984558
  - 30.57927994430065
  - 30.1123618632555
  - 30.90339456498623
  - 30.49990251660347
  - 30.384336605668068
  - 30.459343671798706
  - 30.513036459684372
  - 30.24797409772873
  - 31.28925731778145
  - 30.376971378922462
  - 30.36700139939785
  - 30.579506635665894
  - 30.544505044817924
  - 30.941696733236313
  - 30.452819757163525
  - 30.569310143589973
  - 30.380469352006912
  - 30.264155864715576
  - 30.331491634249687
  - 30.18399466574192
  - 31.61265030503273
  - 30.381237097084522
  - 30.156379744410515
  - 30.56211145222187
  - 30.527881681919098
  - 30.162209302186966
  - 30.61078356206417
  - 30.653810940682888
  - 30.10891802608967
  - 30.109820142388344
  - 30.811169892549515
  - 30.065615221858025
  - 30.36855873465538
  - 30.761421725153923
  - 30.324744194746017
  - 30.16449683904648
  - 30.24740359187126
  - 30.68860825896263
  - 31.04004466533661
  - 30.40036278963089
  - 30.21826323866844
  - 30.563907727599144
  - 30.37394042313099
  - 30.360526263713837
  - 30.73171439766884
  - 30.94637882709503
  - 31.13944299519062
  - 30.552788227796555
  - 30.6407513320446
  - 30.536637991666794
  - 30.48837085068226
  validation_losses:
  - 3.692310899067139e+19
  - 0.416185587644577
  - 0.4050096869468689
  - 0.471763551235199
  - 0.40163594484329224
  - 1250703488.0
  - 1476268261376.0
  - 1917831217152.0
  - 1973552283648.0
  - 1763699458048.0
  - 1573541773312.0
  - 1657693274112.0
  - 1521538957312.0
  - 1845090058240.0
  - 1706080731136.0
  - 1562406944768.0
  - 1719672111104.0
  - 1856942768128.0
  - 1506158182400.0
  - 1061737201664.0
  - 417025359872.0
  - 1026079457280.0
  - 1089525776384.0
  - 1621556854784.0
  - 1233367072768.0
  - 1320672296960.0
  - 1277450780672.0
  - 1471866470400.0
  - 1467057569792.0
  - 1637310136320.0
  - 1347843260416.0
  - 1421979418624.0
  - 1208185913344.0
  - 1621025882112.0
  - 1672005287936.0
  - 1337463930880.0
  - 934970589184.0
  - 1181361504256.0
  - 1351430701056.0
  - 1370405863424.0
  - 1460381679616.0
  - 1572276011008.0
  - 779173691392.0
  - 1034015801344.0
  - 1374249549824.0
  - 1418441523200.0
  - 1564696248320.0
  - 1303767547904.0
  - 1316148346880.0
  - 1338747256832.0
  - 1398368370688.0
  - 848663805952.0
  - 1454415151104.0
  - 1491994673152.0
  - 1138953420800.0
  - 791023648768.0
  - 967411499008.0
  - 1956112236544.0
  - 0.40841448307037354
  - 0.4096522629261017
  - 4.000538313095538e+19
  - 1386.861572265625
  - 0.40910646319389343
  - 0.4062192440032959
  - 9.791732467094048e+19
  - 3.304674978469966e+21
  - 5.89212187993365e+21
  - 7.142312129292445e+21
  - 9.69820681176677e+21
  - 1.0928369553582649e+22
  - 1.274853987538108e+22
  - 9.950493959292438e+21
  - 6.907114453502788e+21
  - 9.194872695462822e+21
  - 0.4124758839607239
  - 0.4061620235443115
  - 5.5507325827835494e+17
  - 97888583680.0
  - 88501747712.0
  - 91119116288.0
  - 79969550336.0
  - 2.215085449176133e+20
  - 4.369854821861591e+21
  - 4.166445241091775e+21
  - 5.026233356927587e+21
  - 1.8305879921614704e+21
  - 3.0371090877334673e+21
  - 2.8651627798603686e+21
  - 5.181647513518562e+21
  - 5.925900003038835e+21
  - 6.715509933256217e+21
  - 2.4783992733613246e+21
  - 6.867218190303819e+21
  - 2.1856576977578712e+21
  - 6.620520009915718e+21
  - 4.949981222786717e+21
  - 5165172736.0
  - 3.596415123280888e+18
  - 9.45727098710189e+21
  - 1.0901030452044696e+22
loss_records_fold3:
  train_losses:
  - 31.150239199399948
  - 32.32809764146805
  - 30.301543042063713
  - 30.438638493418694
  - 30.58438876271248
  - 30.300437226891518
  - 30.263989493250847
  - 30.46104498207569
  - 30.508151918649673
  - 30.468930512666702
  - 30.68587715923786
  - 30.797302171587944
  - 30.468002781271935
  - 30.888366296887398
  - 30.521438270807266
  - 30.913608387112617
  - 30.327477619051933
  - 30.4412073045969
  - 30.48103655874729
  - 30.44719360768795
  - 30.122403115034103
  - 30.066386342048645
  - 29.966956079006195
  - 30.610247880220413
  - 30.627022296190262
  - 30.77545303851366
  - 30.22348366677761
  - 31.84727256000042
  - 30.76579052209854
  - 30.350168228149414
  - 30.0665822327137
  - 30.820028975605965
  - 30.621548742055893
  - 31.0423451513052
  - 30.433709368109703
  - 30.571308799088
  - 30.319617599248886
  - 30.088300690054893
  - 31.145902916789055
  - 31.195249155163765
  - 30.47122898697853
  - 30.672654256224632
  - 30.23554889857769
  - 30.44262683391571
  - 38.94762271642685
  - 30.172366127371788
  - 30.754742443561554
  - 30.677764892578125
  - 31.03107586503029
  - 30.19379599392414
  - 31.242173492908478
  - 30.338828206062317
  - 30.779906764626503
  - 36.251332119107246
  - 31.161598339676857
  - 30.320838123559952
  - 45.07274058461189
  - 31.90801053494215
  - 30.276420012116432
  - 30.859339609742165
  - 30.949322193861008
  - 30.00783570110798
  - 30.94086767733097
  - 30.484931275248528
  - 30.317610189318657
  - 30.931552037596703
  - 30.62766742706299
  - 30.041305348277092
  - 30.448038011789322
  - 30.44758802652359
  - 30.467387169599533
  - 30.700426384806633
  - 30.378110021352768
  - 31.114739157259464
  - 30.085164591670036
  - 30.216143742203712
  - 31.432690039277077
  - 30.347142599523067
  - 30.117916896939278
  - 30.364373594522476
  - 30.15490610897541
  - 31.000231474637985
  - 30.305436179041862
  - 30.450207501649857
  - 30.153373643755913
  - 30.32041521370411
  - 30.915352776646614
  - 30.531814232468605
  - 30.644150659441948
  - 30.411929294466972
  - 30.59878233075142
  - 30.282448559999466
  - 30.380834341049194
  - 30.838155910372734
  - 30.596862375736237
  - 30.166207626461983
  - 31.086100861430168
  - 30.42469508945942
  - 30.512228846549988
  - 30.98457793146372
  validation_losses:
  - 1.004034077185848e+22
  - 1.0756592130695576e+22
  - 9.131824552479449e+21
  - 9.851612925873892e+21
  - 9.196004787819152e+21
  - 1.0735775367317963e+22
  - 9.276705352491957e+21
  - 9.842866935397538e+21
  - 1.1266006669446428e+22
  - 9.725141715238166e+21
  - 1.0632555115658445e+22
  - 1.0602952955307739e+22
  - 8.834185030806253e+21
  - 1.0138323337151366e+22
  - 5.446130408835149e+21
  - 6.030236584455988e+21
  - 1.0066024800533374e+22
  - 1.1130106047090896e+22
  - 9.040326607700023e+21
  - 0.4146232604980469
  - 0.4137958884239197
  - 4.072174904131191e+18
  - 4.5841619878297364e+21
  - 4.663776622042392e+21
  - 6.483250293273465e+21
  - 2.065680114834861e+21
  - 1.8557783137272138e+21
  - 2.7746463383247326e+21
  - 3.808219075106541e+21
  - 2.729295653027065e+21
  - 2.0821770817448958e+21
  - 5.774715289447915e+21
  - 2.0333981719683809e+21
  - 2.537776701173414e+21
  - 5.915771970426832e+21
  - 6.450930210547641e+21
  - 6.518929498371355e+21
  - 3.703736126701499e+21
  - 3.265075389896451e+21
  - 3.5612478639910305e+21
  - 3.6341130097371413e+21
  - 835479076864.0
  - 5375778636169216.0
  - 0.4081915318965912
  - 0.410722941160202
  - 0.4113506078720093
  - 7305845248.0
  - 1.1143686511029744e+21
  - 1.2593814980639326e+20
  - 4.8870814874848146e+20
  - 3.8866462367611984e+20
  - 31451710.0
  - 0.4175058901309967
  - 22.708478927612305
  - 7429468.0
  - 7.1576948165893555
  - 356293.03125
  - 3289556736.0
  - 10962.310546875
  - 121468112.0
  - 6.063603613357687e+20
  - 0.4126715660095215
  - 0.4087934195995331
  - 34406536.0
  - 2.2022855505320765e+21
  - 2.6301004935345094e+21
  - 519466024960.0
  - 0.40625202655792236
  - 0.4295009970664978
  - 0.41032731533050537
  - 0.41630107164382935
  - 333236000.0
  - 1.6718560900200069e+21
  - 1.23604993169127e+21
  - 1.2822185832712587e+21
  - 458748100608.0
  - 601185255424.0
  - 0.4514363408088684
  - 0.4063512086868286
  - 0.41020238399505615
  - 3.793898887782072e+17
  - 657362649088.0
  - 3.0286758159562394e+21
  - 536227577856.0
  - 1.085847020149231
  - 231.36085510253906
  - 35855092.0
  - 1.0267891898430815e+22
  - 1105656545280.0
  - 1092724785152.0
  - 7.647285978073221e+20
  - 6.638481491129579e+21
  - 6.47894654087956e+21
  - 3.530628453074492e+21
  - 3.084671603398127e+21
  - 2.9614978721145673e+21
  - 2.871396887644556e+21
  - 3.3495387120579243e+21
  - 3.2235350313585156e+21
  - 2.8979675624960887e+21
loss_records_fold4:
  train_losses:
  - 31.01018875837326
  - 30.499496176838875
  - 30.405507273972034
  - 30.491091519594193
  - 30.555863693356514
  - 30.052156686782837
  - 30.442902222275734
  - 30.192794874310493
  - 30.751058593392372
  - 30.29514379799366
  - 31.175198763608932
  - 30.195211067795753
  - 30.43164697289467
  - 30.353759840130806
  - 30.63281974941492
  - 30.64288492500782
  - 30.489255532622337
  - 30.464705169200897
  - 30.485223352909088
  - 30.47034280002117
  - 30.446135833859444
  - 29.98678185045719
  - 30.590321347117424
  - 30.671591252088547
  - 30.39962989091873
  - 30.050840139389038
  - 30.131580039858818
  - 30.305653259158134
  - 30.933363273739815
  - 30.65414036810398
  - 30.69337072968483
  - 30.474976927042007
  - 30.34370620548725
  - 30.58808995783329
  - 30.688857331871986
  - 30.781060457229614
  - 30.55532570183277
  - 30.587839238345623
  - 31.070370972156525
  - 30.344480708241463
  - 30.57406160235405
  - 30.074729323387146
  - 31.083324372768402
  - 30.621891796588898
  - 30.20228449255228
  - 30.31223800778389
  - 30.810890451073647
  - 30.587716922163963
  - 30.8755574375391
  - 30.291014090180397
  - 30.536308348178864
  - 30.004071056842804
  - 30.321259200572968
  - 30.21664570271969
  - 30.46535015106201
  - 30.274477805942297
  - 30.082780078053474
  - 31.950797647237778
  - 30.748310640454292
  - 30.803544744849205
  - 31.738033577799797
  - 30.062842339277267
  - 30.464470714330673
  - 30.454313933849335
  - 30.618521869182587
  - 30.36876630783081
  - 30.730315402150154
  - 30.318532168865204
  - 30.14355978369713
  - 29.98710086941719
  - 30.55119101703167
  - 30.876765325665474
  - 30.294706717133522
  - 30.98824243247509
  - 30.57325153052807
  - 30.43590460717678
  - 30.04178260266781
  - 30.27909265458584
  - 30.71462047100067
  - 30.403427705168724
  - 30.436404451727867
  - 30.836439087986946
  - 30.00818534195423
  - 30.14365990459919
  - 30.441516280174255
  - 30.288964793086052
  - 30.405052587389946
  - 31.715114548802376
  - 30.675112009048462
  - 30.35265627503395
  - 30.24666692316532
  - 30.098519027233124
  - 30.804634004831314
  - 30.899514093995094
  - 30.449864104390144
  - 30.68318723142147
  - 30.07830722630024
  - 31.572067067027092
  - 30.588392302393913
  - 30.489748254418373
  validation_losses:
  - 1008820682752.0
  - 1020816719872.0
  - 1106242306048.0
  - 8.505771849929282e+21
  - 0.42236584424972534
  - 0.40664142370224
  - 0.4065828025341034
  - 1.0293720042293785e+21
  - 1.5270747069692722e+22
  - 1.3546274862276192e+22
  - 0.4082965850830078
  - 0.4068285822868347
  - 0.4590245485305786
  - 0.44476884603500366
  - 0.417954683303833
  - 0.40906500816345215
  - 1.4412379065483159e+20
  - 5.773446963202856e+21
  - 6.439194392868667e+21
  - 4.92269560149434e+21
  - 6.401711496120016e+21
  - 7.600220687954921e+21
  - 4.371168184102923e+21
  - 4.263923121751419e+21
  - 3.8980394290247715e+21
  - 5.672140178435017e+21
  - 5.849311787775773e+21
  - 6.869219477388232e+21
  - 7.859392713410931e+21
  - 0.4081008732318878
  - 0.4066724181175232
  - 17668268032.0
  - 2.8816791685437965e+21
  - 1.7001889389632364e+21
  - 6.961131190383423e+21
  - 8.489760427354073e+21
  - 0.4112788438796997
  - 0.40661364793777466
  - 4.880272255954463e+20
  - 6.118364710814234e+21
  - 5.349391399989371e+21
  - 7.241875458054538e+21
  - 5.116596394600916e+21
  - 7.23747600416855e+21
  - 7.146754930324846e+21
  - 6.219431678901915e+21
  - 0.4126255512237549
  - 0.4075959324836731
  - 7.918473396228823e+19
  - 4.724982511303194e+21
  - 0.4093491733074188
  - 0.4416879415512085
  - 3945794816.0
  - 1.9991100162053956e+21
  - 0.4583726227283478
  - 0.4075699746608734
  - 1.7830146397102072e+21
  - 4.244416624390394e+21
  - 6.361668302983204e+21
  - 7.766024085936286e+21
  - 8.608300236096045e+21
  - 5.974087393151793e+21
  - 2.2982951169757094e+21
  - 6.215468511229829e+21
  - 4.0646371493904143e+21
  - 8.663406844086504e+21
  - 7.22943482703388e+21
  - 4.786141957192839e+21
  - 5.477854327560301e+21
  - 8.063723280304544e+21
  - 5.466768717077528e+21
  - 4.724605334834402e+21
  - 6.445666628483152e+21
  - 1.0497738734913202e+22
  - 4.9128411625597e+21
  - 6.350389600666408e+21
  - 5.343432574732406e+21
  - 6.708659395273033e+21
  - 6.786290193849832e+21
  - 7.233040521485544e+21
  - 8.244742652576934e+21
  - 6.744661170694232e+21
  - 0.4070679247379303
  - 0.408819317817688
  - 246016688.0
  - 1.2596411939143203e+21
  - 5.977478040721249e+21
  - 8.547302356843032e+21
  - 6.096851015394285e+21
  - 0.41018807888031006
  - 0.42426618933677673
  - 0.4112869203090668
  - 29984486.0
  - 2.45575995950951e+21
  - 5.904650894096994e+21
  - 8.495959632241148e+21
  - 1.0369660865110413e+22
  - 0.41087597608566284
  - 2.9056534844331527e+23
  - 8.534295353133236e+23
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.140893470790378]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.24698795180722893]'
  mean_eval_accuracy: 0.7149711469882643
  mean_f1_accuracy: 0.04939759036144579
  total_train_time: '0:19:15.764515'
