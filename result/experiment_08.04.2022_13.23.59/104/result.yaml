config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 01:12:45.039641'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/104/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/104/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/104/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/104/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/104/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/104/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/104/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/104/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/104/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/104/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 29.67665058374405
  - 29.90544407069683
  - 30.11385305225849
  - 29.92612174153328
  - 30.21599842607975
  - 30.36562344431877
  - 30.419794365763664
  - 29.94054999947548
  - 29.243375211954117
  - 29.83309033513069
  - 30.061474561691284
  - 30.30591495335102
  - 29.487831234931946
  - 29.931425243616104
  - 29.7040858566761
  - 29.595135495066643
  - 30.277411982417107
  - 29.203196808695793
  - 29.349192336201668
  - 29.249991789460182
  - 30.571832731366158
  - 29.605215653777122
  - 29.549493834376335
  - 29.066335037350655
  - 28.836009979248047
  - 29.816008865833282
  - 29.564593464136124
  - 28.779872953891754
  - 29.480583786964417
  - 29.16427807509899
  - 28.83169350028038
  - 29.81032530963421
  - 29.39322964847088
  - 29.18830819427967
  - 29.26276934146881
  - 29.50395706295967
  - 28.746593713760376
  - 28.827628195285797
  - 29.01061949133873
  - 29.592335000634193
  - 29.556042030453682
  - 28.81663343310356
  - 29.60147486627102
  - 29.065836563706398
  - 29.03080838918686
  - 29.588435366749763
  - 29.099259689450264
  - 29.348701313138008
  - 29.449033826589584
  - 29.502059414982796
  - 29.602024659514427
  - 29.70006573200226
  - 29.149598196148872
  - 29.02602818608284
  - 29.046812146902084
  - 28.871776163578033
  - 28.862514182925224
  - 29.26242008805275
  - 29.928339958190918
  - 29.254727989435196
  - 29.120055615901947
  - 29.248843416571617
  - 29.743474915623665
  - 29.144689291715622
  - 29.693900913000107
  - 29.522891148924828
  - 28.98202048242092
  - 29.334371477365494
  - 29.659778580069542
  - 29.451111510396004
  - 29.539480820298195
  - 29.61733703315258
  - 28.62663894891739
  - 28.776611641049385
  - 28.846597582101822
  - 30.091992139816284
  - 29.589647889137268
  - 29.15004913508892
  - 29.398979038000107
  - 29.387620523571968
  - 28.923877745866776
  - 28.481940910220146
  - 29.10550832748413
  - 29.544422656297684
  - 28.533616825938225
  - 30.526264414191246
  - 29.85030274093151
  - 28.949538975954056
  - 28.766375333070755
  - 29.249147042632103
  - 29.228781834244728
  - 29.173215463757515
  - 29.20331521332264
  - 29.176678031682968
  - 29.14340542256832
  - 29.3834036141634
  - 29.23659510910511
  - 27.876898482441902
  - 29.182190254330635
  - 30.20148479938507
  validation_losses:
  - 0.3910987973213196
  - 0.39042559266090393
  - 0.39183443784713745
  - 0.41138672828674316
  - 0.3972693085670471
  - 0.4206055700778961
  - 0.39822661876678467
  - 0.3944326937198639
  - 0.39173248410224915
  - 0.3938143253326416
  - 0.44915086030960083
  - 0.3932751715183258
  - 0.39298713207244873
  - 0.4551014304161072
  - 0.39204394817352295
  - 0.40501296520233154
  - 0.3986068367958069
  - 0.4049927592277527
  - 0.37879496812820435
  - 0.41826939582824707
  - 0.41243281960487366
  - 0.41379815340042114
  - 0.3889293074607849
  - 0.6980723142623901
  - 0.3935450613498688
  - 0.4069462716579437
  - 1.0418939590454102
  - 0.39144641160964966
  - 0.3933904767036438
  - 0.6059408187866211
  - 0.4628083407878876
  - 0.5081586837768555
  - 0.407831072807312
  - 0.6062800288200378
  - 0.9709489941596985
  - 0.5117762684822083
  - 0.4507022500038147
  - 0.9694980382919312
  - 0.6151607036590576
  - 0.6182900667190552
  - 0.48360735177993774
  - 0.8332697749137878
  - 0.802505373954773
  - 0.7291911840438843
  - 0.6079010367393494
  - 0.42251577973365784
  - 0.4348689317703247
  - 0.4001678228378296
  - 0.6790174841880798
  - 0.9158483743667603
  - 1.5388610363006592
  - 0.4766295552253723
  - 0.8090569376945496
  - 0.5147709250450134
  - 0.5424447655677795
  - 0.7201821208000183
  - 0.6655322909355164
  - 0.4587995409965515
  - 0.8827849626541138
  - 0.43131887912750244
  - 0.4299013912677765
  - 0.4711116552352905
  - 0.7785961031913757
  - 0.5621399879455566
  - 0.6651424169540405
  - 0.7841644287109375
  - 0.8780189156532288
  - 0.7584021687507629
  - 0.714257001876831
  - 0.6933984160423279
  - 0.5642861723899841
  - 0.6916468143463135
  - 0.8475267887115479
  - 0.5152509808540344
  - 0.5892186164855957
  - 0.491148978471756
  - 0.4568324387073517
  - 0.48093080520629883
  - 0.569281280040741
  - 0.6560471057891846
  - 0.670404851436615
  - 0.699501097202301
  - 0.7021134495735168
  - 0.7469969987869263
  - 1.2892954349517822
  - 0.46984606981277466
  - 0.49947497248649597
  - 0.4640866219997406
  - 0.6269689798355103
  - 0.5070726871490479
  - 0.7891865372657776
  - 0.6866655349731445
  - 0.49055352807044983
  - 0.5354223847389221
  - 0.8300284147262573
  - 0.4319157898426056
  - 0.48856204748153687
  - 0.5533456802368164
  - 0.5640446543693542
  - 0.7420642971992493
loss_records_fold3:
  train_losses:
  - 29.137543708086014
  - 29.392202958464622
  - 29.717349529266357
  - 29.78001929819584
  - 29.167960539460182
  - 29.877134799957275
  - 29.291402369737625
  - 29.274280294775963
  - 30.011255398392677
  - 28.985141456127167
  - 28.890600085258484
  - 29.15841692686081
  - 29.906071662902832
  - 29.47001503407955
  - 30.20366722345352
  - 29.10448195040226
  - 30.125661745667458
  - 29.302345648407936
  - 29.124854564666748
  - 29.229571223258972
  - 29.764539256691933
  - 29.37058438360691
  - 29.254639506340027
  - 29.42260940372944
  - 28.909936293959618
  - 29.30497871339321
  - 28.896992281079292
  - 28.88827235996723
  - 29.215405613183975
  - 29.08805125951767
  - 29.514358565211296
  - 30.148382678627968
  - 29.38250970840454
  - 28.81327573955059
  - 29.514299660921097
  - 29.17605470120907
  - 28.254684790968895
  - 29.104611292481422
  - 29.388906061649323
  - 28.891390696167946
  - 29.14950481057167
  - 29.19901715219021
  - 28.841568678617477
  - 28.917537078261375
  - 28.484287679195404
  - 29.354548320174217
  - 29.008776143193245
  - 29.16920855641365
  - 29.066060096025467
  - 29.013705030083656
  - 28.867919579148293
  - 29.01183618605137
  - 29.33028109371662
  - 29.07222919166088
  - 28.90832230448723
  - 29.019405096769333
  - 28.884813025593758
  - 29.10158123075962
  - 29.211187571287155
  - 29.723169580101967
  - 29.453412547707558
  - 29.268473610281944
  - 28.88317920267582
  - 29.37280984222889
  - 28.269482523202896
  - 29.193989768624306
  - 28.69068717956543
  - 29.48035515844822
  - 28.75992925465107
  - 29.299030035734177
  - 28.934003472328186
  - 29.219183892011642
  - 29.308895453810692
  - 29.10572275519371
  - 29.203052908182144
  - 29.219824463129044
  - 29.141992941498756
  - 29.066073805093765
  - 28.84166695177555
  - 29.194948554039
  - 28.7596498131752
  - 28.038660794496536
  - 28.541771233081818
  - 29.501502230763435
  - 29.42681972682476
  - 28.94950971007347
  - 28.85768496990204
  - 28.673766687512398
  - 29.107384607195854
  - 29.360614866018295
  - 28.71418161690235
  - 28.921611413359642
  - 29.02711845934391
  - 28.54877172410488
  - 29.193406835198402
  - 29.206416189670563
  - 28.98302748799324
  - 29.368468329310417
  - 28.841876074671745
  - 28.58334295451641
  validation_losses:
  - 0.772085428237915
  - 2.2161176204681396
  - 0.5656264424324036
  - 0.5307331681251526
  - 0.5433101654052734
  - 0.4565179646015167
  - 0.5910289883613586
  - 0.6045666933059692
  - 0.4819352924823761
  - 0.5220799446105957
  - 0.5550199151039124
  - 0.5766109228134155
  - 0.592709481716156
  - 0.5861145257949829
  - 0.6247156858444214
  - 0.5126937627792358
  - 0.52751624584198
  - 0.5839611291885376
  - 0.59136962890625
  - 0.5780256390571594
  - 0.5584171414375305
  - 0.6194981336593628
  - 0.6418890953063965
  - 0.6116521954536438
  - 0.7430884838104248
  - 0.5677710175514221
  - 0.6433383822441101
  - 0.7161949872970581
  - 0.6243695616722107
  - 0.7938798069953918
  - 0.7096937894821167
  - 0.49980393052101135
  - 0.9928818345069885
  - 0.8181936740875244
  - 0.6542766690254211
  - 0.6550421118736267
  - 0.7914925217628479
  - 0.857245683670044
  - 0.5805211663246155
  - 0.5468568205833435
  - 0.5720669627189636
  - 0.5680702328681946
  - 0.550979733467102
  - 0.5529336333274841
  - 0.5795539021492004
  - 0.4813535213470459
  - 0.5637701153755188
  - 0.5390792489051819
  - 0.5175511240959167
  - 0.5881305932998657
  - 0.7081626653671265
  - 0.5839630365371704
  - 0.6066728234291077
  - 0.5212084054946899
  - 0.5840809345245361
  - 0.4777947962284088
  - 0.5674169659614563
  - 0.7582554817199707
  - 0.5570427775382996
  - 0.622145414352417
  - 0.5919052362442017
  - 0.49168965220451355
  - 0.5583115816116333
  - 0.5646016001701355
  - 0.6499897837638855
  - 0.6771929264068604
  - 0.5369237661361694
  - 0.5963746905326843
  - 0.5123462080955505
  - 0.5824148654937744
  - 0.5046728849411011
  - 0.5177796483039856
  - 0.5467368364334106
  - 0.538470983505249
  - 0.45002561807632446
  - 0.5218417644500732
  - 0.5296196341514587
  - 0.4559606909751892
  - 0.5172827839851379
  - 0.42377224564552307
  - 0.5803637504577637
  - 0.5508189797401428
  - 0.5899622440338135
  - 0.5609612464904785
  - 0.4819806218147278
  - 0.5710574984550476
  - 0.511591911315918
  - 0.5008946657180786
  - 0.4567132592201233
  - 0.5465644001960754
  - 0.5325421690940857
  - 0.520222544670105
  - 0.5806750655174255
  - 0.9201484322547913
  - 0.548600971698761
  - 0.5689888000488281
  - 0.5469571948051453
  - 0.45921963453292847
  - 0.544110119342804
  - 0.6338460445404053
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 64 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8507718696397941, 0.8404802744425386, 0.8456260720411664,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.06451612903225806, 0.16216216216216217, 0.0625, 0.023809523809523808]'
  mean_eval_accuracy: 0.8507235356875503
  mean_f1_accuracy: 0.06259756300078881
  total_train_time: '0:15:27.783505'
