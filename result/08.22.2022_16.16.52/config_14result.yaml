config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.171847'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_14fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_14fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_14fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_14fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_14fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_14fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_14fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_14fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_14fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_14fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 38.12462329864502
  - 9.158538788557053
  - 5.692051076889038
  - 3.962153333425522
  - 3.006621688604355
  - 3.161631762981415
  - 3.0831102788448335
  - 4.060976481437684
  - 2.994785887002945
  - 3.110747539997101
  - 2.132900261878967
  - 1.8587696135044098
  - 1.7285661280155182
  - 4.869369357824326
  - 5.959383314847947
  - 4.789667129516602
  - 4.5948561251163484
  - 2.6953871309757234
  - 2.253728210926056
  - 2.8271328270435334
  - 1.8156067311763764
  - 3.262124490737915
  - 2.468810796737671
  - 2.1076857447624207
  - 5.493870353698731
  - 3.9609752833843235
  - 9.681085413694383
  - 4.333187055587769
  - 2.169999819993973
  - 1.946318182349205
  - 1.9170775711536407
  - 2.258973377943039
  - 1.8814594447612762
  - 1.9479224503040315
  - 2.3544548094272613
  - 1.743418312072754
  - 5.2975548803806305
  - 5.880344986915588
  - 4.6010169148445135
  - 5.049954408407212
  - 3.1983184933662416
  - 2.8699379682540895
  - 4.676591444015503
  - 3.90033141374588
  - 2.3863401830196382
  - 2.6831297963857654
  - 2.0737980365753175
  - 2.318436974287033
  - 3.2825594663619997
  - 4.231301265954971
  - 2.294227957725525
  - 3.213100802898407
  - 2.151683485507965
  - 1.9961855173110963
  - 2.6256485760211947
  - 1.923304831981659
  - 1.9057401180267335
  - 3.3673639416694643
  - 2.07938466668129
  - 2.0186083137989046
  - 2.0255191683769227
  - 1.8964006364345551
  - 1.6246149837970734
  - 2.292327600717545
  - 1.8106175899505617
  - 2.7895090699195864
  - 1.9338208436965942
  - 1.7995319724082948
  - 2.0667560398578644
  - 1.6974518775939942
  - 1.7701251327991487
  - 1.6945783376693726
  - 1.869006484746933
  - 2.600355857610703
  - 1.7186343252658844
  - 1.7693792998790743
  - 1.6340060859918595
  - 1.5960459887981415
  - 1.6227909207344056
  - 1.7859412729740143
  - 1.6827054858207704
  - 1.5770014137029649
  - 1.7947897791862488
  - 1.777676445245743
  - 1.6222038686275484
  - 1.642304676771164
  - 1.7237353086471559
  - 1.6680825471878054
  - 1.7681187391281128
  - 1.750144213438034
  - 1.6389612913131715
  - 1.7871426820755005
  - 1.7961419939994814
  - 1.9559644222259522
  - 2.061909759044647
  - 1.7929884552955628
  - 1.6514966547489167
  - 1.5753315031528474
  - 1.6849855959415436
  - 1.7714601039886475
  validation_losses:
  - 3.1192357540130615
  - 1.9688342809677124
  - 1.1338454484939575
  - 0.6459313035011292
  - 1.1215426921844482
  - 0.7176140546798706
  - 0.6146665215492249
  - 0.4284781813621521
  - 0.46965542435646057
  - 0.88040691614151
  - 0.4454081952571869
  - 0.39547407627105713
  - 0.4453973174095154
  - 0.6677569150924683
  - 0.8302164077758789
  - 0.5589883327484131
  - 0.678300142288208
  - 0.4103405177593231
  - 0.39854833483695984
  - 0.416675865650177
  - 0.44483667612075806
  - 0.44711509346961975
  - 0.44079601764678955
  - 0.4703305959701538
  - 0.42971503734588623
  - 0.4962245225906372
  - 0.7842364311218262
  - 0.4747993052005768
  - 0.478129118680954
  - 0.4551886320114136
  - 0.38683032989501953
  - 0.428240567445755
  - 0.40088996291160583
  - 0.4069215953350067
  - 0.5136244893074036
  - 0.42027997970581055
  - 0.44092610478401184
  - 0.5233644843101501
  - 0.46103131771087646
  - 0.6111054420471191
  - 0.38823437690734863
  - 0.5491104125976562
  - 0.5268142223358154
  - 0.43325334787368774
  - 0.3829759359359741
  - 0.4129403233528137
  - 0.39846745133399963
  - 0.43027234077453613
  - 0.5523170232772827
  - 0.39005735516548157
  - 0.3915659785270691
  - 0.4720420241355896
  - 0.4208347499370575
  - 0.5108768343925476
  - 0.44356825947761536
  - 0.40051960945129395
  - 0.43215158581733704
  - 0.4866687059402466
  - 0.3894146680831909
  - 0.40101736783981323
  - 0.5385702848434448
  - 0.38428187370300293
  - 0.3835770785808563
  - 0.4160405099391937
  - 0.40914881229400635
  - 0.4143140912055969
  - 0.3911300599575043
  - 0.46664518117904663
  - 0.5219467878341675
  - 0.4204462766647339
  - 0.3953539729118347
  - 0.4451320171356201
  - 0.38226398825645447
  - 0.3780997693538666
  - 0.3828466534614563
  - 0.43447667360305786
  - 0.3950810134410858
  - 0.3878926634788513
  - 0.39173197746276855
  - 0.4659293591976166
  - 0.38762348890304565
  - 0.3956632614135742
  - 0.47598037123680115
  - 0.40824010968208313
  - 0.41113778948783875
  - 0.3970418870449066
  - 0.4386694133281708
  - 0.3932677209377289
  - 0.38865014910697937
  - 0.39984580874443054
  - 0.38721248507499695
  - 0.43176475167274475
  - 0.3854924440383911
  - 0.42597100138664246
  - 0.41922861337661743
  - 0.4311144948005676
  - 0.39065003395080566
  - 0.3906651735305786
  - 0.3969874680042267
  - 0.3789365291595459
loss_records_fold1:
  train_losses:
  - 1.7039438903331758
  - 1.6152993500232697
  - 1.7056920111179352
  - 1.7480019688606263
  - 1.9693980157375337
  - 1.541661721467972
  - 1.6000027716159821
  - 1.5869767844676972
  - 1.6031029641628267
  - 1.5882900059223175
  - 1.5819517254829407
  validation_losses:
  - 0.42010965943336487
  - 0.4786265194416046
  - 0.39702436327934265
  - 0.504804253578186
  - 0.40711984038352966
  - 0.39862897992134094
  - 0.3950135111808777
  - 0.40396857261657715
  - 0.4103866219520569
  - 0.40302538871765137
  - 0.3988775312900543
loss_records_fold2:
  train_losses:
  - 1.5930958628654481
  - 1.7194916307926178
  - 1.7606980204582214
  - 2.5449780106544497
  - 1.8743396401405334
  - 1.6349226236343384
  - 1.767714297771454
  - 1.6588518083095551
  - 1.571674180030823
  - 1.5727535128593446
  - 1.6159807801246644
  - 1.699228209257126
  - 1.772802072763443
  - 1.9114332973957062
  - 1.9051445484161378
  - 2.160647076368332
  - 2.2912281811237336
  - 1.9112020909786225
  - 1.7475784838199617
  - 1.6383154928684236
  - 1.6191096022725107
  - 1.657616597414017
  - 1.7532657146453858
  - 2.039849764108658
  - 1.8768107771873475
  - 1.6426892995834352
  - 1.7284253358840944
  - 1.6160520851612092
  - 1.6331464886665346
  - 1.5647565186023713
  - 1.6273903369903566
  - 1.5876536130905152
  - 1.618610441684723
  - 1.6661806225776674
  - 1.5932934105396273
  - 1.8111907720565796
  - 2.1607695877552033
  - 3.096491497755051
  - 2.1687048554420474
  - 1.8594559729099274
  - 1.7906846106052399
  - 1.8393520295619965
  - 1.6868064641952516
  - 1.797962099313736
  - 1.749933058023453
  - 1.9616184175014497
  - 1.890266013145447
  - 1.7677367389202119
  - 1.6676398813724518
  - 1.9128800690174104
  - 1.7570437371730805
  - 1.7481707751750948
  - 1.7475325882434847
  - 1.724495941400528
  - 1.6006778359413147
  - 1.6286711513996126
  - 1.604234218597412
  - 1.6248581171035767
  - 1.7237399041652681
  - 1.625797075033188
  - 1.5411974608898165
  - 1.7862298846244813
  - 1.7092750400304795
  - 1.5848547101020813
  - 1.7445213496685028
  - 1.7440446496009827
  - 1.6146385371685028
  - 1.9936720311641694
  - 1.7490085780620577
  - 1.6431301891803742
  - 1.6945926785469057
  - 1.8325759947299958
  - 1.6341335833072663
  - 1.6914238512516022
  - 1.5434421688318254
  - 1.6748030960559845
  - 1.648910027742386
  - 1.7271268486976625
  - 1.5889965534210206
  - 1.733957087993622
  - 1.610634344816208
  - 1.5890306353569033
  - 1.5884938895702363
  - 1.6450363099575043
  - 1.5498475551605226
  - 1.6030102014541627
  - 1.6448885202407837
  - 1.6458959877490997
  - 1.5883896589279176
  - 1.7251326203346253
  - 1.560974681377411
  - 1.5847159624099731
  - 1.6692705094814302
  - 1.604243588447571
  - 1.5829671561717988
  - 1.6263951659202576
  - 1.6856255769729616
  - 1.6075790107250214
  - 2.030762153863907
  - 1.7508284389972688
  validation_losses:
  - 0.38524630665779114
  - 0.37387123703956604
  - 0.4020175337791443
  - 0.3704867660999298
  - 0.38575050234794617
  - 0.40860286355018616
  - 0.41352930665016174
  - 0.3887852132320404
  - 0.37986576557159424
  - 0.397549569606781
  - 0.3792412281036377
  - 0.40385305881500244
  - 0.412424772977829
  - 0.4047580659389496
  - 0.4065653085708618
  - 0.6905637383460999
  - 0.4889083802700043
  - 0.3885482847690582
  - 0.37900999188423157
  - 0.394491046667099
  - 0.41864535212516785
  - 0.5368473529815674
  - 0.38895559310913086
  - 0.39161577820777893
  - 0.3834925889968872
  - 0.38001683354377747
  - 0.3947293758392334
  - 0.3817998766899109
  - 0.38859519362449646
  - 0.3999934196472168
  - 0.3922000825405121
  - 0.3835045099258423
  - 0.443372517824173
  - 0.40151268243789673
  - 0.39422884583473206
  - 0.4223948121070862
  - 0.8864548206329346
  - 0.39665937423706055
  - 0.3767731487751007
  - 0.3852206766605377
  - 0.38997209072113037
  - 0.3935500979423523
  - 0.4134872853755951
  - 0.3956422507762909
  - 0.38116946816444397
  - 0.3936408460140228
  - 0.38439393043518066
  - 0.3810848891735077
  - 0.4590802788734436
  - 0.4143604338169098
  - 0.4260047376155853
  - 0.39881423115730286
  - 0.40459129214286804
  - 0.37490034103393555
  - 0.3862113654613495
  - 0.40546390414237976
  - 0.39081862568855286
  - 0.37422072887420654
  - 0.3787241578102112
  - 0.39011526107788086
  - 0.4020062983036041
  - 0.38445836305618286
  - 0.3833465278148651
  - 0.3943879008293152
  - 0.3850637376308441
  - 0.3865591585636139
  - 0.39421921968460083
  - 0.3948725163936615
  - 0.43202024698257446
  - 0.4079260230064392
  - 0.44336557388305664
  - 0.39944788813591003
  - 0.3936883509159088
  - 0.3990301191806793
  - 0.4479561448097229
  - 0.4213704466819763
  - 0.4005606770515442
  - 0.3873540163040161
  - 0.3921653926372528
  - 0.40994855761528015
  - 0.389453262090683
  - 0.3757455348968506
  - 0.3979664742946625
  - 0.3857520818710327
  - 0.40103134512901306
  - 0.3802478313446045
  - 0.39628180861473083
  - 0.38111773133277893
  - 0.37667903304100037
  - 0.38448718190193176
  - 0.38869282603263855
  - 0.3849690556526184
  - 0.40952223539352417
  - 0.38632071018218994
  - 0.3935631811618805
  - 0.3976845145225525
  - 0.4870167076587677
  - 0.39184120297431946
  - 0.48863500356674194
  - 0.40580078959465027
loss_records_fold3:
  train_losses:
  - 1.6316135704517365
  - 1.562618124485016
  - 1.589090269804001
  - 1.5422956943511963
  - 1.5936758279800416
  - 1.6051868736743928
  - 1.6747358083724977
  - 1.6274804949760437
  - 1.663398051261902
  - 1.5709321200847626
  - 1.5658309102058412
  - 1.6099011600017548
  - 1.5919937193393707
  - 1.8775235712528229
  - 1.744053226709366
  - 1.6326194047927858
  - 1.6116840541362762
  - 1.69572491645813
  - 1.5750003576278688
  - 1.6587023675441743
  - 1.7544756412506104
  - 1.7214994847774507
  - 1.645829623937607
  - 1.5109876424074173
  validation_losses:
  - 0.38889458775520325
  - 0.3844040334224701
  - 0.3914714455604553
  - 0.39543065428733826
  - 0.38876208662986755
  - 0.4043087065219879
  - 0.3970486521720886
  - 0.41430994868278503
  - 0.3950293958187103
  - 0.40721189975738525
  - 0.40739282965660095
  - 0.39400580525398254
  - 0.40760454535484314
  - 0.414069801568985
  - 0.3971244692802429
  - 0.42386484146118164
  - 0.4121183156967163
  - 0.44211092591285706
  - 0.38774698972702026
  - 0.3805806636810303
  - 0.3798273503780365
  - 0.3798205256462097
  - 0.38056042790412903
  - 0.3814263939857483
loss_records_fold4:
  train_losses:
  - 1.5523392736911774
  - 1.5701653361320496
  - 1.7223445713520051
  - 1.621308821439743
  - 1.590779423713684
  - 1.5854680478572847
  - 1.6151174724102022
  - 1.5837321519851686
  - 1.5789810359477998
  - 1.6012712955474855
  - 1.6764012992382051
  - 1.7627386063337327
  - 1.5969993293285372
  - 1.6424313187599182
  - 1.6339835226535797
  - 1.5296435326337816
  - 1.589663851261139
  - 1.550919085741043
  - 1.5508337676525117
  - 1.5430853366851807
  - 1.5729622423648835
  - 1.653004440665245
  - 1.6902767181396485
  - 1.5923704981803894
  - 1.70044304728508
  - 1.6280412912368776
  - 1.6106666326522827
  - 1.5866169035434723
  - 1.60544513463974
  - 1.5236094117164614
  - 1.6035482347011567
  - 1.6500761926174166
  - 1.5801269918680192
  - 1.6134989798069002
  - 1.628101247549057
  - 1.6203818678855897
  - 1.6682255744934082
  - 1.7177323877811432
  - 1.5669098496437073
  - 1.617382675409317
  - 1.812043845653534
  - 1.5765119969844819
  - 1.5713965356349946
  - 1.5446172773838045
  - 1.627072447538376
  - 1.5842565059661866
  - 1.6275188624858856
  - 1.523576056957245
  - 1.620413762331009
  - 1.5146053254604341
  - 1.5486789345741272
  - 1.5845722436904908
  - 1.5726147770881653
  - 1.658398723602295
  - 1.5483832836151123
  - 1.5752387881278993
  - 1.5936934977769852
  validation_losses:
  - 0.39802315831184387
  - 0.3917117714881897
  - 0.362467497587204
  - 0.38892820477485657
  - 0.36441534757614136
  - 0.37863409519195557
  - 0.3889937400817871
  - 0.38783717155456543
  - 0.3896302580833435
  - 0.389938086271286
  - 0.4016907811164856
  - 0.3871971666812897
  - 0.4610847532749176
  - 0.45683756470680237
  - 0.3910972774028778
  - 0.3950701355934143
  - 0.38523030281066895
  - 0.3900041878223419
  - 0.4283902049064636
  - 0.39841216802597046
  - 0.40583235025405884
  - 0.393694132566452
  - 0.4006480276584625
  - 0.39691656827926636
  - 0.4171815514564514
  - 0.4043598175048828
  - 0.4227831959724426
  - 0.41277647018432617
  - 0.4360724091529846
  - 0.4006456136703491
  - 0.4312342405319214
  - 0.3847300112247467
  - 0.39617347717285156
  - 0.4139530658721924
  - 0.40572497248649597
  - 0.46290960907936096
  - 0.39172685146331787
  - 0.40990716218948364
  - 0.44986721873283386
  - 0.4462665319442749
  - 0.3959032893180847
  - 0.40273335576057434
  - 0.3963223397731781
  - 0.4247114360332489
  - 0.3845893442630768
  - 0.37911391258239746
  - 0.40864819288253784
  - 0.39582982659339905
  - 0.39221155643463135
  - 0.3820953369140625
  - 0.41052794456481934
  - 0.4062251150608063
  - 0.4161677360534668
  - 0.3935849368572235
  - 0.3936172425746918
  - 0.38622695207595825
  - 0.3959480822086334
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 57 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:24:06.924946'
