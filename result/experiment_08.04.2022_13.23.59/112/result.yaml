config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 02:22:41.522486'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/112/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/112/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/112/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/112/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/112/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/112/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/112/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/112/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/112/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/112/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 291.75085158646107
  - 187.61183275282383
  - 98.15790541470051
  - 94.56458792090416
  - 80.84200153872371
  - 97.08928534388542
  - 81.90920436382294
  - 53.23029435425997
  - 77.85628827661276
  - 58.99960256367922
  - 52.98586244881153
  - 60.696840181946754
  - 57.51526099443436
  - 52.90694838762283
  - 44.84768898785114
  - 45.64860126376152
  - 65.57609143853188
  - 53.66355939954519
  - 49.838597267866135
  - 54.950946763157845
  - 39.304251939058304
  - 36.98827424645424
  - 32.77612090110779
  - 41.78939060866833
  - 33.541023179888725
  - 38.315618723630905
  - 35.23837508261204
  - 35.67505723237991
  - 40.085777670145035
  - 32.62068297713995
  - 30.065913945436478
  - 31.096022993326187
  - 32.63800132274628
  - 35.224682092666626
  - 32.15939036011696
  - 30.390071973204613
  - 29.994005143642426
  - 30.07311236858368
  - 29.317361071705818
  - 30.02430360019207
  - 32.24798944592476
  - 30.187134101986885
  - 28.708863139152527
  - 29.888623282313347
  - 29.68267959356308
  - 29.380022794008255
  - 31.75561799108982
  - 32.43450412154198
  - 32.116963520646095
  - 33.16679784655571
  - 34.4563540071249
  - 32.92606192827225
  - 30.852719143033028
  - 29.56299565732479
  - 30.443561017513275
  - 30.01041452586651
  - 30.403703421354294
  - 30.1265676766634
  - 29.644200041890144
  - 29.40702225267887
  - 30.055757835507393
  - 29.28598989546299
  - 29.66822937130928
  - 29.545158341526985
  - 30.089289098978043
  - 29.988979667425156
  - 29.47792437672615
  - 30.90114502608776
  - 31.393962919712067
  - 29.751087874174118
  - 29.656275242567062
  - 30.376122176647186
  - 30.829669624567032
  - 53.52032166719437
  - 33.64904375374317
  - 37.30263788998127
  - 30.887288488447666
  - 29.251549035310745
  - 30.271872654557228
  - 29.829049333930016
  - 30.19739133119583
  - 29.472028210759163
  - 30.758470237255096
  - 30.36154718697071
  - 30.724603444337845
  - 29.296752139925957
  - 29.674277529120445
  - 29.46302454173565
  - 29.49252910912037
  - 29.48426389694214
  - 29.681162104010582
  - 29.741362020373344
  - 29.378487646579742
  - 29.439270481467247
  - 32.075904950499535
  - 34.06146042048931
  - 35.20511665940285
  - 31.786843456327915
  - 30.296326085925102
  - 31.959607899188995
  validation_losses:
  - 2.8518574237823486
  - 0.8716804385185242
  - 0.7627139687538147
  - 0.50221186876297
  - 0.7513846755027771
  - 0.5855557918548584
  - 0.9387263059616089
  - 0.5604458451271057
  - 0.5716785788536072
  - 0.4692525267601013
  - 0.9050213694572449
  - 6.746265411376953
  - 9.61599349975586
  - 0.3889771103858948
  - 0.46454235911369324
  - 0.3856683671474457
  - 0.43746301531791687
  - 0.38505521416664124
  - 0.3952459692955017
  - 0.44585901498794556
  - 0.4385162889957428
  - 0.3808526396751404
  - 0.7556751370429993
  - 0.38529449701309204
  - 0.518629252910614
  - 0.3894061744213104
  - 0.38777029514312744
  - 0.3823532462120056
  - 0.39075401425361633
  - 0.3806777894496918
  - 0.4061635434627533
  - 0.389871746301651
  - 0.37520596385002136
  - 0.3809435963630676
  - 0.38949522376060486
  - 0.41937482357025146
  - 0.37865254282951355
  - 0.3781754970550537
  - 0.37641894817352295
  - 0.4291940927505493
  - 0.38906243443489075
  - 0.37643834948539734
  - 0.376875102519989
  - 0.3746916651725769
  - 0.41237276792526245
  - 0.37873488664627075
  - 0.3823877274990082
  - 0.3836992084980011
  - 0.5456765294075012
  - 0.40000641345977783
  - 0.4362897276878357
  - 0.42643412947654724
  - 0.4101785719394684
  - 0.39068442583084106
  - 0.42517927289009094
  - 0.400371253490448
  - 0.4072122871875763
  - 0.39085128903388977
  - 0.389665812253952
  - 0.39190050959587097
  - 0.4725530445575714
  - 0.5324383974075317
  - 0.38992780447006226
  - 6.624665260314941
  - 0.44976547360420227
  - 0.3909100890159607
  - 0.3919292688369751
  - 0.4236919581890106
  - 9.951531410217285
  - 34600876.0
  - 414.94921875
  - 647574.1875
  - 1353668.0
  - 0.38900497555732727
  - 0.3982546031475067
  - 0.3880644738674164
  - 0.39681971073150635
  - 0.3997378945350647
  - 0.4151251018047333
  - 0.4295727014541626
  - 0.40411829948425293
  - 150.64004516601562
  - 0.39104029536247253
  - 0.8062258362770081
  - 0.8253762125968933
  - 0.43181976675987244
  - 0.4070107042789459
  - 0.4766163229942322
  - 0.3910452723503113
  - 0.5508881211280823
  - 0.4143000543117523
  - 0.39401373267173767
  - 10.93058967590332
  - 27159.75390625
  - 54252.96875
  - 1.2787904739379883
  - 0.4722777009010315
  - 1.4891343116760254
  - 0.41376805305480957
  - 0.4860914349555969
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 73 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 33 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 10 epochs
training_metrics:
  fold_eval_accs: '[0.8421955403087479, 0.8576329331046312, 0.8593481989708405, 0.8576329331046312,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8551832269396945
  mean_f1_accuracy: 0.0
  total_train_time: '0:10:51.085459'
