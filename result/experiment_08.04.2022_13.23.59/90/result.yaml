config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 23:41:01.023189'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/90/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/90/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/90/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/90/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/90/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/90/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/90/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/90/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/90/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/90/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 15.134590208530426
  - 16.353787809610367
  - 17.35624071955681
  - 15.852255582809448
  - 15.667735010385513
  - 17.20871104300022
  - 15.423710897564888
  - 15.752573475241661
  - 16.241083830595016
  - 16.388521522283554
  - 17.034551322460175
  - 15.648919522762299
  - 16.209653943777084
  - 16.574430912733078
  - 16.720096811652184
  - 16.941917687654495
  - 16.08846029639244
  - 15.96339613199234
  - 15.324797540903091
  - 16.582567930221558
  - 16.159681618213654
  - 16.287283092737198
  - 16.038463830947876
  - 15.51993903517723
  - 16.01032704114914
  - 16.186530470848083
  - 17.168270081281662
  - 16.571858644485474
  - 15.587925791740417
  - 16.473390609025955
  - 17.400031477212906
  - 16.31589177250862
  - 15.687807708978653
  - 15.324176043272018
  - 15.299986600875854
  - 15.482869416475296
  - 15.100168108940125
  - 16.8411925137043
  - 15.533463418483734
  - 16.522703781723976
  - 15.491080701351166
  - 15.362134426832199
  - 15.852439284324646
  - 16.60131335258484
  - 15.96286404132843
  - 15.448833271861076
  - 15.395766332745552
  - 15.57480937242508
  - 15.230672419071198
  - 15.059217691421509
  - 15.978268325328827
  - 17.145924776792526
  - 16.135392010211945
  - 16.007111370563507
  - 15.589955985546112
  - 16.22154414653778
  - 15.891105651855469
  - 17.475069627165794
  - 17.769057244062424
  - 17.57393865287304
  - 16.417961925268173
  - 16.81799754500389
  - 16.219728529453278
  - 15.219063580036163
  - 16.004235208034515
  - 16.093967020511627
  - 15.375620871782303
  - 15.18023744225502
  - 15.040927544236183
  - 14.851262018084526
  - 15.524086833000183
  - 16.296211183071136
  - 15.402572095394135
  - 15.195743262767792
  - 15.566669642925262
  - 15.702718049287796
  - 14.950240015983582
  - 15.260789930820465
  - 15.636024296283722
  - 15.975069224834442
  - 15.489789962768555
  - 15.731730937957764
  - 15.368235111236572
  - 17.21152424812317
  - 15.998034209012985
  - 15.729224860668182
  - 15.425179481506348
  - 15.266899049282074
  - 15.265064984560013
  - 15.07647106051445
  - 15.354355067014694
  - 15.359640061855316
  - 15.51503649353981
  - 16.07181541621685
  - 15.490024343132973
  - 15.323449864983559
  - 14.956975892186165
  - 16.02450606226921
  - 14.966855734586716
  - 15.404883325099945
  validation_losses:
  - 0.4156268537044525
  - 0.43740954995155334
  - 0.4298427999019623
  - 0.4150717258453369
  - 0.47215592861175537
  - 0.4066157937049866
  - 0.40681126713752747
  - 0.47806718945503235
  - 0.40820419788360596
  - 0.4240913987159729
  - 0.4410005211830139
  - 0.44225531816482544
  - 0.41888976097106934
  - 0.40917545557022095
  - 0.42817357182502747
  - 0.6707585453987122
  - 0.4007934629917145
  - 0.4288721978664398
  - 0.4751080274581909
  - 0.42169031500816345
  - 0.4508625268936157
  - 0.4160524904727936
  - 0.44302940368652344
  - 0.41555675864219666
  - 0.4099354147911072
  - 0.42058393359184265
  - 0.49084463715553284
  - 0.3963075876235962
  - 0.4100259244441986
  - 0.539508044719696
  - 0.4433334171772003
  - 0.4113316237926483
  - 0.49421894550323486
  - 0.4246310591697693
  - 0.3979083299636841
  - 0.412238746881485
  - 0.46114596724510193
  - 0.4672338664531708
  - 0.42064616084098816
  - 0.42292195558547974
  - 0.4038217067718506
  - 0.42403021454811096
  - 0.42632728815078735
  - 0.4678385555744171
  - 0.4079177975654602
  - 0.4052416682243347
  - 0.40877243876457214
  - 0.4779222905635834
  - 0.4483897089958191
  - 0.42778876423835754
  - 0.42221033573150635
  - 0.42377445101737976
  - 0.43682485818862915
  - 0.4079318940639496
  - 0.43723708391189575
  - 0.4462018609046936
  - 0.5490300059318542
  - 0.5992836952209473
  - 0.39757829904556274
  - 0.5685396194458008
  - 0.8296290040016174
  - 0.5248350501060486
  - 0.40851444005966187
  - 0.40050044655799866
  - 0.6482310891151428
  - 0.43973487615585327
  - 0.4074332118034363
  - 0.3985702097415924
  - 0.41462841629981995
  - 0.41642630100250244
  - 0.40348777174949646
  - 0.4423055648803711
  - 0.40870481729507446
  - 0.4040694534778595
  - 0.41808444261550903
  - 0.4137548804283142
  - 0.4105055332183838
  - 0.39779168367385864
  - 0.4210599958896637
  - 0.570306658744812
  - 0.46820059418678284
  - 0.4082818925380707
  - 0.44227153062820435
  - 0.40642091631889343
  - 0.5284898281097412
  - 0.42818382382392883
  - 0.39828959107398987
  - 0.3978186547756195
  - 0.4113656282424927
  - 0.4295668303966522
  - 0.41737374663352966
  - 0.40849748253822327
  - 0.4123546779155731
  - 0.481428325176239
  - 0.42002472281455994
  - 0.41244223713874817
  - 0.4472702443599701
  - 0.4263690710067749
  - 0.4561276137828827
  - 197.7875518798828
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 84 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 57 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 44 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:08:17.263524'
