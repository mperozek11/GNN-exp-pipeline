config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:09:13.146136'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_48fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_48fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_48fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_48fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_48fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_48fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_48fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_48fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_48fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_48fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.100219374895096
  - 5.928448209166527
  - 5.93121874332428
  - 5.8159703642129905
  - 5.781930696964264
  - 5.817881643772125
  - 5.720852965116501
  - 5.7063566774129875
  - 5.798831799626351
  - 5.77937705218792
  - 5.618823766708374
  - 5.744065850973129
  - 5.7039583176374435
  - 5.800962743163109
  - 5.705681312084199
  - 5.715113586187363
  - 5.708479514718056
  - 5.694144302606583
  - 5.661547991633416
  - 5.61757425069809
  - 5.609108428657056
  - 5.690233132243157
  - 5.623492202162743
  - 5.604745697975159
  - 5.570592850446701
  validation_losses:
  - 0.4377577602863312
  - 0.5240808725357056
  - 0.4214785397052765
  - 0.5657223463058472
  - 0.44389989972114563
  - 0.93362957239151
  - 0.4520193040370941
  - 0.38899537920951843
  - 0.40603989362716675
  - 0.4073248505592346
  - 0.5398313403129578
  - 0.8415795564651489
  - 0.4144686162471771
  - 0.38285234570503235
  - 0.38388746976852417
  - 0.383547842502594
  - 0.39942753314971924
  - 0.38341549038887024
  - 0.4095208942890167
  - 0.38817909359931946
  - 0.3906898498535156
  - 0.3865099251270294
  - 0.3915640115737915
  - 0.39234283566474915
  - 0.3903382420539856
loss_records_fold1:
  train_losses:
  - 5.617467167973519
  - 5.584280860424042
  - 5.5832092553377155
  - 5.641279801726341
  - 5.6875446259975435
  - 5.709213966131211
  - 5.6307003706693655
  - 5.703720262646676
  - 5.655816206336022
  - 5.5536490976810455
  - 5.6020666599273685
  - 5.550262987613678
  - 5.565944850444794
  - 5.532051938772202
  - 5.558821195363999
  - 5.566612470149995
  - 5.55747995376587
  - 5.533719226717949
  - 5.578264978528023
  - 5.586014142632485
  - 5.513927054405213
  - 5.516557748615742
  - 5.477909150719643
  - 5.4664985805749895
  - 5.436929041147232
  - 5.510218676924706
  - 5.4819845229387285
  - 5.482114309072495
  - 5.526230025291444
  - 5.454821938276291
  - 5.579194992780685
  - 5.545744113624096
  - 5.42098452448845
  - 5.5154153496027
  - 5.479071155190468
  - 5.432737010717393
  - 5.427330711483956
  - 5.560757115483284
  - 5.483094295859337
  - 5.421256768703461
  - 5.53965346813202
  - 5.559387037158013
  - 5.468547624349594
  - 5.4614291653037075
  - 5.39984884262085
  - 5.448939302563668
  - 5.499681773781777
  - 5.36627463400364
  - 5.463823753595353
  - 5.44517474770546
  - 5.395176097750664
  - 5.53553421497345
  - 5.5969993591308596
  - 5.452078515291214
  - 5.41914456486702
  - 5.382811206579209
  - 5.441620960831642
  - 5.489225113391877
  - 5.435981744527817
  - 5.5561637639999395
  - 5.499193418025971
  - 5.463987144827843
  - 5.460856169462204
  - 5.571018958091736
  - 5.472749224305153
  - 5.476437522470952
  - 5.4608147025108345
  - 5.390111732482911
  - 5.410583508014679
  - 5.523764336109162
  - 5.451857918500901
  - 5.522171458601952
  - 5.433633583784104
  - 5.487069946527481
  - 5.423370537161827
  - 5.396338468790055
  - 5.456267669796944
  - 5.441279363632202
  - 5.367869704961777
  - 5.40720029771328
  - 5.3885885149240496
  - 5.336937841773033
  - 5.393485847115517
  - 5.40459335744381
  - 5.38078787624836
  - 5.411655768752098
  - 5.339801007509232
  - 5.44566747546196
  - 5.467585879564286
  - 5.368686527013779
  - 5.314912980794907
  - 5.361309388279915
  - 5.392505192756653
  - 5.448632854223252
  - 5.454510587453843
  - 5.383106479048729
  - 5.425823485851288
  - 5.414886358380318
  - 5.436806944012642
  - 5.434563022851944
  validation_losses:
  - 0.382517009973526
  - 0.5565164089202881
  - 0.46807214617729187
  - 0.38934123516082764
  - 0.3873382806777954
  - 0.3917751610279083
  - 0.3952106535434723
  - 0.39325204491615295
  - 0.3876970112323761
  - 0.40222615003585815
  - 0.8738279342651367
  - 0.3980942666530609
  - 0.4034925103187561
  - 0.4023410975933075
  - 0.5092467069625854
  - 0.5579116940498352
  - 0.39693397283554077
  - 0.5233404636383057
  - 0.392539918422699
  - 0.8099344372749329
  - 0.42228397727012634
  - 0.4080274701118469
  - 0.39517107605934143
  - 0.46712079644203186
  - 0.5697391629219055
  - 0.4751201868057251
  - 0.45542779564857483
  - 0.4657842516899109
  - 0.611836850643158
  - 0.5600776672363281
  - 0.44701144099235535
  - 0.5306917428970337
  - 0.66086745262146
  - 0.45984023809432983
  - 0.6481912732124329
  - 0.6008400321006775
  - 0.6677770614624023
  - 0.426993727684021
  - 0.4844706952571869
  - 0.5395111441612244
  - 0.40301811695098877
  - 0.4099826514720917
  - 0.4931037724018097
  - 0.5212557911872864
  - 0.6933919191360474
  - 0.6515249609947205
  - 0.5847988724708557
  - 0.5745390057563782
  - 0.5131065845489502
  - 0.6553946733474731
  - 0.5977732539176941
  - 0.44449371099472046
  - 0.50312340259552
  - 0.5896481871604919
  - 0.5524484515190125
  - 0.5349639654159546
  - 0.6377038955688477
  - 0.5594016909599304
  - 0.727381706237793
  - 0.40170547366142273
  - 0.4303762912750244
  - 0.4379894435405731
  - 0.5616052746772766
  - 1.2879574298858643
  - 0.46511223912239075
  - 0.5671611428260803
  - 0.7028953433036804
  - 0.896485447883606
  - 0.6256007552146912
  - 0.9402955174446106
  - 0.96732497215271
  - 1.106112003326416
  - 1.3046202659606934
  - 2.9116756916046143
  - 1.0932039022445679
  - 1.1069660186767578
  - 0.658653736114502
  - 0.5838364362716675
  - 0.7562599182128906
  - 1.3525336980819702
  - 0.8509002327919006
  - 1.1689071655273438
  - 1.3166416883468628
  - 1.1051130294799805
  - 1.1694294214248657
  - 0.8974177241325378
  - 0.7226930260658264
  - 1.1301708221435547
  - 1.3783015012741089
  - 1.2305675745010376
  - 0.7647945284843445
  - 1.5886446237564087
  - 1.483111023902893
  - 1.954886794090271
  - 2.389350652694702
  - 1.9721407890319824
  - 1.367638111114502
  - 0.7376763820648193
  - 0.8993526101112366
  - 0.7928827404975891
loss_records_fold2:
  train_losses:
  - 5.496436807513238
  - 5.385968485474587
  - 5.353553551435471
  - 5.360972529649735
  - 5.5051936358213425
  - 5.396358439326287
  - 5.387256479263306
  - 5.392619267106056
  - 5.320065930485725
  - 5.327715110778809
  - 5.475654762983322
  - 5.353634425997734
  - 5.317566686868668
  - 5.3487120330333715
  - 5.32116013020277
  - 5.377145564556122
  - 5.3392229110002525
  - 5.358631673455239
  - 5.3842374920845035
  - 5.385242500901223
  - 5.317054098844529
  - 5.426191464066505
  - 5.455277818441392
  - 5.4479059994220735
  - 5.312459263205529
  - 5.286099773645401
  - 5.321556395292283
  - 5.413808429241181
  - 5.394633927941323
  - 5.308292488753796
  - 5.418634894490243
  - 5.4062341302633286
  - 5.434495627880096
  - 5.403051415085793
  - 5.339807191491127
  - 5.258803194761277
  - 5.223222106695175
  - 5.334636393189431
  - 5.386995342373848
  - 5.250693216919899
  - 5.2122484862804415
  - 5.324412608146668
  - 5.307872116565704
  - 5.277903592586518
  - 5.370643720030785
  - 5.321307924389839
  - 5.324099805951119
  - 5.2574235111475
  - 5.305131649971009
  - 5.345248068869115
  - 5.3662990629673
  - 5.318449580669403
  - 5.33401649594307
  - 5.2255583882331855
  - 5.304585963487625
  - 5.330910390615464
  - 5.2297352939844135
  - 5.201501169800759
  - 5.32149977684021
  - 5.220808789134026
  - 5.309549635648728
  - 5.4563421458005905
  - 5.367620888352395
  - 5.5840785801410675
  - 5.503117027878762
  - 5.320983117818833
  - 5.3651478245854385
  - 5.335483232140541
  - 5.304780977964402
  - 5.281957696378232
  - 5.250871565937996
  - 5.231736642122269
  - 5.281925192475319
  - 5.229901990294457
  - 5.261112061142922
  - 5.186569094657898
  - 5.355838182568551
  - 5.177644619345665
  - 5.568708893656731
  - 5.476823553442955
  - 5.3074886202812195
  - 5.314998599886895
  - 5.199029657244683
  - 5.343650820851327
  - 5.292267760634423
  - 5.354753676056863
  - 5.3727165251970295
  - 5.2683640152215965
  - 5.270024886727334
  - 5.222904655337334
  - 5.342423634231091
  - 5.29256671667099
  - 5.278272843360901
  - 5.234582501649857
  - 5.270429614186288
  - 5.280102351307869
  - 5.205497720837593
  - 5.187047944962979
  - 5.236341932415963
  - 5.3961783602833755
  validation_losses:
  - 0.5727989673614502
  - 0.5009616017341614
  - 0.5058324933052063
  - 0.6038699150085449
  - 22.51543426513672
  - 0.6365936398506165
  - 0.44741618633270264
  - 0.6510860323905945
  - 0.7079122066497803
  - 0.5927199125289917
  - 0.5669872164726257
  - 0.6973536014556885
  - 0.6727040410041809
  - 0.8758841156959534
  - 0.8478496670722961
  - 0.9162120223045349
  - 0.6192737221717834
  - 1.2326186895370483
  - 1.0293073654174805
  - 1.1407142877578735
  - 0.7656640410423279
  - 1.2596172094345093
  - 0.9941789507865906
  - 0.7631680369377136
  - 1.3121840953826904
  - 1.1238902807235718
  - 3.110017776489258
  - 1.3530627489089966
  - 1.2379542589187622
  - 0.6101528406143188
  - 3.6036715507507324
  - 5.155635833740234
  - 0.9814296364784241
  - 1.9908061027526855
  - 1.711105227470398
  - 2.6838648319244385
  - 1.0695741176605225
  - 1.1829487085342407
  - 1.064775824546814
  - 3.2198357582092285
  - 3.9011335372924805
  - 18.037715911865234
  - 1.3434635400772095
  - 0.9613039493560791
  - 0.7844775319099426
  - 1.5576390027999878
  - 2.301239490509033
  - 0.6956328749656677
  - 0.8853543400764465
  - 0.7586216330528259
  - 0.6332324147224426
  - 0.6513369679450989
  - 0.7639508247375488
  - 1.3412531614303589
  - 0.8756586313247681
  - 0.778289258480072
  - 5.179874420166016
  - 1.5379163026809692
  - 9.58521556854248
  - 0.8638337254524231
  - 0.9134018421173096
  - 0.4390406906604767
  - 0.6272466778755188
  - 0.48277518153190613
  - 0.9658488035202026
  - 1.0286496877670288
  - 0.5657006502151489
  - 0.916806697845459
  - 0.6574587821960449
  - 0.6481000185012817
  - 0.5071535706520081
  - 0.5446168184280396
  - 0.6964836120605469
  - 0.7347581386566162
  - 0.8401697874069214
  - 0.6682560443878174
  - 0.6368613839149475
  - 1.0540770292282104
  - 3.092712163925171
  - 0.4757632911205292
  - 0.5303928852081299
  - 1.4401079416275024
  - 0.5285550355911255
  - 2.9963245391845703
  - 0.4894042909145355
  - 0.9004153609275818
  - 2.44453763961792
  - 1.87847101688385
  - 0.8214302062988281
  - 0.8514523506164551
  - 0.7180314064025879
  - 1.1041818857192993
  - 1.1300376653671265
  - 0.8026781678199768
  - 1.1194283962249756
  - 1.481645941734314
  - 1.111725091934204
  - 1.9571067094802856
  - 0.7931508421897888
  - 0.599380612373352
loss_records_fold3:
  train_losses:
  - 5.359832739830018
  - 5.264925929903985
  - 5.429235842823982
  - 5.347774296998978
  - 5.338229575753212
  - 5.270567497611046
  - 5.323336836695671
  - 5.291122072935105
  - 5.267144256830216
  - 5.275235721468926
  - 5.289725002646446
  - 5.250195425748825
  - 5.2771467313170435
  - 5.323609256744385
  - 5.341805517673492
  - 5.368109753727913
  - 5.334299659729004
  - 5.22370982915163
  - 5.339886358380318
  - 5.31097632944584
  - 5.18736498951912
  - 5.435006502270699
  - 5.2640267521142965
  - 5.260026299953461
  - 5.222243225574494
  - 5.3254190325737
  - 5.392534446716309
  - 5.2435914278030396
  - 5.271397995948792
  - 5.309219205379486
  - 5.302804386615754
  - 5.396763136982918
  - 5.2649197518825535
  - 5.305987912416459
  - 5.2846986055374146
  - 5.226716431975365
  - 5.315222588181496
  - 5.183075091242791
  - 5.268404287099838
  - 5.300190046429634
  - 5.270110654830933
  - 5.284806448221207
  - 5.3399828493595125
  - 5.236558668315411
  - 5.284708839654923
  - 5.323968917131424
  - 5.180260446667671
  - 5.2753873616456985
  - 5.274028009176255
  - 5.2798938870429994
  - 5.265874579548836
  - 5.384850731492043
  - 5.4304452538490295
  - 5.289556372165681
  - 5.276837915182114
  - 5.188431856036186
  - 5.186301875114442
  - 5.252496466040611
  - 5.331104889512062
  - 5.2831092357635505
  - 5.245619544386864
  - 5.246566763520241
  - 5.361180374026299
  - 5.303715559840203
  - 5.3100626349449165
  - 5.264528089761734
  - 5.192611917853355
  - 5.301628151535988
  - 5.253544175624848
  - 5.321200680732727
  - 5.406377863883972
  - 5.413601514697075
  - 5.387136712670326
  - 5.283226329088212
  - 5.410451000928879
  - 5.249793814122677
  - 5.322037249803543
  - 5.268249806761742
  - 5.24368656873703
  - 5.234871527552605
  - 5.270014765858651
  - 5.302352246642113
  - 5.217945045232773
  - 5.189674282073975
  - 5.367756938934327
  - 5.346025988459587
  - 5.254881313443184
  - 5.1996463477611545
  - 5.277753058075906
  - 5.540136128664017
  - 5.288743218779564
  - 5.353230077028275
  - 5.295208731293679
  - 5.458680301904678
  - 5.194443629682064
  - 5.208393573760986
  - 5.2764894515275955
  - 5.177698874473572
  - 5.238824933767319
  - 5.348112341761589
  validation_losses:
  - 6.720163345336914
  - 0.7210568189620972
  - 2.0600099563598633
  - 1.4763189554214478
  - 4.722748279571533
  - 4.841968059539795
  - 3.426433801651001
  - 9.560043334960938
  - 5.377510070800781
  - 7.610754489898682
  - 9.00242805480957
  - 1.4450253248214722
  - 1.060955286026001
  - 0.8895971775054932
  - 13.221264839172363
  - 2.096013069152832
  - 2.3798439502716064
  - 0.8708232641220093
  - 2.870497941970825
  - 36.860992431640625
  - 4.423129081726074
  - 0.9434177279472351
  - 0.4753530025482178
  - 1.9302608966827393
  - 2.256885528564453
  - 1.2949763536453247
  - 0.8460140824317932
  - 0.96787428855896
  - 0.6807563900947571
  - 1.188434362411499
  - 0.770043671131134
  - 0.5484530329704285
  - 0.7829518914222717
  - 1.3895184993743896
  - 1.1262730360031128
  - 0.6477010250091553
  - 0.4876099228858948
  - 0.5598217844963074
  - 0.9856523871421814
  - 1.2192835807800293
  - 0.906292736530304
  - 5.121551036834717
  - 0.6957659721374512
  - 5.880563735961914
  - 4.941734313964844
  - 1.7910792827606201
  - 10.084723472595215
  - 3.836317777633667
  - 1.0271275043487549
  - 6.900726795196533
  - 1.2910680770874023
  - 0.6350199580192566
  - 8.240623474121094
  - 0.6084285974502563
  - 0.48629412055015564
  - 0.9667844176292419
  - 4.579501152038574
  - 13.740293502807617
  - 5.950888156890869
  - 4.200933933258057
  - 5.543081760406494
  - 1.2506840229034424
  - 0.8342682719230652
  - 1.0997991561889648
  - 1.528947114944458
  - 2.7293272018432617
  - 1.5944606065750122
  - 15.462996482849121
  - 0.5882191061973572
  - 1.0770668983459473
  - 1.3203320503234863
  - 4.503475666046143
  - 2.9339003562927246
  - 5.028470993041992
  - 5.155464172363281
  - 3.743567943572998
  - 1.3616702556610107
  - 5.357167720794678
  - 12.135886192321777
  - 2.6741912364959717
  - 1.4095770120620728
  - 2.767174482345581
  - 5.390641212463379
  - 1.8137370347976685
  - 0.43394935131073
  - 5.9694719314575195
  - 3.3448307514190674
  - 5.740847587585449
  - 10.563170433044434
  - 0.860979437828064
  - 2.7458174228668213
  - 3.652625560760498
  - 0.6737582087516785
  - 6.643087863922119
  - 11.186975479125977
  - 3.724942445755005
  - 0.7832146883010864
  - 0.9775118827819824
  - 4.966343879699707
  - 0.5099647045135498
loss_records_fold4:
  train_losses:
  - 5.418484953045845
  - 5.304895040392876
  - 5.342055726051331
  - 5.3328790932893755
  - 5.2025584250688555
  - 5.300256013870239
  - 5.2412804663181305
  - 5.220022255182267
  - 5.2229146540164955
  - 5.293576806783676
  - 5.373568779230118
  - 5.211999741196633
  - 5.201590538024902
  - 5.212271493673325
  - 5.224666282534599
  - 5.202414587140083
  - 5.312565290927887
  - 5.278695878386498
  - 5.319143310189247
  - 5.244014976918698
  - 5.391012555360795
  - 5.343834632635117
  - 5.374515044689179
  - 5.328769645094872
  - 5.293289563059807
  - 5.230804553627968
  - 5.296860724687576
  - 5.2546156495809555
  - 5.278004539012909
  - 5.250251081585884
  - 5.307194477319718
  - 5.264834704995156
  - 5.317904087901116
  - 5.275577947497368
  - 5.27910884320736
  - 5.240877094864846
  - 5.277228721976281
  - 5.217147228121758
  - 5.244016939401627
  - 5.212699607014656
  - 5.156059849262238
  - 5.233580625057221
  - 5.2806881159543995
  - 5.41864921450615
  - 5.5683817520737655
  - 5.722024923563004
  - 5.32785074710846
  - 5.303449454903603
  - 5.409991332888604
  - 5.3055342674255375
  - 5.198729917407036
  - 5.2385168969631195
  - 5.285857585072518
  - 5.254963836073876
  - 5.225373905897141
  - 5.302652794122697
  - 5.274927321076394
  - 5.241075599193573
  - 5.272874674201012
  - 5.261586758494378
  - 5.3215141743421555
  - 5.26263544857502
  - 5.279028221964836
  - 5.2529232710599905
  - 5.24621157348156
  - 5.183290287852287
  - 5.223163333535195
  - 5.212080353498459
  - 5.081914508342743
  - 5.2678600490093235
  - 5.28121946156025
  - 5.278125968575478
  - 5.290030243992806
  - 5.216416242718697
  - 5.245906063914299
  - 5.28957102894783
  - 5.352727457880974
  - 5.360327655076981
  - 5.2246703743934635
  - 5.181845852732659
  - 5.176273694634438
  - 5.251777029037476
  - 5.362115216255188
  - 5.246510463953019
  - 5.1200331270694734
  - 5.153362241387367
  - 5.149826076626778
  - 5.3295624837279325
  - 5.171423321962357
  - 5.259677666425706
  - 5.230935844779015
  - 5.201394900679588
  - 5.154523941874505
  - 5.187895688414574
  - 5.632750177383423
  - 5.491104158759118
  - 5.391295985877514
  - 5.32039969265461
  - 5.282567343115807
  - 5.401429581642152
  validation_losses:
  - 0.6213290691375732
  - 0.6035259366035461
  - 0.7094845175743103
  - 0.722201406955719
  - 0.6126657724380493
  - 0.7389995455741882
  - 0.8367981910705566
  - 0.7079792618751526
  - 0.6392799615859985
  - 0.5478941202163696
  - 0.6581810116767883
  - 0.6165697574615479
  - 0.6237319111824036
  - 0.9530891180038452
  - 0.8754246234893799
  - 1.049405813217163
  - 0.6553292274475098
  - 0.5082988739013672
  - 0.5726960301399231
  - 0.4775099754333496
  - 0.48700475692749023
  - 0.4951290488243103
  - 0.4835217595100403
  - 0.5699830651283264
  - 1.0953882932662964
  - 2.105886936187744
  - 0.6527278423309326
  - 1.3851900100708008
  - 1.7211954593658447
  - 0.7948595285415649
  - 0.8478775024414062
  - 0.9209783673286438
  - 0.8660917282104492
  - 0.6443411707878113
  - 0.645401656627655
  - 0.9431408643722534
  - 3.407492160797119
  - 0.8719748854637146
  - 0.7749134302139282
  - 0.7131126523017883
  - 0.6349948048591614
  - 0.5798360109329224
  - 1.529866099357605
  - 0.7919632792472839
  - 0.411079078912735
  - 0.4683745801448822
  - 0.5286707878112793
  - 1.9966458082199097
  - 0.8018398880958557
  - 2.0235424041748047
  - 1.135586142539978
  - 2.7095260620117188
  - 4.3773322105407715
  - 6.142197132110596
  - 0.674292266368866
  - 0.5740680694580078
  - 1.0283300876617432
  - 1.1347355842590332
  - 0.617064893245697
  - 0.8068856000900269
  - 2.4963598251342773
  - 4.665404796600342
  - 0.6450341939926147
  - 0.5416819453239441
  - 0.5305287837982178
  - 0.9604138731956482
  - 0.5700324773788452
  - 0.5992234349250793
  - 0.6270512938499451
  - 0.47836944460868835
  - 0.5363626480102539
  - 0.5701335668563843
  - 0.6283913254737854
  - 0.5258473753929138
  - 0.4466496706008911
  - 0.5476744174957275
  - 8.747547149658203
  - 0.49342599511146545
  - 10.571447372436523
  - 8.17626667022705
  - 1.2556716203689575
  - 2.5328333377838135
  - 0.5028373003005981
  - 0.49038320779800415
  - 4.249143600463867
  - 6.861050605773926
  - 3.066988945007324
  - 20.583646774291992
  - 8.622655868530273
  - 0.9784810543060303
  - 1.8982681035995483
  - 5.084019184112549
  - 1.834349513053894
  - 1.3589835166931152
  - 0.3809064030647278
  - 0.37680384516716003
  - 0.48433148860931396
  - 0.4249902069568634
  - 0.4804117977619171
  - 0.6291114091873169
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8284734133790738, 0.8353344768439108, 0.8113207547169812,
    0.7869415807560137]'
  fold_eval_f1: '[0.0, 0.15254237288135594, 0.19999999999999998, 0.2361111111111111,
    0.24390243902439024]'
  mean_eval_accuracy: 0.8235975785868803
  mean_f1_accuracy: 0.16651118460337147
  total_train_time: '0:45:12.241048'
