config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.860341'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_8fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_8fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_8fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_8fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_8fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_8fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_8fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_8fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_8fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_8fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 59.68316704630852
  - 17.278575653582813
  - 23.07617486715317
  - 17.089141586422922
  - 12.386475571990013
  - 13.328631955385209
  - 21.11838382780552
  - 11.698173063993455
  - 9.821334357559682
  - 8.851746739447117
  - 15.211904913187027
  - 10.891028410196306
  - 9.456661334633827
  - 6.561236265301705
  - 8.099122107028961
  - 6.694342210888863
  - 7.501496344804764
  - 8.120702849328518
  - 8.98063039779663
  - 8.78212463259697
  - 6.942605730891228
  - 7.73045294880867
  - 7.954962354898453
  - 6.979975065588952
  - 6.447499373555184
  - 6.362234672904015
  - 6.512432596087456
  - 6.578584855794907
  - 6.250212439894677
  - 8.192835173010826
  - 5.815213826298714
  - 6.784975790977478
  - 6.315478092432023
  - 6.883955937623978
  - 6.120106634497643
  - 6.369568729400635
  - 6.492193880677224
  - 6.13160991370678
  - 6.186349406838417
  - 6.097211115062237
  - 5.873585000634193
  - 5.859802985191346
  - 6.487460511922837
  - 5.888837012648583
  - 5.897381207346917
  - 5.797211572527885
  - 5.937743380665779
  - 6.263569766283036
  - 5.892179125547409
  - 5.796927943825722
  - 5.926032254099846
  - 5.907013419270516
  - 5.737147974967957
  - 5.818009904026986
  - 6.143027073144913
  - 6.037526533007622
  - 5.765482115745545
  - 5.8274513483047485
  - 5.899867433309556
  - 5.87470364868641
  - 6.072112029790879
  - 5.7747863084077835
  - 6.137617307901383
  - 5.875317579507828
  - 5.968524146080018
  - 5.783023351430893
  - 5.8598654001951225
  - 5.843256032466889
  - 6.030642035603524
  - 5.912299302220345
  - 5.840451136231422
  - 6.056714555621148
  - 6.096753591299057
  - 6.143484061956406
  - 6.558820122480393
  - 6.571246692538262
  - 6.221973162889481
  - 5.953493431210518
  - 6.360853073000908
  - 6.357852509617806
  - 5.881048187613487
  - 6.051785615086556
  - 5.826086407899857
  - 5.881823894381523
  - 5.894030122458935
  - 5.898984664678574
  - 6.362925817072392
  - 5.900811669230461
  - 5.8142729878425605
  - 5.943926313519478
  - 6.104869368672372
  - 6.513943958282471
  - 6.6228897184133535
  - 6.237394377589226
  - 6.291391623020172
  - 5.990148396790028
  - 5.973682630062104
  - 5.927494820952416
  - 6.059947186708451
  - 5.913290137052536
  validation_losses:
  - 7.545292377471924
  - 1.4281439781188965
  - 0.5146527886390686
  - 1.5423827171325684
  - 0.7485145330429077
  - 0.39388519525527954
  - 1.357088565826416
  - 0.4823967516422272
  - 0.5385070443153381
  - 0.6113551259040833
  - 0.4177258312702179
  - 0.5514801144599915
  - 0.3846515715122223
  - 0.40195006132125854
  - 0.38560792803764343
  - 0.38539594411849976
  - 0.5228040218353271
  - 0.3915574848651886
  - 0.4084540903568268
  - 0.3987548351287842
  - 0.39344844222068787
  - 0.41645127534866333
  - 0.4302169382572174
  - 0.3912467658519745
  - 0.44167354702949524
  - 0.4033755362033844
  - 0.38629376888275146
  - 0.4047009348869324
  - 0.38986635208129883
  - 0.3916214108467102
  - 0.387008935213089
  - 0.4005574584007263
  - 0.39182910323143005
  - 0.3898674249649048
  - 0.39270249009132385
  - 0.3913283348083496
  - 0.41976648569107056
  - 0.38769224286079407
  - 0.46115240454673767
  - 0.38672903180122375
  - 0.3858395218849182
  - 2.577794313430786
  - 0.38433966040611267
  - 0.586125910282135
  - 0.7130031585693359
  - 0.3875507414340973
  - 0.5151818990707397
  - 0.3931449353694916
  - 0.7297268509864807
  - 0.3945927321910858
  - 0.42229926586151123
  - 0.5031552314758301
  - 0.3874346613883972
  - 0.4352594316005707
  - 0.4569113850593567
  - 0.3831665813922882
  - 0.3825405240058899
  - 0.41705381870269775
  - 0.5817728638648987
  - 0.37625280022621155
  - 0.46922624111175537
  - 0.3828972578048706
  - 0.39217814803123474
  - 0.37973201274871826
  - 0.4043729305267334
  - 0.4104086756706238
  - 0.380781352519989
  - 0.5558689832687378
  - 0.37807098031044006
  - 0.41460585594177246
  - 0.3802700340747833
  - 85.3729248046875
  - 0.3924042284488678
  - 0.3978717029094696
  - 0.47959667444229126
  - 0.4333352744579315
  - 0.45741036534309387
  - 0.5152527093887329
  - 4.248350620269775
  - 0.3898518979549408
  - 0.47063693404197693
  - 0.39880499243736267
  - 0.4516257345676422
  - 0.3922031819820404
  - 2363.8798828125
  - 2261.6201171875
  - 0.7318857312202454
  - 0.398293137550354
  - 309.3612976074219
  - 7117.6533203125
  - 32039.119140625
  - 0.5277296304702759
  - 0.39790427684783936
  - 0.43253064155578613
  - 3.9854278564453125
  - 0.3960101306438446
  - 0.44851577281951904
  - 0.39111122488975525
  - 0.41089925169944763
  - 0.40340954065322876
loss_records_fold1:
  train_losses:
  - 6.131963115930557
  - 5.973177033662797
  - 5.909045159816742
  - 5.811529603600502
  - 5.863802662491799
  - 5.837722152471542
  - 5.867970386147499
  - 5.815530574321747
  - 6.050182461738586
  - 6.01048369705677
  - 5.864615944027901
  - 5.851984766125679
  - 5.829112607240678
  - 5.864633932709694
  - 5.815565097332001
  - 5.790358358621598
  - 5.995869141817093
  - 5.940000838041306
  - 5.805294999480248
  - 6.10877178311348
  - 6.002982178330422
  validation_losses:
  - 0.42112997174263
  - 0.4249260425567627
  - 0.44078800082206726
  - 0.42435917258262634
  - 0.5000697374343872
  - 0.42518484592437744
  - 0.41008460521698
  - 0.404842734336853
  - 0.40842491388320923
  - 0.4483319818973541
  - 0.410541296005249
  - 0.4118862748146057
  - 0.407142698764801
  - 0.4082554876804352
  - 0.41844433546066284
  - 0.4080038368701935
  - 0.4074160158634186
  - 0.4106508791446686
  - 0.4205794930458069
  - 0.41165891289711
  - 0.4059884250164032
loss_records_fold2:
  train_losses:
  - 5.92378141283989
  - 6.083991914987564
  - 5.943011340498924
  - 6.0871687024831775
  - 6.124182051420212
  - 6.018748492002487
  - 6.101027517020703
  - 6.130063745379449
  - 5.979891896247864
  - 6.017581653594971
  - 5.905442771315575
  - 5.9433502674102785
  - 5.98869036436081
  - 5.973293721675873
  - 6.040039971470833
  - 5.912770918011666
  - 5.981259965896607
  - 6.226298961043359
  - 5.863780510425568
  - 6.051426362991333
  - 5.900555917620659
  - 6.071523433923722
  - 5.9662232309579855
  - 6.347929103672505
  - 6.026309172809125
  - 6.171782238781453
  - 6.423121640086174
  - 6.285006171464921
  - 6.343232792615891
  - 5.963501685857773
  - 5.987435767054558
  - 5.912203305959702
  - 5.879726439714432
  - 5.881470212340355
  - 5.986787766218185
  - 6.066460290551186
  - 5.959474959969521
  - 6.024385842680932
  - 5.938653090596199
  - 5.995510789752007
  - 5.9672669082880025
  - 5.984987071156502
  - 5.938704258203507
  - 6.0012897014617925
  - 6.0436855256557465
  - 6.081037256121636
  - 6.119257268309593
  - 6.166541489958764
  - 5.962578904628754
  - 6.192963087558747
  - 6.248444509506226
  - 5.985422593355179
  - 5.96064992249012
  - 5.893334205448628
  - 5.942249110341073
  - 5.908551505208016
  - 5.9201388478279116
  - 6.005650594830513
  - 5.896616932749748
  - 5.991297236084939
  - 6.292630064487458
  - 6.066550892591477
  - 6.133794334530831
  - 6.004946446418763
  - 5.95670340359211
  - 5.935976180434228
  - 6.051432684063911
  - 6.1177513837814335
  - 5.967890027165414
  - 5.914930364489556
  - 5.965753933787346
  - 5.970383909344673
  - 5.9752831876277925
  - 6.325230896472931
  - 5.887941811978817
  - 6.069798532128335
  - 5.990840530395508
  - 5.933268910646439
  - 5.960687747597695
  - 6.159921583533287
  - 6.076782912015915
  - 6.404899707436562
  - 5.8907909244298935
  - 5.980686420202256
  - 5.892432215809823
  - 5.9362193793058395
  - 5.943835258483887
  - 5.966075757145882
  - 6.2138879060745245
  validation_losses:
  - 0.385313481092453
  - 0.38600608706474304
  - 0.3849908709526062
  - 2435776.25
  - 0.42048880457878113
  - 0.38394567370414734
  - 0.3849774897098541
  - 0.43087634444236755
  - 0.41861727833747864
  - 0.38692960143089294
  - 0.38445165753364563
  - 0.41221338510513306
  - 0.3897189199924469
  - 1.336799144744873
  - 0.4091689884662628
  - 0.4220094084739685
  - 0.4022551476955414
  - 0.38335469365119934
  - 0.3937954306602478
  - 0.4567582309246063
  - 0.40456217527389526
  - 0.38403400778770447
  - 0.4274131953716278
  - 5105.30224609375
  - 6469.740234375
  - 33000528.0
  - 0.4335690438747406
  - 1550096768.0
  - 1837733120.0
  - 0.38472339510917664
  - 420890240.0
  - 578721280.0
  - 734387328.0
  - 648091456.0
  - 485637568.0
  - 735334848.0
  - 710964480.0
  - 643443840.0
  - 708339584.0
  - 702005568.0
  - 710139904.0
  - 795910144.0
  - 665146176.0
  - 708522048.0
  - 848684032.0
  - 744387456.0
  - 554997056.0
  - 684972736.0
  - 607907200.0
  - 614479296.0
  - 7652732928.0
  - 1253833600.0
  - 532595904.0
  - 247799328.0
  - 305606080.0
  - 227590000.0
  - 302825120.0
  - 433679392.0
  - 240152864.0
  - 229729888.0
  - 395121120.0
  - 383054880.0
  - 374496832.0
  - 387917216.0
  - 424895104.0
  - 319822048.0
  - 327170816.0
  - 280633760.0
  - 314746656.0
  - 317542048.0
  - 420322784.0
  - 330374240.0
  - 316250944.0
  - 457663552.0
  - 317261408.0
  - 389840352.0
  - 196477216.0
  - 259360912.0
  - 241165264.0
  - 331970944.0
  - 33877825536.0
  - 0.38351932168006897
  - 0.39581018686294556
  - 0.38783231377601624
  - 0.39393356442451477
  - 0.39235395193099976
  - 0.3935735523700714
  - 0.3921736180782318
  - 0.3902846574783325
loss_records_fold3:
  train_losses:
  - 5.872271770238877
  - 5.814936417341233
  - 6.199608200788498
  - 6.108526909351349
  - 6.122496476769448
  - 5.863869261741638
  - 6.05387569963932
  - 5.825873489677907
  - 5.882344757020474
  - 5.84074189066887
  - 5.956715485453606
  - 5.812941405177117
  - 5.9705847889184955
  - 6.029611217975617
  validation_losses:
  - 0.3982483446598053
  - 0.421955406665802
  - 0.45974183082580566
  - 0.4649547338485718
  - 0.39914315938949585
  - 0.4348439872264862
  - 0.39848819375038147
  - 0.41907963156700134
  - 0.40942326188087463
  - 0.40300798416137695
  - 0.398272842168808
  - 0.39925986528396606
  - 0.4082569479942322
  - 0.3982265591621399
loss_records_fold4:
  train_losses:
  - 5.955070118606091
  - 6.030497518181801
  - 5.861205971240998
  - 6.0054567694664005
  - 5.940986141562462
  - 5.951433077454567
  - 6.036560696363449
  - 5.916076135635376
  - 6.1180167436599735
  - 5.8535185128450395
  - 6.151763930916786
  - 5.843851602077485
  - 6.095462787151337
  - 6.185975632071496
  - 5.840646788477898
  - 5.956271556019783
  - 5.92772940993309
  - 8.276370367407798
  - 6.014030432701111
  - 5.908562433719635
  - 5.86392002105713
  - 5.940975253283978
  - 6.055991241335869
  - 6.315443266928196
  - 5.927981415390969
  - 6.1277332052588465
  - 6.139261478185654
  - 6.071473422646523
  - 5.840971836447716
  - 5.98478094637394
  - 5.9122856825590135
  - 5.900393977761269
  - 5.907441763579846
  - 5.950554901361466
  - 6.0057548731565475
  - 6.0348759099841125
  - 5.969649803638458
  - 6.039689677953721
  - 6.041205832362175
  - 5.954932861030102
  - 5.940444201231003
  - 5.927541805803776
  - 6.203848358243704
  - 6.118523651361466
  - 6.031156671047211
  - 5.8481740921735765
  - 5.9160656303167345
  - 6.032435563206673
  - 5.927542421221734
  - 6.296488538384438
  - 5.928702622652054
  - 5.880753774940968
  - 5.932873845100403
  - 5.931981331110001
  - 6.1220506787300115
  - 6.079732975363732
  - 5.914174813032151
  - 5.834601187705994
  - 5.9672762751579285
  - 6.089104023575783
  validation_losses:
  - 0.40157797932624817
  - 0.4058809280395508
  - 0.3994419574737549
  - 7703340032.0
  - 0.3927837312221527
  - 0.39706045389175415
  - 0.39719802141189575
  - 0.39464330673217773
  - 2408931584.0
  - 0.40942659974098206
  - 0.401153028011322
  - 0.39245742559432983
  - 0.41300836205482483
  - 0.44462329149246216
  - 0.3946080803871155
  - 0.3963615894317627
  - 0.4138326942920685
  - 6390369792.0
  - 1203057280.0
  - 0.3979758024215698
  - 0.39197683334350586
  - 0.3919067680835724
  - 0.43238288164138794
  - 0.392119437456131
  - 0.39918243885040283
  - 0.3936324715614319
  - 38472318976.0
  - 0.39418843388557434
  - 0.4041191637516022
  - 0.393214613199234
  - 0.39467042684555054
  - 0.39453524351119995
  - 0.42059972882270813
  - 0.3919016122817993
  - 0.41881316900253296
  - 0.39120036363601685
  - 0.3916633427143097
  - 0.44336646795272827
  - 0.43097811937332153
  - 0.3915453851222992
  - 0.3912436366081238
  - 48040464384.0
  - 0.45127832889556885
  - 0.4285789728164673
  - 0.3923729956150055
  - 0.3909367024898529
  - 0.4103604853153229
  - 0.3917659819126129
  - 0.3958176374435425
  - 0.3916163146495819
  - 0.4157526195049286
  - 0.4051404297351837
  - 0.39866703748703003
  - 0.44634848833084106
  - 0.40244993567466736
  - 0.3951285183429718
  - 0.39215314388275146
  - 0.3943338692188263
  - 0.39890119433403015
  - 0.3940503001213074
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 89 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 60 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:28:09.318591'
