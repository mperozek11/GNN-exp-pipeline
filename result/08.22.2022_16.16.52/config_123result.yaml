config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 19:03:11.889804'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_123fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_123fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_123fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_123fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_123fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_123fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_123fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_123fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_123fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_123fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 13.9268363237381
  - 7.411540460586548
  - 4.235148406028748
  - 7.942461311817169
  - 6.782427752017975
  - 3.3789441704750063
  - 1.522471046447754
  - 1.2972461491823197
  - 1.193665212392807
  - 1.0536995112895966
  - 1.0698108434677125
  - 1.0884155273437501
  - 1.0828025341033936
  - 4.247206628322601
  - 5.025880205631257
  - 2.3845067381858827
  - 4.361731952428818
  - 1.969879221916199
  - 1.8668885231018066
  - 1.9195541322231293
  - 1.2062578558921815
  - 1.8282618522644043
  - 2.471542376279831
  - 0.9963731288909913
  - 1.3019370019435883
  - 1.0534561097621917
  - 6.254206496477128
  - 3.363554149866104
  - 4.402253890037537
  - 1.5625960528850555
  - 1.375416523218155
  - 2.1027216136455538
  - 1.2007733047008515
  - 1.1223660707473755
  - 1.2997889697551728
  - 9.508338552713395
  - 1.5553354501724244
  - 2.1318953812122348
  - 1.3172915577888489
  - 1.4323275089263916
  - 1.752967154979706
  - 7.650012791156769
  - 1.2588869124650957
  - 1.0008985877037049
  - 1.057274580001831
  - 1.1382195353507996
  - 0.9788170099258423
  - 1.206443327665329
  - 0.9335868537425995
  - 0.8906112015247345
  - 2.3964566230773925
  - 1.6199339985847474
  - 3.20022206902504
  - 4.552600347995758
  - 1.2351938158273699
  - 1.9253514230251314
  - 4.698408168554306
  - 5.594029891490937
  - 1.1601127326488496
  - 1.372479033470154
  - 1.2246598422527315
  - 1.407598638534546
  - 1.2550775408744812
  - 0.9845415472984315
  - 0.9101828277111054
  - 0.8219399988651276
  - 0.8661940038204193
  - 0.9506408214569092
  - 0.8156667292118073
  - 0.8479804396629333
  - 0.8524815201759339
  - 0.8710213780403138
  validation_losses:
  - 29.840091705322266
  - 55.599159240722656
  - 7.754653453826904
  - 1.8845739364624023
  - 0.5689524412155151
  - 0.6250380277633667
  - 0.5864529013633728
  - 0.5688807368278503
  - 0.47287967801094055
  - 0.4671975374221802
  - 0.44920045137405396
  - 0.6194869875907898
  - 0.39699438214302063
  - 4.865604877471924
  - 0.6972886919975281
  - 1.8112297058105469
  - 1.1197526454925537
  - 0.9318640828132629
  - 0.5115703344345093
  - 0.8339826464653015
  - 0.6197678446769714
  - 0.6356523036956787
  - 0.44748061895370483
  - 0.40968552231788635
  - 0.39189624786376953
  - 0.4687701165676117
  - 1.3172153234481812
  - 1.3551959991455078
  - 0.6750688552856445
  - 0.5452768206596375
  - 0.6588228344917297
  - 0.42001616954803467
  - 0.47165587544441223
  - 0.6030269265174866
  - 0.43833866715431213
  - 0.41014233231544495
  - 0.46933168172836304
  - 0.5386987924575806
  - 0.502288281917572
  - 0.3898344933986664
  - 0.4515879452228546
  - 0.45234227180480957
  - 0.4022211730480194
  - 0.548376739025116
  - 0.4104623794555664
  - 0.4562528729438782
  - 0.41289064288139343
  - 0.45063167810440063
  - 0.42371660470962524
  - 0.43022698163986206
  - 0.45631396770477295
  - 0.4757697880268097
  - 0.5254078507423401
  - 0.4358769357204437
  - 0.5354021787643433
  - 0.4732835292816162
  - 0.4317356050014496
  - 0.4803312122821808
  - 0.6390566229820251
  - 0.4827614426612854
  - 0.42767223715782166
  - 0.4206998646259308
  - 0.5552805066108704
  - 0.40324777364730835
  - 0.38843879103660583
  - 0.40115630626678467
  - 0.3956560492515564
  - 0.3870784342288971
  - 0.3925083577632904
  - 0.39884865283966064
  - 0.3875750005245209
  - 0.3897913098335266
loss_records_fold1:
  train_losses:
  - 0.7871113479137422
  - 0.7620363682508469
  - 0.7507384389638901
  - 0.8230245232582093
  - 0.7981900274753571
  - 0.849268925189972
  - 0.7980951070785522
  - 0.8653079569339752
  - 1.3069445729255678
  - 1.0837035536766053
  - 0.8479916304349899
  - 0.8643155336380005
  - 0.7872083425521851
  - 0.8093014597892761
  - 0.7647743582725526
  - 0.8023607909679413
  validation_losses:
  - 0.4192122519016266
  - 0.40197306871414185
  - 0.40320059657096863
  - 0.4100763201713562
  - 0.4006102681159973
  - 0.39906010031700134
  - 0.4104601740837097
  - 0.43663814663887024
  - 0.4153538942337036
  - 0.5155693292617798
  - 0.39884456992149353
  - 0.40089190006256104
  - 0.4063909947872162
  - 0.40156811475753784
  - 0.4040411710739136
  - 0.4019189774990082
loss_records_fold2:
  train_losses:
  - 0.7704798787832261
  - 0.8444339752197266
  - 0.8079655766487122
  - 0.8060796201229096
  - 0.8213086783885957
  - 0.8651133835315705
  - 0.9707911014556885
  - 0.9603923618793488
  - 1.5180539786815643
  - 1.1357556402683258
  - 0.9209982991218567
  - 0.8584232866764069
  - 0.8445678353309631
  - 1.8651869535446168
  - 6.837735444307327
  - 1.5212032914161684
  - 0.8515609502792358
  - 0.8802292704582215
  - 0.7924914836883545
  - 0.8458607375621796
  - 1.6135981857776642
  - 1.6131141543388368
  - 1.0398392081260681
  - 2.6575559794902803
  - 1.0194810211658478
  - 1.251910996437073
  - 0.8647618889808655
  - 1.0671750068664552
  - 0.8719163656234742
  - 0.8720861852169037
  - 0.9152478992938996
  - 0.8772060215473175
  - 0.8731842458248139
  - 0.8364488303661347
  - 0.91594398021698
  - 1.8494859516620636
  - 1.581825876235962
  - 3.63486025929451
  - 8.661634123325348
  - 1.5436121702194214
  - 1.6336190104484558
  - 1.2003636896610261
  - 1.9292567908763887
  - 1.4267494678497314
  - 6.031050121784211
  - 0.9355408966541291
  - 0.9518917918205262
  - 0.9139716088771821
  - 0.7882889151573181
  - 0.7847634971141816
  - 1.0377091586589813
  - 0.8199619054794312
  - 0.7592599898576737
  - 1.0947895586490632
  - 0.8916267335414887
  - 0.7896749675273895
  - 0.8438438475131989
  - 0.854787403345108
  - 0.8593496143817902
  - 0.9252285778522492
  - 3.1912067890167237
  - 1.417584264278412
  - 0.9379240810871124
  - 1.013370031118393
  - 0.8972183495759964
  - 1.020138567686081
  - 0.8031443357467651
  - 0.8695601403713227
  - 0.872546148300171
  - 0.8380080699920655
  - 0.8391569614410401
  validation_losses:
  - 0.38199368119239807
  - 0.3876187801361084
  - 0.37973129749298096
  - 0.3786980211734772
  - 0.3954034447669983
  - 0.3808746337890625
  - 0.38258686661720276
  - 0.3887469172477722
  - 0.3998209834098816
  - 0.3875904083251953
  - 0.3822284936904907
  - 0.38428452610969543
  - 0.46324291825294495
  - 0.3882669508457184
  - 0.39066553115844727
  - 0.3830145597457886
  - 0.39944738149642944
  - 0.3916946351528168
  - 0.38302671909332275
  - 0.384757936000824
  - 1.1091850996017456
  - 0.3864724636077881
  - 0.40130332112312317
  - 0.42878031730651855
  - 0.39818164706230164
  - 0.39989158511161804
  - 0.3881373107433319
  - 0.38340070843696594
  - 0.38158220052719116
  - 0.40676555037498474
  - 0.39337533712387085
  - 0.3848242163658142
  - 0.3945544958114624
  - 0.3964717388153076
  - 0.4083016514778137
  - 0.4422987103462219
  - 0.3904763460159302
  - 0.3896426260471344
  - 0.3892149329185486
  - 0.3804994225502014
  - 0.39652949571609497
  - 2.165418863296509
  - 2.1235480308532715
  - 0.3893052041530609
  - 0.3800983428955078
  - 0.46976542472839355
  - 1.9577620029449463
  - 0.38273563981056213
  - 0.3825847804546356
  - 0.3811410665512085
  - 0.393938809633255
  - 0.3861730098724365
  - 0.3845330774784088
  - 0.3936520218849182
  - 0.38791459798812866
  - 0.3845476508140564
  - 0.405855268239975
  - 0.38037362694740295
  - 0.3820347189903259
  - 0.3819524049758911
  - 0.670978307723999
  - 0.3802964985370636
  - 0.3896503448486328
  - 0.38929519057273865
  - 0.4038844108581543
  - 0.3841305077075958
  - 0.38799166679382324
  - 0.38000431656837463
  - 0.38100355863571167
  - 0.38258421421051025
  - 0.3816785514354706
loss_records_fold3:
  train_losses:
  - 0.7966089129447937
  - 0.7707426965236664
  - 0.7956280231475831
  - 0.7975373268127441
  - 0.8390704870224
  - 0.9866182506084442
  - 0.8612117409706116
  - 1.28166241645813
  - 0.7927860200405121
  - 0.7679609298706055
  - 0.7430002570152283
  validation_losses:
  - 0.392758309841156
  - 0.3987865447998047
  - 0.3946065306663513
  - 0.39418116211891174
  - 0.39600005745887756
  - 0.40185558795928955
  - 0.40987181663513184
  - 0.4038291871547699
  - 0.40019696950912476
  - 0.396807998418808
  - 0.3968189060688019
loss_records_fold4:
  train_losses:
  - 0.7994933664798737
  - 0.8171688914299011
  - 0.7952538907527924
  - 0.7525410801172256
  - 0.8107347428798676
  - 0.8831440448760987
  - 0.815953665971756
  - 0.8357805013656616
  - 0.8115766942501068
  - 0.8001419723033906
  - 0.8084727883338929
  validation_losses:
  - 0.3915867209434509
  - 0.3910475969314575
  - 0.3910609483718872
  - 0.39445459842681885
  - 0.405744343996048
  - 0.3970266580581665
  - 0.3991268277168274
  - 0.39136141538619995
  - 0.39150571823120117
  - 0.3922860026359558
  - 0.39313384890556335
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 72 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 71 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.855917667238422, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.0
  total_train_time: '0:15:51.103501'
