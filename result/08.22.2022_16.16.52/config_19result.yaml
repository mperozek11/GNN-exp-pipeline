config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:33:16.244607'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_19fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_19fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_19fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_19fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_19fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_19fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_19fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_19fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_19fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_19fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.0372177064418793
  - 0.8087092995643617
  - 0.8186253786087037
  - 0.8044313490390778
  - 0.806606662273407
  - 0.7716553926467896
  - 0.7780569672584534
  - 0.8109171032905579
  - 0.7572802782058716
  - 0.7402989238500596
  - 0.7567849516868592
  - 0.7630390226840973
  - 0.8189492523670197
  - 0.8104197323322296
  - 0.8299470603466035
  - 0.7808118999004364
  - 0.7725519061088563
  - 0.7561542212963105
  validation_losses:
  - 0.41075992584228516
  - 0.4053939878940582
  - 0.3999246656894684
  - 0.3957534730434418
  - 0.39300498366355896
  - 0.3930485248565674
  - 0.38836073875427246
  - 0.40032869577407837
  - 0.38488295674324036
  - 0.3810460865497589
  - 0.38219383358955383
  - 0.43434226512908936
  - 0.38201668858528137
  - 0.38826265931129456
  - 0.3903824985027313
  - 0.39142492413520813
  - 0.38496455550193787
  - 0.38718998432159424
loss_records_fold1:
  train_losses:
  - 0.7446587502956391
  - 0.735964259505272
  - 0.7363286584615708
  - 0.7866236329078675
  - 0.7365273654460908
  - 0.7439447700977326
  - 0.8446785271167756
  - 0.7769316077232361
  - 0.7590695321559906
  - 0.7376426666975022
  - 0.7621585786342622
  - 0.7525404274463654
  - 0.7540689170360566
  validation_losses:
  - 0.3914148509502411
  - 0.3896014094352722
  - 0.389767587184906
  - 0.3906855881214142
  - 0.39008772373199463
  - 0.38742363452911377
  - 0.40039440989494324
  - 0.3924133777618408
  - 0.39013510942459106
  - 0.3885810971260071
  - 0.38918304443359375
  - 0.392769992351532
  - 0.38945844769477844
loss_records_fold2:
  train_losses:
  - 0.7570263385772705
  - 0.7538131773471832
  - 0.7809257745742798
  - 0.754290795326233
  - 0.7675806641578675
  - 0.7404806852340698
  - 0.7830327153205872
  - 0.7688975214958191
  - 0.7618793189525604
  - 0.7526800334453583
  - 0.7466482937335969
  - 0.8081270039081574
  - 0.7809888243675233
  - 0.7947850406169892
  - 0.7685535728931427
  - 0.743438071012497
  - 0.7544203817844392
  - 0.7213721692562104
  - 0.8264546573162079
  - 0.7834951281547546
  - 0.7620765924453736
  - 0.7566764235496521
  - 0.7113504141569138
  - 0.7705714106559753
  - 0.7525633275508881
  - 0.722469848394394
  - 0.7241118490695954
  - 0.7724549472332001
  - 0.7582291841506958
  - 0.7576717615127564
  - 0.8009728252887727
  - 0.7719356000423432
  - 0.7368191242218018
  - 0.7274336755275727
  - 0.7085523158311844
  - 0.7419092953205109
  - 0.7345371127128602
  - 0.7063713133335114
  - 0.7262501955032349
  - 0.7896654963493348
  - 0.7424935281276703
  - 0.7583489894866944
  - 0.774379450082779
  - 0.7244956254959107
  - 0.7318619906902314
  - 0.737426793575287
  - 0.751429432630539
  - 0.7169451147317887
  - 0.8132009327411652
  - 0.710935789346695
  - 0.7523372709751129
  - 0.7439055562019349
  - 0.7340148448944093
  - 0.7347383081912995
  - 0.7002447217702866
  - 0.7817119240760804
  - 0.7992694318294525
  - 0.7534137070178986
  - 0.7778815090656281
  - 0.729696261882782
  - 0.7198601424694062
  - 0.7213648319244386
  - 0.7376083433628082
  - 0.7413885831832886
  - 0.734533965587616
  - 0.7334271550178528
  - 0.7322629153728486
  - 0.7261306345462799
  - 0.7323360919952393
  - 0.754883998632431
  - 0.7425099670886994
  - 0.7327046871185303
  - 0.7019828587770462
  - 0.7273025393486023
  - 0.8064021229743958
  - 0.7526098787784576
  - 0.7357007443904877
  - 0.7245722591876984
  - 0.7419378399848938
  - 0.7402571499347688
  - 0.7923605978488922
  - 0.7353844225406647
  - 0.7137158274650575
  - 0.7173390865325928
  - 0.6967485189437866
  - 0.7430045127868653
  - 0.7626237392425538
  - 0.7475277066230774
  - 0.7274576246738435
  - 0.7127558469772339
  - 0.7494628310203553
  - 0.7117160797119141
  - 0.7122207283973694
  - 0.7680617690086365
  - 0.7295792400836945
  - 0.7073812663555146
  - 0.750115180015564
  - 0.7323669195175171
  - 0.7159431993961335
  - 0.6945734143257142
  validation_losses:
  - 0.3914090394973755
  - 0.41096019744873047
  - 0.39225703477859497
  - 0.4050949513912201
  - 0.555038332939148
  - 0.4048287570476532
  - 0.39093464612960815
  - 0.3891315758228302
  - 0.3987768590450287
  - 0.38915401697158813
  - 0.5987408757209778
  - 0.4194856882095337
  - 0.39133134484291077
  - 0.38535070419311523
  - 0.3973782956600189
  - 0.3964349031448364
  - 0.3940925896167755
  - 0.38760024309158325
  - 0.39865022897720337
  - 0.4054625332355499
  - 0.3918133080005646
  - 0.5307409167289734
  - 0.5215920209884644
  - 0.4945577383041382
  - 0.8384355306625366
  - 0.3851207494735718
  - 0.40841394662857056
  - 0.542694628238678
  - 0.5327757596969604
  - 0.4200224280357361
  - 0.7191987633705139
  - 0.5154674649238586
  - 0.4551747739315033
  - 0.38396814465522766
  - 0.37944307923316956
  - 0.3820638656616211
  - 0.49756765365600586
  - 0.527773916721344
  - 0.37892934679985046
  - 0.4347740113735199
  - 0.38610267639160156
  - 0.4042341113090515
  - 0.41327548027038574
  - 0.4637974798679352
  - 0.547317624092102
  - 0.40576210618019104
  - 0.39661216735839844
  - 0.3815593421459198
  - 0.39628851413726807
  - 0.37962040305137634
  - 0.3792230188846588
  - 0.43959006667137146
  - 0.5024327039718628
  - 0.7224666476249695
  - 0.531741738319397
  - 0.4093380272388458
  - 0.4901152551174164
  - 0.574947714805603
  - 0.8055606484413147
  - 0.9182620048522949
  - 0.9761623740196228
  - 0.4084891974925995
  - 0.4020851254463196
  - 0.394607275724411
  - 0.9781008362770081
  - 0.7521486282348633
  - 0.8561546802520752
  - 0.7245283722877502
  - 0.6018330454826355
  - 0.6149345636367798
  - 0.5869927406311035
  - 0.651303231716156
  - 0.38389644026756287
  - 0.4189864695072174
  - 0.43697336316108704
  - 0.41767770051956177
  - 0.38498011231422424
  - 0.3821418881416321
  - 0.3952001929283142
  - 0.5569096803665161
  - 0.5638377666473389
  - 0.49710726737976074
  - 0.3939213752746582
  - 0.38572272658348083
  - 0.40070098638534546
  - 0.4720691442489624
  - 0.8181058168411255
  - 0.7025609612464905
  - 0.6654093861579895
  - 0.6103350520133972
  - 0.45695796608924866
  - 0.40371012687683105
  - 0.591177761554718
  - 0.5244152545928955
  - 0.5642930269241333
  - 0.38321951031684875
  - 0.5331032276153564
  - 0.649429440498352
  - 0.37918373942375183
  - 0.4086013436317444
loss_records_fold3:
  train_losses:
  - 0.826698362827301
  - 0.7750521242618561
  - 0.7551946163177491
  - 0.728838711977005
  - 0.7375574171543122
  - 0.7391432702541352
  - 0.7368122220039368
  - 0.7336585342884064
  - 0.765339231491089
  - 0.7479472041130066
  - 0.7065531373023988
  - 0.7219349205493928
  - 0.7348358929157257
  - 0.7442848920822144
  - 0.7697912395000458
  - 0.7398697435855865
  - 0.7101070106029511
  - 0.7212694644927979
  - 0.7710382223129273
  - 0.7641831040382385
  - 0.7880296885967255
  - 0.7473632156848908
  - 0.7725860536098481
  - 0.7407930642366409
  - 0.7700477302074433
  - 0.7565357148647309
  - 0.717641967535019
  - 0.7513857603073121
  - 0.7297105014324189
  - 0.73357053399086
  - 0.7632835030555726
  - 0.8305633127689362
  - 0.7354854315519334
  - 0.751023030281067
  - 0.7615928828716279
  - 0.7144595175981522
  - 0.7364975094795227
  - 0.7378411591053009
  - 0.7920568406581879
  - 0.7829292058944702
  - 0.7155244886875153
  - 0.7282440841197968
  - 0.7549933433532715
  - 0.7408664762973786
  - 0.7730450570583344
  - 0.7716859877109528
  - 0.7481395542621613
  - 0.7503237009048462
  - 0.7707718670368195
  - 0.7417166888713838
  - 0.7197835206985475
  - 0.7559143781661988
  - 0.7057494789361954
  - 0.7447416484355927
  - 0.7454696953296662
  - 0.8091124951839448
  - 0.7640287816524506
  - 0.7514913976192474
  - 0.7176183819770814
  - 0.8033428847789765
  - 0.7749956727027894
  - 0.7255312502384186
  - 0.7946388065814972
  - 0.737676465511322
  - 0.7464656174182892
  - 0.7267392575740814
  - 0.7302893877029419
  - 0.7736156105995179
  - 0.7522854089736939
  - 0.7544781863689423
  - 0.7452639579772949
  - 0.7211309254169465
  - 0.7229479908943177
  - 0.7557777404785156
  - 0.7747891247272491
  - 0.7406891644001008
  - 0.7390781939029694
  - 0.7664945304393769
  - 0.8636855363845826
  - 0.7415070354938508
  - 0.7134192138910294
  - 0.7471208810806275
  - 0.7523712038993836
  - 0.7425519704818726
  - 0.7628492891788483
  - 0.8126115202903748
  - 0.7688199877738953
  - 0.7333375096321106
  - 0.7374418318271637
  - 0.7252179503440858
  - 0.7278386294841767
  - 0.7458305776119233
  - 0.7277390003204346
  - 0.7318355739116669
  - 0.7184624016284943
  - 0.68564230799675
  - 0.7268729209899902
  - 0.7351883113384248
  - 0.729215031862259
  - 0.7112952589988709
  validation_losses:
  - 0.5573673844337463
  - 1.1060479879379272
  - 1.0937939882278442
  - 0.7107670903205872
  - 1.0256496667861938
  - 0.6023901104927063
  - 0.9645569324493408
  - 1.6813275814056396
  - 0.44357970356941223
  - 0.37510621547698975
  - 0.37420058250427246
  - 0.6030686497688293
  - 1.1154634952545166
  - 1.1976649761199951
  - 1.0287266969680786
  - 0.5411160588264465
  - 0.3782762885093689
  - 0.5761845707893372
  - 0.5546036958694458
  - 0.4038707911968231
  - 0.4297008216381073
  - 0.3785644471645355
  - 0.580158531665802
  - 1.3930530548095703
  - 2.014746904373169
  - 0.4095470905303955
  - 0.4624672532081604
  - 0.48608022928237915
  - 0.4609369933605194
  - 0.4773414134979248
  - 0.4314817190170288
  - 0.5689188241958618
  - 0.47704219818115234
  - 0.43569979071617126
  - 0.37154853343963623
  - 0.36248475313186646
  - 0.3895776569843292
  - 0.38459303975105286
  - 0.44709840416908264
  - 0.5401374101638794
  - 0.5162054896354675
  - 0.9149512648582458
  - 1.0192757844924927
  - 0.5588906407356262
  - 0.6095670461654663
  - 0.4824908673763275
  - 0.4215262234210968
  - 0.4187091588973999
  - 0.4602866470813751
  - 0.48568427562713623
  - 0.42420774698257446
  - 0.4882373511791229
  - 0.5076735615730286
  - 0.4526402950286865
  - 0.41954684257507324
  - 0.46847373247146606
  - 0.592223048210144
  - 0.7459056377410889
  - 0.5154695510864258
  - 0.3902076482772827
  - 0.5580770373344421
  - 0.47687312960624695
  - 0.5318726301193237
  - 0.37997525930404663
  - 0.37302330136299133
  - 0.37286052107810974
  - 0.3776685893535614
  - 0.471367210149765
  - 0.7246270775794983
  - 0.543531060218811
  - 1.0649181604385376
  - 0.5786704421043396
  - 0.7359185814857483
  - 0.4943787455558777
  - 0.38513416051864624
  - 0.4840814173221588
  - 0.48476916551589966
  - 0.597091019153595
  - 0.495217502117157
  - 0.3899400532245636
  - 0.3843855559825897
  - 0.40094321966171265
  - 0.4193134009838104
  - 0.48462679982185364
  - 0.4775615334510803
  - 0.3825339376926422
  - 0.3979261815547943
  - 0.44416478276252747
  - 0.4344615638256073
  - 0.4679615795612335
  - 0.6623157262802124
  - 0.5987032651901245
  - 0.516010046005249
  - 0.5194358229637146
  - 0.5423427820205688
  - 0.6788886189460754
  - 0.8691481351852417
  - 0.5059911608695984
  - 0.48465606570243835
  - 0.5073716044425964
loss_records_fold4:
  train_losses:
  - 0.7661201536655426
  - 0.7262847542762757
  - 0.7137286722660066
  - 0.729746150970459
  - 0.7148894786834717
  - 0.7162519037723541
  - 0.7456463336944581
  - 0.7310684561729431
  - 0.7214065372943879
  - 0.7213480114936829
  - 0.7224805593490601
  - 0.7424440145492555
  - 0.7349807858467102
  - 0.736811101436615
  - 0.7478206634521485
  - 0.703135558962822
  - 0.7409866809844972
  - 0.7190581560134888
  - 0.74403076171875
  - 0.7053658366203308
  - 0.7521689891815186
  - 0.7290268182754517
  - 0.7300965070724488
  - 0.7372166752815247
  - 0.7094069242477418
  - 0.6883135348558427
  - 0.7087330102920533
  - 0.7207370162010194
  - 0.7900388419628144
  - 0.722632247209549
  - 0.7338688910007477
  - 0.711512315273285
  - 0.7529751896858216
  - 0.7346907436847687
  - 0.7234504997730256
  - 0.7669357836246491
  - 0.7145228266716004
  - 0.7736632585525514
  - 0.7158444911241532
  - 0.7554853916168214
  - 0.7570393025875092
  - 0.7908847868442536
  - 0.7424844264984132
  - 0.7511598706245423
  - 0.8102442502975464
  - 0.7142205059528351
  - 0.7658467888832092
  - 0.7383712410926819
  - 0.7397061944007874
  - 0.7421684622764588
  - 0.77573521733284
  - 0.7097985208034516
  - 0.7126692116260529
  - 0.7416957437992097
  - 0.7700811266899109
  - 0.7312748670578003
  - 0.7116165518760682
  - 0.7273785829544068
  - 0.7231278121471405
  - 0.7057305037975312
  - 0.6896007657051086
  - 0.7006823211908341
  - 0.7283548176288606
  - 0.7235485196113587
  - 0.7317977786064148
  - 0.7258871912956238
  - 0.802894800901413
  - 0.7316004574298859
  - 0.7361625075340271
  - 0.7897566258907318
  - 0.7513680040836335
  - 0.8202659785747528
  - 0.8118029773235321
  - 0.7425753951072693
  - 0.7242177456617356
  - 0.7568424701690675
  - 0.7315016448497773
  - 0.7641477525234223
  - 0.7688383221626283
  validation_losses:
  - 0.3820769786834717
  - 0.37417906522750854
  - 0.3800598978996277
  - 0.37279948592185974
  - 0.35573917627334595
  - 0.366608589887619
  - 0.3624175488948822
  - 0.39008790254592896
  - 0.3787125051021576
  - 0.3693836033344269
  - 0.3642437160015106
  - 0.3609398901462555
  - 0.4214596152305603
  - 0.44887346029281616
  - 0.4424464702606201
  - 0.3803774416446686
  - 0.44625216722488403
  - 0.47219061851501465
  - 0.45512694120407104
  - 0.4449872374534607
  - 0.3811620771884918
  - 0.3693302571773529
  - 0.36635205149650574
  - 0.43102431297302246
  - 0.51393723487854
  - 0.4447377026081085
  - 0.6464375853538513
  - 0.562317430973053
  - 0.4954893887042999
  - 0.3976817727088928
  - 0.4385327398777008
  - 0.3956942856311798
  - 0.3635193407535553
  - 0.38689395785331726
  - 0.37546592950820923
  - 0.3714514374732971
  - 0.38184359669685364
  - 0.49606600403785706
  - 0.3874201476573944
  - 0.3713653087615967
  - 0.394734263420105
  - 0.37573736906051636
  - 0.39752277731895447
  - 0.39677655696868896
  - 0.42297837138175964
  - 0.3588402569293976
  - 0.35588639974594116
  - 0.3547101318836212
  - 0.3580033481121063
  - 0.36741533875465393
  - 0.4315590560436249
  - 0.40534666180610657
  - 0.3605056405067444
  - 0.38212642073631287
  - 0.3725006580352783
  - 0.4226296842098236
  - 0.41362711787223816
  - 0.36882394552230835
  - 0.35925373435020447
  - 0.37490683794021606
  - 0.35317450761795044
  - 0.35309410095214844
  - 0.5264305472373962
  - 0.36118173599243164
  - 0.3799915015697479
  - 0.35729560256004333
  - 0.37814974784851074
  - 0.3803567886352539
  - 0.3752582371234894
  - 0.3788287937641144
  - 0.380362868309021
  - 0.38551461696624756
  - 0.3956039845943451
  - 0.37576672434806824
  - 0.38349342346191406
  - 0.37355974316596985
  - 0.37396854162216187
  - 0.37830424308776855
  - 0.3754113018512726
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 79 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8439108061749572, 0.8421955403087479,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.08080808080808081, 0.31343283582089554, 0.023809523809523808]'
  mean_eval_accuracy: 0.8520957483805178
  mean_f1_accuracy: 0.08361008808770003
  total_train_time: '0:24:50.472587'
