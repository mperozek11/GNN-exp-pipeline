config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 01:09:36.224353'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/102/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/102/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/102/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/102/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/102/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/102/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/102/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/102/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/102/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/102/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 3.90656179189682
  - 3.6715989112854004
  - 4.327861130237579
  - 4.022442042827606
  - 3.8589049577713013
  - 3.9040682911872864
  - 3.62636162340641
  - 3.7487592101097107
  - 3.9236316978931427
  - 3.9931022226810455
  - 3.8009754717350006
  - 4.028113275766373
  - 3.97660231590271
  - 3.7751580476760864
  - 3.732458382844925
  - 3.5919342786073685
  - 3.965373009443283
  - 3.7731705009937286
  - 3.915018081665039
  - 4.019257128238678
  - 3.786126524209976
  - 3.722829282283783
  - 3.6237781047821045
  - 3.6819140911102295
  - 3.599407970905304
  - 3.77383753657341
  - 3.7210478484630585
  - 3.675903409719467
  - 3.752524495124817
  - 4.062981963157654
  - 3.659912794828415
  - 4.189757019281387
  - 3.7907274067401886
  - 3.7179228961467743
  - 3.6653529703617096
  - 3.717871129512787
  - 4.2280667424201965
  - 3.791810929775238
  - 3.7042244970798492
  - 3.6411499828100204
  - 3.6079870462417603
  - 3.6914102733135223
  - 3.543853133916855
  - 3.556137055158615
  - 3.7909133434295654
  - 3.702781170606613
  - 3.6584108471870422
  - 3.4822962433099747
  - 3.7288093864917755
  - 3.689907044172287
  - 3.6642361879348755
  - 3.7739650309085846
  - 4.211994230747223
  - 3.7306343019008636
  - 3.8717762529850006
  - 3.8273521065711975
  - 3.6222563087940216
  - 3.6498073041439056
  - 3.7055170238018036
  - 3.5963324308395386
  - 3.768114387989044
  - 3.693437248468399
  - 4.02133184671402
  - 3.7193390130996704
  - 4.010562568902969
  - 3.717745929956436
  - 3.612796187400818
  - 4.215782880783081
  - 3.9960677325725555
  - 3.732193559408188
  - 3.6871029138565063
  - 3.6274220645427704
  - 3.704797148704529
  - 4.074168771505356
  - 3.5905436128377914
  - 3.724846690893173
  - 3.7898519039154053
  - 3.746696889400482
  - 3.599688559770584
  - 3.6345684230327606
  - 3.6902979910373688
  - 3.7695289254188538
  - 3.6283317506313324
  - 3.6231196224689484
  - 3.64789941906929
  - 3.686168372631073
  - 3.8726255297660828
  - 3.8802700340747833
  - 3.813190996646881
  - 3.6363807916641235
  - 3.655517339706421
  - 3.674856185913086
  - 3.5814996361732483
  - 3.6298415064811707
  - 3.7121909856796265
  - 3.4746211022138596
  - 3.7209599912166595
  - 3.550765037536621
  - 3.8430526554584503
  - 3.9591448307037354
  validation_losses:
  - 0.4010857343673706
  - 0.39209815859794617
  - 0.41344693303108215
  - 0.4046699106693268
  - 0.3917706310749054
  - 0.3899085521697998
  - 0.3919409215450287
  - 0.42545056343078613
  - 0.406289279460907
  - 0.39992010593414307
  - 0.4185197651386261
  - 0.4589076042175293
  - 0.4152199625968933
  - 0.3908599317073822
  - 0.39163723587989807
  - 0.3937937617301941
  - 0.5968858599662781
  - 0.5652698874473572
  - 0.3856164216995239
  - 0.39613813161849976
  - 0.3943304717540741
  - 0.4199771583080292
  - 0.39687037467956543
  - 0.426849365234375
  - 0.40385758876800537
  - 0.5589444041252136
  - 0.40127596259117126
  - 0.39351361989974976
  - 0.39131590723991394
  - 0.40496569871902466
  - 0.4109507203102112
  - 0.523444652557373
  - 0.48367181420326233
  - 0.3850962817668915
  - 0.38810068368911743
  - 0.3859340250492096
  - 0.3895207345485687
  - 0.4172714650630951
  - 0.42873239517211914
  - 0.3858344852924347
  - 0.3922934830188751
  - 0.535419225692749
  - 0.48216676712036133
  - 0.5867509245872498
  - 0.47307607531547546
  - 0.6481837630271912
  - 0.6501078605651855
  - 0.7668009400367737
  - 0.3932279646396637
  - 0.3996354937553406
  - 0.38512685894966125
  - 0.4484301805496216
  - 0.4015054404735565
  - 0.41133415699005127
  - 0.40030723810195923
  - 0.3952201306819916
  - 0.41613274812698364
  - 0.509958803653717
  - 0.6422406435012817
  - 0.5553299188613892
  - 0.6748427152633667
  - 0.6351932287216187
  - 0.4899085760116577
  - 0.5397626161575317
  - 0.5251287817955017
  - 0.6375088691711426
  - 0.6005042791366577
  - 0.3987690210342407
  - 0.7325864434242249
  - 0.5257949233055115
  - 0.6128697991371155
  - 0.696672260761261
  - 0.6017237901687622
  - 0.6100909113883972
  - 0.4956516623497009
  - 0.4117559492588043
  - 0.5838173031806946
  - 0.6518814563751221
  - 0.5032996535301208
  - 0.3979852497577667
  - 0.4573706090450287
  - 0.5513771772384644
  - 0.6466383934020996
  - 0.472385436296463
  - 0.493196576833725
  - 0.45401906967163086
  - 0.4700396955013275
  - 0.5386004447937012
  - 0.4730522036552429
  - 0.3979293704032898
  - 0.3915826380252838
  - 0.40503448247909546
  - 0.5008605718612671
  - 0.44898566603660583
  - 0.4566267430782318
  - 0.43613311648368835
  - 0.5804916620254517
  - 0.5074423551559448
  - 0.5708187222480774
  - 0.4798319339752197
loss_records_fold2:
  train_losses:
  - 3.7529925107955933
  - 3.7037143409252167
  - 3.696052998304367
  - 3.6080442517995834
  - 3.480852857232094
  - 3.859727203845978
  - 3.927914470434189
  - 3.681934878230095
  - 3.888892650604248
  - 3.7197905778884888
  - 3.772504538297653
  - 3.6544277369976044
  - 3.5391271263360977
  - 3.720089763402939
  - 3.560879945755005
  - 3.649755507707596
  - 3.532339334487915
  - 3.6901476681232452
  - 3.4616628736257553
  - 3.781108260154724
  - 3.451590970158577
  - 3.7626712024211884
  - 3.5920866429805756
  - 3.809349298477173
  - 3.5509868264198303
  - 3.4515181183815002
  - 3.5016540586948395
  - 3.764196455478668
  - 3.641989767551422
  - 3.7142647206783295
  - 3.699442744255066
  - 3.751218318939209
  - 3.9715614020824432
  - 3.5774371325969696
  - 3.7883415818214417
  - 3.5910874605178833
  - 3.4967786371707916
  - 3.5100239515304565
  - 3.4694158732891083
  - 3.541466236114502
  - 3.7225908041000366
  - 3.596152424812317
  - 3.82335501909256
  - 3.5573716163635254
  - 3.4818319529294968
  - 3.520486503839493
  - 3.485193580389023
  - 3.4353920221328735
  - 3.4758098423480988
  - 3.5367794036865234
  - 3.881312519311905
  - 3.8820981979370117
  - 3.687623769044876
  - 3.6009891629219055
  - 3.6122965216636658
  - 3.897277384996414
  - 3.5326460003852844
  - 3.695376545190811
  - 3.5980288237333298
  - 3.490875229239464
  - 3.7180572152137756
  - 3.9042948484420776
  - 3.5525007247924805
  - 3.8561865389347076
  - 3.8334844410419464
  - 3.4745631963014603
  - 3.628716379404068
  - 3.636984407901764
  - 3.4674371629953384
  - 3.5063672214746475
  - 3.7280682921409607
  - 3.86830872297287
  - 3.7294400930404663
  - 3.689974784851074
  - 3.4735881239175797
  - 3.5191240310668945
  - 3.503043919801712
  - 3.6505963504314423
  - 3.570836067199707
  - 3.793117195367813
  - 3.6499939560890198
  - 3.7862164974212646
  - 3.559890568256378
  - 3.6472466588020325
  - 3.463813528418541
  - 3.6176289319992065
  - 3.5778151154518127
  - 3.6202596724033356
  - 3.601087212562561
  - 3.5716051757335663
  - 3.712675303220749
  - 3.5720628798007965
  - 3.6166382133960724
  - 3.6412988007068634
  - 3.4859111607074738
  - 3.660493105649948
  - 3.514711558818817
  - 3.550511062145233
  - 3.620574414730072
  - 3.876159965991974
  validation_losses:
  - 0.4544607698917389
  - 0.47916507720947266
  - 0.4236052632331848
  - 0.4999965727329254
  - 0.9600669145584106
  - 0.6100757122039795
  - 0.45208051800727844
  - 0.38149699568748474
  - 0.38316214084625244
  - 0.4144335985183716
  - 0.3851306736469269
  - 0.3840313255786896
  - 0.3868819773197174
  - 0.3878478705883026
  - 0.39992383122444153
  - 0.4257117211818695
  - 0.38823387026786804
  - 0.4206926226615906
  - 0.4180067777633667
  - 0.47734349966049194
  - 0.392158180475235
  - 0.40629151463508606
  - 0.38135847449302673
  - 0.4447612464427948
  - 0.43949973583221436
  - 0.4385453760623932
  - 0.5470371842384338
  - 0.7332754731178284
  - 0.45590466260910034
  - 0.7068976759910583
  - 0.3823387324810028
  - 0.38973721861839294
  - 0.40783292055130005
  - 0.4826544225215912
  - 0.5979772210121155
  - 0.5661550164222717
  - 0.4142884314060211
  - 0.49708884954452515
  - 0.5128203630447388
  - 0.43281397223472595
  - 0.469835102558136
  - 0.5016122460365295
  - 0.47043126821517944
  - 0.39642593264579773
  - 0.42679229378700256
  - 0.43523332476615906
  - 0.5506563186645508
  - 0.6293221116065979
  - 0.5172767043113708
  - 0.4381529688835144
  - 0.42759039998054504
  - 0.39365848898887634
  - 0.5552033185958862
  - 0.4682205319404602
  - 0.41772764921188354
  - 0.41288384795188904
  - 0.3928724229335785
  - 0.420829713344574
  - 0.4085857570171356
  - 0.3985525071620941
  - 0.4587116241455078
  - 0.38501638174057007
  - 0.5430464744567871
  - 0.38548338413238525
  - 0.3881673216819763
  - 0.39745184779167175
  - 0.3837359845638275
  - 0.4804282486438751
  - 0.49958914518356323
  - 0.6511045098304749
  - 0.5755586624145508
  - 0.6884402632713318
  - 0.6285762786865234
  - 0.49601608514785767
  - 0.4669209420681
  - 0.4926074743270874
  - 0.4133731424808502
  - 0.547990620136261
  - 0.46686220169067383
  - 0.5928894281387329
  - 0.45329535007476807
  - 0.44396325945854187
  - 0.46172064542770386
  - 0.5045302510261536
  - 0.4128926992416382
  - 0.49817660450935364
  - 0.5401944518089294
  - 0.5945280194282532
  - 0.439609557390213
  - 0.3770773410797119
  - 0.38400378823280334
  - 0.3773658275604248
  - 0.42085185647010803
  - 0.4739223122596741
  - 0.5184905529022217
  - 0.546002984046936
  - 0.4953996539115906
  - 0.43691912293434143
  - 0.7497876286506653
  - 0.3944781422615051
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 10 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 55 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8507718696397941, 0.8490566037735849, 0.8542024013722127,
    0.8608247422680413]'
  fold_eval_f1: '[0.0, 0.10309278350515462, 0.10204081632653061, 0.15841584158415842,
    0.0898876404494382]'
  mean_eval_accuracy: 0.854497710031653
  mean_f1_accuracy: 0.09068741637305637
  total_train_time: '0:01:57.409776'
