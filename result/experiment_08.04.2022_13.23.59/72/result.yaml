config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 20:58:05.670172'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/72/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/72/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/72/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/72/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/72/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/72/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/72/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/72/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/72/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/72/fold_4_state_dict.pt
loss_records_fold3:
  train_losses:
  - 29.496753558516502
  - 30.15642301738262
  - 29.15164126455784
  - 29.720538154244423
  - 29.752622857689857
  - 29.60050205886364
  - 29.123580291867256
  - 29.289152204990387
  - 29.886582598090172
  - 29.70978806912899
  - 29.223804712295532
  - 29.4182126224041
  - 29.126576602458954
  - 29.728685811161995
  - 29.405571192502975
  - 29.552847772836685
  - 29.98013712465763
  - 29.738586619496346
  - 29.760969772934914
  - 29.463050946593285
  - 29.931378215551376
  - 28.874275475740433
  - 29.407513171434402
  - 30.023253351449966
  - 29.171581387519836
  - 29.568635940551758
  - 29.060880288481712
  - 29.11335590481758
  - 29.301495775580406
  - 29.43306066095829
  - 29.455637380480766
  - 29.178969383239746
  - 29.72929121553898
  - 29.427257269620895
  - 29.27677120268345
  - 29.48117484152317
  - 29.3284133374691
  - 29.100977405905724
  - 29.73906534910202
  - 30.27543094754219
  - 29.453739657998085
  - 29.81249611079693
  - 29.2909032702446
  - 29.341578155755997
  - 29.352964982390404
  - 29.80732388794422
  - 28.757400259375572
  - 28.770401269197464
  - 29.376304551959038
  - 29.58489081263542
  - 29.644143104553223
  - 28.97603575885296
  - 29.312884375452995
  - 29.194741889834404
  - 29.564365446567535
  - 29.92994660139084
  - 29.229379922151566
  - 29.963293612003326
  - 29.173239678144455
  - 28.776803329586983
  - 29.717588931322098
  - 29.22775512933731
  - 29.35597251355648
  - 29.397510051727295
  - 29.525045558810234
  - 28.670264437794685
  - 29.239931613206863
  - 29.303785622119904
  - 29.011517390608788
  - 29.09661276638508
  - 29.265366211533546
  - 29.077075615525246
  - 28.902367666363716
  - 29.141104251146317
  - 29.448706045746803
  - 29.233288004994392
  - 28.44780695438385
  - 30.1653144210577
  - 29.454031378030777
  - 29.336498945951462
  - 29.187787786126137
  - 29.123999059200287
  - 29.683682143688202
  - 29.03209836781025
  - 31.71671238541603
  - 29.552726298570633
  - 29.44220231473446
  - 29.46312226355076
  - 29.089841708540916
  - 29.34643219411373
  - 29.229005977511406
  - 29.651277795433998
  - 29.282483145594597
  - 29.839655220508575
  - 29.130233123898506
  - 29.223508298397064
  - 29.2954453676939
  - 29.249265506863594
  - 28.925198167562485
  - 29.109288722276688
  validation_losses:
  - 0.3911902904510498
  - 0.39669135212898254
  - 0.4350390136241913
  - 0.43432748317718506
  - 0.40199002623558044
  - 0.38308820128440857
  - 0.7614481449127197
  - 0.3899940252304077
  - 0.4797336161136627
  - 1.328768014907837
  - 0.618186354637146
  - 0.7535558342933655
  - 1.2046855688095093
  - 1.7927372455596924
  - 0.6599090099334717
  - 0.738257884979248
  - 0.7591655254364014
  - 0.6630039811134338
  - 1.2624965906143188
  - 0.40534448623657227
  - 0.38761425018310547
  - 0.44313403964042664
  - 0.48399001359939575
  - 0.5249345898628235
  - 0.5857336521148682
  - 0.6302171349525452
  - 0.47233840823173523
  - 0.4076101779937744
  - 0.4791352450847626
  - 0.3969913125038147
  - 0.4069387912750244
  - 0.6156506538391113
  - 0.44675344228744507
  - 0.4873115122318268
  - 0.8929773569107056
  - 2.4576499462127686
  - 1.95915949344635
  - 3.592656373977661
  - 0.39005905389785767
  - 0.38275691866874695
  - 0.39087361097335815
  - 0.3925396502017975
  - 1.5786436796188354
  - 0.4973672330379486
  - 0.44290250539779663
  - 1.2048163414001465
  - 1.9611519575119019
  - 2.4691998958587646
  - 0.4067840278148651
  - 0.3959593176841736
  - 0.44894149899482727
  - 0.6523870229721069
  - 0.4126671552658081
  - 0.5380038619041443
  - 0.3907066583633423
  - 0.5681560635566711
  - 4.18991231918335
  - 0.9005824327468872
  - 0.9689223766326904
  - 0.4095049500465393
  - 3.400629997253418
  - 9.09305477142334
  - 10.997673034667969
  - 16.04273796081543
  - 6.903683185577393
  - 1.6270971298217773
  - 4.162731647491455
  - 0.3788613975048065
  - 0.6158095002174377
  - 0.3828922510147095
  - 0.39382365345954895
  - 0.396208256483078
  - 1.071581244468689
  - 0.3954612910747528
  - 0.3843843936920166
  - 0.6918649673461914
  - 0.3857862651348114
  - 7.076231479644775
  - 7.107998371124268
  - 1.7891006469726562
  - 2.376394033432007
  - 6.350819110870361
  - 7.866232872009277
  - 8.609901428222656
  - 19.203672409057617
  - 0.8266012072563171
  - 1.1189589500427246
  - 0.4457636773586273
  - 2.1490252017974854
  - 5.433290004730225
  - 1.8308537006378174
  - 2.796971082687378
  - 3.5631275177001953
  - 10.259227752685547
  - 4.0388264656066895
  - 5.138617515563965
  - 6.989776611328125
  - 2.0572428703308105
  - 2.9782726764678955
  - 4.692846298217773
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 57 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.023809523809523808]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0047619047619047615
  total_train_time: '0:10:21.798721'
