config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:12:14.150320'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_117fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_117fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_117fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_117fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_117fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_117fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_117fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_117fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_117fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_117fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.8422703802585603
  - 3.4026131689548493
  - 3.2756521940231327
  - 3.2225787878036503
  - 3.290997803211212
  - 3.2707146137952807
  - 3.26872952580452
  - 3.249529403448105
  - 3.5775093674659733
  - 3.18656125664711
  - 3.1973111510276797
  - 3.107977652549744
  - 3.285164323449135
  - 3.6709966301918033
  - 3.182896775007248
  - 3.095673155784607
  - 3.1172583997249603
  validation_losses:
  - 0.7826271057128906
  - 0.4084683060646057
  - 0.3956848084926605
  - 0.40152648091316223
  - 0.40900638699531555
  - 0.4251905381679535
  - 0.4087098240852356
  - 0.40673890709877014
  - 0.4202670753002167
  - 0.3912232518196106
  - 0.41311776638031006
  - 0.4077299237251282
  - 0.39247748255729675
  - 0.3998400568962097
  - 0.40079808235168457
  - 0.3941749334335327
  - 0.39317786693573
loss_records_fold1:
  train_losses:
  - 3.1824824512004852
  - 3.2267696261405945
  - 3.1595643103122715
  - 3.297289401292801
  - 3.061753100156784
  - 3.5311880856752396
  - 3.4083424985408786
  - 3.101627007126808
  - 2.9669724285602572
  - 3.0672307312488556
  - 3.064866277575493
  - 3.106267410516739
  - 3.2091841578483584
  - 3.149211359024048
  - 3.064977526664734
  - 3.1629370987415317
  - 3.208009469509125
  - 3.050877732038498
  - 3.1273313939571383
  - 3.052568346261978
  - 3.0461938679218292
  - 2.978218185901642
  - 3.0830901950597767
  - 2.985065793991089
  - 2.9441327512264253
  - 3.041902178525925
  - 2.9537252664566043
  - 2.97411938905716
  - 3.0444197297096256
  - 2.997986295819283
  - 3.0477831304073337
  - 2.9585426032543185
  - 3.05944139957428
  - 2.9981582820415498
  - 3.036601546406746
  - 2.9703744649887085
  - 3.0479349493980408
  - 2.9985506892204286
  - 3.2367745250463487
  - 3.16863813996315
  - 3.0111109197139743
  - 2.996996793150902
  - 2.961073982715607
  - 3.0121799230575563
  - 2.9064849615097046
  - 2.8874149799346926
  - 2.9368011504411697
  - 2.9245189100503923
  - 2.990876293182373
  - 2.9733951151371003
  - 2.910662353038788
  - 2.8821664422750475
  - 3.0095831930637362
  - 2.9737990081310275
  - 2.9875483840703967
  - 2.9577620863914493
  - 2.9265575885772708
  - 2.918181359767914
  - 3.009274235367775
  - 2.891081115603447
  - 2.9167280912399294
  - 2.8216960668563846
  - 2.995721882581711
  - 2.9010999441146854
  - 2.9837108314037324
  - 2.902587813138962
  - 3.0001348078250887
  - 2.939365985989571
  - 2.906302469968796
  - 2.9221720695495605
  - 2.9022671282291412
  - 2.883680483698845
  - 2.9839502036571504
  - 2.954626697301865
  - 2.987670582532883
  - 2.898735851049423
  - 2.9539343506097797
  - 2.980321764945984
  - 2.899134758114815
  - 2.9639572858810426
  - 2.8726129263639453
  - 2.921585524082184
  - 2.9056218385696413
  - 2.8383360356092453
  - 2.8723350763320923
  - 2.9591807425022125
  - 2.9580821841955185
  - 2.8356922537088396
  - 2.9339364349842074
  - 2.889073032140732
  - 2.9482765555381776
  - 2.8560320019721988
  - 2.8752359956502915
  - 2.9137343525886537
  - 2.902633157372475
  - 2.9244736433029175
  - 2.9490097880363466
  - 2.9663035929203034
  - 2.896257197856903
  - 2.8796941071748736
  validation_losses:
  - 0.40135422348976135
  - 0.4187681972980499
  - 0.38813456892967224
  - 0.3958510160446167
  - 0.3968835175037384
  - 0.4016544818878174
  - 0.4109756350517273
  - 0.396776407957077
  - 0.4225638508796692
  - 0.39765435457229614
  - 0.40810781717300415
  - 0.3965502381324768
  - 0.3950285315513611
  - 0.42058080434799194
  - 0.4159058630466461
  - 0.519819974899292
  - 0.4015476405620575
  - 0.46673479676246643
  - 1.4750664234161377
  - 0.5121733546257019
  - 0.5656696557998657
  - 1.089503526687622
  - 0.40147891640663147
  - 1.2100828886032104
  - 0.6424146890640259
  - 0.9888606667518616
  - 0.5579635500907898
  - 0.4631405770778656
  - 0.5981878042221069
  - 0.9930037260055542
  - 0.43025484681129456
  - 0.6327795386314392
  - 0.4484359323978424
  - 0.8527565002441406
  - 0.391128808259964
  - 0.385178804397583
  - 0.406338095664978
  - 0.3917867839336395
  - 0.38994044065475464
  - 0.3932269513607025
  - 0.6544227600097656
  - 0.529904842376709
  - 0.4316839873790741
  - 0.44531869888305664
  - 0.5077638030052185
  - 0.4713444709777832
  - 0.4941219091415405
  - 0.4062296450138092
  - 0.5716204643249512
  - 0.7371923327445984
  - 1.7702624797821045
  - 0.6619640588760376
  - 0.9046406149864197
  - 0.9501487612724304
  - 0.38386040925979614
  - 0.4014744162559509
  - 0.42701655626296997
  - 0.4414498805999756
  - 0.5721039772033691
  - 0.8426598906517029
  - 1.3978101015090942
  - 2.2850608825683594
  - 0.5537424683570862
  - 1.1836224794387817
  - 1.0780709981918335
  - 0.9817874431610107
  - 0.6518936157226562
  - 0.7392206192016602
  - 0.5940613150596619
  - 1.0765718221664429
  - 0.9329414367675781
  - 0.8067399263381958
  - 1.017127513885498
  - 1.036203145980835
  - 0.9672808051109314
  - 0.4616435468196869
  - 0.5413762927055359
  - 0.5621597170829773
  - 1.1208293437957764
  - 1.0006870031356812
  - 1.0520929098129272
  - 1.1513527631759644
  - 1.3658874034881592
  - 0.9797229766845703
  - 1.021896481513977
  - 0.5324384570121765
  - 0.8385697603225708
  - 0.6391807794570923
  - 1.0339479446411133
  - 0.7202337980270386
  - 0.9261478781700134
  - 0.8171018362045288
  - 0.8436142206192017
  - 0.9576919078826904
  - 1.697998285293579
  - 0.9535688161849976
  - 0.6394149661064148
  - 0.6509739756584167
  - 0.6401911973953247
  - 0.7654786705970764
loss_records_fold2:
  train_losses:
  - 2.949392127990723
  - 2.914142769575119
  - 2.89254749417305
  - 2.902532559633255
  - 2.909008598327637
  - 2.8987050592899326
  - 2.9891629546880725
  - 2.914393585920334
  - 2.985681837797165
  - 2.901786682009697
  - 2.903548389673233
  - 2.9070325404405595
  - 2.9281670570373537
  - 2.807211208343506
  - 2.9946133285760883
  - 2.8736556351184848
  - 2.8635339707136156
  - 2.892020434141159
  - 2.929881522059441
  - 2.835158482193947
  - 2.8252625465393066
  - 2.9037537753582003
  - 3.141115289926529
  - 2.94146948158741
  - 2.9793157398700716
  - 2.885698512196541
  - 2.922248446941376
  - 2.8317598432302478
  - 2.85387482047081
  - 2.8761608064174653
  - 2.8993007779121402
  - 2.915027487277985
  - 2.8700031042099
  - 2.7783035933971405
  - 2.9332268953323366
  - 2.784185826778412
  - 2.891552719473839
  - 2.902142453193665
  - 2.8832233965396883
  - 2.850820428133011
  - 2.8516453683376315
  - 2.8497369199991227
  - 2.9447328031063082
  - 2.842841124534607
  - 2.816300877928734
  - 2.8825664937496187
  - 2.8062809616327287
  - 2.9738373517990113
  - 2.972786247730255
  - 3.1876081407070163
  - 3.1102022469043735
  - 2.881282413005829
  - 2.8980509817600253
  - 2.8384133875370026
  - 2.870206427574158
  - 2.79087789952755
  - 2.884235674142838
  - 2.9328851044178013
  - 2.8354031622409823
  - 2.824083572626114
  - 2.8330170810222626
  - 2.923185455799103
  - 2.839189270138741
  - 2.854629084467888
  - 2.9217778742313385
  - 2.814229938387871
  - 2.8962160766124727
  - 2.778164100646973
  - 2.8389271885156635
  - 2.8299455821514132
  - 2.9361362099647526
  - 2.8131308913230897
  - 2.8397414684295654
  - 2.8831773638725284
  - 2.8755328893661503
  - 2.836369028687477
  - 2.8488860160112384
  - 2.819372379779816
  - 2.841066783666611
  - 2.8029198527336123
  - 2.8080752402544022
  - 2.978432053327561
  - 2.8425392270088197
  - 2.8158272534608844
  - 2.8160743057727817
  - 2.7762428641319277
  - 2.7978793740272523
  - 2.8412341058254245
  - 2.8461911201477053
  - 2.86659314930439
  - 2.879179605841637
  - 2.920157712697983
  - 2.8307170212268833
  - 2.764558178186417
  - 2.985770905017853
  - 2.775210875272751
  - 2.7808799862861635
  - 2.791351264715195
  - 2.882524162530899
  - 2.8487985670566562
  validation_losses:
  - 0.5698756575584412
  - 0.6360785961151123
  - 0.887202262878418
  - 0.6967618465423584
  - 0.9301671981811523
  - 2.72914719581604
  - 0.41878148913383484
  - 0.4027293920516968
  - 0.41042640805244446
  - 0.7456499934196472
  - 1.2291884422302246
  - 0.7715609073638916
  - 0.5449178814888
  - 0.980995774269104
  - 0.6019872426986694
  - 0.6568309664726257
  - 0.3981286585330963
  - 1.0133721828460693
  - 0.8010528683662415
  - 0.9433742761611938
  - 0.8453383445739746
  - 1.3172687292099
  - 2.0313003063201904
  - 1.7765849828720093
  - 1.4984667301177979
  - 0.7828771471977234
  - 1.9345976114273071
  - 2.250814914703369
  - 2.3136048316955566
  - 1.9137070178985596
  - 0.7779912948608398
  - 4.779637336730957
  - 1.5817254781723022
  - 2.3884823322296143
  - 2.405902147293091
  - 0.8880239725112915
  - 0.6786938905715942
  - 0.39717575907707214
  - 1.179552674293518
  - 1.679499626159668
  - 2.117692232131958
  - 1.4515485763549805
  - 2.796719789505005
  - 1.9165728092193604
  - 1.0997782945632935
  - 1.4923712015151978
  - 1.7421094179153442
  - 32.55387878417969
  - 2.4164769649505615
  - 0.6553524136543274
  - 0.3871704041957855
  - 0.4885658025741577
  - 2.4485321044921875
  - 2.786726713180542
  - 6.621542930603027
  - 4.675942420959473
  - 1.4487104415893555
  - 0.9757786989212036
  - 1.563429594039917
  - 1.043558955192566
  - 1.3028688430786133
  - 0.5690246820449829
  - 0.8097986578941345
  - 1.1498751640319824
  - 0.6178291440010071
  - 0.5444663763046265
  - 0.9053173065185547
  - 0.9393277168273926
  - 0.7715698480606079
  - 1.3248389959335327
  - 0.48437628149986267
  - 0.7943301796913147
  - 0.6370090246200562
  - 0.9986321330070496
  - 0.9412896633148193
  - 1.2173304557800293
  - 0.6905263662338257
  - 1.3054500818252563
  - 1.0946775674819946
  - 1.0329395532608032
  - 0.6026987433433533
  - 0.5208601951599121
  - 1.6005853414535522
  - 5.187885284423828
  - 1.243605136871338
  - 1.3454653024673462
  - 1.2186099290847778
  - 4.733262538909912
  - 0.5341182351112366
  - 3.7948944568634033
  - 0.824569046497345
  - 0.7918937802314758
  - 2.337756633758545
  - 3.6539535522460938
  - 0.40144458413124084
  - 2.684377431869507
  - 4.873097896575928
  - 2.0934031009674072
  - 2.01151442527771
  - 1.3743419647216797
loss_records_fold3:
  train_losses:
  - 2.829611432552338
  - 2.876610952615738
  - 2.8874032229185107
  - 3.295099037885666
  - 2.965138226747513
  - 2.952942705154419
  - 2.9632062911987305
  - 3.003854423761368
  - 2.927897104620934
  - 2.927412581443787
  - 2.9578446447849274
  - 2.8971845656633377
  - 2.8989326000213627
  - 2.9209965825080872
  - 2.932471549510956
  - 2.8532456159591675
  - 2.870685911178589
  - 2.8786411345005036
  - 2.855097597837448
  - 2.8249095499515535
  - 2.8741558074951175
  - 2.9026261508464817
  - 2.9903492808341983
  - 2.8401974886655807
  - 2.8914923608303074
  - 2.867732745409012
  - 2.9033222228288653
  validation_losses:
  - 1.2268985509872437
  - 0.5199701189994812
  - 2.2233874797821045
  - 0.7592432498931885
  - 0.6919134259223938
  - 0.800984799861908
  - 0.7462552785873413
  - 1.1118204593658447
  - 1.0195317268371582
  - 0.4458286464214325
  - 2.0609874725341797
  - 1.4972902536392212
  - 3.2351980209350586
  - 1.1821062564849854
  - 1.3571994304656982
  - 1.505629062652588
  - 1.8375937938690186
  - 2.6620476245880127
  - 4.363028526306152
  - 1.3464572429656982
  - 1.6402016878128052
  - 0.9191447496414185
  - 0.5489369034767151
  - 0.5401148200035095
  - 0.5284374952316284
  - 0.4653169810771942
  - 0.41786712408065796
loss_records_fold4:
  train_losses:
  - 2.859875568747521
  - 2.897191047668457
  - 2.861406895518303
  - 2.912518519163132
  - 2.8432433247566227
  - 2.9273044109344486
  - 2.8985201954841617
  - 2.8614786952733997
  - 2.8410951793193817
  - 2.838911893963814
  - 2.873504209518433
  - 2.9185183346271515
  - 2.8603217184543612
  - 2.8047777950763706
  - 2.8145504891872406
  - 2.9145295292139055
  - 2.840340292453766
  - 2.925215804576874
  - 2.8491225957870485
  - 2.8365311264991764
  - 2.874913763999939
  - 2.8232129871845246
  - 2.8622361332178117
  - 2.8493286192417147
  - 2.8189567983150483
  - 2.91881902217865
  - 2.761518505215645
  - 2.815532898902893
  - 2.8250539004802704
  - 2.829903021454811
  - 2.896977210044861
  - 2.8426143616437916
  - 2.933942419290543
  - 2.823532515764237
  - 2.8485201746225357
  - 2.804385295510292
  - 2.7411267906427383
  - 2.7835029095411303
  - 2.8658429443836213
  - 2.8418597877025604
  - 2.796582481265068
  - 2.7615235924720767
  - 2.8523466259241106
  - 2.859590357542038
  - 2.8450802087783815
  - 2.770524597167969
  - 2.8288966834545137
  - 2.8605714708566667
  - 2.8856436103582386
  - 2.8124577075243
  - 2.762910613417626
  - 2.870940864086151
  - 2.7600499927997593
  - 2.7932608872652054
  - 2.796675390005112
  - 2.889320942759514
  - 2.8405826151371003
  - 2.828666400909424
  - 2.799063503742218
  - 2.9097283482551575
  - 2.830493450164795
  - 2.8529682844877247
  - 2.8764459609985353
  - 2.9060897022485737
  - 2.8054567337036134
  - 2.7851312458515167
  - 2.7793678760528566
  - 2.8966711133718492
  - 2.8200178861618044
  - 2.779067349433899
  - 2.8272766262292865
  - 2.793300443887711
  - 2.793194627761841
  - 2.838069012761116
  - 2.868419870734215
  - 2.8501123785972595
  - 2.8399553835392
  - 2.8457773417234424
  - 2.812674954533577
  - 2.8902253746986393
  - 2.745135644078255
  - 2.760161274671555
  - 2.8148895651102066
  - 2.788535022735596
  - 2.921688175201416
  - 2.8546274602413177
  - 2.8007535606622698
  - 2.7569558531045915
  - 2.759366196393967
  - 2.775129199028015
  - 2.7959227681159975
  - 2.701655584573746
  - 2.8304645329713822
  - 2.7874249011278156
  - 2.7738516271114353
  - 2.8094188243150713
  - 2.8233371615409855
  - 2.8305242836475375
  - 2.8197550386190415
  - 2.7612405478954316
  validation_losses:
  - 0.4070059061050415
  - 0.6161157488822937
  - 0.506595253944397
  - 0.6138683557510376
  - 0.4060417711734772
  - 0.47737357020378113
  - 0.4301525056362152
  - 0.4070788621902466
  - 0.47006598114967346
  - 0.47583281993865967
  - 2.58296537399292
  - 0.47283726930618286
  - 0.7371803522109985
  - 0.41788449883461
  - 0.9246584177017212
  - 2.6395013332366943
  - 1.2246129512786865
  - 1.0291991233825684
  - 0.7379905581474304
  - 1.9391964673995972
  - 2.3251683712005615
  - 0.4275403618812561
  - 0.47521793842315674
  - 0.5729316473007202
  - 0.5734715461730957
  - 1.8421224355697632
  - 2.3208982944488525
  - 1.3299051523208618
  - 1.5141242742538452
  - 0.40694835782051086
  - 0.49542906880378723
  - 0.4104410707950592
  - 0.9281341433525085
  - 0.7299347519874573
  - 1.2956470251083374
  - 0.6385195255279541
  - 0.8595668077468872
  - 0.6256721615791321
  - 1.1856576204299927
  - 1.2651402950286865
  - 0.6253203749656677
  - 0.9053860902786255
  - 0.9577630758285522
  - 1.0784752368927002
  - 0.6631243824958801
  - 1.050763726234436
  - 0.80540931224823
  - 0.5942041873931885
  - 0.6656379103660583
  - 0.5831743478775024
  - 1.3443551063537598
  - 0.566932737827301
  - 0.8618403077125549
  - 0.7535516619682312
  - 0.7234874963760376
  - 0.9390386939048767
  - 0.597027599811554
  - 0.7191904187202454
  - 0.6958078145980835
  - 0.6777580976486206
  - 0.7041249871253967
  - 0.5963581800460815
  - 0.6366987228393555
  - 0.5530198216438293
  - 0.5175029635429382
  - 0.7795005440711975
  - 1.2667852640151978
  - 1.0333542823791504
  - 0.8596003651618958
  - 0.9235294461250305
  - 0.6788775324821472
  - 0.6768589019775391
  - 0.668707549571991
  - 0.8008177280426025
  - 0.8032290935516357
  - 0.7310868501663208
  - 0.6634489297866821
  - 0.6376453638076782
  - 0.7958837151527405
  - 0.7737154364585876
  - 0.8321811556816101
  - 0.7597803473472595
  - 0.7903763651847839
  - 0.8418232202529907
  - 0.595541775226593
  - 1.1857976913452148
  - 1.0991277694702148
  - 0.9136602282524109
  - 0.7314851880073547
  - 0.6502739191055298
  - 0.7690731883049011
  - 0.6943986415863037
  - 0.8265787959098816
  - 0.8090085983276367
  - 0.7450945973396301
  - 0.7989488840103149
  - 0.6976457834243774
  - 0.6866578459739685
  - 0.7798813581466675
  - 0.6488323211669922
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8456260720411664, 0.8216123499142367, 0.8319039451114922,
    0.8144329896907216]'
  fold_eval_f1: '[0.0, 0.09999999999999998, 0.26760563380281693, 0.26865671641791045,
    0.15625]'
  mean_eval_accuracy: 0.8342416579724496
  mean_f1_accuracy: 0.15850247004414547
  total_train_time: '0:26:43.295124'
