config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:26:12.750183'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_84fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_84fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_84fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_84fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_84fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_84fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_84fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_84fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_84fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_84fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 7.060984200239182
  - 6.916970711946488
  - 6.538704726099969
  - 6.360342800617218
  - 7.135045525431633
  - 6.516350224614143
  - 6.153528732061386
  - 6.341362833976746
  - 6.723172181844712
  - 6.5002897709608085
  - 6.21342101097107
  - 6.302299347519875
  - 6.112237825989723
  - 6.184994021058083
  - 6.041115999221802
  - 6.043140846490861
  - 6.093371185660363
  - 6.190393435955048
  - 6.062339130043984
  - 5.9732501924037935
  - 5.92500160932541
  - 6.143204137682915
  - 6.416804873943329
  - 6.103724879026413
  - 6.115296167135239
  - 6.041350308060647
  - 5.979973322153092
  - 6.114612603187561
  - 5.973513728380204
  - 6.0618820011615755
  - 5.9561173051595695
  - 6.052522510290146
  - 6.0248908609151846
  - 5.957265663146973
  - 6.002408912777901
  - 6.076228627562523
  - 6.041004759073258
  - 5.885468977689744
  - 6.106154844164848
  validation_losses:
  - 0.4165230691432953
  - 0.4308755099773407
  - 0.4089045822620392
  - 0.4011891782283783
  - 0.4127208888530731
  - 0.41468846797943115
  - 0.41277366876602173
  - 0.39093253016471863
  - 0.40553754568099976
  - 0.42929762601852417
  - 0.4753558933734894
  - 0.41216355562210083
  - 0.4000084698200226
  - 0.393095463514328
  - 0.38897737860679626
  - 0.39644762873649597
  - 0.4229951500892639
  - 0.4110642373561859
  - 0.39356622099876404
  - 0.3925529420375824
  - 0.6334055066108704
  - 0.65097975730896
  - 0.4004461169242859
  - 0.3996657133102417
  - 0.39236921072006226
  - 0.41483569145202637
  - 0.40293920040130615
  - 0.3922174870967865
  - 0.3950992524623871
  - 0.3885163962841034
  - 0.9603832960128784
  - 0.3894919157028198
  - 0.41090601682662964
  - 0.3931744694709778
  - 0.4000992178916931
  - 0.4023132026195526
  - 0.4025724530220032
  - 0.40901604294776917
  - 0.3918771743774414
loss_records_fold1:
  train_losses:
  - 6.011124068498612
  - 6.1238477498292925
  - 6.0390757650136955
  - 5.979713350534439
  - 6.096326118707657
  - 5.938003033399582
  - 6.010517850518227
  - 6.0118247121572495
  - 6.022240871191025
  - 6.06948842406273
  - 5.953575795888901
  - 5.90679253935814
  validation_losses:
  - 0.39409616589546204
  - 0.39805737137794495
  - 0.40811872482299805
  - 0.3990689516067505
  - 0.3848399519920349
  - 0.40181779861450195
  - 0.40497827529907227
  - 0.40013387799263
  - 0.3894466459751129
  - 0.3847604990005493
  - 0.3945929706096649
  - 0.38426247239112854
loss_records_fold2:
  train_losses:
  - 6.0314594388008125
  - 6.053721788525582
  - 6.031232395768166
  - 5.9225488126277925
  - 5.991966605186462
  - 6.125628802180291
  - 6.1008960783481605
  - 5.9663334637880325
  - 6.053305131196976
  - 5.933310836553574
  - 5.927681463956834
  - 5.9589986383914955
  - 6.0216176301240925
  - 5.974678206443787
  - 5.8915015906095505
  - 5.976650747656823
  - 5.982259646058083
  - 6.006159690022469
  - 5.886976489424706
  - 5.96937869489193
  - 5.784980893135071
  - 6.0824358582496645
  - 6.062813490629196
  - 5.837106397747994
  - 6.084170663356781
  - 5.970788916945458
  - 6.275662669539452
  validation_losses:
  - 0.4034748077392578
  - 0.386905699968338
  - 0.40351352095603943
  - 0.39039966464042664
  - 0.3937484920024872
  - 0.39605456590652466
  - 0.3969777226448059
  - 0.402688205242157
  - 0.3960646390914917
  - 0.40871137380599976
  - 0.39245209097862244
  - 0.3953312635421753
  - 0.3914807140827179
  - 0.4011920094490051
  - 0.41179153323173523
  - 0.3916565477848053
  - 0.39827772974967957
  - 0.41514846682548523
  - 0.3911619782447815
  - 0.4005008637905121
  - 0.4434259235858917
  - 0.3981001079082489
  - 0.39867791533470154
  - 0.3962131440639496
  - 0.39734527468681335
  - 0.40057530999183655
  - 0.39624667167663574
loss_records_fold3:
  train_losses:
  - 6.219408461451531
  - 5.869276782870293
  - 6.0818398058414465
  - 6.0250349849462514
  - 6.076274241507054
  - 6.006268638372422
  - 5.979468232393265
  - 5.963470363616944
  - 5.9449326455593114
  - 6.089144679903985
  - 6.106022900342942
  validation_losses:
  - 0.39252474904060364
  - 0.3755064010620117
  - 0.3856127858161926
  - 0.3890743851661682
  - 0.3845609128475189
  - 0.38002774119377136
  - 0.3874303996562958
  - 0.38087886571884155
  - 0.38530245423316956
  - 0.3931083381175995
  - 0.37577614188194275
loss_records_fold4:
  train_losses:
  - 6.019866132736206
  - 6.096780803799629
  - 6.090044507384301
  - 5.827052396535874
  - 6.078330558538437
  - 5.971125784516335
  - 6.006793865561486
  - 5.888227906823158
  - 5.897736422717571
  - 6.202523437142372
  - 5.913380026817322
  - 6.012608486413956
  - 5.876398608088493
  - 6.067286184430123
  - 5.99881537258625
  validation_losses:
  - 0.386614054441452
  - 0.38860613107681274
  - 0.3788127601146698
  - 0.38005512952804565
  - 0.38715559244155884
  - 0.39837199449539185
  - 0.3883506953716278
  - 0.38044244050979614
  - 0.4259422719478607
  - 0.39222660660743713
  - 0.3759439289569855
  - 0.37836015224456787
  - 0.3836832642555237
  - 0.3878352642059326
  - 0.38095802068710327
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:10:49.088936'
