config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:03:08.625112'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_82fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_82fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_82fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_82fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_82fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_82fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_82fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_82fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_82fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_82fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.7091149628162385
  - 1.5345593214035036
  - 1.484463232755661
  - 1.474622365832329
  - 1.530595600605011
  - 1.4919566035270693
  - 1.5559401929378511
  - 1.4454224586486817
  - 1.5469236552715302
  - 1.4820509254932404
  - 1.569679927825928
  - 1.5145482361316682
  - 1.4587664186954499
  - 1.4646969616413117
  - 1.50118003487587
  - 1.5691210865974428
  - 1.5609421372413637
  - 1.4908474147319795
  - 1.4474734276533128
  - 1.4472242176532746
  - 1.4875412523746492
  - 1.4639316558837892
  - 1.4945762097835542
  - 1.5147821009159088
  - 1.4273544251918793
  - 1.4494146049022676
  - 1.500440376996994
  - 1.455554258823395
  - 1.454002457857132
  - 1.4338547348976136
  - 1.4252443283796312
  - 1.413489207625389
  - 1.4592779755592347
  - 1.4683061122894288
  - 1.4295829474925996
  - 1.4157222151756288
  - 1.4335400938987732
  - 1.4320721983909608
  - 1.461591672897339
  - 1.4215264141559603
  - 1.4334595739841463
  - 1.3864574968814851
  - 1.4314023435115815
  - 1.4378126859664917
  - 1.426949214935303
  - 1.3851008653640748
  - 1.403710550069809
  - 1.4739372849464418
  - 1.4338857412338257
  - 1.4452372431755067
  - 1.4508256435394289
  - 1.5314184606075287
  - 1.3819838881492617
  - 1.4062107622623445
  - 1.4085722863674164
  - 1.481518042087555
  - 1.4922288775444033
  - 1.494908905029297
  - 1.444091033935547
  - 1.4226291924715042
  - 1.428018879890442
  - 1.385111528635025
  - 1.3921694695949556
  - 1.4387503266334534
  - 1.403453803062439
  - 1.3848247408866883
  - 1.4077420830726624
  - 1.3895739793777466
  - 1.3895269155502321
  - 1.4979785144329072
  - 1.476127588748932
  - 1.3727062165737154
  - 1.4022262871265412
  - 1.3675448179244996
  - 1.3913448333740235
  - 1.3746349751949312
  - 1.3929256081581116
  - 1.4031661927700043
  - 1.4123168170452118
  - 1.4035328209400177
  - 1.3838942408561707
  - 1.4366380691528322
  - 1.4053737223148346
  - 1.4065385639667511
  - 1.408685165643692
  - 1.402893215417862
  - 1.469703906774521
  - 1.4273756742477417
  - 1.3839219689369202
  - 1.3781812101602555
  - 1.3818661123514175
  - 1.3822023987770082
  - 1.3852697491645813
  - 1.444402426481247
  - 1.3836447775363923
  - 1.4477865397930145
  - 1.4167065978050233
  - 1.366497340798378
  - 1.4153652727603914
  - 1.3617716997861864
  validation_losses:
  - 0.44358399510383606
  - 0.39261680841445923
  - 0.39033591747283936
  - 0.3882278800010681
  - 0.43022727966308594
  - 0.39637112617492676
  - 0.4082532823085785
  - 0.38555485010147095
  - 0.38628238439559937
  - 0.48426303267478943
  - 0.3984033167362213
  - 0.3923892676830292
  - 0.3998444080352783
  - 0.3859465420246124
  - 0.49651196599006653
  - 0.401561975479126
  - 0.5408109426498413
  - 0.3990861773490906
  - 0.3832670748233795
  - 0.38869693875312805
  - 0.385853111743927
  - 0.6158915162086487
  - 0.4513266086578369
  - 0.3840179145336151
  - 0.3798539340496063
  - 0.5507256388664246
  - 0.42457321286201477
  - 0.37909963726997375
  - 0.3905826210975647
  - 0.506193995475769
  - 0.6796308755874634
  - 0.492351770401001
  - 0.611970841884613
  - 0.4146415591239929
  - 0.3867848217487335
  - 0.4758282005786896
  - 0.3816121220588684
  - 0.3879775106906891
  - 0.40088963508605957
  - 1.0604593753814697
  - 0.42313352227211
  - 0.6178725361824036
  - 0.39708560705184937
  - 0.4413209855556488
  - 0.405275821685791
  - 0.47083353996276855
  - 0.39227673411369324
  - 0.3850252032279968
  - 0.7504313588142395
  - 0.6776217222213745
  - 1.9618868827819824
  - 0.39527279138565063
  - 0.536852240562439
  - 0.7512370347976685
  - 1.1932241916656494
  - 0.4042207896709442
  - 0.3992770314216614
  - 0.40063461661338806
  - 0.4565381705760956
  - 0.8489190340042114
  - 2.4287519454956055
  - 1.278456449508667
  - 0.7308791279792786
  - 0.6402443051338196
  - 0.6528224945068359
  - 1.8102408647537231
  - 1.8496001958847046
  - 1.0629457235336304
  - 0.40262559056282043
  - 0.47060254216194153
  - 1.6507868766784668
  - 1.7193164825439453
  - 2.247978687286377
  - 0.4407133162021637
  - 2.511528730392456
  - 2.1678290367126465
  - 2.7058706283569336
  - 0.3708248734474182
  - 0.40466827154159546
  - 0.5489878058433533
  - 0.9348574280738831
  - 1.4062211513519287
  - 1.1051496267318726
  - 0.3767082691192627
  - 0.4043141305446625
  - 0.4208140969276428
  - 0.644951581954956
  - 0.5295858383178711
  - 1.1989209651947021
  - 0.47960400581359863
  - 0.4829555153846741
  - 0.4205702543258667
  - 1.044087529182434
  - 1.0383802652359009
  - 0.616402268409729
  - 0.6799073219299316
  - 0.4076572358608246
  - 0.39052486419677734
  - 0.368154376745224
  - 0.9659935235977173
loss_records_fold1:
  train_losses:
  - 1.3760661065578461
  - 1.3694034755229951
  - 1.3427673548460008
  - 1.4094552516937258
  - 1.3880915164947512
  - 1.3724072635173798
  - 1.3611883342266085
  - 1.3821536958217622
  - 1.3714751839637758
  - 1.3768559992313385
  - 1.3700691044330597
  - 1.390309637784958
  - 1.3849003851413728
  - 1.3816579639911652
  - 1.4108332931995393
  - 1.397732135653496
  - 1.3571581840515137
  - 1.349638718366623
  - 1.3610306680202484
  - 1.3677212297916412
  - 1.4001490831375123
  - 1.3754803001880647
  - 1.3887422800064089
  - 1.407809215784073
  - 1.3824549943208695
  - 1.3824886798858644
  - 1.3591231644153596
  - 1.3435698211193086
  - 1.3650701880455018
  - 1.3947993397712708
  - 1.3621671199798584
  - 1.397418224811554
  - 1.3766014635562898
  - 1.3458247661590577
  - 1.3904861927032472
  - 1.4006150782108309
  - 1.3702279329299927
  - 1.3527391672134401
  - 1.3285731911659242
  - 1.368323600292206
  - 1.347302097082138
  - 1.3775199294090272
  - 1.388981968164444
  - 1.3596106857061387
  - 1.3754389107227327
  - 1.3832700550556183
  - 1.364282947778702
  - 1.3469116628170015
  - 1.3754614382982255
  - 1.3761267602443696
  - 1.384450513124466
  - 1.375097244977951
  - 1.3332909405231477
  - 1.3465853214263916
  - 1.3794916093349459
  - 1.3736114203929901
  - 1.3646689832210541
  - 1.4224073827266694
  - 1.3760288059711456
  - 1.3579163938760759
  - 1.3569022119045258
  - 1.345821487903595
  - 1.3736535787582398
  - 1.3609458684921265
  - 1.3816568076610567
  - 1.370965373516083
  - 1.3686548590660097
  - 1.471089017391205
  - 1.3731248378753662
  - 1.3796743988990785
  - 1.3788046300411225
  - 1.380198782682419
  - 1.4122727930545809
  - 1.354056739807129
  - 1.3552735269069673
  - 1.4289716958999634
  - 1.3480450034141542
  - 1.3379542589187623
  - 1.39417924284935
  - 1.3623710095882418
  - 1.391775053739548
  - 1.3674294352531433
  - 1.3393119126558304
  - 1.3293636620044709
  - 1.405106782913208
  - 1.3630663096904756
  - 1.3382706046104431
  - 1.3279430806636812
  - 1.3794224202632905
  - 1.3589003622531892
  - 1.3701550722122193
  - 1.3904352307319643
  - 1.3877154767513276
  - 1.3360528856515885
  - 1.3771176099777223
  - 1.4057816326618195
  - 1.3435448735952378
  - 1.3457264065742494
  - 1.3509940326213838
  - 1.3324976533651354
  validation_losses:
  - 0.6795752048492432
  - 0.827873170375824
  - 0.9061970710754395
  - 0.7632731199264526
  - 0.5332561731338501
  - 0.45448631048202515
  - 0.4358341693878174
  - 0.5113308429718018
  - 0.5293792486190796
  - 0.5128095149993896
  - 0.37105250358581543
  - 0.39836403727531433
  - 0.8452239036560059
  - 0.8770211935043335
  - 0.3767242133617401
  - 0.4681895077228546
  - 0.486636757850647
  - 0.9229293465614319
  - 0.6673153042793274
  - 0.4696443974971771
  - 0.5089589953422546
  - 0.7671404480934143
  - 0.3849044442176819
  - 0.42144671082496643
  - 0.4430481791496277
  - 1.2108759880065918
  - 0.4462311863899231
  - 0.422962486743927
  - 0.45677658915519714
  - 0.4942794740200043
  - 0.5072361826896667
  - 0.6759036183357239
  - 0.5211774110794067
  - 0.4024062752723694
  - 0.41810354590415955
  - 0.576873242855072
  - 0.4804106652736664
  - 0.5037205219268799
  - 0.4391785264015198
  - 0.5764888525009155
  - 0.4328315854072571
  - 0.45485368371009827
  - 0.4775371849536896
  - 0.4846310317516327
  - 0.7001249194145203
  - 1.5310121774673462
  - 1.0381728410720825
  - 0.3936397135257721
  - 1.4483572244644165
  - 1.3391616344451904
  - 0.6384530663490295
  - 1.2640241384506226
  - 1.249954342842102
  - 0.44908496737480164
  - 0.5787538886070251
  - 0.43837761878967285
  - 0.8470796346664429
  - 0.4679906964302063
  - 0.4832232892513275
  - 0.5512012243270874
  - 0.933124840259552
  - 1.100671410560608
  - 0.3763648569583893
  - 0.5512259006500244
  - 0.6036481857299805
  - 0.5594959855079651
  - 0.6169824004173279
  - 0.3988342881202698
  - 0.4091304540634155
  - 0.4117819666862488
  - 0.45642080903053284
  - 0.40616896748542786
  - 0.3869452476501465
  - 0.44585147500038147
  - 0.4508228600025177
  - 0.4902683198451996
  - 0.4286591708660126
  - 0.4544312357902527
  - 0.43980878591537476
  - 0.7755493521690369
  - 0.745357871055603
  - 0.3988551199436188
  - 0.3755982220172882
  - 0.7734395861625671
  - 0.7299281358718872
  - 0.8253263235092163
  - 0.6221187114715576
  - 0.5706164240837097
  - 0.6108800768852234
  - 0.5593187808990479
  - 0.39472633600234985
  - 0.39448267221450806
  - 0.4547313451766968
  - 0.8379026055335999
  - 0.4808664917945862
  - 0.5226879119873047
  - 0.6177642941474915
  - 0.5599188208580017
  - 0.5389704704284668
  - 0.80219966173172
loss_records_fold2:
  train_losses:
  - 1.3860540866851807
  - 1.3641506075859071
  - 1.4175106406211855
  - 1.369793063402176
  - 1.32784164249897
  - 1.3401106297969818
  - 1.39048889875412
  - 1.3721979796886445
  - 1.3635341703891755
  - 1.340274304151535
  - 1.3703923225402832
  - 1.3332487463951113
  - 1.3174203842878343
  - 1.3260697662830354
  - 1.3604337751865387
  - 1.346059074997902
  - 1.3553309440612793
  - 1.3725325345993042
  - 1.3273162186145784
  - 1.3287787020206452
  - 1.3137380570173265
  - 1.3483667314052583
  - 1.3778852820396423
  - 1.3433118879795076
  - 1.3158859550952913
  - 1.3533044695854188
  - 1.4055835783481598
  - 1.3694519698619843
  - 1.337459409236908
  - 1.3743360579013826
  - 1.3632476449012758
  - 1.3862022519111634
  - 1.3819122135639192
  - 1.3587866485118867
  - 1.342696648836136
  - 1.3729143798351289
  - 1.3289990812540056
  - 1.336290615797043
  - 1.3278694272041323
  - 1.3163542389869691
  - 1.3198843717575075
  - 1.325258082151413
  - 1.2882856726646423
  - 1.3328351736068726
  - 1.3083000123500825
  - 1.3050532639026642
  - 1.3179929971694948
  - 1.3952170968055726
  - 1.4049281775951385
  - 1.3605714917182923
  - 1.343968278169632
  - 1.2936432480812075
  - 1.325177437067032
  - 1.2879622101783754
  - 1.3137549191713334
  - 1.3248527705669404
  - 1.3268524587154389
  - 1.3561837732791902
  - 1.374339407682419
  - 1.325354200601578
  - 1.3169387489557267
  - 1.3508334994316102
  - 1.2951247513294222
  - 1.3682611644268037
  - 1.3554199159145357
  - 1.2969146132469178
  - 1.3096372663974762
  - 1.352586990594864
  - 1.3208073914051057
  - 1.2805137455463411
  - 1.3107025802135468
  - 1.3000226616859436
  - 1.3571369469165804
  - 1.3169363528490068
  - 1.3488012850284576
  - 1.3645605981349946
  - 1.3252453684806824
  - 1.3228096902370454
  - 1.334306687116623
  - 1.3156868875026704
  - 1.3108692824840547
  - 1.2897676587104798
  - 1.2718519806861879
  - 1.2874350845813751
  - 1.278338646888733
  - 1.266806498169899
  - 1.3580169379711151
  - 1.3456517815589906
  - 1.2937403559684755
  - 1.2759973585605622
  - 1.2555078864097595
  - 1.283519220352173
  - 1.2506777971982956
  - 1.2658734977245332
  - 1.2751474201679232
  - 1.3327288925647736
  - 1.288940852880478
  - 1.2801302313804628
  - 1.251661041378975
  - 1.305072057247162
  validation_losses:
  - 0.5008853077888489
  - 0.5896344780921936
  - 0.567814290523529
  - 0.5113887786865234
  - 0.5596800446510315
  - 0.8675024509429932
  - 0.7867531180381775
  - 0.747200071811676
  - 0.5161281824111938
  - 1.3273961544036865
  - 2.4160730838775635
  - 0.7261215448379517
  - 0.8315008878707886
  - 0.7563449740409851
  - 0.7812513709068298
  - 0.7546918988227844
  - 0.5017158389091492
  - 0.7275711297988892
  - 0.9088118076324463
  - 1.0523326396942139
  - 1.097825050354004
  - 1.4294630289077759
  - 1.0809887647628784
  - 0.9633142352104187
  - 0.817961573600769
  - 1.0812697410583496
  - 0.5439698696136475
  - 1.5663564205169678
  - 1.38381028175354
  - 2.528146266937256
  - 1.9878524541854858
  - 2.764059066772461
  - 2.9155139923095703
  - 1.8685531616210938
  - 2.0596232414245605
  - 2.9447476863861084
  - 1.6200412511825562
  - 1.829071283340454
  - 0.97761470079422
  - 0.6265549063682556
  - 0.5803083181381226
  - 0.5068739652633667
  - 0.6634587049484253
  - 0.8166708946228027
  - 0.7860609889030457
  - 0.67115718126297
  - 2.9000494480133057
  - 0.9544532299041748
  - 0.6826257705688477
  - 0.6044802665710449
  - 0.619480550289154
  - 0.516765832901001
  - 0.5713817477226257
  - 0.537845253944397
  - 0.6861978769302368
  - 0.6179938912391663
  - 0.6870997548103333
  - 0.49620679020881653
  - 0.620747983455658
  - 0.9688930511474609
  - 1.044639229774475
  - 1.2277811765670776
  - 1.1245485544204712
  - 1.0401849746704102
  - 0.8522419929504395
  - 0.6240274906158447
  - 0.6649307608604431
  - 0.7409058809280396
  - 1.092059850692749
  - 1.2062504291534424
  - 0.4701296389102936
  - 0.47832605242729187
  - 0.942511796951294
  - 0.8814431428909302
  - 0.7823104858398438
  - 0.7839531302452087
  - 0.747553288936615
  - 0.737463116645813
  - 0.860930323600769
  - 0.9032973051071167
  - 0.780668318271637
  - 0.8701761364936829
  - 0.7752662301063538
  - 1.934551477432251
  - 1.743678331375122
  - 0.9688708186149597
  - 0.8500570058822632
  - 0.5956087708473206
  - 0.6110430359840393
  - 0.6162145733833313
  - 0.6094800233840942
  - 0.674044132232666
  - 0.6978017091751099
  - 0.7215133309364319
  - 0.6719399094581604
  - 0.7189782857894897
  - 0.5989512205123901
  - 0.825626015663147
  - 0.9470628499984741
  - 0.7146753072738647
loss_records_fold3:
  train_losses:
  - 1.3143842637538912
  - 1.3009761571884155
  - 1.3017450213432313
  - 1.3133752524852753
  - 1.2814839273691179
  - 1.3425843596458436
  - 1.3012874454259873
  - 1.424397373199463
  - 1.3732620358467102
  - 1.3602203339338304
  - 1.3531885743141174
  - 1.2969313621520997
  - 1.2999658524990083
  - 1.3306455194950104
  - 1.3282374113798143
  - 1.3299896061420442
  - 1.3552050769329071
  - 1.316583824157715
  - 1.3919822216033937
  - 1.309644192457199
  - 1.3112185895442963
  - 1.3028118669986726
  - 1.2935604006052017
  - 1.3034746170043947
  - 1.3039036065340044
  - 1.2944133311510087
  - 1.2643181800842287
  - 1.3388820052146913
  - 1.321206831932068
  - 1.341692778468132
  - 1.3549148797988892
  - 1.320638281106949
  - 1.4108531951904297
  - 1.3305102586746216
  - 1.283364546298981
  - 1.315532773733139
  - 1.3044723302125931
  - 1.3226306915283204
  - 1.3731224030256273
  - 1.2764813750982285
  - 1.3143488585948946
  - 1.2839305877685547
  - 1.3104574203491213
  - 1.3328103959560396
  - 1.2933686524629593
  - 1.35230250954628
  - 1.362547641992569
  - 1.315680518746376
  - 1.302525669336319
  - 1.3254882812500002
  - 1.3042826354503632
  - 1.2709974974393845
  - 1.2510042995214463
  - 1.28269162774086
  - 1.255384585261345
  - 1.2552297681570055
  - 1.3076880633831025
  - 1.365938597917557
  - 1.3127729177474976
  - 1.2941670238971712
  - 1.3106467783451081
  - 1.321349447965622
  - 1.3154899895191194
  - 1.2948880970478058
  - 1.2613566339015962
  - 1.2386394977569581
  - 1.2848157703876497
  - 1.280428946018219
  - 1.2972229182720185
  - 1.3111893504858019
  - 1.282749092578888
  - 1.3462490975856782
  - 1.3261735022068024
  - 1.2631830394268038
  - 1.2652060449123383
  - 1.243819233775139
  - 1.3051011234521868
  - 1.2678701370954515
  - 1.3011185735464097
  - 1.2417853385210038
  - 1.2794080495834352
  - 1.2288794577121736
  - 1.2334545314311982
  - 1.2536284655332566
  - 1.3251328647136689
  - 1.2902476310729982
  - 1.2937872767448426
  - 1.2998694479465485
  - 1.3192901492118836
  - 1.2768789529800415
  - 1.2947063505649568
  - 1.2766440570354463
  - 1.2592060148715973
  - 1.2814174622297287
  - 1.239451801776886
  - 1.312821513414383
  - 1.2679497301578522
  - 1.285543030500412
  - 1.28084996342659
  - 1.3377662241458894
  validation_losses:
  - 1.1723051071166992
  - 2.136092185974121
  - 1.3887567520141602
  - 0.5507218241691589
  - 0.7409220337867737
  - 0.7072508335113525
  - 1.4549634456634521
  - 2.0567150115966797
  - 1.30770742893219
  - 0.4606911242008209
  - 0.5363650321960449
  - 0.6227983832359314
  - 8.133620262145996
  - 0.9318812489509583
  - 0.6627473831176758
  - 0.6378645300865173
  - 0.7623138427734375
  - 0.6725605130195618
  - 0.6972898244857788
  - 0.6534134149551392
  - 0.541636049747467
  - 0.6514139175415039
  - 0.7040417790412903
  - 0.7048904895782471
  - 0.676774799823761
  - 0.6638190746307373
  - 0.5875597596168518
  - 0.6593817472457886
  - 0.8999857306480408
  - 0.6042576432228088
  - 0.8127148747444153
  - 0.5825863480567932
  - 0.45604413747787476
  - 0.6579897999763489
  - 0.637272834777832
  - 0.833461582660675
  - 0.8957796096801758
  - 0.8149698376655579
  - 0.6480118036270142
  - 0.8067442178726196
  - 0.905971109867096
  - 0.8586722016334534
  - 0.967667281627655
  - 0.7881325483322144
  - 0.6568695902824402
  - 0.7693958282470703
  - 0.8303269743919373
  - 0.847509503364563
  - 0.7579618692398071
  - 0.6997064352035522
  - 0.7411696910858154
  - 0.7627008557319641
  - 0.8326007127761841
  - 0.770334005355835
  - 0.7745195031166077
  - 0.9008259773254395
  - 0.5951804518699646
  - 0.7934423089027405
  - 0.810267448425293
  - 0.9139350056648254
  - 0.8881850838661194
  - 0.8794808983802795
  - 0.907429575920105
  - 0.6673135161399841
  - 0.8886852860450745
  - 0.8367408514022827
  - 0.6941137909889221
  - 0.8622348308563232
  - 0.7897168397903442
  - 0.8947936296463013
  - 1.1188271045684814
  - 0.7103464007377625
  - 0.7473008632659912
  - 0.8617279529571533
  - 0.8885122537612915
  - 0.7308114171028137
  - 1.0461994409561157
  - 0.9897130131721497
  - 0.8956031203269958
  - 0.8055751323699951
  - 0.9892961382865906
  - 0.7033115029335022
  - 0.9187883138656616
  - 0.7158817052841187
  - 0.8416321277618408
  - 0.5794543623924255
  - 0.7767424583435059
  - 0.660214364528656
  - 0.6078087091445923
  - 0.7104390859603882
  - 0.7165020704269409
  - 0.6916037201881409
  - 0.7735590934753418
  - 0.8030455112457275
  - 0.6551633477210999
  - 0.7342050075531006
  - 0.7825153470039368
  - 0.6837011575698853
  - 3.3531832695007324
  - 0.7001088857650757
loss_records_fold4:
  train_losses:
  - 1.3573677182197572
  - 1.3235323309898377
  - 1.3274733006954194
  - 1.2643495976924897
  - 1.3204413771629335
  - 1.281183397769928
  - 1.3068985342979431
  - 1.2845973551273346
  - 1.2971015453338623
  - 1.251540756225586
  - 1.2697555392980577
  - 1.3174774140119554
  - 1.333563929796219
  - 1.3028734445571901
  - 1.2637815713882448
  - 1.2687059611082079
  - 1.2498097181320191
  - 1.29621921479702
  - 1.3158097803592683
  - 1.2940553665161134
  - 1.2692806452512742
  - 1.2740959435701371
  - 1.2688999295234682
  - 1.289521497488022
  - 1.250851795077324
  - 1.2593235403299332
  - 1.29156194627285
  - 1.2494736284017565
  - 1.3088200092315674
  - 1.2533496081829072
  - 1.2556149154901506
  - 1.2823597371578217
  - 1.2671455085277559
  - 1.2962766468524933
  - 1.259664696455002
  - 1.221821165084839
  - 1.2467272609472275
  - 1.2974882006645203
  - 1.3155898422002794
  - 1.2623447626829147
  - 1.325156879425049
  - 1.2826074063777924
  - 1.2426558315753937
  - 1.2728419691324235
  - 1.2560319751501083
  - 1.2875110268592835
  - 1.4323756575584412
  - 1.4350111961364747
  - 1.3828332901000977
  - 1.3501710057258607
  - 1.3545470148324967
  - 1.3005522251129151
  - 1.2812072694301606
  - 1.3024771332740785
  - 1.3283088266849519
  - 1.2510061025619508
  - 1.3347088873386384
  - 1.2791557401418687
  - 1.2395760774612428
  - 1.2754755258560182
  - 1.2697948873043061
  - 1.2835321485996247
  - 1.256574547290802
  - 1.2595085680484772
  - 1.257982435822487
  - 1.2612997472286225
  - 1.2542690724134447
  - 1.2899258136749268
  - 1.2724363118410111
  - 1.245549175143242
  - 1.2540631651878358
  - 1.2700228154659272
  - 1.267675906419754
  - 1.312508621811867
  - 1.2541654467582704
  - 1.28944650888443
  - 1.3062134057283403
  - 1.2505959540605547
  - 1.2452797412872316
  - 1.2739591598510742
  - 1.2479342877864839
  - 1.2806283295154572
  - 1.2438208848237993
  - 1.2416403770446778
  - 1.290433728694916
  - 1.249261522293091
  - 1.2338118433952332
  - 1.255420932173729
  - 1.2201720952987671
  - 1.2298254013061525
  - 1.211471724510193
  - 1.2596328794956209
  - 1.285568130016327
  - 1.277522885799408
  - 1.213186502456665
  - 1.2786222845315933
  - 1.2523525059223175
  - 1.2245265781879426
  - 1.2404786229133606
  - 1.1980818778276443
  validation_losses:
  - 0.6790699362754822
  - 0.7172560095787048
  - 0.718197762966156
  - 0.7623378038406372
  - 0.7159884572029114
  - 0.7956106066703796
  - 0.6115716695785522
  - 0.7796590924263
  - 0.8201159238815308
  - 0.7433786392211914
  - 0.7192240953445435
  - 0.6351983547210693
  - 0.7254819273948669
  - 0.7718271017074585
  - 0.8372028470039368
  - 0.772643506526947
  - 0.5945866703987122
  - 0.7993062138557434
  - 0.7906139492988586
  - 0.9126132130622864
  - 0.7444562911987305
  - 0.749843955039978
  - 0.8162825703620911
  - 0.7259754538536072
  - 1.0425498485565186
  - 0.7423645257949829
  - 0.7501921653747559
  - 0.5624877214431763
  - 0.7617445588111877
  - 0.833854079246521
  - 0.8285692930221558
  - 0.7388073801994324
  - 0.7132763862609863
  - 0.8265289664268494
  - 0.6210325956344604
  - 0.7978945970535278
  - 0.6862455606460571
  - 0.7645781636238098
  - 0.9975866079330444
  - 0.8393577933311462
  - 0.7129238843917847
  - 0.7355945110321045
  - 0.9874997735023499
  - 0.7813811302185059
  - 0.8706550598144531
  - 0.6986976265907288
  - 0.6859490871429443
  - 0.6456931233406067
  - 0.4301028847694397
  - 0.46879541873931885
  - 0.4860683083534241
  - 0.5499155521392822
  - 0.6184526681900024
  - 0.5430301427841187
  - 0.583418607711792
  - 0.5672114491462708
  - 0.6633418798446655
  - 0.6407520174980164
  - 0.645264208316803
  - 0.6884527802467346
  - 0.5890839695930481
  - 0.6446992754936218
  - 0.6642206311225891
  - 0.6317479014396667
  - 0.6879408359527588
  - 0.7033212184906006
  - 0.7031782865524292
  - 0.767578661441803
  - 0.6553621888160706
  - 0.6382567286491394
  - 0.6592898964881897
  - 0.7399515509605408
  - 0.572252094745636
  - 0.6059028506278992
  - 0.6326549053192139
  - 0.6829556226730347
  - 0.7443363666534424
  - 0.6428585648536682
  - 0.7305244207382202
  - 0.6935083866119385
  - 0.7127789855003357
  - 0.6368261575698853
  - 0.6359209418296814
  - 0.5795354247093201
  - 0.6305274367332458
  - 0.639804482460022
  - 0.723213255405426
  - 0.6372367739677429
  - 0.7341358661651611
  - 0.8720694184303284
  - 0.8121381998062134
  - 0.729254961013794
  - 0.7292875647544861
  - 0.6634328961372375
  - 0.6644040942192078
  - 0.7166334390640259
  - 0.7127793431282043
  - 0.7360928654670715
  - 0.6577883362770081
  - 0.6776712536811829
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8216123499142367, 0.8387650085763293, 0.8319039451114922,
    0.8419243986254296]'
  fold_eval_f1: '[0.16161616161616163, 0.11864406779661017, 0.27692307692307694, 0.19672131147540983,
    0.17857142857142858]'
  mean_eval_accuracy: 0.8383677270664238
  mean_f1_accuracy: 0.18649520927653743
  total_train_time: '0:43:00.079977'
