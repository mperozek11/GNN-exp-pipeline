config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:43:57.202539'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_57fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_57fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_57fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_57fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_57fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_57fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_57fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_57fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_57fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_57fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 37.52342293262482
  - 12.480281269550325
  - 15.821101158857346
  - 10.379472920298577
  - 8.545760267972947
  - 5.6245918571949005
  - 8.41671572327614
  - 4.604915311932564
  - 3.7320395320653916
  - 3.9655894100666047
  - 4.239034289121628
  - 5.844216710329056
  - 8.064966154098512
  - 5.596669262647629
  - 3.6211866915225985
  - 5.603388574719429
  - 10.138267147541047
  - 6.406308865547181
  - 4.605290359258652
  - 5.761754184961319
  - 4.828935188055039
  - 7.054203587770463
  - 4.84213650226593
  - 9.523446944355966
  - 3.469777002930641
  - 5.6526179939508445
  - 4.021589976549149
  - 4.86778833270073
  - 7.525251543521882
  - 3.7258041560649873
  - 3.361462113261223
  - 3.794988548755646
  - 4.010672092437744
  - 4.434365472197533
  - 3.4909707903862
  - 3.2583302438259127
  - 3.5276528835296634
  - 4.118068259954453
  - 3.51756386756897
  - 3.2240305840969086
  - 4.260792446136475
  - 4.832625460624695
  - 3.631033140420914
  - 4.366660556197167
  - 3.5570592641830445
  - 6.061572241783143
  - 4.053307843208313
  - 3.554071253538132
  - 3.3768452972173693
  - 3.580913239717484
  - 2.956105500459671
  - 3.5398029625415806
  - 2.9757609724998475
  - 3.114077937602997
  - 3.106182298064232
  - 3.173914593458176
  - 3.218532934784889
  - 3.019564500451088
  - 3.3362847834825518
  - 3.0882197558879856
  - 3.156531438231468
  - 2.9532644897699356
  - 3.196650215983391
  - 3.2118235290050507
  - 3.21065046787262
  - 3.1167141735553745
  - 2.9455892086029056
  - 2.9101727664470673
  - 2.8454237997531893
  - 2.857303160429001
  - 3.0712881207466127
  - 3.2723139464855198
  - 3.4443700641393664
  - 3.3475502669811252
  - 2.922467377781868
  - 3.041750687360764
  - 2.9755015641450884
  - 2.838036695122719
  - 2.965105351805687
  - 2.9442795872688294
  validation_losses:
  - 1.6732096672058105
  - 1.8509827852249146
  - 1.1311432123184204
  - 0.49939683079719543
  - 0.4416413903236389
  - 0.49538761377334595
  - 0.5634850859642029
  - 0.722743570804596
  - 0.41193780303001404
  - 0.5389145612716675
  - 0.8784317970275879
  - 0.42100900411605835
  - 0.3995063900947571
  - 0.5210583806037903
  - 0.38190150260925293
  - 0.4016680419445038
  - 0.4946368634700775
  - 0.4164964258670807
  - 0.44229498505592346
  - 0.4716830551624298
  - 0.4296538233757019
  - 0.40535876154899597
  - 0.4990529716014862
  - 0.4566124379634857
  - 0.3949527442455292
  - 0.3839664161205292
  - 0.579498827457428
  - 0.4449783265590668
  - 0.3998473882675171
  - 0.3967573344707489
  - 0.3858768343925476
  - 0.42204561829566956
  - 0.3895540237426758
  - 0.3854757249355316
  - 0.3859761655330658
  - 0.3833293616771698
  - 0.42116984724998474
  - 0.3858596086502075
  - 0.4200422763824463
  - 0.4261886179447174
  - 1.0487900972366333
  - 0.37899938225746155
  - 0.41900551319122314
  - 0.389059454202652
  - 0.42840468883514404
  - 0.3946308195590973
  - 0.4369726777076721
  - 0.5063465237617493
  - 0.38543933629989624
  - 0.3811165690422058
  - 0.4499186873435974
  - 0.3779374361038208
  - 0.43166354298591614
  - 0.3828849196434021
  - 0.38518837094306946
  - 0.5099902153015137
  - 0.38115188479423523
  - 0.3957875967025757
  - 0.4748196005821228
  - 0.38477516174316406
  - 0.3923742473125458
  - 0.3985525965690613
  - 0.38581204414367676
  - 0.4312724769115448
  - 0.40951210260391235
  - 0.3789689838886261
  - 0.4705671966075897
  - 0.46330928802490234
  - 0.40931272506713867
  - 0.3792803883552551
  - 0.3816981613636017
  - 0.3983001708984375
  - 0.3803427815437317
  - 0.40390250086784363
  - 0.3813032805919647
  - 0.37719470262527466
  - 0.37669435143470764
  - 0.3805364668369293
  - 0.38329043984413147
  - 0.3898966610431671
loss_records_fold1:
  train_losses:
  - 3.0019908338785175
  - 2.920187458395958
  - 2.991790467500687
  - 2.887231016159058
  - 2.823502978682518
  - 2.924360740184784
  - 2.9238747835159304
  - 2.8984569728374483
  - 2.8703025072813038
  - 3.0050040185451508
  - 2.8868048757314684
  - 2.89143622815609
  - 2.8210492730140686
  - 3.1708518445491793
  - 3.005669850111008
  - 3.1219002425670626
  - 2.9144333600997925
  - 2.8841454148292542
  - 2.9697244405746464
  - 3.2141624212265016
  - 3.127541270852089
  - 3.065057384967804
  - 2.9658310234546663
  - 3.0046239584684376
  - 3.048889008164406
  - 3.07294602394104
  - 2.971187010407448
  - 3.0075124561786652
  - 2.8566120088100435
  - 2.954137033224106
  - 3.011238443851471
  - 3.1679497152566913
  - 2.8917894065380096
  - 2.9768423557281496
  - 2.860201907157898
  - 2.897526687383652
  - 2.7944490492343905
  - 2.8411705255508424
  - 2.925573003292084
  - 2.8568114757537844
  - 2.8080026119947434
  - 2.91963367164135
  - 2.905003398656845
  - 2.9806695967912678
  - 2.7889782398939134
  - 2.807348355650902
  - 2.838695523142815
  - 2.868162965774536
  - 2.8434540688991548
  - 2.814938944578171
  - 2.8358110278844837
  - 2.899420917034149
  - 2.8056301325559616
  - 2.7759656369686128
  - 2.807929435372353
  - 2.9194293916225433
  - 2.823248353600502
  - 2.859446755051613
  - 2.8347775965929034
  - 2.883673289418221
  - 2.827432253956795
  - 2.8080641597509386
  - 2.788019359111786
  - 2.9596740245819095
  - 2.800173446536064
  - 2.9108900606632235
  - 2.8746473073959353
  - 2.765869176387787
  - 2.9259544909000397
  - 2.8219944536685944
  - 2.889540466666222
  - 2.857606416940689
  - 2.768698358535767
  - 2.742961505055428
  validation_losses:
  - 0.4211004078388214
  - 0.4709581732749939
  - 0.40353062748908997
  - 0.41178154945373535
  - 0.43268418312072754
  - 0.3940703570842743
  - 0.44681352376937866
  - 0.3988080620765686
  - 0.3968741297721863
  - 0.4041427671909332
  - 0.5306670069694519
  - 0.4095808267593384
  - 0.41862183809280396
  - 0.43506598472595215
  - 0.5132418274879456
  - 0.40461719036102295
  - 0.3981270492076874
  - 0.4047459661960602
  - 0.408645361661911
  - 0.5366094708442688
  - 0.40387681126594543
  - 0.40638741850852966
  - 0.41310587525367737
  - 0.39971598982810974
  - 0.420286625623703
  - 0.4277667999267578
  - 0.40248095989227295
  - 0.40068554878234863
  - 0.42589911818504333
  - 0.4122397005558014
  - 0.42273861169815063
  - 0.4134939908981323
  - 0.5270176529884338
  - 0.39285027980804443
  - 0.441058486700058
  - 0.39720484614372253
  - 0.47960716485977173
  - 0.39300036430358887
  - 0.3941338062286377
  - 0.39469432830810547
  - 0.419515997171402
  - 0.4166785180568695
  - 0.39731892943382263
  - 0.47875139117240906
  - 0.4128665328025818
  - 0.4082224369049072
  - 0.3973883092403412
  - 0.47474434971809387
  - 0.4809892475605011
  - 0.400068074464798
  - 0.39310815930366516
  - 0.39578425884246826
  - 0.4243106544017792
  - 0.392839640378952
  - 0.40897226333618164
  - 0.3935541808605194
  - 0.4205319881439209
  - 0.40193673968315125
  - 0.3969971537590027
  - 0.4717757999897003
  - 0.4618381857872009
  - 0.41109779477119446
  - 0.607757031917572
  - 0.4640348255634308
  - 0.4027542471885681
  - 0.4620914161205292
  - 0.4249209761619568
  - 0.4427734315395355
  - 0.3962280750274658
  - 0.3945193290710449
  - 0.3920384645462036
  - 0.3989453911781311
  - 0.40814441442489624
  - 0.38878923654556274
loss_records_fold2:
  train_losses:
  - 2.9015397757291796
  - 2.8662273317575457
  - 2.7786874115467075
  - 2.9736425012350085
  - 3.067305174469948
  - 2.85631582736969
  - 3.2535301595926285
  - 2.9830250024795535
  - 2.953146988153458
  - 2.9447430193424227
  - 2.914418011903763
  - 3.0204956948757173
  - 2.9358629286289215
  - 2.900139033794403
  - 2.839019811153412
  - 2.864821743965149
  - 2.8558909058570863
  - 2.948768743872643
  - 3.0103535652160645
  - 2.864769768714905
  - 2.848893812298775
  - 2.8538956582546238
  validation_losses:
  - 0.3687450885772705
  - 0.3741772770881653
  - 0.37495046854019165
  - 0.3905934989452362
  - 0.3772849142551422
  - 0.3948045074939728
  - 0.38393038511276245
  - 0.39089009165763855
  - 0.37908145785331726
  - 0.3875808119773865
  - 0.4001513719558716
  - 0.37695589661598206
  - 0.43490344285964966
  - 0.37020257115364075
  - 0.3702751696109772
  - 0.3871292471885681
  - 0.3795357644557953
  - 0.37007254362106323
  - 0.37883830070495605
  - 0.3755984604358673
  - 0.3711909353733063
  - 0.37478309869766235
loss_records_fold3:
  train_losses:
  - 2.838246202468872
  - 2.79514052271843
  - 2.779575222730637
  - 2.825502344965935
  - 2.857986414432526
  - 2.904284113645554
  - 2.798368698358536
  - 2.8902850031852725
  - 2.9065924257040026
  - 2.8683506548404694
  - 3.193825060129166
  - 2.893046820163727
  - 2.8971884667873384
  - 2.870150047540665
  - 2.808040475845337
  - 2.8123504132032395
  - 2.811763805150986
  - 2.772068038582802
  - 2.854147630929947
  - 2.8508184373378755
  - 2.9571414381265644
  - 2.976994970440865
  - 2.759828418493271
  - 2.8301949501037598
  - 2.8180853068828586
  - 3.1814490020275117
  - 3.0362796902656557
  - 2.8868083119392396
  - 2.8671825736761094
  - 3.0425321698188785
  - 2.84801167845726
  - 2.878382584452629
  - 2.8686420261859897
  - 2.8455572158098223
  - 2.862741905450821
  - 2.868899935483933
  - 2.90364271402359
  - 2.8937034964561463
  - 2.977319222688675
  - 2.80402107834816
  - 2.9588928014039997
  - 3.6158129572868347
  - 3.1552283406257633
  - 2.9613253951072696
  - 2.9021867513656616
  - 2.8701548814773563
  - 2.8838035941123965
  - 2.84675087928772
  - 2.872925114631653
  - 2.8790626347064974
  - 2.884650558233261
  - 2.871253949403763
  - 3.0221018850803376
  - 2.8612144470214846
  - 2.9221100568771363
  - 3.0837246596813204
  - 2.8647496700286865
  - 2.9150354325771333
  - 2.835684469342232
  - 2.8258829295635226
  - 2.8760617613792423
  - 2.9243838369846347
  - 3.0179236710071566
  - 2.8450625956058504
  - 2.8833761870861054
  - 2.9779330015182497
  - 2.8274711191654207
  validation_losses:
  - 0.3750995993614197
  - 0.3685847520828247
  - 0.3719990849494934
  - 0.3757031261920929
  - 0.37879377603530884
  - 0.37577536702156067
  - 0.377282977104187
  - 0.3780921399593353
  - 0.3789162039756775
  - 0.4696255624294281
  - 0.3696114718914032
  - 0.37386077642440796
  - 0.45632654428482056
  - 0.41625338792800903
  - 0.36991021037101746
  - 0.39005109667778015
  - 0.377234548330307
  - 0.37272122502326965
  - 0.37203651666641235
  - 0.3701750338077545
  - 0.502007007598877
  - 0.400539755821228
  - 0.43300163745880127
  - 0.3696024715900421
  - 0.4052468538284302
  - 0.43588879704475403
  - 0.40805500745773315
  - 0.3741253912448883
  - 0.45609050989151
  - 0.3824771046638489
  - 0.3940543234348297
  - 0.3913877308368683
  - 0.41317248344421387
  - 0.37259441614151
  - 0.40583762526512146
  - 0.3744118809700012
  - 2.555426597595215
  - 458.9092712402344
  - 0.3911136984825134
  - 6.756909370422363
  - 84568.734375
  - 0.39185163378715515
  - 0.438874214887619
  - 0.4104239046573639
  - 0.3885965943336487
  - 0.40780144929885864
  - 0.6522432565689087
  - 30.370309829711914
  - 0.386656790971756
  - 0.37582215666770935
  - 0.40991348028182983
  - 0.3808104693889618
  - 0.37886765599250793
  - 0.3764778971672058
  - 0.39499974250793457
  - 0.38284263014793396
  - 0.37656766176223755
  - 0.3757040202617645
  - 0.38626450300216675
  - 0.3899655044078827
  - 7.897467136383057
  - 0.5220289826393127
  - 0.37969040870666504
  - 0.3835679590702057
  - 0.39164507389068604
  - 0.3908635973930359
  - 0.38390710949897766
loss_records_fold4:
  train_losses:
  - 2.845564976334572
  - 2.9970297276973725
  - 2.9214574813842775
  - 2.9378501176834106
  - 2.817456614971161
  - 3.001116615533829
  - 2.8999111115932465
  - 2.865049803256989
  - 2.9330630004405975
  - 2.8533602833747866
  - 2.908758682012558
  validation_losses:
  - 0.39304402470588684
  - 0.37777701020240784
  - 0.38117948174476624
  - 0.3797204792499542
  - 0.38604244589805603
  - 0.37840011715888977
  - 0.37955233454704285
  - 0.38517996668815613
  - 0.3901253938674927
  - 0.37965917587280273
  - 0.38913002610206604
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 80 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 74 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 67 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:24:10.268539'
