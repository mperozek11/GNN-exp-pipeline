config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 19:09:48.125614'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_126fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_126fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_126fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_126fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_126fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_126fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_126fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_126fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_126fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_126fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 26.634020757675174
  - 14.545769584178926
  - 6.683369380235672
  - 3.0940791189670565
  - 6.3369105696678165
  - 9.01940085887909
  - 4.764309388399124
  - 4.78526759147644
  - 2.696620661020279
  - 3.515078151226044
  - 3.6537183165550235
  - 2.847723996639252
  - 2.1139891147613525
  - 12.566743487119675
  - 8.094400078058243
  - 5.924462145566941
  - 2.967348259687424
  - 1.9387034296989443
  - 2.6509575247764587
  - 5.718255752325058
  - 4.118408519029617
  - 8.82980365753174
  - 12.100619101524353
  - 2.967627388238907
  - 10.764679324626924
  - 13.491823357343675
  - 7.916825044155122
  - 11.088799554109574
  - 4.557831007242203
  - 5.014804166555405
  - 3.0393415629863743
  - 3.211192005872727
  - 2.2199739038944246
  - 2.5324566900730137
  - 3.872984081506729
  - 16.174385899305346
  - 6.408413189649583
  - 3.3326071918010713
  - 2.376755249500275
  - 3.1528820216655733
  - 1.9849675655364991
  - 13.23423022031784
  - 3.873189663887024
  - 2.175769805908203
  - 2.136010080575943
  - 2.445514899492264
  - 2.938425147533417
  - 2.9547497212886813
  - 1.7876342773437501
  - 1.9117100954055788
  - 8.764270436763764
  - 4.489478307962417
  - 4.8828651964664465
  - 2.5523495316505436
  - 1.7046380937099457
  - 1.774020290374756
  - 9.70662415623665
  - 3.4098593413829805
  - 4.500926965475083
  - 6.144072061777115
  - 3.6414199471473694
  - 3.1366185754537583
  - 3.1533461630344393
  - 3.4783180892467502
  - 2.0276610910892487
  - 1.9344050943851472
  - 1.9579795718193056
  - 1.6850859820842743
  - 2.504913938045502
  - 1.8148060381412507
  - 2.7639071941375732
  - 1.9133224904537203
  - 1.7887031018733979
  - 2.0134097635746
  - 1.7092763960361481
  - 1.8788448929786683
  - 2.280685544013977
  - 2.3537012815475467
  - 1.8676855862140656
  - 1.8205722749233246
  - 1.8843529880046845
  - 1.8129821658134462
  - 1.6904139637947084
  - 1.847713589668274
  - 1.7990097641944887
  - 1.8764858663082125
  - 1.6931486636400224
  - 1.6357381880283357
  - 3.3744079232215882
  - 1.8346988499164583
  - 1.6184152960777283
  - 1.9470428705215455
  - 1.5812690675258638
  - 1.6490155816078187
  - 1.7733562707901003
  - 1.6448684334754944
  - 1.6235495150089265
  - 1.629687386751175
  - 2.6323664724826816
  - 1.6720235109329225
  validation_losses:
  - 12.87417221069336
  - 1.3486311435699463
  - 0.6134395599365234
  - 0.8393529057502747
  - 0.7577816843986511
  - 0.8760523796081543
  - 0.7751426100730896
  - 0.49460914731025696
  - 0.5113980174064636
  - 1.384526014328003
  - 1.105098009109497
  - 1.0644162893295288
  - 0.8281872868537903
  - 0.43154582381248474
  - 1.091822862625122
  - 0.5187147855758667
  - 0.42507314682006836
  - 0.42393234372138977
  - 0.506973385810852
  - 0.43014901876449585
  - 0.41311174631118774
  - 0.48816412687301636
  - 0.6754658222198486
  - 0.4512210786342621
  - 0.5012028217315674
  - 17.834835052490234
  - 0.49676084518432617
  - 0.6023650765419006
  - 0.6581682562828064
  - 0.47338345646858215
  - 0.4419824779033661
  - 0.40446019172668457
  - 0.4166117012500763
  - 0.3929167091846466
  - 0.5079837441444397
  - 0.5346883535385132
  - 0.5782454013824463
  - 0.46515125036239624
  - 0.47031766176223755
  - 0.3921743333339691
  - 0.4497147798538208
  - 0.45809921622276306
  - 0.40336042642593384
  - 0.4640049636363983
  - 0.3972933888435364
  - 0.42982473969459534
  - 0.3940235376358032
  - 0.4188971221446991
  - 0.38844865560531616
  - 0.4069080352783203
  - 0.4044603705406189
  - 0.3936309516429901
  - 0.4246470332145691
  - 0.3899115324020386
  - 0.39667636156082153
  - 0.6418888568878174
  - 0.37521079182624817
  - 0.3840792775154114
  - 0.41903597116470337
  - 0.47943687438964844
  - 0.3940483331680298
  - 0.43488115072250366
  - 0.3979582190513611
  - 0.47271573543548584
  - 0.6358783841133118
  - 0.40299326181411743
  - 0.3909252882003784
  - 0.414943128824234
  - 0.41540756821632385
  - 0.3908509612083435
  - 0.5378005504608154
  - 0.39377206563949585
  - 0.4169767498970032
  - 0.3888576030731201
  - 0.3869277238845825
  - 0.395883172750473
  - 0.3962811529636383
  - 0.41833773255348206
  - 0.3912787139415741
  - 0.3897828757762909
  - 0.43228232860565186
  - 0.3962465226650238
  - 0.5284837484359741
  - 0.4279243052005768
  - 0.41789284348487854
  - 0.380636066198349
  - 0.3971800208091736
  - 0.40169718861579895
  - 0.42322176694869995
  - 0.39490342140197754
  - 0.3855523467063904
  - 0.41654086112976074
  - 0.3946426212787628
  - 0.397579550743103
  - 0.43317437171936035
  - 0.3940257132053375
  - 0.3979407250881195
  - 0.4280736446380615
  - 0.39560237526893616
  - 0.39645522832870483
loss_records_fold1:
  train_losses:
  - 1.6328167736530306
  - 2.510915440320969
  - 1.89805446267128
  - 1.7645559310913086
  - 1.6112150579690934
  - 2.0208318889141084
  - 1.8294951796531678
  - 1.6458507895469667
  - 1.5647386431694033
  - 1.548170965909958
  - 1.597547346353531
  - 2.550877332687378
  - 1.9166646957397462
  - 1.6572684586048128
  - 1.6388653457164766
  - 2.9918692052364353
  - 1.5991948604583741
  - 1.562226873636246
  - 1.6460404813289644
  - 1.613380128145218
  - 1.5975053668022157
  - 1.6681543052196504
  - 1.5383672416210175
  - 1.7062702655792237
  - 2.016377991437912
  - 1.8449159562587738
  - 1.844877314567566
  - 1.8127526998519898
  - 1.5483335316181184
  - 1.8043689846992494
  - 3.0259824991226196
  validation_losses:
  - 0.4137837290763855
  - 0.4076751172542572
  - 0.40566933155059814
  - 0.46374523639678955
  - 0.39537370204925537
  - 0.5291973352432251
  - 0.3993619680404663
  - 0.4014168679714203
  - 0.41244277358055115
  - 0.43317562341690063
  - 0.4104876220226288
  - 0.4241337478160858
  - 0.4287823736667633
  - 0.42541688680648804
  - 0.40873414278030396
  - 0.41386160254478455
  - 0.4105132520198822
  - 0.5008281469345093
  - 0.4209742248058319
  - 0.41871321201324463
  - 0.4008457362651825
  - 0.391762375831604
  - 0.42312753200531006
  - 0.46252819895744324
  - 0.4822383522987366
  - 0.4778313636779785
  - 0.418484091758728
  - 0.42641565203666687
  - 0.3982732892036438
  - 0.395950049161911
  - 0.401492714881897
loss_records_fold2:
  train_losses:
  - 2.28956179022789
  - 3.1006199777126313
  - 2.284998464584351
  - 1.60302694439888
  - 1.7010214865207673
  - 1.6036530315876008
  - 1.9467477917671205
  - 1.7190601438283921
  - 1.663090854883194
  - 1.6083418011665345
  - 1.781685745716095
  - 2.00747806429863
  - 1.6302880644798279
  - 1.6508237719535828
  - 1.6287706553936006
  - 1.637346076965332
  - 1.7137094020843506
  - 3.8326773524284365
  - 2.3270952820777895
  - 1.8789555728435516
  - 1.731566333770752
  - 1.6898187279701233
  - 2.1783021450042725
  - 2.8906469345092773
  - 1.80039798617363
  - 1.9206092178821565
  - 1.6103388011455537
  - 1.5811250805854797
  - 1.6174827337265016
  - 1.6332121312618257
  - 1.6709283947944642
  - 1.6621436774730682
  - 2.0240142941474915
  - 1.9228188455104829
  - 1.599507087469101
  - 1.7161914169788361
  - 2.0626504182815553
  - 1.7725272536277772
  - 1.803468245267868
  - 1.8872052490711213
  - 1.5985306322574617
  - 1.5575127333402634
  - 2.825662332773209
  - 2.6251399993896487
  - 1.6763462960720064
  - 1.6478924691677095
  - 1.612546217441559
  - 1.63395614027977
  - 1.5942401856184008
  - 1.5971030235290529
  - 1.8262733757495881
  - 1.5886097013950349
  - 1.9185614526271821
  - 2.1200988560914995
  - 1.658267939090729
  - 1.653491133451462
  - 1.689750736951828
  - 1.7821132063865663
  - 3.2324058651924137
  - 2.3790064334869387
  - 1.6954079866409302
  - 1.6713431239128114
  - 1.6127127707004547
  - 2.1887969791889192
  - 1.5827100396156313
  validation_losses:
  - 0.44631293416023254
  - 0.37547042965888977
  - 0.4287838637828827
  - 0.42519915103912354
  - 0.39075765013694763
  - 0.39726224541664124
  - 0.3866233825683594
  - 0.41722407937049866
  - 0.4101462662220001
  - 0.4299025535583496
  - 0.4198184907436371
  - 1.0498411655426025
  - 0.40216535329818726
  - 0.4013594388961792
  - 0.3848837912082672
  - 0.39730051159858704
  - 0.377175897359848
  - 0.39112335443496704
  - 0.41720351576805115
  - 0.4013896584510803
  - 0.41418886184692383
  - 0.4122985303401947
  - 0.3922291398048401
  - 0.5630954504013062
  - 0.39522647857666016
  - 0.3826187551021576
  - 0.39715027809143066
  - 0.404369592666626
  - 0.4318316578865051
  - 0.4039951264858246
  - 0.4195995330810547
  - 0.4113709032535553
  - 0.40747708082199097
  - 0.41967150568962097
  - 0.43912452459335327
  - 0.6579161286354065
  - 0.4017258286476135
  - 0.4132709503173828
  - 0.4872208833694458
  - 0.8354116082191467
  - 0.38575756549835205
  - 0.3865891396999359
  - 0.39866140484809875
  - 0.41071218252182007
  - 0.40234139561653137
  - 0.4077717661857605
  - 0.38822105526924133
  - 0.3935980498790741
  - 0.3904060423374176
  - 0.40046045184135437
  - 0.4066781997680664
  - 0.3895185589790344
  - 0.4020566940307617
  - 0.3936957120895386
  - 0.4163955748081207
  - 0.42188727855682373
  - 0.4218549132347107
  - 0.41529473662376404
  - 5.063878059387207
  - 0.4291500449180603
  - 0.38270044326782227
  - 0.3832685649394989
  - 0.39271482825279236
  - 0.3746146261692047
  - 0.3786014914512634
loss_records_fold3:
  train_losses:
  - 1.6042920708656312
  - 2.880119651556015
  - 1.6625900864601135
  - 1.7032008707523347
  - 1.61854048371315
  - 1.5823813617229463
  - 1.7352160513401031
  - 1.5706820845603944
  - 1.74512380361557
  - 1.8786429524421693
  - 1.6353865444660187
  - 1.665952730178833
  - 1.7314484834671022
  - 1.5853340148925783
  - 5.268812650442124
  - 1.751119214296341
  - 1.5884223759174347
  - 1.790861713886261
  - 1.6175689816474916
  - 1.6325979471206666
  - 1.667140918970108
  - 1.6149933815002442
  - 1.6655453026294709
  validation_losses:
  - 0.38016873598098755
  - 0.4023641347885132
  - 0.3933603763580322
  - 0.39540883898735046
  - 0.39490601420402527
  - 0.4489993751049042
  - 0.4098680913448334
  - 0.39616480469703674
  - 0.4085538387298584
  - 0.406534343957901
  - 0.4133926033973694
  - 0.40200257301330566
  - 0.431335985660553
  - 0.4052453935146332
  - 0.4051211178302765
  - 0.41303306818008423
  - 0.4258274435997009
  - 0.4106709063053131
  - 0.4069584310054779
  - 0.410444438457489
  - 0.40732884407043457
  - 0.4091434180736542
  - 0.4090571105480194
loss_records_fold4:
  train_losses:
  - 1.606516891717911
  - 2.1106132149696353
  - 1.5924564003944397
  - 1.6290675580501557
  - 1.6239844262599945
  - 1.5678604125976563
  - 1.6157855987548828
  - 1.5938557267189026
  - 1.6328852832317353
  - 1.7413630306720735
  - 1.6040912926197053
  - 1.5960976004600527
  - 1.6689086854457855
  - 1.70898352265358
  - 1.6007533907890321
  - 1.6024535417556764
  - 1.5866940140724184
  - 1.5712258219718933
  - 1.5477119565010071
  - 1.6344247460365295
  - 1.5724918305873872
  - 1.6556495964527131
  - 1.75631400346756
  - 1.6815986812114716
  - 1.6298052191734316
  - 1.5752953469753266
  - 1.615679669380188
  - 1.6112901210784913
  - 1.6010909974575043
  - 1.625216144323349
  - 1.6644590318202974
  - 1.7423991322517396
  - 1.6047605931758882
  - 1.6127246916294098
  - 1.6997309625148773
  - 2.4179238557815554
  - 1.8058260798454286
  - 1.650671273469925
  - 1.599938178062439
  - 1.6656567931175232
  - 1.5964645624160767
  - 1.5547344446182252
  - 2.1932012617588046
  - 1.6441961109638215
  - 1.696381026506424
  - 1.6387924134731293
  - 3.1128353774547577
  - 1.6309354305267334
  - 1.6050655364990236
  - 1.6328879952430726
  - 1.64269158244133
  - 1.5230155289173126
  - 1.6330266058444978
  - 1.6257810294628143
  - 1.5879261434078218
  - 1.5726683259010317
  - 1.6009943306446077
  - 1.659655886888504
  - 1.562607538700104
  - 1.6407509684562684
  - 1.621963793039322
  - 1.543928787112236
  - 1.5920495033264161
  - 1.709010273218155
  - 1.668621575832367
  - 1.6631627261638642
  - 1.6357122182846071
  - 1.5672035545110703
  - 1.5777302324771882
  - 1.6934985756874086
  - 1.6350439488887787
  - 1.606826025247574
  - 1.6958543956279755
  - 1.8042860686779023
  - 1.749714505672455
  - 1.6145914196968079
  - 1.7321893751621247
  - 1.5978141546249391
  - 1.576059126853943
  - 1.5459851026535034
  - 1.5846147745847703
  validation_losses:
  - 0.426771342754364
  - 0.4201682209968567
  - 0.4022856056690216
  - 0.40943247079849243
  - 0.39326784014701843
  - 0.4054839611053467
  - 0.43039670586586
  - 0.421088308095932
  - 0.40243735909461975
  - 0.4165616035461426
  - 0.4018608629703522
  - 0.4304518699645996
  - 0.4059072434902191
  - 0.4416148364543915
  - 0.4070435166358948
  - 0.40601450204849243
  - 0.4084679186344147
  - 0.40969976782798767
  - 0.4060077369213104
  - 0.4161624312400818
  - 0.4005802273750305
  - 0.41347429156303406
  - 0.3996635675430298
  - 0.3958074748516083
  - 0.4004562199115753
  - 0.4252614378929138
  - 0.41066911816596985
  - 0.4228966534137726
  - 0.4283777177333832
  - 0.4111829102039337
  - 0.436815470457077
  - 0.4054955542087555
  - 0.4162199795246124
  - 0.40587398409843445
  - 0.4024325907230377
  - 0.4332856833934784
  - 0.41231805086135864
  - 0.4175078570842743
  - 0.4346306324005127
  - 0.4098958969116211
  - 0.40388235449790955
  - 0.39586082100868225
  - 0.408498078584671
  - 0.4662453532218933
  - 0.4238816797733307
  - 0.40696844458580017
  - 0.4102594554424286
  - 0.41274428367614746
  - 0.41014429926872253
  - 0.42127418518066406
  - 0.4101702570915222
  - 0.42885175347328186
  - 0.39568477869033813
  - 0.4172493517398834
  - 0.41129496693611145
  - 0.4049711525440216
  - 0.4048844277858734
  - 0.402432382106781
  - 0.39504384994506836
  - 0.4052375853061676
  - 0.4060025215148926
  - 0.40768253803253174
  - 0.40370020270347595
  - 0.39407777786254883
  - 0.4082915186882019
  - 0.39769476652145386
  - 0.41295892000198364
  - 0.4052965044975281
  - 0.4054413437843323
  - 0.43419018387794495
  - 0.41368794441223145
  - 0.3958437144756317
  - 0.41032981872558594
  - 0.3966045677661896
  - 0.4407587945461273
  - 0.4215433597564697
  - 0.4237646758556366
  - 0.41704437136650085
  - 0.408309668302536
  - 0.3963271379470825
  - 0.40574193000793457
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 31 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 65 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 81 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8542024013722127, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.0
  total_train_time: '0:18:17.886651'
