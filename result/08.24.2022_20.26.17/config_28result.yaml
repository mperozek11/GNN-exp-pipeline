config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:00:41.914303'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_28fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_28fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_28fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_28fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_28fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_28fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_28fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_28fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_28fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_28fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 83.55930831171572
  - 22.818491551280022
  - 21.627344250679016
  - 23.53350315690041
  - 20.087440471351147
  - 21.65704006999731
  - 21.56093969345093
  - 24.756354066729546
  - 15.245587241649629
  - 14.969903790950776
  - 18.627241653203964
  - 10.932120153307915
  - 8.51782922744751
  - 8.501015320420265
  - 10.230726271867752
  - 8.240432316064835
  - 9.60793680548668
  - 11.156890657544137
  - 8.483580401539802
  - 8.99386394917965
  - 7.745534178614617
  - 8.877387180924416
  - 7.826653987169266
  - 7.4379088401794435
  - 6.787762761116028
  - 6.992266684770584
  - 6.859437564015389
  - 6.945894286036491
  - 6.967031937837601
  - 6.551028603315354
  - 6.384443709254265
  - 7.046846616268159
  - 7.033723771572113
  - 6.39671682715416
  - 6.381017863750458
  - 6.542815020680428
  - 6.955270358920098
  - 7.003173917531967
  - 6.4060931473970415
  - 6.622184693813324
  - 6.887223336100579
  - 6.423866972327232
  - 6.751864284276962
  - 7.14217301607132
  - 6.7066011309623725
  - 6.338052749633789
  - 6.351069527864457
  - 6.4792128771543505
  - 7.025669747591019
  - 6.6867630839347845
  - 6.633163496851921
  - 6.539820045232773
  - 6.8831118762493135
  - 6.539719933271408
  - 6.128500691056252
  - 6.507356435060501
  - 6.331734046339989
  - 6.533167719841003
  - 6.510930699110031
  - 6.47778822183609
  - 6.2859878748655325
  - 6.48700849711895
  - 6.399450582265854
  - 6.595795127749444
  - 6.434580034017563
  - 6.366069141030312
  - 6.272909659147263
  - 6.253420501947403
  - 6.217817294597626
  - 6.9394261062145235
  - 6.3446256756782535
  - 6.381166636943817
  - 6.617793062329293
  - 6.996396550536156
  - 6.309570866823197
  - 7.06830450296402
  - 6.2249938398599625
  - 6.606492033600808
  - 6.327848270535469
  - 6.307252311706543
  - 6.250191061198712
  - 6.355075842142106
  - 6.0601358920335775
  - 6.264418983459473
  - 6.207078683376313
  - 6.414734148979187
  - 6.523250469565392
  - 6.305614963173866
  - 6.2604592949152
  - 6.639778599143028
  - 6.32527369260788
  - 6.589243942499161
  - 6.280938225984574
  - 6.628340041637421
  - 6.331116765737534
  - 6.494988834857941
  - 6.647291392087936
  - 6.571138137578965
  - 6.496063634753227
  - 6.464590606093407
  validation_losses:
  - 2.1468400955200195
  - 0.5867630839347839
  - 0.4588009715080261
  - 0.7513370513916016
  - 0.6016676425933838
  - 0.7127286791801453
  - 0.7498887181282043
  - 0.4344344139099121
  - 0.4401431679725647
  - 0.45109447836875916
  - 0.4128592312335968
  - 0.41066429018974304
  - 0.4203531742095947
  - 0.642804741859436
  - 0.4267628490924835
  - 0.3981058597564697
  - 0.629330575466156
  - 0.3881120979785919
  - 0.40901830792427063
  - 0.4797094464302063
  - 0.41614383459091187
  - 0.49837756156921387
  - 0.5623456239700317
  - 0.3861393630504608
  - 0.40073010325431824
  - 0.41799452900886536
  - 0.39879080653190613
  - 0.40589091181755066
  - 0.40186741948127747
  - 0.3919595777988434
  - 0.3985922038555145
  - 0.4230068027973175
  - 0.44755467772483826
  - 0.40107131004333496
  - 0.3911849558353424
  - 0.45601192116737366
  - 0.5291534662246704
  - 0.4354085326194763
  - 0.4202457070350647
  - 0.4007629454135895
  - 0.4206302762031555
  - 0.40704360604286194
  - 0.394365519285202
  - 0.3905813694000244
  - 0.4290176033973694
  - 0.46895676851272583
  - 0.41118311882019043
  - 0.3965897560119629
  - 0.4667322337627411
  - 0.3860301375389099
  - 0.3927801251411438
  - 0.4627208411693573
  - 0.4012801945209503
  - 0.4768058657646179
  - 0.4954181909561157
  - 0.39917463064193726
  - 0.3896339237689972
  - 0.4104540944099426
  - 0.46313348412513733
  - 0.4077501595020294
  - 0.417545348405838
  - 0.4112188220024109
  - 0.389669805765152
  - 0.4006671607494354
  - 0.3967778980731964
  - 0.40318915247917175
  - 0.39505916833877563
  - 0.4167874753475189
  - 0.39687252044677734
  - 0.43118658661842346
  - 0.38799405097961426
  - 0.4055767059326172
  - 0.43014010787010193
  - 0.38761791586875916
  - 0.4012552797794342
  - 0.39027753472328186
  - 0.3912673592567444
  - 0.3902941644191742
  - 0.4131377637386322
  - 0.4057919979095459
  - 0.42872899770736694
  - 0.4100968539714813
  - 0.46607840061187744
  - 0.4078829288482666
  - 0.39926132559776306
  - 0.400031715631485
  - 0.4380187690258026
  - 0.40085092186927795
  - 0.4830414950847626
  - 0.5328962802886963
  - 0.46401989459991455
  - 0.43025708198547363
  - 0.4525546133518219
  - 0.4768977463245392
  - 0.39620816707611084
  - 0.4047621786594391
  - 0.4961501955986023
  - 1.6359044313430786
  - 0.4698755443096161
  - 2.0418131351470947
loss_records_fold1:
  train_losses:
  - 6.16768778860569
  - 6.386279103159905
  - 6.230005961656571
  - 6.268068075180054
  - 6.276402598619462
  - 6.361576919257641
  - 6.44469333589077
  - 6.334475660324097
  - 6.481164819002152
  - 6.503641593456269
  - 6.6676237136125565
  - 6.241996574401856
  - 6.091564431786537
  - 6.313513332605362
  - 6.195413979887963
  - 6.028747808933258
  - 6.275831377506257
  - 6.187326511740685
  - 6.333592161536217
  - 6.657048197090626
  - 6.673724906146527
  - 6.416362135112286
  validation_losses:
  - 0.42852073907852173
  - 0.3938536047935486
  - 0.40809953212738037
  - 0.4262543320655823
  - 0.45091161131858826
  - 0.43581315875053406
  - 0.4082190692424774
  - 0.4197984039783478
  - 0.4727231562137604
  - 0.4453793168067932
  - 0.41969773173332214
  - 0.4179063141345978
  - 0.41688990592956543
  - 0.4075396955013275
  - 0.4239065647125244
  - 0.44493749737739563
  - 0.4165468215942383
  - 0.400770902633667
  - 0.40893808007240295
  - 0.414488285779953
  - 0.3989878296852112
  - 0.39899495244026184
loss_records_fold2:
  train_losses:
  - 6.374263942241669
  - 6.67787625193596
  - 6.780227389931679
  - 6.499977946281433
  - 15.44309643805027
  - 9.69235602170229
  - 6.5662792176008224
  - 6.464577472209931
  - 6.346737471222878
  - 6.623532220721245
  - 6.726407253742218
  - 8.386177319288254
  - 6.792986771464348
  - 6.554375982284546
  - 6.487510550022126
  - 6.549796706438065
  - 6.790965506434441
  - 6.4939911544322975
  - 6.437929004430771
  - 6.385672765970231
  - 6.431590053439141
  - 6.29912094771862
  - 6.736757230758667
  - 6.577206881344319
  - 6.4801942884922035
  - 7.0070779681205755
  - 6.593205443024636
  - 6.725964981317521
  - 6.62076390683651
  - 6.5401264339685445
  - 6.598789930343628
  - 6.347179037332535
  - 6.267369943857194
  - 6.756346747279167
  - 6.831704077124596
  - 6.395044586062432
  - 8.681187775731088
  - 6.484296676516533
  - 6.49760354757309
  - 6.8778814584016805
  - 9.214165309071541
  - 8.21899127662182
  - 10.970854353904725
  - 7.468884789943695
  - 7.414092263579369
  - 7.366547730565071
  - 8.018896692991257
  - 7.052217918634415
  - 6.974775013327599
  - 6.834392511844635
  - 7.21947589814663
  - 6.98770478963852
  - 6.796691608428955
  - 6.443161311745644
  - 6.851581591367722
  - 7.428053998947144
  - 6.840162149071694
  - 7.395355045795441
  - 7.131960412859917
  - 6.441599664092064
  - 6.942040771245956
  - 7.053856810927392
  - 6.712850672006607
  - 6.9163863360881805
  - 6.645589956641198
  - 7.046726313233376
  - 6.622717720270157
  - 6.796015301346779
  - 6.928815162181855
  - 6.592636904120446
  - 7.058046305179596
  - 6.770050647854806
  - 7.520451647043228
  - 6.555804014205933
  - 7.006423455476761
  - 6.597734498977662
  - 6.852948713302613
  - 6.767104896903039
  - 6.941687747836113
  - 6.60098974108696
  - 6.857554578781128
  - 6.897653159499169
  - 6.4473857462406166
  - 6.529170867800713
  - 6.481190851330758
  - 6.758707761764526
  - 6.9510602235794074
  - 6.728043752908707
  - 6.457200664281846
  - 6.687594914436341
  - 6.609095856547356
  - 6.791236221790314
  - 6.541670256853104
  - 6.854402947425843
  - 6.638297656178475
  - 6.906404492259026
  - 6.405335074663163
  - 6.722960805892945
  - 6.899994778633118
  - 6.48072262108326
  validation_losses:
  - 0.6937582492828369
  - 1007559.625
  - 427.19915771484375
  - 20793670.0
  - 6380565.5
  - 3326606.0
  - 161030816.0
  - 0.41944068670272827
  - 0.43013784289360046
  - 0.41064175963401794
  - 0.8997308015823364
  - 0.414971262216568
  - 0.4113765060901642
  - 0.4023614823818207
  - 0.4653898775577545
  - 0.41609987616539
  - 0.4472642242908478
  - 0.41008204221725464
  - 0.4947413504123688
  - 0.4113602638244629
  - 0.4069362282752991
  - 0.41458895802497864
  - 0.41332027316093445
  - 0.42472168803215027
  - 0.3994002044200897
  - 0.43140709400177
  - 0.4140526354312897
  - 0.4019322693347931
  - 0.4219292998313904
  - 0.4188814163208008
  - 0.39720094203948975
  - 0.431709885597229
  - 0.4565202593803406
  - 0.4267091453075409
  - 0.46468719840049744
  - 0.4250441789627075
  - 0.4032510817050934
  - 0.46036913990974426
  - 3691.496337890625
  - 0.43763741850852966
  - 0.4414612352848053
  - 20.306472778320312
  - 5.179556369781494
  - 855.5775146484375
  - 0.42295053601264954
  - 0.48745980858802795
  - 0.48476022481918335
  - 0.41633766889572144
  - 0.40928298234939575
  - 0.41913893818855286
  - 0.4654923379421234
  - 0.4115651845932007
  - 0.42625629901885986
  - 0.3963558077812195
  - 0.43337568640708923
  - 0.41402286291122437
  - 0.40700241923332214
  - 0.48640549182891846
  - 0.42647385597229004
  - 0.4292411208152771
  - 0.4355214536190033
  - 0.4726884663105011
  - 0.4563244879245758
  - 0.43643948435783386
  - 0.4478645920753479
  - 0.40833741426467896
  - 0.43151694536209106
  - 0.5014957189559937
  - 0.40140390396118164
  - 7.468986511230469
  - 0.4571116268634796
  - 0.437934547662735
  - 0.40619808435440063
  - 0.4856630265712738
  - 0.3973979353904724
  - 0.4063463509082794
  - 0.4035121500492096
  - 0.4506911337375641
  - 0.4206126928329468
  - 0.39873772859573364
  - 0.5437849164009094
  - 0.4321487545967102
  - 0.4476170241832733
  - 0.39621883630752563
  - 0.4333652853965759
  - 0.44237130880355835
  - 0.4131896495819092
  - 0.4243395924568176
  - 0.40098580718040466
  - 0.4796988070011139
  - 0.39187267422676086
  - 0.39786574244499207
  - 0.3881281614303589
  - 0.44152671098709106
  - 0.41992294788360596
  - 0.4131058156490326
  - 0.40768852829933167
  - 0.41572126746177673
  - 0.4398564100265503
  - 0.41216665506362915
loss_records_fold3:
  train_losses:
  - 6.507188004255295
  - 6.765014639496804
  - 6.46086337864399
  - 6.674237161874771
  - 6.545915186405182
  - 6.498194229602814
  - 6.592112582921982
  - 6.786398187279701
  - 6.557948511838913
  - 6.355222073197365
  - 6.454482972621918
  - 6.829669031500817
  - 6.545809108018876
  - 6.468254113197327
  - 6.622643250226975
  - 6.507923829555512
  - 6.3326597034931185
  - 6.364748427271843
  - 6.522925344109535
  - 6.606759077310563
  - 6.364547574520111
  - 6.361547037959099
  - 6.429354006052018
  - 6.736750227212906
  - 6.3855558991432195
  - 6.675411105155945
  - 6.415145432949067
  - 6.463056099414826
  - 6.56430701315403
  - 6.420632702112198
  - 6.476400138437748
  - 6.713975024223328
  - 6.666841372847557
  - 6.40208654999733
  - 6.3653374999761585
  - 6.458352187275887
  - 6.6272254467010505
  - 6.549522581696511
  - 6.591872465610504
  - 6.85255532860756
  - 6.63768699169159
  - 6.461586984992028
  - 6.330387890338898
  - 6.516219991445542
  - 6.475804853439332
  - 7.015127411484719
  - 6.373328804969788
  - 6.436146467924118
  - 6.762098476290703
  - 6.718785190582276
  - 6.603456607460976
  - 6.219254207611084
  - 6.35241842865944
  - 6.721436724066734
  - 6.497327357530594
  - 6.37840819656849
  - 6.346473151445389
  - 6.388832288980485
  - 6.753209346532822
  - 6.28881248831749
  - 6.423380881547928
  - 6.426799947023392
  - 6.532035785913468
  - 6.249981153011323
  - 6.612793451547623
  - 6.4750032246112825
  - 6.5519259750843055
  - 6.580188146233559
  - 6.509424272179604
  - 6.539803764224053
  - 6.427717572450638
  - 6.333599975705147
  - 6.726188120245934
  - 6.5406332373619085
  - 6.32846839427948
  - 6.431514897942543
  - 6.542556974291802
  - 6.47343248128891
  - 6.679769229888916
  - 6.585423135757447
  - 6.555213680863381
  - 6.526423588395119
  - 6.648418539762497
  - 6.417216473817826
  - 6.2814913421869285
  - 6.300295460224152
  - 6.545620328187943
  - 6.456702277064323
  - 6.6390455782413484
  - 6.625284877419472
  - 6.518836435675621
  - 6.384127670526505
  - 6.4047814309597015
  - 6.42115806043148
  - 6.3680526405572895
  - 6.5284415066242225
  - 6.427649146318436
  - 6.843517526984215
  - 6.52638692855835
  - 6.467609387636185
  validation_losses:
  - 0.41651538014411926
  - 0.4154888391494751
  - 0.47394296526908875
  - 0.42967793345451355
  - 0.44802433252334595
  - 0.4704440236091614
  - 0.4213579297065735
  - 0.4318799376487732
  - 0.4274691939353943
  - 0.4246881306171417
  - 0.4711580276489258
  - 0.44323664903640747
  - 0.40130746364593506
  - 0.42247432470321655
  - 1.707431674003601
  - 0.46006298065185547
  - 2140.654296875
  - 2712.4697265625
  - 0.44675981998443604
  - 0.5102065205574036
  - 502.64093017578125
  - 20.281665802001953
  - 1329.306396484375
  - 9246.61328125
  - 0.4435732662677765
  - 0.42164260149002075
  - 4935.28662109375
  - 0.40208539366722107
  - 234.4613800048828
  - 0.4571341276168823
  - 3118.3115234375
  - 0.4698169231414795
  - 1044.3470458984375
  - 7505.8193359375
  - 12282.470703125
  - 10849.5537109375
  - 0.4171217978000641
  - 0.4397910237312317
  - 13649.443359375
  - 0.42503589391708374
  - 32101.8671875
  - 8029.15283203125
  - 883.3578491210938
  - 9704.3154296875
  - 5239.69287109375
  - 0.41640520095825195
  - 51962.69140625
  - 2019.5662841796875
  - 12688.3798828125
  - 14554.6748046875
  - 39746.484375
  - 36315.6796875
  - 7272.1923828125
  - 64.27259826660156
  - 0.43594616651535034
  - 11043.23828125
  - 6486.28466796875
  - 0.5183729529380798
  - 1763.3946533203125
  - 6177.3935546875
  - 0.4123491942882538
  - 0.409706175327301
  - 208.5884552001953
  - 16764.490234375
  - 0.4167250394821167
  - 11434.720703125
  - 64819.3828125
  - 12415.8251953125
  - 0.4592339098453522
  - 5328.3291015625
  - 2230.595458984375
  - 60.10175323486328
  - 2675.450927734375
  - 984.3604736328125
  - 0.422345906496048
  - 24785.513671875
  - 2340.743408203125
  - 138728.078125
  - 36038.73046875
  - 1499.7252197265625
  - 122.01750183105469
  - 287.8728942871094
  - 0.41116809844970703
  - 1137.2774658203125
  - 142.5431365966797
  - 0.4007989764213562
  - 35882.234375
  - 91.65723419189453
  - 0.433118999004364
  - 4676.88330078125
  - 11068.724609375
  - 0.40846115350723267
  - 0.4124012589454651
  - 0.41642096638679504
  - 0.444647878408432
  - 0.4074519872665405
  - 0.4204178750514984
  - 0.47854745388031006
  - 0.434735506772995
  - 0.4085310697555542
loss_records_fold4:
  train_losses:
  - 6.393301132321358
  - 6.520446276664734
  - 6.524114787578583
  - 6.594790354371071
  - 6.639119219779968
  - 6.448483112454415
  - 6.446913361549377
  - 7.09304341673851
  - 6.5842459321022035
  - 6.701491355895996
  - 6.492138865590096
  - 6.436154106259346
  - 6.449527797102928
  - 6.23806301355362
  - 6.373627066612244
  - 6.460509952902794
  - 6.620161336660385
  - 6.548943993449211
  - 6.33619459271431
  - 6.449173107743263
  validation_losses:
  - 0.4193996787071228
  - 0.41097912192344666
  - 0.44035208225250244
  - 0.46007540822029114
  - 0.42137548327445984
  - 0.4132203459739685
  - 0.42017096281051636
  - 0.43021678924560547
  - 0.4638511538505554
  - 0.4352710247039795
  - 16038590.0
  - 0.41623660922050476
  - 0.4030648469924927
  - 0.4505752623081207
  - 0.41313400864601135
  - 0.40992453694343567
  - 0.4059790372848511
  - 0.41504982113838196
  - 0.4024721086025238
  - 0.4033190906047821
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:34:19.181880'
