config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.860642'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_14fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_14fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_14fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_14fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_14fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_14fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_14fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_14fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_14fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_14fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 38.42207379341126
  - 11.127952384948731
  - 5.377184730768204
  - 3.8740577638149265
  - 2.995829546451569
  - 2.7081810235977173
  - 2.4580430269241336
  - 3.4955349504947666
  - 2.946763211488724
  - 3.4498175144195558
  - 2.4287014961242677
  - 2.354014015197754
  - 1.7925754725933076
  - 5.101342797279358
  - 6.753400212526322
  - 5.5781026721000675
  - 4.666622191667557
  - 2.7890911519527437
  - 2.022053837776184
  - 4.0421118080616
  - 2.466798883676529
  - 3.780967092514038
  - 2.2103644490242007
  - 1.978926742076874
  - 5.579511219263077
  - 3.144620895385742
  - 6.1096716105937965
  - 4.206496554613113
  - 2.0568880021572116
  - 2.1653240114450454
  - 1.9340797662734985
  - 3.0180246174335483
  - 2.133291006088257
  - 2.392186212539673
  - 2.197517478466034
  - 1.7314378559589387
  - 4.7975126802921295
  - 2.9395329117774964
  - 3.401071673631668
  - 6.1558473169803625
  - 2.7427861094474792
  - 2.7272264778614046
  - 4.755998802185059
  - 2.6096656918525696
  - 2.321922039985657
  - 1.8249887734651566
  - 1.786359751224518
  - 1.856732937693596
  - 1.8520712077617647
  - 2.7481015861034397
  - 1.9141931951045992
  - 2.9143964231014254
  - 2.0233685553073886
  - 2.0100288271903994
  - 2.133920246362686
  - 1.8292252600193024
  - 2.002173537015915
  - 2.769067579507828
  - 2.0641005218029025
  - 1.9436231195926668
  - 2.1141700983047484
  - 2.13968603014946
  - 1.6664492964744568
  - 2.581663125753403
  - 1.8953843414783478
  - 2.851680260896683
  - 1.858773547410965
  - 1.7195494532585145
  - 2.4113454997539523
  - 1.6347439229488374
  - 2.6530072391033173
  - 2.9084079146385196
  - 1.8818516671657564
  - 2.7409085869789127
  - 1.7763848662376405
  - 1.8657041788101196
  - 1.7893734276294708
  - 1.6230021595954895
  - 1.6565425634384157
  - 1.833610796928406
  - 1.646236377954483
  - 1.618501102924347
  - 1.9087863922119142
  - 1.7500272691249847
  - 1.6527910172939302
  - 1.7781570792198182
  - 1.9638830184936524
  - 1.677499842643738
  - 1.7096510589122773
  - 1.690219658613205
  - 1.6628956973552704
  - 1.8186605453491211
  - 1.7680690348148347
  - 1.7280499994754792
  - 1.7223281979560854
  - 1.6546236276626587
  - 1.6264309227466585
  - 1.5807454586029053
  - 1.7688311576843263
  - 1.8445277690887452
  validation_losses:
  - 4.061183929443359
  - 3.074836015701294
  - 1.0565071105957031
  - 0.7692953944206238
  - 1.3695729970932007
  - 0.48902979493141174
  - 0.5789973139762878
  - 0.4606153666973114
  - 0.5389432311058044
  - 0.9136416912078857
  - 0.8905141353607178
  - 0.42509567737579346
  - 0.4696427285671234
  - 1.1580777168273926
  - 1.190832495689392
  - 0.5968194603919983
  - 0.9043231010437012
  - 0.4305041432380676
  - 0.44159749150276184
  - 0.5775107741355896
  - 0.43561413884162903
  - 0.5054165124893188
  - 0.49281424283981323
  - 0.5211981534957886
  - 0.44450920820236206
  - 0.42471230030059814
  - 0.6235435605049133
  - 0.4742254912853241
  - 0.43082818388938904
  - 0.4253676235675812
  - 0.3931223154067993
  - 0.4233950078487396
  - 0.42359858751296997
  - 0.4056394100189209
  - 0.46544507145881653
  - 0.5565817952156067
  - 0.46482035517692566
  - 0.5188703536987305
  - 0.45436257123947144
  - 0.4021790325641632
  - 0.39442238211631775
  - 0.4420030117034912
  - 0.4555046558380127
  - 0.4278319180011749
  - 0.4046429693698883
  - 0.41580840945243835
  - 0.4223484396934509
  - 0.4085831642150879
  - 0.49661245942115784
  - 0.39917951822280884
  - 0.4000786244869232
  - 0.4075873792171478
  - 0.46253153681755066
  - 0.4308147132396698
  - 0.4637337327003479
  - 0.41072532534599304
  - 0.3911205232143402
  - 0.4688969850540161
  - 0.43566760420799255
  - 0.384746253490448
  - 0.5716454386711121
  - 0.40781277418136597
  - 0.40024077892303467
  - 0.4141364097595215
  - 0.42772015929222107
  - 0.40537190437316895
  - 0.3869558572769165
  - 0.5153059363365173
  - 0.4235352575778961
  - 0.3991609513759613
  - 0.9338752031326294
  - 0.46382594108581543
  - 0.3877274692058563
  - 0.41503506898880005
  - 0.4793645143508911
  - 0.43144819140434265
  - 0.3965442478656769
  - 0.3893527090549469
  - 0.41095080971717834
  - 0.40150555968284607
  - 0.3857955038547516
  - 0.3851264417171478
  - 0.4086800813674927
  - 0.40248140692710876
  - 0.4262695610523224
  - 0.4110705852508545
  - 0.43915361166000366
  - 0.38347139954566956
  - 0.3838485777378082
  - 0.39447981119155884
  - 0.39104244112968445
  - 0.43212446570396423
  - 0.4054710268974304
  - 0.3952607810497284
  - 0.4283623695373535
  - 0.41657882928848267
  - 0.3887735605239868
  - 0.3872496485710144
  - 0.4092661738395691
  - 0.4041809141635895
loss_records_fold1:
  train_losses:
  - 1.7828543722629548
  - 1.6513483166694642
  - 1.740978252887726
  - 1.831531220674515
  - 1.9889418363571167
  - 1.7622005820274353
  - 1.7578853487968447
  - 1.6595111310482027
  - 1.7850613653659821
  - 1.617754727602005
  - 1.6138778328895569
  - 1.7911280900239945
  - 2.5551126897335052
  - 2.1418800592422484
  - 4.097472870349884
  - 2.1015503466129304
  - 1.7719042241573335
  - 2.262141650915146
  - 1.7709670186042787
  - 1.8620045304298403
  - 1.7460421562194826
  - 1.787415248155594
  - 1.9645622849464417
  - 2.1287262022495272
  - 2.020006775856018
  - 1.775399124622345
  - 1.6942136764526368
  - 1.6653931081295015
  - 2.646843278408051
  - 1.85106076002121
  - 1.7551959037780762
  - 1.9366749048233034
  - 1.987594622373581
  - 1.8199100583791734
  - 1.7073677361011506
  - 1.7810091376304626
  - 1.664991706609726
  - 1.7400475025177002
  - 1.7095965206623078
  - 2.359647309780121
  - 1.6556858837604524
  - 1.643289029598236
  - 1.6413095772266388
  - 1.7273800820112228
  - 1.6650870382785798
  - 1.7388995528221132
  validation_losses:
  - 0.4088760316371918
  - 0.45517513155937195
  - 0.4225580096244812
  - 0.41948193311691284
  - 0.40914928913116455
  - 0.45672252774238586
  - 0.40486589074134827
  - 0.40617862343788147
  - 0.4164334535598755
  - 0.41070789098739624
  - 0.4102913737297058
  - 0.5228825211524963
  - 0.5006006956100464
  - 0.41444042325019836
  - 0.48229584097862244
  - 0.4325709342956543
  - 0.4041789472103119
  - 0.446880578994751
  - 0.44760948419570923
  - 0.46762844920158386
  - 0.4302607476711273
  - 0.41007286310195923
  - 0.48195087909698486
  - 0.4871549904346466
  - 0.4063081741333008
  - 0.41151076555252075
  - 0.42894259095191956
  - 0.41449788212776184
  - 0.422468900680542
  - 0.47226399183273315
  - 0.45670756697654724
  - 0.4045329988002777
  - 0.4297613203525543
  - 0.42986106872558594
  - 0.4189603924751282
  - 0.4371715486049652
  - 0.44929540157318115
  - 0.42411288619041443
  - 0.4150677025318146
  - 0.44932469725608826
  - 0.41071662306785583
  - 0.41491490602493286
  - 0.4220563769340515
  - 0.4253537654876709
  - 0.40978607535362244
  - 0.4068846106529236
loss_records_fold2:
  train_losses:
  - 1.7383865416049957
  - 3.0797663629055023
  - 3.3849555373191835
  - 2.0979591786861422
  - 1.8226684033870697
  - 1.7181528449058534
  - 1.7392427325248718
  - 1.659981507062912
  - 1.754413777589798
  - 1.698067718744278
  - 1.834720426797867
  - 1.9681541800498963
  - 1.8042056113481522
  - 1.6487818956375122
  - 1.8625900447368622
  - 1.7688458979129793
  - 1.7201566219329836
  - 1.695991110801697
  - 1.6967882454395296
  - 1.6816045343875885
  - 1.6653790831565858
  - 1.6251576483249666
  - 1.6360372483730317
  - 1.6588423043489458
  - 1.654146957397461
  - 1.586156064271927
  - 2.402506023645401
  - 1.612692505121231
  - 1.6569118231534958
  - 1.7253882706165315
  - 1.7413453817367555
  - 1.6394720494747164
  - 1.7791499316692354
  - 1.7652070164680482
  - 1.674275028705597
  - 1.7460374593734742
  - 1.761700761318207
  - 1.6892958402633669
  - 1.702599507570267
  - 1.578003314137459
  - 1.7034864902496338
  - 1.649469429254532
  - 1.6447548389434816
  - 1.618508142232895
  - 1.649159049987793
  - 1.6570548474788667
  validation_losses:
  - 0.395933598279953
  - 0.5200456380844116
  - 0.3969140350818634
  - 0.43299970030784607
  - 0.4149012863636017
  - 0.3923985958099365
  - 0.43123000860214233
  - 0.4008905291557312
  - 0.40076035261154175
  - 0.39311856031417847
  - 0.40801945328712463
  - 0.3950521945953369
  - 0.4064375162124634
  - 0.4429703652858734
  - 0.4128657281398773
  - 0.41566431522369385
  - 0.4087289869785309
  - 0.4067055284976959
  - 0.38897961378097534
  - 0.40251603722572327
  - 0.41473323106765747
  - 0.41565564274787903
  - 0.3860665261745453
  - 0.395649254322052
  - 0.3873893618583679
  - 0.4036940634250641
  - 0.38955235481262207
  - 0.40766239166259766
  - 0.401088684797287
  - 0.39925193786621094
  - 0.39163705706596375
  - 0.39140576124191284
  - 0.3993544280529022
  - 0.41220948100090027
  - 0.4069918394088745
  - 0.39858606457710266
  - 0.39611580967903137
  - 0.40330222249031067
  - 0.4124450981616974
  - 0.4688263237476349
  - 0.41547608375549316
  - 0.4132389426231384
  - 0.3976518213748932
  - 0.4057997167110443
  - 0.41334787011146545
  - 0.3966539204120636
loss_records_fold3:
  train_losses:
  - 1.6307344675064088
  - 1.7611625373363495
  - 1.6511017262935639
  - 1.6330987513065338
  - 1.5857511937618256
  - 1.6665557563304902
  - 1.6541583180427553
  - 1.6807361960411074
  - 1.6708390295505524
  - 1.6297566890716553
  - 1.6629891455173493
  - 1.651515781879425
  - 1.742760267853737
  - 1.6896572530269625
  - 1.667054456472397
  - 1.6535690307617188
  - 1.7223522663116455
  - 1.6255380749702455
  - 1.6173100113868715
  - 1.6469332784414292
  - 1.6476059675216677
  - 1.6624986290931703
  - 1.6316897749900818
  - 1.664079338312149
  validation_losses:
  - 0.401594877243042
  - 0.4044918119907379
  - 0.41900086402893066
  - 0.40439456701278687
  - 0.4262732267379761
  - 0.40638574957847595
  - 0.41448622941970825
  - 0.4288237690925598
  - 0.4035329520702362
  - 0.4108756482601166
  - 0.44792723655700684
  - 0.41637834906578064
  - 0.4534122347831726
  - 0.4346228837966919
  - 0.4176103472709656
  - 0.4228696823120117
  - 0.40543097257614136
  - 0.4182558059692383
  - 0.4096466302871704
  - 0.40685901045799255
  - 0.40760666131973267
  - 0.4174331724643707
  - 0.42732858657836914
  - 0.41194960474967957
loss_records_fold4:
  train_losses:
  - 1.6216748416423798
  - 1.6760669887065889
  - 1.7749834179878237
  - 1.6807658851146698
  - 1.6439476847648622
  - 1.6216052860021593
  - 1.6363245964050295
  - 1.6733480215072634
  - 1.645472025871277
  - 1.6535644054412844
  - 1.6139021188020708
  - 1.6513747811317445
  - 1.650843930244446
  - 1.670139276981354
  - 1.6877456963062287
  - 1.630198383331299
  - 1.6270346224308014
  - 1.5948555320501328
  - 1.6033072233200074
  validation_losses:
  - 0.44510239362716675
  - 0.42932644486427307
  - 0.41731518507003784
  - 0.42486608028411865
  - 0.4270305633544922
  - 0.41172581911087036
  - 0.440809428691864
  - 0.4177480936050415
  - 0.3981643617153168
  - 0.41476893424987793
  - 0.43541958928108215
  - 0.40130415558815
  - 0.44641125202178955
  - 0.4240993559360504
  - 0.4195091724395752
  - 0.4133223593235016
  - 0.40331968665122986
  - 0.40998920798301697
  - 0.40160536766052246
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 46 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 46 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:19:34.181569'
