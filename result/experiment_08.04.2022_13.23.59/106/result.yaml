config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 01:52:50.539304'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/106/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/106/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/106/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/106/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/106/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/106/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/106/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/106/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/106/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/106/fold_4_state_dict.pt
loss_records_fold3:
  train_losses:
  - 15.428301274776459
  - 15.519182413816452
  - 14.701085567474365
  - 15.379499390721321
  - 14.891824066638947
  - 15.164651960134506
  - 14.998251885175705
  - 15.008371084928513
  - 14.813146144151688
  - 15.358073487877846
  - 15.410749077796936
  - 15.15691363811493
  - 14.914730682969093
  - 15.41129656136036
  - 14.88175892829895
  - 14.994714602828026
  - 15.224121659994125
  - 15.128430992364883
  - 14.771680533885956
  - 15.251069962978363
  - 14.743283748626709
  - 14.779534697532654
  - 14.734160304069519
  - 14.853717535734177
  - 15.32920752465725
  - 15.565300330519676
  - 15.115004613995552
  - 15.119276851415634
  - 14.98809015750885
  - 15.011676520109177
  - 15.084663301706314
  - 14.957754075527191
  - 14.847417086362839
  - 14.798050045967102
  - 14.942085057497025
  - 14.88978561758995
  - 14.970385894179344
  - 14.94049747288227
  - 14.96534389257431
  - 15.062361776828766
  - 15.099068135023117
  - 15.320941597223282
  - 15.478658705949783
  - 15.186771273612976
  - 15.173542872071266
  - 15.15794113278389
  - 15.010319471359253
  - 15.037293583154678
  - 15.166254192590714
  - 15.519538968801498
  - 14.727588176727295
  - 15.622263222932816
  - 15.212054163217545
  - 14.490686058998108
  - 15.302977740764618
  - 15.048516631126404
  - 15.12410244345665
  - 15.086404234170914
  - 15.1575448513031
  - 15.167231738567352
  - 14.860974103212357
  - 15.029063314199448
  - 14.828435808420181
  - 14.747046336531639
  - 14.842181384563446
  - 14.662301003932953
  - 14.912409096956253
  - 15.027283385396004
  - 14.860432863235474
  - 14.392499476671219
  - 14.979661777615547
  - 15.194011092185974
  - 14.920489490032196
  - 14.478165000677109
  - 14.871964871883392
  - 14.838482677936554
  - 14.621051400899887
  - 14.803207904100418
  - 14.84496521949768
  - 14.748360678553581
  - 14.953791558742523
  - 14.668612882494926
  - 14.558439418673515
  - 14.680575266480446
  - 14.628253817558289
  - 15.020806446671486
  - 14.997834175825119
  - 14.92725694179535
  - 14.898242324590683
  - 14.775854468345642
  - 14.781854093074799
  - 14.7257519364357
  - 14.782921463251114
  - 14.528304249048233
  - 14.92904956638813
  - 14.90194071829319
  - 14.77252747118473
  - 14.668437778949738
  - 14.283167123794556
  - 14.556465864181519
  validation_losses:
  - 0.3783700168132782
  - 0.37870505452156067
  - 0.37953680753707886
  - 0.40934038162231445
  - 0.38850992918014526
  - 1.1244477033615112
  - 1.207534670829773
  - 0.6836735606193542
  - 0.4084145724773407
  - 0.3881114721298218
  - 0.3803781569004059
  - 0.3831981420516968
  - 0.403819739818573
  - 0.392766535282135
  - 0.3802017271518707
  - 0.4911499619483948
  - 0.5706026554107666
  - 0.5172803997993469
  - 2.700515031814575
  - 0.42467260360717773
  - 0.5944362282752991
  - 1.1151665449142456
  - 0.8060517311096191
  - 1.6589000225067139
  - 4.823873996734619
  - 0.46678030490875244
  - 0.5180100202560425
  - 0.37996360659599304
  - 0.3874303996562958
  - 0.3918899595737457
  - 0.3920253813266754
  - 0.4784742295742035
  - 0.7738984823226929
  - 0.584199845790863
  - 1.0174635648727417
  - 0.906099259853363
  - 1.1417841911315918
  - 0.46841171383857727
  - 0.6888157725334167
  - 0.4053835868835449
  - 0.4534558355808258
  - 0.4794788658618927
  - 0.4532833993434906
  - 0.7448083758354187
  - 1.6341571807861328
  - 1.4135977029800415
  - 0.8998883366584778
  - 0.5586378574371338
  - 1.2293100357055664
  - 0.6033982038497925
  - 0.45433536171913147
  - 0.38446128368377686
  - 0.516602635383606
  - 0.4847964942455292
  - 0.45833510160446167
  - 0.37163469195365906
  - 0.5283390283584595
  - 0.5309993624687195
  - 0.7718120217323303
  - 1.326876163482666
  - 3.652970790863037
  - 0.6552428007125854
  - 1.024074673652649
  - 0.6578065156936646
  - 0.6391464471817017
  - 0.8535520434379578
  - 0.6510135531425476
  - 0.7527590394020081
  - 0.4928605854511261
  - 1.3373351097106934
  - 0.5136368870735168
  - 0.4327990412712097
  - 1.0819921493530273
  - 0.6113415360450745
  - 1.0827662944793701
  - 1.7017369270324707
  - 2.6404943466186523
  - 0.3859994113445282
  - 0.42126160860061646
  - 0.42967650294303894
  - 0.6165496110916138
  - 0.671494722366333
  - 0.7512572407722473
  - 0.46467724442481995
  - 0.4881778359413147
  - 0.5662400126457214
  - 0.4770423173904419
  - 5.224967956542969
  - 0.6373520493507385
  - 0.6008116602897644
  - 0.5214206576347351
  - 0.557117223739624
  - 0.4544025659561157
  - 0.5321022272109985
  - 0.582615852355957
  - 0.5341264009475708
  - 0.47834670543670654
  - 0.6029331684112549
  - 0.4882568120956421
  - 0.6603478789329529
loss_records_fold4:
  train_losses:
  - 14.848962277173996
  - 14.725897282361984
  - 14.90238407254219
  - 14.654587239027023
  - 14.451502084732056
  - 14.672038242220879
  - 14.369175300002098
  - 14.764280289411545
  - 14.505897641181946
  - 14.600347846746445
  - 14.680357113480568
  - 14.403174132108688
  - 14.569211512804031
  - 14.674805641174316
  - 14.3900106549263
  - 14.716356545686722
  - 14.419749438762665
  - 14.814425200223923
  - 14.725091397762299
  - 14.344167605042458
  - 14.831748336553574
  - 14.868187695741653
  - 14.628498882055283
  - 14.743409663438797
  - 14.895035564899445
  - 14.593519508838654
  - 14.792143419384956
  - 14.751197442412376
  - 14.422938048839569
  - 14.581367045640945
  - 15.076460897922516
  - 14.436593294143677
  - 14.700625211000443
  - 14.808693245053291
  - 14.832752704620361
  - 14.964495301246643
  - 15.024477154016495
  - 15.008718937635422
  - 15.144324243068695
  - 15.132445693016052
  - 14.604189425706863
  - 14.756408676505089
  - 14.616176515817642
  - 14.803076207637787
  - 14.668976962566376
  - 15.099116623401642
  - 14.969383895397186
  - 14.238961890339851
  - 14.676749095320702
  - 14.41523739695549
  - 15.249023973941803
  - 14.983462065458298
  - 14.70125737786293
  - 14.97168579697609
  - 14.927337110042572
  - 14.820091754198074
  - 14.736006319522858
  - 15.00767320394516
  - 14.838193967938423
  - 15.000389531254768
  - 15.13209867477417
  - 14.723503708839417
  - 14.577932864427567
  - 15.01916429400444
  - 14.821763083338737
  - 14.417511254549026
  - 14.480301260948181
  - 15.398128017783165
  - 14.363041341304779
  - 14.96501711010933
  - 14.651020511984825
  - 14.504514992237091
  - 14.961287826299667
  - 14.548254430294037
  - 14.733085751533508
  - 14.41585773229599
  - 14.80987274646759
  - 14.78171294927597
  - 14.603116571903229
  - 14.87254723906517
  - 14.581662118434906
  - 14.705014690756798
  - 14.707030057907104
  - 15.007333725690842
  - 15.187170892953873
  - 14.836205422878265
  - 14.501698315143585
  - 14.528558492660522
  - 14.335965603590012
  - 15.203424513339996
  - 14.818426504731178
  - 14.537505596876144
  - 14.391424357891083
  - 14.889872148633003
  - 14.465959578752518
  - 14.42361530661583
  - 14.577472135424614
  - 14.474269539117813
  - 14.44691213965416
  - 14.140202671289444
  validation_losses:
  - 0.5023636221885681
  - 0.46631720662117004
  - 0.5005762577056885
  - 0.365153431892395
  - 0.4136746823787689
  - 0.5040998458862305
  - 0.4141084849834442
  - 0.42848634719848633
  - 0.5184741616249084
  - 0.4225810468196869
  - 0.40621498227119446
  - 0.372546911239624
  - 0.5121634006500244
  - 0.39788028597831726
  - 0.38207489252090454
  - 0.4660540223121643
  - 0.5804491639137268
  - 1.176168441772461
  - 0.3936748206615448
  - 0.4194229543209076
  - 0.4199059009552002
  - 0.5194377899169922
  - 0.653522789478302
  - 0.8501423597335815
  - 0.5437257289886475
  - 0.4124835431575775
  - 0.3835526406764984
  - 0.49010223150253296
  - 0.43157848715782166
  - 0.4525989294052124
  - 0.4141591787338257
  - 0.42224571108818054
  - 0.39312803745269775
  - 0.46086353063583374
  - 0.3749638795852661
  - 0.42515698075294495
  - 0.4231680631637573
  - 0.38075336813926697
  - 0.4023069739341736
  - 0.40827369689941406
  - 0.3925338685512543
  - 0.45228609442710876
  - 0.38272759318351746
  - 0.4144309163093567
  - 0.38659781217575073
  - 0.39372748136520386
  - 0.4119860529899597
  - 0.40390241146087646
  - 0.4169365167617798
  - 0.4222675561904907
  - 0.4331638216972351
  - 0.3931466341018677
  - 0.4902661442756653
  - 0.5714815855026245
  - 0.3890868127346039
  - 0.4199336767196655
  - 0.4447215497493744
  - 3.3139355182647705
  - 0.89129638671875
  - 0.40171006321907043
  - 0.4115257263183594
  - 0.4606640040874481
  - 0.47883784770965576
  - 0.4241805970668793
  - 0.43946710228919983
  - 0.49004635214805603
  - 1.5559810400009155
  - 0.4048845171928406
  - 0.4241941273212433
  - 0.3923162519931793
  - 0.4595545530319214
  - 0.4457445740699768
  - 0.5404718518257141
  - 0.46762076020240784
  - 0.4615950584411621
  - 0.4393880069255829
  - 0.43355873227119446
  - 0.490275114774704
  - 0.45827457308769226
  - 0.44170424342155457
  - 1.309915542602539
  - 2.0373053550720215
  - 0.3724800646305084
  - 0.3938196897506714
  - 0.3763922154903412
  - 0.39562246203422546
  - 0.3846406638622284
  - 0.3889465928077698
  - 0.401212602853775
  - 0.39015302062034607
  - 0.3808359205722809
  - 0.4904671907424927
  - 0.4395104944705963
  - 0.4006575345993042
  - 0.39533987641334534
  - 0.3910202383995056
  - 0.4122668206691742
  - 0.4008048176765442
  - 0.41789764165878296
  - 0.3896887004375458
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 10 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8147512864493996,
    0.8556701030927835]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.21739130434782608, 0.0]'
  mean_eval_accuracy: 0.849007090944457
  mean_f1_accuracy: 0.043478260869565216
  total_train_time: '0:06:31.489748'
