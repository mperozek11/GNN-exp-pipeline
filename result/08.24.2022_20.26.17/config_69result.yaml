config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:02:43.575832'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_69fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_69fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_69fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_69fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_69fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_69fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_69fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_69fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_69fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_69fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.4943812102079392
  - 3.2035967946052555
  - 3.2159010112285618
  - 3.2762147307395937
  - 3.3168603777885437
  - 3.117501825094223
  - 3.140967696905136
  - 3.0689861565828327
  - 3.154147052764893
  - 3.069824177026749
  - 3.1990291714668277
  - 3.1333154261112215
  - 3.210818311572075
  - 3.050659731030464
  - 3.214892041683197
  - 3.5644696533679965
  - 3.088867238163948
  - 3.1254263103008273
  - 3.09133318066597
  - 3.024091047048569
  - 3.154036742448807
  validation_losses:
  - 0.4016623795032501
  - 0.41658270359039307
  - 0.397034615278244
  - 0.4170096218585968
  - 0.4073030352592468
  - 0.40433117747306824
  - 0.3979799449443817
  - 0.3942359387874603
  - 0.3889569938182831
  - 0.478596568107605
  - 0.40029364824295044
  - 0.41142386198043823
  - 0.390266090631485
  - 0.3980960547924042
  - 0.42518848180770874
  - 0.3953194320201874
  - 0.40194493532180786
  - 0.39343002438545227
  - 0.39257940649986267
  - 0.3934996724128723
  - 0.3946906626224518
loss_records_fold1:
  train_losses:
  - 3.1554331421852115
  - 3.1218504428863527
  - 2.9992476373910906
  - 3.0316806852817537
  - 2.987142962217331
  - 3.09942090511322
  - 3.0972884178161624
  - 3.0068139493465424
  - 3.065489941835404
  - 3.0685305058956147
  - 3.0184255182743076
  - 3.0149557381868366
  - 3.0696198761463167
  - 3.0593358635902406
  - 2.9840530395507816
  - 2.9580854535102845
  - 3.0164603114128115
  - 3.1564424574375156
  - 2.962793979048729
  - 2.9512130260467533
  - 3.062231856584549
  - 3.0376487612724308
  - 2.996354776620865
  - 3.0941194713115694
  - 2.998450216650963
  - 3.117820590734482
  - 3.120305958390236
  - 3.01793991625309
  - 3.122520810365677
  - 3.0678315788507464
  - 3.062079602479935
  - 3.0169345468282702
  - 3.0648339211940767
  - 3.076887792348862
  - 2.985737028717995
  - 3.0403736472129825
  - 3.0945801854133608
  - 3.0263294696807863
  - 2.9844370961189273
  - 3.0615714609622957
  - 2.949300056695938
  - 3.0589569449424747
  - 2.9683503031730654
  - 3.122703540325165
  - 3.070289018750191
  - 3.025249108672142
  - 3.030052548646927
  - 3.0025049448013306
  validation_losses:
  - 0.4134674668312073
  - 0.4096319079399109
  - 0.40171533823013306
  - 0.40125179290771484
  - 0.3831349015235901
  - 0.39649805426597595
  - 0.406530499458313
  - 0.39597997069358826
  - 0.42540243268013
  - 0.39831340312957764
  - 0.38805848360061646
  - 0.4059261083602905
  - 0.3950774371623993
  - 0.3871959149837494
  - 0.40191397070884705
  - 0.38839811086654663
  - 0.434751033782959
  - 0.39855802059173584
  - 0.39568135142326355
  - 0.40219759941101074
  - 0.4148310422897339
  - 0.39917096495628357
  - 0.39211785793304443
  - 0.3987102210521698
  - 0.5455745458602905
  - 0.4181345999240875
  - 0.4722460210323334
  - 0.3909963071346283
  - 0.39981400966644287
  - 0.3927706778049469
  - 0.41645997762680054
  - 0.39369192719459534
  - 0.42280158400535583
  - 0.3904274106025696
  - 0.4114916920661926
  - 0.3986225128173828
  - 0.38994550704956055
  - 0.39733412861824036
  - 0.4093641936779022
  - 0.4057668149471283
  - 0.41296565532684326
  - 0.5177558064460754
  - 0.41378524899482727
  - 0.42335933446884155
  - 0.3988710343837738
  - 0.39051946997642517
  - 0.3936493396759033
  - 0.3886653482913971
loss_records_fold2:
  train_losses:
  - 3.019414269924164
  - 3.0270843356847763
  - 2.9875506043434146
  - 3.1879844665527344
  - 3.106094068288803
  - 3.0281364172697067
  - 3.0078616827726368
  - 3.041525161266327
  - 3.031023317575455
  - 2.9620021283626556
  - 3.0564064860343936
  - 3.0032514303922655
  - 2.9690678060054783
  - 2.969602543115616
  - 2.97127605676651
  - 3.0438122630119326
  - 2.9596293807029728
  - 3.0223159134387974
  - 3.012260812520981
  - 2.93546034693718
  - 3.0209129214286805
  - 2.9298923492431643
  - 3.113239759206772
  - 2.982913237810135
  - 3.0943202853202822
  - 3.0281426787376406
  validation_losses:
  - 0.384113609790802
  - 0.7994769215583801
  - 0.5580276846885681
  - 0.3941129744052887
  - 0.39915731549263
  - 0.39850932359695435
  - 0.40334874391555786
  - 0.4355832040309906
  - 0.3888806104660034
  - 0.5015032887458801
  - 0.3981458246707916
  - 0.4095703065395355
  - 0.39949092268943787
  - 0.39619261026382446
  - 0.3951011002063751
  - 0.4054079055786133
  - 0.40306246280670166
  - 0.395520955324173
  - 0.3966984450817108
  - 0.4116666316986084
  - 0.3947424590587616
  - 0.3858702480792999
  - 0.389095664024353
  - 0.3841206431388855
  - 0.3900628983974457
  - 0.3892349600791931
loss_records_fold3:
  train_losses:
  - 3.0416176319122314
  - 3.0524283915758135
  - 3.056013470888138
  - 2.992143833637238
  - 3.0123090893030167
  - 3.041672110557556
  - 3.023304927349091
  - 3.0705220937728885
  - 3.007495933771134
  - 2.958417421579361
  - 3.0123740911483767
  - 2.9716293931007387
  - 2.9542108178138733
  - 2.9780026733875276
  - 3.008870714902878
  - 3.020496538281441
  - 3.065647393465042
  - 2.943093514442444
  - 2.988611972332001
  - 3.02976376414299
  - 3.0097375571727754
  - 3.0364989638328552
  - 2.994271928071976
  - 2.952516257762909
  - 3.070095527172089
  - 2.9656002402305606
  - 2.9870063751935962
  - 2.9867521584033967
  - 3.018334996700287
  - 2.9919403284788135
  - 3.0301816880702974
  - 3.0547549426555634
  - 3.0271321058273317
  - 3.0597207605838777
  - 2.9910364687442783
  - 3.0125347673892975
  - 3.0824575781822205
  - 2.935897183418274
  - 3.052629142999649
  - 2.992249545454979
  - 3.0026852786540985
  - 2.9288282990455627
  - 3.022288531064987
  - 3.0123459786176685
  - 2.9692631512880325
  - 2.9977085143327713
  - 3.007075119018555
  - 2.9373445093631747
  - 2.9213353931903843
  - 2.9749721348285676
  - 2.9706266939640047
  - 2.9834039926528932
  - 3.0030814111232758
  - 3.0326540350914004
  - 3.0313407480716705
  - 2.999365210533142
  - 2.954965740442276
  - 2.9679992318153383
  - 3.026567536592484
  - 3.058767604827881
  - 2.950604009628296
  - 2.949367940425873
  - 2.966907608509064
  - 3.046091061830521
  - 3.0040417909622192
  - 2.91695955991745
  - 3.0485485196113586
  - 3.0048717975616457
  - 2.980116307735443
  - 2.8856487900018695
  - 3.045498895645142
  - 2.941729724407196
  - 3.0703510820865634
  - 2.9141723901033405
  - 2.965589112043381
  - 2.985035198926926
  - 2.9930746972560884
  - 2.9919067919254303
  - 2.9208340764045717
  - 2.9581352293491365
  - 2.975424873828888
  - 3.0067373335361482
  - 3.0655126810073856
  - 2.9795908451080324
  - 3.006978303194046
  - 2.9597957849502565
  - 2.915612205862999
  - 3.0008680284023286
  - 3.043509316444397
  - 3.0960424274206164
  - 3.002244818210602
  - 2.9674705207347873
  - 2.9136395156383514
  - 2.9585674285888675
  - 2.991794833540917
  - 2.977860701084137
  - 2.980438452959061
  - 3.0046327829360964
  - 2.9681732237339022
  - 2.923478975892067
  validation_losses:
  - 0.3821585774421692
  - 0.37623074650764465
  - 0.385475754737854
  - 0.39071840047836304
  - 0.4032551050186157
  - 1.533886432647705
  - 1.5177050828933716
  - 0.5894456505775452
  - 0.4385946989059448
  - 1.0095471143722534
  - 2.6309800148010254
  - 2.4465372562408447
  - 0.738021731376648
  - 0.41246962547302246
  - 0.3822460174560547
  - 0.9137070178985596
  - 0.39249151945114136
  - 0.39155006408691406
  - 0.38100650906562805
  - 0.38200843334198
  - 2.020287036895752
  - 0.5840073823928833
  - 0.9477432370185852
  - 1.1739801168441772
  - 0.39978206157684326
  - 1.0225774049758911
  - 0.69785475730896
  - 0.4018518030643463
  - 0.42767053842544556
  - 1.2197062969207764
  - 0.3927096724510193
  - 0.38709139823913574
  - 0.4088212847709656
  - 0.39459726214408875
  - 0.3939548432826996
  - 0.3892822265625
  - 0.41058313846588135
  - 0.412804514169693
  - 0.41252759099006653
  - 0.6442664861679077
  - 0.46794429421424866
  - 0.6810533404350281
  - 0.5713770985603333
  - 0.4079222083091736
  - 0.9588620066642761
  - 1.8714386224746704
  - 4.5089592933654785
  - 3.5220024585723877
  - 1.45188570022583
  - 2.1271135807037354
  - 4.55436897277832
  - 0.42550113797187805
  - 4.24228572845459
  - 2.082343578338623
  - 0.3920053243637085
  - 1.3844469785690308
  - 2.5967071056365967
  - 0.7095979452133179
  - 0.38222765922546387
  - 0.38684841990470886
  - 0.4449193477630615
  - 0.39551445841789246
  - 0.43798935413360596
  - 0.43418893218040466
  - 0.4321482479572296
  - 0.40608301758766174
  - 0.45784351229667664
  - 0.38747909665107727
  - 0.512148916721344
  - 0.39985522627830505
  - 0.4127029478549957
  - 0.4056224524974823
  - 0.43166762590408325
  - 0.5140613913536072
  - 0.47058770060539246
  - 0.4576188325881958
  - 0.43644607067108154
  - 1.1826645135879517
  - 0.45822736620903015
  - 3.5637969970703125
  - 0.5509128570556641
  - 0.5255123972892761
  - 0.5721077919006348
  - 0.5681730508804321
  - 3.692880392074585
  - 1.6743550300598145
  - 1.1100417375564575
  - 2.1977791786193848
  - 1.8593776226043701
  - 0.44014912843704224
  - 0.40086933970451355
  - 0.4345436990261078
  - 0.46763327717781067
  - 0.524093508720398
  - 0.4649803936481476
  - 0.44014859199523926
  - 0.4925902187824249
  - 0.43173104524612427
  - 0.4670504629611969
  - 0.4955245554447174
loss_records_fold4:
  train_losses:
  - 2.9224050819873812
  - 2.976435858011246
  - 2.9875184118747713
  - 2.9655839800834656
  - 2.9776439845561984
  - 3.0311276227235795
  - 3.0168401777744296
  - 2.948363494873047
  - 2.9519081830978395
  - 3.0152229905128483
  - 2.9482442498207093
  - 2.9822757065296175
  - 2.9878152847290043
  - 2.934911549091339
  - 2.9877627432346348
  - 2.92348747253418
  - 2.9841586858034135
  - 2.9470508098602295
  - 2.923531758785248
  - 2.9766674220561984
  - 2.979164382815361
  - 2.976688760519028
  - 2.9370340526103975
  - 2.9530594646930695
  - 3.000632101297379
  - 2.977762883901596
  - 3.0354735374450685
  - 2.86518566608429
  - 3.018395334482193
  - 3.031182637810707
  - 2.8907852947711947
  - 2.98895446062088
  - 2.895913237333298
  - 2.975571382045746
  - 2.9922283411026003
  - 2.8749642312526706
  - 2.9634143471717835
  - 2.9068428933620454
  - 2.931440705060959
  - 2.9616872251033786
  - 2.9861468851566317
  - 2.929268568754196
  - 2.971122506260872
  - 2.918482992053032
  - 2.999812349677086
  - 2.9821021378040315
  - 2.9270743876695633
  - 2.921108686923981
  - 2.951616430282593
  - 2.8542267203330995
  - 2.9396279335021975
  - 2.977520352602005
  - 2.9233369052410128
  - 2.88278286755085
  - 2.968593317270279
  - 2.934012770652771
  - 2.8972399771213535
  - 2.9461423367261887
  - 2.8989842236042023
  - 2.9453300535678864
  - 2.854603844881058
  - 2.978189790248871
  - 2.9349672853946687
  - 2.964064449071884
  - 2.8850839614868167
  - 2.93324448466301
  - 2.923808062076569
  - 2.9989771723747256
  - 2.9830766916275024
  - 2.8925829946994783
  - 2.9271127343177796
  - 2.9166518539190296
  - 2.9814670503139498
  - 2.9349471509456637
  - 2.8822043359279634
  - 2.9640672981739047
  - 2.906017610430718
  - 2.9932702958583834
  - 2.9478372216224673
  - 2.9403452754020694
  - 2.948821485042572
  - 2.8917204558849336
  - 2.8913094997406006
  - 2.937702411413193
  - 2.9906844198703766
  - 2.87926943898201
  - 2.912349128723145
  - 2.9257220804691317
  - 2.923189187049866
  - 2.9708286821842194
  - 3.0038282334804536
  - 2.8772647976875305
  - 2.9787750840187073
  - 2.913970130681992
  - 2.983524399995804
  - 2.8631561160087586
  - 2.9156314849853517
  - 2.935003447532654
  - 2.9061301887035373
  - 2.896582692861557
  validation_losses:
  - 0.420769065618515
  - 0.4525638818740845
  - 0.46380478143692017
  - 0.40945422649383545
  - 0.4224269390106201
  - 0.4415745139122009
  - 0.4187597930431366
  - 0.4229060113430023
  - 0.44391247630119324
  - 0.40791964530944824
  - 0.4414123296737671
  - 0.4481443464756012
  - 0.4439493417739868
  - 0.43140754103660583
  - 0.44627103209495544
  - 0.46767571568489075
  - 0.42723003029823303
  - 0.43332499265670776
  - 0.45581328868865967
  - 0.4360044300556183
  - 0.41085001826286316
  - 0.5362439751625061
  - 0.5736581087112427
  - 0.44688042998313904
  - 0.49491605162620544
  - 0.48356950283050537
  - 0.517088770866394
  - 0.5581573843955994
  - 0.39733967185020447
  - 0.40732088685035706
  - 0.3854515254497528
  - 0.41121983528137207
  - 0.4918561279773712
  - 0.383513480424881
  - 0.37979739904403687
  - 0.42925602197647095
  - 0.4550887644290924
  - 0.5158191323280334
  - 0.47948694229125977
  - 0.479521244764328
  - 0.38702771067619324
  - 0.3981185853481293
  - 0.3719465434551239
  - 0.48841533064842224
  - 0.4536609947681427
  - 0.38757777214050293
  - 0.6215369701385498
  - 0.48506397008895874
  - 0.6035323143005371
  - 0.40693747997283936
  - 0.4943300187587738
  - 0.3817053735256195
  - 0.44249197840690613
  - 0.5680480599403381
  - 0.4953218996524811
  - 0.38111916184425354
  - 0.47910428047180176
  - 0.49991416931152344
  - 0.4982421398162842
  - 0.4576796889305115
  - 0.5903587341308594
  - 0.5780171751976013
  - 0.8163056969642639
  - 0.3807179033756256
  - 0.5501231551170349
  - 0.6345592737197876
  - 0.6383265852928162
  - 0.5589237809181213
  - 0.5085094571113586
  - 0.5236989259719849
  - 0.5254058241844177
  - 0.6154518723487854
  - 0.5254858732223511
  - 0.4296536445617676
  - 0.3851853311061859
  - 0.39055201411247253
  - 0.426822692155838
  - 0.6816657781600952
  - 0.9562792181968689
  - 1.0402777194976807
  - 0.6258024573326111
  - 0.6458603739738464
  - 0.5699645280838013
  - 0.5712196230888367
  - 0.851824939250946
  - 0.6316414475440979
  - 0.5413793325424194
  - 0.4165075421333313
  - 0.5393884778022766
  - 0.517294704914093
  - 0.4091847836971283
  - 0.8499841094017029
  - 0.6804505586624146
  - 0.658794105052948
  - 0.38016700744628906
  - 0.42449814081192017
  - 0.6647641658782959
  - 0.7057879567146301
  - 0.7224664092063904
  - 0.9073853492736816
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 48 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8419243986254296]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.09803921568627451]'
  mean_eval_accuracy: 0.8551773325552745
  mean_f1_accuracy: 0.0196078431372549
  total_train_time: '0:27:20.296019'
