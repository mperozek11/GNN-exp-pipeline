config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 04:20:57.746245'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/123/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/123/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/123/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/123/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/123/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/123/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/123/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/123/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/123/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/123/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 680.6987512111664
  - 358.2448642253876
  - 194.81215977668762
  - 193.00877177715302
  - 159.58416014909744
  - 100.38259914517403
  - 87.83998748660088
  - 39.009804248809814
  - 44.51968714594841
  - 28.138785511255264
  - 32.347837656736374
  - 35.56307190656662
  - 44.05310741066933
  - 34.238549679517746
  - 22.860307902097702
  - 25.443343862891197
  - 36.412126272916794
  - 21.981590554118156
  - 21.676499992609024
  - 20.455477237701416
  - 22.21089616417885
  - 22.876912236213684
  - 17.959770411252975
  - 23.9282785654068
  - 23.804151862859726
  - 22.48757517337799
  - 31.928871542215347
  - 22.99912941455841
  - 24.00059112906456
  - 34.635542660951614
  - 34.58553609251976
  - 33.061748921871185
  - 26.527502834796906
  - 17.85526132583618
  - 20.224076330661774
  - 23.363308310508728
  - 21.13897281885147
  - 18.860115736722946
  - 21.0112724006176
  - 20.47754144668579
  - 19.82372272014618
  - 33.24993512034416
  - 20.970307797193527
  - 18.80236268043518
  - 19.360818296670914
  - 17.84363701939583
  - 17.83808344602585
  - 21.69187307357788
  - 21.266179740428925
  - 19.681737959384918
  - 72.66810953617096
  - 41.55802321434021
  - 29.812778040766716
  - 24.62180870771408
  - 21.000451117753983
  - 29.61008670926094
  - 19.705021113157272
  - 39.092824935913086
  - 31.02263516187668
  - 24.23064735531807
  - 22.40683126449585
  - 19.512601137161255
  - 18.99020564556122
  - 19.599513679742813
  - 29.59541679918766
  - 56.888565719127655
  - 35.72920408844948
  - 30.667740762233734
  - 44.8053240776062
  - 22.32162055373192
  - 31.052897602319717
  - 68.44049429893494
  - 20.62215928733349
  - 18.79175180196762
  - 19.13143277168274
  - 30.184752821922302
  - 24.777400076389313
  - 18.1952086687088
  - 20.33355987071991
  - 21.987433284521103
  - 24.83371675014496
  - 18.851258128881454
  - 21.976173728704453
  - 20.904608637094498
  - 25.045814961194992
  - 21.45980016887188
  - 21.26384299993515
  - 19.280095100402832
  - 23.903401851654053
  - 20.774251133203506
  - 21.97060889005661
  - 23.170031905174255
  - 17.7658851146698
  - 20.739856123924255
  - 27.45566475391388
  - 29.02656352519989
  - 28.817323833703995
  - 23.08018320798874
  - 20.806722581386566
  - 19.727997317910194
  validation_losses:
  - 73.03328704833984
  - 2.8102500438690186
  - 3.617262601852417
  - 1.1221930980682373
  - 0.6985563039779663
  - 0.736059844493866
  - 0.7473611831665039
  - 1.1124482154846191
  - 0.5792551636695862
  - 0.5289602279663086
  - 0.7268108129501343
  - 0.7262008190155029
  - 0.7322297692298889
  - 0.765059232711792
  - 0.5945889949798584
  - 0.6071134805679321
  - 0.573740541934967
  - 0.6000335812568665
  - 0.5415827035903931
  - 0.5609171986579895
  - 0.56061190366745
  - 0.4154702126979828
  - 0.5228512287139893
  - 0.670879065990448
  - 0.5715829730033875
  - 2.5141916275024414
  - 75.47267150878906
  - 0.5272284746170044
  - 1.6875724792480469
  - 0.8953956365585327
  - 1.5376542806625366
  - 0.5709846615791321
  - 0.48402276635169983
  - 0.4426403045654297
  - 0.4745522141456604
  - 0.9036999344825745
  - 10589.775390625
  - 21508.763671875
  - 19286.072265625
  - 22617.55078125
  - 21838.671875
  - 22116.49609375
  - 23706.45703125
  - 25295.26171875
  - 22362.6796875
  - 22744.357421875
  - 0.47341641783714294
  - 73388195840.0
  - 335739680.0
  - 12890.3125
  - 1.9414293766021729
  - 341378304.0
  - 737996672.0
  - 382895264.0
  - 20967.9765625
  - 22080.716796875
  - 22978.478515625
  - 22643.220703125
  - 0.4067833721637726
  - 0.7056965827941895
  - 0.509797990322113
  - 0.7004656791687012
  - 0.5060651898384094
  - 0.5828619003295898
  - 0.5827915668487549
  - 5722.76513671875
  - 0.8735038638114929
  - 0.8628929257392883
  - 10855.7734375
  - 21869.03515625
  - 23529.029296875
  - 21766.26953125
  - 0.46996384859085083
  - 0.47425583004951477
  - 0.52060467004776
  - 23979.24609375
  - 22849.490234375
  - 19459.568359375
  - 22255.455078125
  - 19148.994140625
  - 17411.732421875
  - 18339.4765625
  - 0.4773634970188141
  - 0.5121493339538574
  - 0.4403532147407532
  - 17249.314453125
  - 23437.603515625
  - 22504.025390625
  - 25312.251953125
  - 16017.5205078125
  - 17992.1484375
  - 20919.146484375
  - 23043.48828125
  - 1220.7625732421875
  - 0.6110585331916809
  - 0.6555200815200806
  - 0.6746892333030701
  - 0.595128059387207
  - 0.5469364523887634
  - 0.8001510500907898
loss_records_fold1:
  train_losses:
  - 23.692742556333542
  - 30.33112494647503
  - 22.851875364780426
  - 20.728307604789734
  - 22.76144602894783
  - 37.71978136897087
  - 24.576661467552185
  - 47.8493150472641
  - 24.953858375549316
  - 25.710206300020218
  - 24.798360913991928
  - 22.14861434698105
  - 23.127623349428177
  - 23.42736953496933
  - 19.625640153884888
  - 24.16904515028
  - 21.848093926906586
  - 21.326223105192184
  - 21.772209584712982
  - 25.061945095658302
  - 18.747789084911346
  - 18.729707926511765
  - 23.401880085468292
  - 24.10635358095169
  - 27.341264337301254
  - 25.09295266866684
  - 18.765774369239807
  - 26.910764575004578
  - 20.287526786327362
  - 21.752537608146667
  - 24.84125754237175
  - 19.179000943899155
  - 19.14500778913498
  - 19.028781563043594
  - 19.688634932041168
  - 22.388301968574524
  - 19.35107347369194
  - 24.048491165041924
  - 26.924143999814987
  - 23.57810053229332
  - 21.783873945474625
  - 19.99465522170067
  - 21.213004052639008
  - 22.657929226756096
  - 26.027439326047897
  - 22.503256291151047
  - 128.30248737335205
  - 168.12879610061646
  - 185.94113063812256
  - 1093.3193691670895
  - 3453.9615437090397
  - 1226.898601859808
  - 593.4812634885311
  - 290.4177244603634
  - 657.7651942968369
  - 464.8779526948929
  - 376.8760792016983
  - 51.1161103695631
  - 92.54654225707054
  - 92.63844257593155
  - 55.920129865407944
  - 81.40635398030281
  - 54.082820028066635
  - 89.78703871369362
  - 71.39144858717918
  - 59.74966290593147
  - 90.2134519815445
  - 34.13852718472481
  - 40.19256389141083
  - 75.95664191246033
  - 87.69528615474701
  - 48.944935247302055
  - 36.7794414460659
  - 57.320003032684326
  - 63.289110600948334
  - 32.62794154882431
  - 29.964400738477707
  - 26.31418389081955
  - 63.80214715003967
  - 34.02334351837635
  - 74.49110804498196
  - 87.70229184627533
  - 73.75825506448746
  - 30.703239172697067
  - 26.709789723157883
  - 26.190971106290817
  - 39.41160191595554
  - 43.94590750336647
  - 31.317366510629654
  - 28.32476782798767
  - 25.362417578697205
  - 40.4696983397007
  - 48.047260880470276
  - 38.162623167037964
  - 58.99112233519554
  - 51.527780413627625
  - 34.37633752822876
  - 30.83217227458954
  - 27.871317595243454
  - 28.45617350935936
  validation_losses:
  - 21484.396484375
  - 25641.703125
  - 0.5094254016876221
  - 0.4939480125904083
  - 0.4656643867492676
  - 0.6982951760292053
  - 0.6273411512374878
  - 0.7860881090164185
  - 0.9827278256416321
  - 10804.376953125
  - 21313.392578125
  - 19318.216796875
  - 19491.421875
  - 23205.455078125
  - 19237.033203125
  - 24325.32421875
  - 19779.005859375
  - 16521.6328125
  - 24753.560546875
  - 27695.33203125
  - 21884.267578125
  - 22481.630859375
  - 16885.025390625
  - 22635.0
  - 20163.982421875
  - 19136.884765625
  - 0.4979724586009979
  - 0.7551750540733337
  - 0.48670679330825806
  - 1.206013798713684
  - 22330.892578125
  - 27347.68359375
  - 27626.470703125
  - 21546.9375
  - 21748.62890625
  - 24432.51953125
  - 20462.93359375
  - 22260.6875
  - 20498.775390625
  - 24109.55859375
  - 24222.162109375
  - 23218.013671875
  - 58501582848.0
  - 436679081984.0
  - 18205.955078125
  - 0.4764772355556488
  - 1.4596744775772095
  - 4.471912384033203
  - 5.103326320648193
  - 166.25112915039062
  - 300.88421630859375
  - 3.0400009155273438
  - 1.1404486894607544
  - 0.7367295026779175
  - 1.371351718902588
  - 0.7081573605537415
  - 0.596136212348938
  - 0.7868472337722778
  - 0.5736703276634216
  - 0.9596712589263916
  - 0.7977066040039062
  - 0.836723268032074
  - 0.7048906683921814
  - 0.8672621846199036
  - 1.4772231578826904
  - 0.7525217533111572
  - 0.8366942405700684
  - 0.4447871744632721
  - 1.074059009552002
  - 0.8801466822624207
  - 0.6289258599281311
  - 0.4888453483581543
  - 0.615740180015564
  - 1.132871389389038
  - 0.5984997749328613
  - 0.535438597202301
  - 0.7078496813774109
  - 0.8991419076919556
  - 0.5567290782928467
  - 0.47303640842437744
  - 0.6768522262573242
  - 1.3600339889526367
  - 0.9813408851623535
  - 0.48083654046058655
  - 0.5123633146286011
  - 0.6077973246574402
  - 1.0420862436294556
  - 0.47859278321266174
  - 0.6981543898582458
  - 0.5103049278259277
  - 0.6112534999847412
  - 0.7231780886650085
  - 0.46542197465896606
  - 0.7973183989524841
  - 0.828666627407074
  - 0.47395044565200806
  - 0.4237329661846161
  - 0.7065548896789551
  - 0.6254526972770691
  - 0.9745246171951294
loss_records_fold2:
  train_losses:
  - 29.99257829785347
  - 25.59301009774208
  - 34.34978622198105
  - 26.743312150239944
  - 24.196885526180267
  - 27.833748921751976
  - 21.97711269557476
  - 35.45913773775101
  - 39.81223455071449
  - 25.882730424404144
  - 50.98038199543953
  - 42.80824640393257
  - 30.085196882486343
  - 44.73250027000904
  - 36.93229687213898
  - 32.108917593955994
  - 20.776297375559807
  - 26.67097097635269
  - 38.30893814563751
  - 23.79343707859516
  - 41.13822937011719
  - 37.07432305812836
  - 33.68683871626854
  - 23.396887555718422
  - 29.323681950569153
  - 23.47540971636772
  - 27.325568199157715
  - 34.72334569692612
  - 30.673687875270844
  - 25.548436611890793
  - 24.647540479898453
  - 22.10831429064274
  - 21.09084391593933
  - 26.158451855182648
  - 28.720172584056854
  - 27.3304260969162
  - 28.52473682165146
  - 21.917796045541763
  - 26.003130063414574
  - 31.08804515004158
  - 21.09091004729271
  - 34.50090101361275
  - 30.559518933296204
  - 24.240591317415237
  - 20.985675513744354
  - 25.8903828561306
  - 21.608323648571968
  - 19.200815826654434
  - 19.939418524503708
  - 26.97254964709282
  - 22.35133245587349
  - 20.088290885090828
  - 26.765553176403046
  - 28.352704405784607
  - 24.14280104637146
  - 18.21768355369568
  - 22.40760450065136
  - 26.05826362967491
  - 20.831403583288193
  - 32.163949221372604
  - 21.564352214336395
  - 27.530934661626816
  - 19.82203632593155
  - 23.525118559598923
  - 24.189184814691544
  - 24.75700195133686
  - 23.008455336093903
  - 22.36443680524826
  - 20.40847034752369
  - 21.63141655921936
  - 17.23121190071106
  - 19.629376620054245
  - 19.83841685950756
  - 19.643159210681915
  - 21.27717825770378
  - 18.667877584695816
  - 21.169661730527878
  - 24.75752428174019
  - 19.481143474578857
  - 20.379329413175583
  - 20.812945127487183
  - 17.825674295425415
  - 25.175834715366364
  - 20.952685832977295
  - 22.050087317824364
  - 27.11969342827797
  - 26.845009833574295
  - 26.513765960931778
  - 26.634534865617752
  - 28.03695273399353
  - 23.779608815908432
  - 18.609164088964462
  - 20.60564896464348
  - 20.78064513206482
  - 20.721946328878403
  - 20.535965636372566
  - 21.878671377897263
  - 23.783940225839615
  - 24.06232199072838
  - 19.276011675596237
  validation_losses:
  - 0.5681559443473816
  - 22653.783203125
  - 508380032.0
  - 1089536983040.0
  - 2008674868396032.0
  - 5.264188445151986e+18
  - 1.5418367113809835e+20
  - 2.5130390265545936e+20
  - 2.4864725386433397e+20
  - 2.565495758745554e+20
  - 2.5615728771795098e+20
  - 2.57103131600629e+20
  - 2.5023005802712216e+20
  - 2.4898980891099084e+20
  - 2.5554377782183802e+20
  - 2.587081546943913e+20
  - 2.6233976243609823e+20
  - 2.523757065902154e+20
  - 2.4875891146915788e+20
  - 2.5066878955488385e+20
  - 2.8114352233339316e+20
  - 2.52095515843086e+20
  - 2.558548956320335e+20
  - 2.559475712681155e+20
  - 2.4972204847071756e+20
  - 2.4660870652987913e+20
  - 2.451916911283735e+20
  - 2.5303898478064802e+20
  - 2.5378988964976787e+20
  - 2.5152389294194478e+20
  - 2.4983131353823943e+20
  - 2.5524666339173387e+20
  - 2.5039470329631185e+20
  - 2.605080464329676e+20
  - 2.564294388360581e+20
  - 2.4316239728377804e+20
  - 2.4812283079834993e+20
  - 2.4852356320425568e+20
  - 2.5080118834705413e+20
  - 2.4737167963862545e+20
  - 2.554389283930133e+20
  - 2.575991432861513e+20
  - 2.4825600364670616e+20
  - 2.5223938474055723e+20
  - 2.5311781536631305e+20
  - 2.4289040449534532e+20
  - 2.4417553127807595e+20
  - 2.433369293615247e+20
  - 2.4517559427814287e+20
  - 2.5357327706300298e+20
  - 2.4967856058681577e+20
  - 2.4594567463005114e+20
  - 2.44166049089798e+20
  - 2.5271643204952365e+20
  - 2.5494349724963045e+20
  - 2.6056543214384447e+20
  - 2.4647317632859295e+20
  - 2.5735976641064495e+20
  - 2.588200058132617e+20
  - 2.593465927181292e+20
  - 2.4897286763583006e+20
  - 2.5689341515079352e+20
  - 2.5464444767906143e+20
  - 2.7158764038811288e+20
  - 2.511473497918501e+20
  - 2.4666369970345397e+20
  - 2.5764558665730857e+20
  - 2.5480904017169298e+20
  - 2.5604485605694112e+20
  - 2.4797966558832047e+20
  - 2.594852191441592e+20
  - 2.557289707643276e+20
  - 2.4519871041060523e+20
  - 2.6294827615137458e+20
  - 2.5296256432447108e+20
  - 2.4580697783527696e+20
  - 2.506433336616776e+20
  - 2.555424232235126e+20
  - 2.4835434396669444e+20
  - 2.5916398582698816e+20
  - 2.5311401545412746e+20
  - 2.4820327986513104e+20
  - 2.467204696878193e+20
  - 2.540776098525243e+20
  - 0.9809054732322693
  - 0.6623691320419312
  - 0.9593521952629089
  - 1451943.75
  - 5.924268980893123e+17
  - 1.0615488337842432e+20
  - 2.4853619439383557e+20
  - 2.4732208726616624e+20
  - 2.5084729746667654e+20
  - 2.5200250595546916e+20
  - 2.5610660462995702e+20
  - 2.544145881762051e+20
  - 2.453557382632377e+20
  - 2.512520760753725e+20
  - 2.5118341377324116e+20
  - 2.490236562769403e+20
loss_records_fold4:
  train_losses:
  - 22.528450518846512
  - 57.86337158083916
  - 24.730972051620483
  - 24.795662611722946
  - 20.431102752685547
  - 20.90510642528534
  - 20.237499356269836
  - 30.128949880599976
  - 24.85405158996582
  - 19.603973492980003
  - 25.22339165210724
  - 23.21509611606598
  - 20.725301653146744
  - 23.08759245276451
  - 19.582981020212173
  - 24.004470884799957
  - 20.066744565963745
  - 25.11594222486019
  - 24.01306787133217
  - 23.317269176244736
  - 21.36026021838188
  - 22.316789507865906
  - 26.89119592308998
  - 28.193885892629623
  - 20.099846482276917
  - 20.163156360387802
  - 18.706303656101227
  - 28.723167181015015
  - 24.63030008971691
  - 24.845061987638474
  - 24.03684851527214
  - 21.80987909436226
  - 25.862814992666245
  - 31.737001717090607
  - 22.95042872428894
  - 30.04272621870041
  - 26.747281104326248
  - 18.901710271835327
  - 24.458845794200897
  - 23.303348630666733
  - 19.73794224858284
  - 17.8704554438591
  - 19.355188012123108
  - 23.593259751796722
  - 26.07429614663124
  - 27.342443868517876
  - 27.57890233397484
  - 22.702894628047943
  - 20.12901245057583
  - 25.145833536982536
  - 41.2277267575264
  - 24.59584292769432
  - 22.707917898893356
  - 27.381306052207947
  - 29.787990629673004
  - 29.974547177553177
  - 22.01436749100685
  - 17.79176700115204
  - 27.571829795837402
  - 26.928246274590492
  - 17.381938189268112
  - 22.27145206928253
  - 23.24228546023369
  - 19.69987951219082
  - 22.90157178044319
  - 23.916049271821976
  - 22.874506175518036
  - 19.367992609739304
  - 20.272188007831573
  - 19.29793095588684
  - 21.08343595266342
  - 17.489813446998596
  - 18.60238765180111
  - 24.655854016542435
  - 21.59590819478035
  - 20.930264860391617
  - 21.602810338139534
  - 20.99380426108837
  - 29.16436132788658
  - 29.20087243616581
  - 23.330526247620583
  - 25.756822615861893
  - 22.165435135364532
  - 23.975611567497253
  - 18.648291170597076
  - 19.060967221856117
  - 17.475178211927414
  - 26.586155623197556
  - 20.53058448433876
  - 23.608599215745926
  - 23.82347810268402
  - 22.369825899600983
  - 28.966816246509552
  - 22.90176013112068
  - 18.514755189418793
  - 23.33897139132023
  - 21.139817237854004
  - 24.664197981357574
  - 22.72440841794014
  - 19.684256374835968
  validation_losses:
  - 0.44100213050842285
  - 0.6067347526550293
  - 0.5707629919052124
  - 0.843447744846344
  - 0.4063170254230499
  - 3227223552.0
  - 338943770624.0
  - 608358957056.0
  - 270138572800.0
  - 1.6021180219717957e+27
  - 4.1861129623042515e+28
  - 1.6699999269040457e+29
  - 126414004224.0
  - 125260.7890625
  - 0.4418441951274872
  - 0.6913220882415771
  - 0.47080516815185547
  - 0.5409384965896606
  - 0.5470646023750305
  - 0.8069338202476501
  - 8222514176.0
  - 4.227719623077294e+26
  - 5.458144237466116e+27
  - 8.324606525468542e+27
  - 500664893440.0
  - 618319314944.0
  - 245573533696.0
  - 5144672351551488.0
  - 1.4505276376588576e+28
  - 3.485926090090481e+27
  - 9.390334845631494e+27
  - 6.444524286445679e+28
  - 1.3731054072573388e+29
  - 9.306956979128272e+28
  - 1.05665519021575e+29
  - 1.2590635623525748e+29
  - 0.40729179978370667
  - 0.495678573846817
  - 1.0657072067260742
  - 0.4013655185699463
  - 0.46467989683151245
  - 0.41133788228034973
  - 0.5254819989204407
  - 0.7931495308876038
  - 0.6231272220611572
  - 0.5187498331069946
  - 0.7816419005393982
  - 0.4691477417945862
  - 0.7200102210044861
  - 0.6321251392364502
  - 0.7453747391700745
  - 5.793968283239155e+25
  - 4.524637220338707e+28
  - 324105371648.0
  - 465609228288.0
  - 586624204800.0
  - 536658870272.0
  - 443733803008.0
  - 326702170112.0
  - 0.47742754220962524
  - 0.6184432506561279
  - 0.7420157194137573
  - 0.7980481386184692
  - 3.062448466034956e+27
  - 9.959299018232105e+28
  - 2.0694105060281064e+29
  - 438999252992.0
  - 412722167808.0
  - 735519637504.0
  - 3.80425422573142e+17
  - 1.8388721537326153e+28
  - 1.0254960716884794e+29
  - 4.2025481553450017e+27
  - 3.3395678517222386e+28
  - 2186700914688.0
  - 1.35737877987328e+16
  - 0.48707491159439087
  - 12156658688.0
  - 306537103360.0
  - 618245849088.0
  - 244866154496.0
  - 709650743296.0
  - 426184343552.0
  - 633741574144.0
  - 364029444096.0
  - 480248463360.0
  - 593949032448.0
  - 634863026176.0
  - 614684033024.0
  - 814285389824.0
  - 822494035968.0
  - 800643612672.0
  - 650751115264.0
  - 462233206784.0
  - 698563559424.0
  - 588077596672.0
  - 357280743424.0
  - 524650414080.0
  - 887409082368.0
  - 582674808832.0
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 96 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.7444253859348199, 0.8593481989708405, 0.8507718696397941,
    0.14261168384879724]'
  fold_eval_f1: '[0.0, 0.12865497076023394, 0.0, 0.0, 0.24736048265460026]'
  mean_eval_accuracy: 0.6909580142997765
  mean_f1_accuracy: 0.07520309068296685
  total_train_time: '0:13:07.980775'
