config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:31:08.251276'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_101fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_101fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_101fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_101fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_101fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_101fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_101fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_101fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_101fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_101fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.9183805346488954
  - 3.266696846485138
  - 3.3274407207965853
  - 3.1780169636011126
  - 3.2853257775306703
  - 3.2057407140731815
  - 3.1334152162075046
  - 3.1526925921440125
  - 3.2708551406860353
  - 3.142480766773224
  - 3.1223951041698457
  - 3.0803839683532717
  - 3.2143506437540057
  - 3.6192831993103027
  - 3.127389818429947
  - 3.103629612922669
  - 3.040801203250885
  - 2.9595960140228272
  - 3.188638144731522
  - 3.1453336477279663
  validation_losses:
  - 0.6023792028427124
  - 0.40397852659225464
  - 0.3957349956035614
  - 0.3987177312374115
  - 0.42185118794441223
  - 0.4388105869293213
  - 0.40709877014160156
  - 0.4109465479850769
  - 0.4500117897987366
  - 0.3853021264076233
  - 0.41478231549263
  - 0.4018745720386505
  - 0.39439770579338074
  - 0.40567851066589355
  - 0.39440271258354187
  - 0.39284735918045044
  - 0.39946672320365906
  - 0.40202367305755615
  - 0.40132826566696167
  - 0.3935406506061554
loss_records_fold1:
  train_losses:
  - 3.28816140294075
  - 3.063183066248894
  - 3.2548826277256016
  - 3.530883723497391
  - 3.1195382952690127
  - 2.965171939134598
  - 3.068588310480118
  - 3.0273177802562716
  - 3.074220907688141
  - 3.1905270636081697
  - 3.1867602318525314
  - 3.030098080635071
  - 3.2705524206161503
  - 3.1596779346466066
  validation_losses:
  - 0.3900090754032135
  - 0.4014427959918976
  - 0.42053866386413574
  - 0.4137059450149536
  - 0.40182894468307495
  - 0.4342042803764343
  - 0.3975217640399933
  - 0.4372849464416504
  - 0.39365068078041077
  - 0.3933370113372803
  - 0.3963562548160553
  - 0.39463743567466736
  - 0.3919435441493988
  - 0.39431309700012207
loss_records_fold2:
  train_losses:
  - 3.12582380771637
  - 3.0181697756052017
  - 3.00778883099556
  - 3.0420293450355533
  - 3.0711337327957153
  - 2.95977840423584
  - 3.0775284051895144
  - 3.0009989947080613
  - 3.0137263000011445
  - 2.9715798974037173
  - 3.00577447116375
  - 2.9706028401851654
  - 3.055235794186592
  - 3.032634118199349
  - 3.0056636035442352
  - 3.10359964966774
  - 3.104779303073883
  - 2.9569080770015717
  - 3.0208069264888766
  - 2.9831642091274264
  - 2.9698739230632785
  - 2.989353919029236
  - 2.8931847870349885
  - 3.120821884274483
  - 3.075774669647217
  - 2.952903014421463
  - 2.985969358682633
  - 2.996805286407471
  - 2.9942465066909794
  - 3.008104920387268
  - 2.9757146418094638
  - 2.9835634887218476
  - 2.923698878288269
  - 2.989721119403839
  - 2.962964409589768
  - 3.009788724780083
  - 2.995248019695282
  - 3.000197851657868
  - 2.973623698949814
  - 2.9894537389278413
  - 2.9162878423929217
  - 3.054919955134392
  - 3.014588284492493
  - 2.9550402343273166
  - 3.0246798336505893
  - 3.0440587997436523
  - 2.9321268379688266
  - 3.0517929762601854
  - 3.010589295625687
  - 3.0456848919391635
  - 2.9803195685148243
  - 2.9591665625572205
  - 2.9717143177986145
  - 2.922942078113556
  - 2.8873477935791017
  - 2.9339251816272736
  validation_losses:
  - 0.424747496843338
  - 0.4031708240509033
  - 0.3967653214931488
  - 0.3947083652019501
  - 0.41073864698410034
  - 0.39454376697540283
  - 0.40658649802207947
  - 0.3871515393257141
  - 0.39058560132980347
  - 0.4021437466144562
  - 0.45087507367134094
  - 0.4083946943283081
  - 0.4029110074043274
  - 0.4164566993713379
  - 0.39610999822616577
  - 0.3860604166984558
  - 0.39239975810050964
  - 0.41437646746635437
  - 0.4248410761356354
  - 0.4035091996192932
  - 0.3918890655040741
  - 0.3791359066963196
  - 0.3975990414619446
  - 0.45494014024734497
  - 0.3870601952075958
  - 0.39967775344848633
  - 0.39491891860961914
  - 0.3919263482093811
  - 0.389338880777359
  - 1.1152212619781494
  - 0.38411128520965576
  - 0.4152342677116394
  - 0.38963231444358826
  - 0.3955627977848053
  - 0.4063388705253601
  - 0.3939785957336426
  - 0.42153581976890564
  - 0.39280107617378235
  - 0.4204300045967102
  - 0.39135706424713135
  - 0.3947850465774536
  - 0.3980136513710022
  - 0.40807247161865234
  - 0.4206661283969879
  - 0.3948879837989807
  - 0.41725802421569824
  - 0.38832300901412964
  - 0.4086073935031891
  - 0.3927365243434906
  - 0.4194205105304718
  - 0.4274483621120453
  - 0.42253604531288147
  - 0.39291512966156006
  - 0.3927351236343384
  - 0.39444324374198914
  - 0.3997201919555664
loss_records_fold3:
  train_losses:
  - 3.0071934163570404
  - 3.0166205346584323
  - 3.0166595190763474
  - 3.055689084529877
  - 2.9349733620882037
  - 3.11612645983696
  - 3.0629545867443086
  - 3.0086160540580753
  - 2.9918303340673447
  - 3.1024189800024033
  - 2.9990812242031097
  - 2.9004029870033268
  - 2.973321318626404
  - 3.0519628107547763
  - 2.9712778568267826
  - 2.994735538959503
  - 2.976412719488144
  - 2.933988830447197
  - 2.9619406640529635
  - 3.003452324867249
  - 2.9543145060539246
  - 2.9841162741184237
  - 2.989711427688599
  - 2.9488096594810487
  - 3.0039391338825228
  - 3.0279696524143223
  - 2.957570853829384
  - 2.997621512413025
  - 2.9240173995494843
  - 2.9798351347446443
  - 2.9399189829826358
  - 2.986890560388565
  - 3.001407414674759
  - 2.9547731995582582
  - 2.9655613362789155
  - 2.9247286915779114
  - 2.9908294498920442
  - 2.9669571936130525
  - 2.94773246049881
  - 3.0257999420166017
  - 2.950390034914017
  - 2.9301810294389727
  - 3.0784605085849766
  - 2.960895258188248
  - 2.9698729813098907
  - 2.9593195497989657
  - 2.9974938809871676
  - 2.883409321308136
  - 2.9039746820926666
  - 3.00445311665535
  - 2.9760136246681217
  - 2.93870165348053
  - 2.9728647232055665
  - 2.9149040162563327
  - 2.955752617120743
  - 3.0081021130084995
  - 2.9468558907508853
  - 2.9407771795988085
  - 2.9157567441463472
  - 2.9876652240753176
  - 2.9341340959072113
  - 2.9422247022390366
  - 2.969831356406212
  - 2.9164025962352755
  - 2.888503083586693
  - 3.1782307744026186
  - 3.129280406236649
  - 3.0068387240171432
  - 2.983938226103783
  - 2.986607885360718
  - 2.996781101822853
  - 2.9298903912305834
  - 2.910994464159012
  - 3.0250699281692506
  - 2.970681327581406
  - 2.9661060690879824
  - 2.9236115455627445
  - 3.0998520791530613
  - 3.089688849449158
  - 2.9895620286464695
  - 2.9587693929672243
  - 2.933841496706009
  - 2.927879083156586
  - 2.986240178346634
  - 2.954525274038315
  - 2.9755390316247943
  - 2.950523680448532
  - 2.8944687247276306
  - 2.822767561674118
  - 2.9177386969327928
  - 2.958927184343338
  - 2.904312491416931
  - 3.003228259086609
  - 2.8853307306766514
  - 2.9516547411680225
  - 3.0129129678010944
  - 2.9674726247787477
  - 2.9086062967777253
  - 2.8753969073295593
  - 2.9230392158031466
  validation_losses:
  - 0.39855581521987915
  - 0.446372926235199
  - 0.4392656981945038
  - 0.37151846289634705
  - 0.3926946222782135
  - 0.38506609201431274
  - 0.37614795565605164
  - 0.37815752625465393
  - 0.3746422231197357
  - 0.3976256549358368
  - 0.391681969165802
  - 0.5686242580413818
  - 0.9579192399978638
  - 0.3743858337402344
  - 0.372426837682724
  - 0.4132823050022125
  - 0.3734455704689026
  - 0.37979212403297424
  - 0.3721567690372467
  - 0.3874130845069885
  - 1.4668543338775635
  - 0.4002511203289032
  - 1.3490736484527588
  - 0.3812260031700134
  - 0.3676036596298218
  - 0.39545243978500366
  - 0.41034412384033203
  - 0.41531893610954285
  - 0.5484201312065125
  - 0.3837999999523163
  - 0.37926533818244934
  - 0.36948832869529724
  - 0.47577691078186035
  - 0.4804267883300781
  - 0.8862013816833496
  - 0.4656592607498169
  - 1.0628975629806519
  - 0.47234585881233215
  - 0.5879236459732056
  - 0.5506547689437866
  - 0.9069194197654724
  - 0.8871312737464905
  - 0.4344410300254822
  - 0.40438100695610046
  - 0.5334497690200806
  - 0.893398642539978
  - 0.47662353515625
  - 0.7256925106048584
  - 10.367650032043457
  - 0.4481486678123474
  - 0.4164484739303589
  - 0.682017982006073
  - 0.6211907267570496
  - 0.5950812697410583
  - 0.5124854445457458
  - 0.713391900062561
  - 0.42405521869659424
  - 0.8010092973709106
  - 0.6737596988677979
  - 0.3972412645816803
  - 1.8885138034820557
  - 1.2412357330322266
  - 0.6294115781784058
  - 0.48830848932266235
  - 2.1874892711639404
  - 0.4215678870677948
  - 0.4802018105983734
  - 0.43935319781303406
  - 0.5000501871109009
  - 0.42748212814331055
  - 0.5448991656303406
  - 0.4171028435230255
  - 0.6126291155815125
  - 0.5777062177658081
  - 0.8389564156532288
  - 0.4466162323951721
  - 0.862208366394043
  - 0.7644174098968506
  - 0.41349971294403076
  - 0.43739309906959534
  - 0.4134078621864319
  - 0.5034911036491394
  - 0.6019522547721863
  - 2.0783238410949707
  - 2.769644260406494
  - 0.4191783666610718
  - 1.654826283454895
  - 0.4458979368209839
  - 4.553197383880615
  - 1.0386602878570557
  - 3.3904190063476562
  - 0.37414151430130005
  - 7.793072700500488
  - 1.0621020793914795
  - 1.2906354665756226
  - 0.4585484564304352
  - 0.5421359539031982
  - 0.7680380344390869
  - 0.38579121232032776
  - 0.4014510214328766
loss_records_fold4:
  train_losses:
  - 2.9529125273227694
  - 2.938718721270561
  - 2.919448709487915
  - 2.928864449262619
  - 2.920018815994263
  - 2.901562732458115
  - 2.9908889055252077
  - 2.9305413454771045
  - 2.914388927817345
  - 2.953692501783371
  - 2.891134774684906
  - 2.9057436823844913
  validation_losses:
  - 0.388211190700531
  - 0.3732629120349884
  - 0.4032299816608429
  - 0.389035701751709
  - 0.38397976756095886
  - 0.4234471619129181
  - 0.4026012718677521
  - 0.38065478205680847
  - 0.3737478256225586
  - 0.38293296098709106
  - 0.3839549124240875
  - 0.3794911205768585
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 56 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.855917667238422, 0.8576329331046312,
    0.8539518900343642]'
  fold_eval_f1: '[0.0, 0.0, 0.04545454545454545, 0.0, 0.0]'
  mean_eval_accuracy: 0.8565536713173358
  mean_f1_accuracy: 0.00909090909090909
  total_train_time: '0:19:48.942556'
