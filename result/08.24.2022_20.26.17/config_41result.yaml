config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:21:45.692512'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_41fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_41fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_41fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_41fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_41fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_41fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_41fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_41fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_41fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_41fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 37.09388646483421
  - 17.736619931459426
  - 14.951208227872849
  - 15.991656190156938
  - 14.868371677398683
  - 7.9663404583930975
  - 8.759159505367279
  - 4.494291031360627
  - 4.175981885194779
  - 4.422709050774574
  - 3.9923782348632812
  - 5.637303382158279
  - 5.358933889865876
  - 3.8500131130218507
  - 3.6479252815246586
  - 5.915350982546807
  - 7.42282187640667
  - 5.459301927685738
  - 7.364726424217224
  - 4.628632038831711
  - 3.982408854365349
  - 4.197295022010803
  - 5.273588505387306
  - 7.347899103164673
  - 5.018950352072716
  - 5.70756018459797
  - 4.576507344841957
  - 11.20350058078766
  - 6.159318771958351
  - 3.4558332890272143
  - 3.619911825656891
  - 4.669687080383301
  - 5.934893250465393
  - 5.853037559986115
  - 6.068255543708801
  - 4.552855390310287
  - 3.835088711977005
  - 5.0968028485775
  - 3.6179115861654285
  - 3.1039648294448856
  - 5.56862132549286
  - 3.908766061067581
  - 3.225815033912659
  - 3.2664252758026127
  - 3.340415993332863
  - 5.062391945719719
  - 3.8036184847354892
  - 3.767305147647858
  - 3.3668171733617784
  - 3.440840184688568
  - 3.3468032747507097
  - 3.4565312415361404
  - 3.151262551546097
  - 3.503868943452835
  - 3.2436895191669466
  - 3.3933040440082554
  - 3.5311222940683367
  - 3.0782688438892367
  - 3.9156886994838715
  - 3.3315899908542637
  - 3.403099924325943
  - 3.1112609386444094
  - 3.0448384523391727
  - 3.2250575184822083
  - 3.2257347047328953
  - 3.308349269628525
  - 2.9581788182258606
  - 2.9517726182937625
  - 2.9155578792095187
  - 2.9578333377838137
  - 3.4455295592546467
  - 3.6476135283708575
  - 3.681118166446686
  - 3.794737142324448
  - 2.9572044253349308
  - 3.8174721479415896
  - 3.0832662254571916
  - 2.9062264978885652
  - 3.356771978735924
  - 3.185939383506775
  - 3.460428148508072
  - 3.354778528213501
  - 2.82563688158989
  - 3.0755588710308075
  - 3.2469437539577486
  - 2.890858030319214
  - 3.097831702232361
  - 3.093286371231079
  - 3.3209949284791946
  - 3.144326287508011
  - 2.993633872270584
  - 3.1854951083660126
  - 3.5831997334957126
  - 2.905368593335152
  - 3.1015397340059283
  - 3.6735058188438416
  - 5.32238838672638
  - 4.253234833478928
  - 4.0571533888578415
  - 3.0273232012987137
  validation_losses:
  - 4.177773952484131
  - 1.2607494592666626
  - 2.31229567527771
  - 3.4718070030212402
  - 0.5913817882537842
  - 0.6281237602233887
  - 0.6736194491386414
  - 0.4767671823501587
  - 0.42318272590637207
  - 0.5521714687347412
  - 0.5834636092185974
  - 0.5215246081352234
  - 0.39962124824523926
  - 0.3943865895271301
  - 0.40496641397476196
  - 0.4521016478538513
  - 0.4724240005016327
  - 0.38737502694129944
  - 0.4377656877040863
  - 0.4221245050430298
  - 0.4907069802284241
  - 0.3807256519794464
  - 1.065628170967102
  - 0.803806722164154
  - 0.40229693055152893
  - 0.4363726079463959
  - 0.43479523062705994
  - 0.5478079319000244
  - 0.3815073072910309
  - 0.3851916193962097
  - 0.5391666889190674
  - 0.8866265416145325
  - 0.3787274658679962
  - 2.9867076873779297
  - 0.4103298485279083
  - 0.959492027759552
  - 0.43332332372665405
  - 0.4557724595069885
  - 0.42970016598701477
  - 0.3753531873226166
  - 0.44919660687446594
  - 0.4274766743183136
  - 0.5095553994178772
  - 0.40629807114601135
  - 0.453403502702713
  - 0.3774079978466034
  - 0.4443889856338501
  - 0.4255926012992859
  - 0.3787645697593689
  - 0.4068443477153778
  - 0.5585867762565613
  - 0.375934898853302
  - 0.3739537000656128
  - 0.3762395679950714
  - 0.3819238245487213
  - 0.3925086259841919
  - 0.4246574938297272
  - 0.3814871311187744
  - 0.4308961033821106
  - 0.38384491205215454
  - 0.3825569450855255
  - 0.3808870315551758
  - 0.3799299895763397
  - 0.3977923095226288
  - 0.44647344946861267
  - 0.38169577717781067
  - 0.4950432777404785
  - 0.3874777853488922
  - 0.38497042655944824
  - 0.4027481973171234
  - 0.3739067018032074
  - 0.40164148807525635
  - 0.38123202323913574
  - 0.40281954407691956
  - 0.3814992606639862
  - 0.38010305166244507
  - 0.37830162048339844
  - 0.3883206844329834
  - 0.37869077920913696
  - 0.39132702350616455
  - 0.3723180890083313
  - 0.37413936853408813
  - 0.37283849716186523
  - 0.36931106448173523
  - 0.3704923689365387
  - 0.46324416995048523
  - 0.37986528873443604
  - 0.41091206669807434
  - 0.44393330812454224
  - 0.3764342963695526
  - 0.3738204836845398
  - 2.6563191413879395
  - 6.5901288986206055
  - 23.27000617980957
  - 0.4599628746509552
  - 4.169872760772705
  - 0.5845398902893066
  - 17.51544761657715
  - 0.4169046878814697
  - 87.66277313232422
loss_records_fold1:
  train_losses:
  - 3.4230465263128282
  - 3.421160686016083
  - 2.890069878101349
  - 2.941263028979302
  - 2.9969987452030185
  - 2.9180644512176515
  - 2.988491985201836
  - 2.9150314509868624
  - 2.810304912924767
  - 3.036637452244759
  - 3.1098055571317675
  - 3.0330365121364595
  - 2.8833646506071093
  - 2.8270481020212177
  - 2.817078995704651
  - 2.784757912158966
  - 2.780225816369057
  - 2.8563303709030152
  - 2.831456291675568
  - 2.850670799612999
  - 2.8425313532352448
  - 3.06448247730732
  - 2.894273442029953
  - 2.8798842400312425
  - 2.8032598078250888
  - 2.8771242409944535
  - 2.8095228731632234
  - 2.899803334474564
  - 2.8352458059787753
  - 2.8230860829353333
  - 2.8537593960762027
  - 2.8238210260868075
  - 2.7463413536548615
  - 2.794497948884964
  - 2.803895488381386
  - 2.9248070597648623
  - 2.8317556351423265
  - 2.866944479942322
  - 2.908724844455719
  - 2.9137735009193424
  - 2.7941571444272997
  - 2.765079247951508
  - 2.7809443801641467
  - 2.998044654726982
  - 2.9058959603309633
  - 2.858496788144112
  - 2.9659799814224246
  - 2.7910738736391068
  - 2.851056629419327
  - 2.7686614960432054
  - 2.8965059846639636
  - 3.1315020442008974
  - 2.8540380090475086
  - 2.8635658591985704
  - 2.822079220414162
  - 3.0067315578460696
  - 2.9077029764652256
  - 2.8074652135372165
  - 2.844100496172905
  - 2.8557575672864917
  - 2.804878702759743
  - 2.7909257233142855
  - 2.7769448518753053
  validation_losses:
  - 0.6167823672294617
  - 0.40408679842948914
  - 0.41182953119277954
  - 0.48458513617515564
  - 0.4270585775375366
  - 0.4070175290107727
  - 0.4015560746192932
  - 0.4909684956073761
  - 0.40973106026649475
  - 0.402871698141098
  - 0.3997862637042999
  - 0.432151734828949
  - 0.40195512771606445
  - 0.42528781294822693
  - 0.4282377362251282
  - 0.39454638957977295
  - 0.4045768976211548
  - 0.4182356297969818
  - 0.39807239174842834
  - 0.39336735010147095
  - 0.416041761636734
  - 0.4603191018104553
  - 0.39696264266967773
  - 0.41873064637184143
  - 0.3964771628379822
  - 0.3924529254436493
  - 0.4608801305294037
  - 0.4085443317890167
  - 0.6944297552108765
  - 2.0486698150634766
  - 0.4572373032569885
  - 0.39858606457710266
  - 0.42027902603149414
  - 0.3889389634132385
  - 0.4398445785045624
  - 0.9546399116516113
  - 0.5515598058700562
  - 0.6116104125976562
  - 0.39550092816352844
  - 0.4312242567539215
  - 0.3952844440937042
  - 0.4911729097366333
  - 3.2206997871398926
  - 15.205493927001953
  - 0.4080430269241333
  - 0.5483058094978333
  - 0.43805500864982605
  - 0.3985806405544281
  - 0.9410790205001831
  - 0.4309443235397339
  - 0.4056148827075958
  - 0.4478349983692169
  - 0.43687474727630615
  - 0.6872232556343079
  - 2.545719623565674
  - 2.4711337089538574
  - 286.4127502441406
  - 208.0242156982422
  - 41.28464889526367
  - 1.6743621826171875
  - 0.40774425864219666
  - 0.40902918577194214
  - 0.40022191405296326
loss_records_fold2:
  train_losses:
  - 2.932115125656128
  - 2.958650314807892
  - 2.8610500514507295
  - 2.8908062100410463
  - 2.9142209112644197
  - 2.864949080348015
  - 2.8635698705911636
  - 2.8874567091465
  - 3.0528985142707827
  - 2.956418472528458
  - 2.90505468249321
  - 3.004234212636948
  - 2.875543159246445
  - 2.923508617281914
  - 2.927550658583641
  - 2.81234347820282
  - 2.876421803236008
  - 2.835882925987244
  - 2.7732522010803224
  - 2.8887360811233522
  - 2.9257596462965014
  - 2.9383620351552966
  - 2.9062343150377274
  - 2.8952979087829593
  - 2.7928807675838474
  - 2.9303371489048007
  - 2.8159748852252964
  - 2.848346269130707
  - 2.8115412056446076
  - 2.8706640124320986
  - 3.043771243095398
  - 2.9749544590711596
  - 2.9314576715230944
  - 2.881983530521393
  - 2.9191447257995606
  - 2.8256203413009646
  - 2.859632979333401
  - 2.8223592311143877
  - 2.805267333984375
  - 2.9075387656688694
  - 3.1019925236701966
  - 2.9700295954942706
  - 2.9808667182922366
  - 2.986675047874451
  - 2.8459090799093247
  - 2.840242490172386
  - 2.9095717787742617
  - 2.9721703052520754
  - 2.8216684222221375
  - 2.967961287498474
  - 3.305811434984207
  - 3.1216850101947786
  - 2.916367828845978
  - 2.8168397903442384
  - 2.9340017259120943
  - 2.937126386165619
  - 3.147015500068665
  - 3.2047087818384172
  - 2.846573901176453
  - 2.8788660109043125
  - 2.9330716490745545
  - 2.869075530767441
  - 2.9814728647470474
  - 2.828132274746895
  - 2.8208663105964664
  - 2.8862170338630677
  - 2.9040585994720463
  - 2.8284950733184817
  - 2.8145014077425006
  - 2.8383196234703068
  - 2.910680922865868
  - 2.877527502179146
  - 2.9245911628007892
  - 2.8811708837747574
  - 2.838158217072487
  - 2.868847531080246
  - 2.9924751400947573
  - 2.9336454749107364
  - 2.8809173941612247
  - 3.041592836380005
  - 2.817936211824417
  - 2.828877478837967
  - 2.911144244670868
  - 2.9648905843496323
  - 3.7089915931224824
  - 3.0662497133016586
  - 3.059332194924355
  - 2.9380326628685
  - 2.928231230378151
  - 2.994895386695862
  - 2.9107464402914047
  - 2.9915408849716187
  - 2.900887125730515
  - 2.865117371082306
  - 2.874137753248215
  - 2.9144828021526337
  - 2.974103438854218
  - 2.8960222542285923
  - 2.901570385694504
  - 2.9608838200569156
  validation_losses:
  - 0.4375040829181671
  - 0.3931237757205963
  - 0.38014036417007446
  - 0.3940833806991577
  - 0.36947309970855713
  - 0.37871021032333374
  - 0.4081368148326874
  - 0.3686734437942505
  - 0.3895518481731415
  - 0.3750680983066559
  - 0.4495971202850342
  - 0.381417453289032
  - 0.38156765699386597
  - 0.3705211579799652
  - 0.40526270866394043
  - 62426.6640625
  - 0.37936145067214966
  - 35477.00390625
  - 6512.63525390625
  - 20384.896484375
  - 2376.471923828125
  - 152745.4375
  - 128985.96875
  - 1240.0216064453125
  - 61706.1484375
  - 8784.3173828125
  - 0.49021637439727783
  - 28424.44140625
  - 42415.91015625
  - 5826.75341796875
  - 49949.11328125
  - 1421.27001953125
  - 0.3906439542770386
  - 66891.875
  - 0.530096173286438
  - 1284.5418701171875
  - 15743.7724609375
  - 124939.6640625
  - 0.4214172959327698
  - 23715.14453125
  - 1.1828793287277222
  - 0.3743368983268738
  - 0.3795623183250427
  - 26.794883728027344
  - 414.1439514160156
  - 0.3850418031215668
  - 4278.19580078125
  - 771.9588623046875
  - 3886.597900390625
  - 0.38300299644470215
  - 0.6076827049255371
  - 0.5017610192298889
  - 0.5770413279533386
  - 0.38764265179634094
  - 0.49159789085388184
  - 3.302543878555298
  - 0.3797307014465332
  - 0.6835115551948547
  - 0.44230952858924866
  - 1.291487455368042
  - 0.7931333184242249
  - 0.7857559323310852
  - 0.5461089611053467
  - 0.3941999077796936
  - 0.3684673309326172
  - 0.5604182481765747
  - 0.3703134059906006
  - 0.6963179707527161
  - 3.5275707244873047
  - 0.6272158622741699
  - 1.6678024530410767
  - 2.160224437713623
  - 0.631644606590271
  - 1.434033751487732
  - 0.6160923838615417
  - 0.3686956465244293
  - 4.240848541259766
  - 0.45113059878349304
  - 1.0020196437835693
  - 0.6703099012374878
  - 3.5637454986572266
  - 1.9532359838485718
  - 8.032281875610352
  - 0.38358259201049805
  - 0.4398441016674042
  - 0.3813892900943756
  - 0.3772827684879303
  - 0.37752026319503784
  - 0.3782746493816376
  - 0.3893943130970001
  - 0.3752609193325043
  - 4.809399604797363
  - 3776.952880859375
  - 1076810.375
  - 1853344.5
  - 2787780.75
  - 1171923.125
  - 1475534.125
  - 10646564.0
  - 1514298.875
loss_records_fold3:
  train_losses:
  - 2.9057934254407884
  - 3.1879040122032167
  - 3.501432716846466
  - 3.639682132005692
  - 3.08185538649559
  - 3.005825012922287
  - 2.9263824701309207
  - 3.041311901807785
  - 2.9850805163383485
  - 3.179820501804352
  - 2.908280459046364
  - 2.9554592043161394
  - 2.9388754040002825
  - 2.9502507686614994
  - 3.1668599247932434
  - 2.9920103371143343
  - 3.308862468600273
  - 3.0155994951725007
  - 2.8941006749868396
  - 2.959239423274994
  - 3.0166133403778077
  - 3.0388814210891724
  - 2.9525135993957523
  - 2.836048159003258
  - 2.928671079874039
  - 2.9634525716304783
  - 2.8920143336057667
  - 2.9260382056236267
  - 2.900089716911316
  - 2.969353860616684
  - 3.094657799601555
  - 2.993620276451111
  - 2.8516807645559314
  - 2.913686352968216
  - 2.9675695776939395
  - 2.9352852255105972
  - 2.9306479692459106
  - 2.9058351904153827
  - 2.941298371553421
  - 3.047105920314789
  - 2.909709042310715
  - 2.9009812414646152
  - 2.8545558750629425
  - 2.9119293272495272
  - 2.875080755352974
  - 2.869848132133484
  - 2.882222199440003
  - 2.8542976021766666
  - 2.905857375264168
  - 3.1126066327095034
  - 2.94967534840107
  - 2.8924735307693483
  - 2.895055347681046
  - 2.8895858168601993
  - 2.8803897321224214
  - 2.93119635283947
  - 2.8447524875402452
  - 2.8695144176483156
  - 2.8957840502262115
  - 2.872455984354019
  - 2.842924898862839
  - 2.8757084846496586
  - 2.8591228783130647
  validation_losses:
  - 1.21174156665802
  - 1.2467522621154785
  - 0.7614423632621765
  - 0.47475144267082214
  - 0.41152626276016235
  - 0.40717613697052
  - 0.3899579644203186
  - 0.4050857126712799
  - 127.22285461425781
  - 1253.2386474609375
  - 3731.612548828125
  - 380.2074279785156
  - 0.39118245244026184
  - 0.39639952778816223
  - 1908.7684326171875
  - 2107.454345703125
  - 0.4052937626838684
  - 0.41277557611465454
  - 0.41149190068244934
  - 0.3920792043209076
  - 0.4624405801296234
  - 0.39377692341804504
  - 0.3861466348171234
  - 0.4379866123199463
  - 0.38992777466773987
  - 0.40844854712486267
  - 0.39797985553741455
  - 0.3812144994735718
  - 0.38580405712127686
  - 0.43254444003105164
  - 38983.35546875
  - 0.39450615644454956
  - 0.38980692625045776
  - 0.39770379662513733
  - 0.389828085899353
  - 0.42551514506340027
  - 0.391026109457016
  - 0.4318542182445526
  - 0.4109344780445099
  - 0.3993121087551117
  - 0.37671899795532227
  - 56459.60546875
  - 0.38539206981658936
  - 0.3959064483642578
  - 0.401985764503479
  - 0.38644152879714966
  - 0.4209561347961426
  - 0.39745447039604187
  - 0.4470953941345215
  - 0.40244704484939575
  - 0.3988410234451294
  - 0.3999626636505127
  - 0.41287609934806824
  - 0.4117436110973358
  - 0.38463497161865234
  - 0.39793500304222107
  - 38409.03515625
  - 0.38312941789627075
  - 0.3828389346599579
  - 0.3911403715610504
  - 0.38573697209358215
  - 0.39036470651626587
  - 0.38724133372306824
loss_records_fold4:
  train_losses:
  - 2.8918898850679398
  - 2.95657357275486
  - 3.022966891527176
  - 2.9928147703409196
  - 3.0187031447887422
  - 2.8862517535686494
  - 2.855949664115906
  - 2.9375588804483415
  - 3.0009578436613085
  - 3.0440631568431855
  - 2.8989820897579195
  - 2.878031286597252
  - 2.8865289568901065
  - 2.9359329611063005
  - 2.9071986973285675
  - 2.946649187803269
  - 3.0652217984199526
  - 2.9184564828872683
  - 3.0017117828130724
  - 2.9398242592811585
  - 2.94036437869072
  - 2.992970043420792
  - 2.8744138121604923
  - 2.940091133117676
  - 2.9201802343130114
  validation_losses:
  - 0.3736880123615265
  - 0.3840799331665039
  - 0.4023492932319641
  - 0.42236649990081787
  - 0.3730463981628418
  - 0.3753304183483124
  - 0.3782241940498352
  - 0.424809992313385
  - 0.3726162314414978
  - 0.3698663115501404
  - 0.3738720118999481
  - 0.39707764983177185
  - 0.38597941398620605
  - 0.4000343382358551
  - 0.3764949440956116
  - 0.37284430861473083
  - 0.38934528827667236
  - 0.3766065239906311
  - 0.44554606080055237
  - 0.44443950057029724
  - 0.4089830815792084
  - 0.3786895275115967
  - 0.3834632933139801
  - 0.390755295753479
  - 0.37928593158721924
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 63 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 63 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8490566037735849, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8565554396326618
  mean_f1_accuracy: 0.0
  total_train_time: '0:33:43.149766'
