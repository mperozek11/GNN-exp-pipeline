config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 18:50:50.525325'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/52/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/52/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/52/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/52/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/52/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/52/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/52/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/52/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/52/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/52/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 148.71425330638885
  - 44.90400940179825
  - 22.248189836740494
  - 13.343513697385788
  - 9.242583602666855
  - 10.878167003393173
  - 11.796209126710892
  - 15.488761097192764
  - 15.681190833449364
  - 15.688258439302444
  - 8.301276236772537
  - 8.91165080666542
  - 9.155110538005829
  - 17.043389171361923
  - 17.094540417194366
  - 13.209256440401077
  - 11.462880104780197
  - 8.577639013528824
  - 10.300316333770752
  - 13.354408293962479
  - 9.957194745540619
  - 9.545427322387695
  - 8.43948444724083
  - 8.116453766822815
  - 17.675134629011154
  - 16.96660414338112
  - 18.76656973361969
  - 36.009871423244476
  - 22.978627920150757
  - 13.310393124818802
  - 13.357246786355972
  - 14.029509633779526
  - 12.815399140119553
  - 12.239923030138016
  - 10.549457788467407
  - 8.749454572796822
  - 21.98717314004898
  - 13.528230726718903
  - 16.004654943943024
  - 11.642384320497513
  - 17.236085832118988
  - 15.399328291416168
  - 24.89829233288765
  - 9.746578842401505
  - 8.700026005506516
  - 8.316477000713348
  - 8.075474679470062
  - 8.050481632351875
  - 8.699300527572632
  - 12.34375724196434
  - 11.01666709780693
  - 10.34127563238144
  - 8.28329011797905
  - 7.794385612010956
  - 8.353790000081062
  - 8.273567944765091
  - 9.75176426768303
  - 9.530478447675705
  - 11.008828192949295
  - 9.905708700418472
  - 8.388662040233612
  - 7.713334560394287
  - 7.401140600442886
  - 9.598469987511635
  - 7.951726019382477
  - 8.820866793394089
  - 8.22445634007454
  - 8.427928298711777
  - 9.326754838228226
  - 8.368593007326126
  - 11.042721629142761
  - 8.490539222955704
  - 7.561504691839218
  - 11.938592314720154
  - 8.304435580968857
  - 7.8373260498046875
  - 7.629237577319145
  - 7.475322246551514
  - 7.459626495838165
  - 8.739724963903427
  - 8.579543828964233
  - 9.858851373195648
  - 14.358733177185059
  - 9.18655863404274
  - 8.01640397310257
  - 14.899324893951416
  - 8.919367492198944
  - 8.17401984333992
  - 8.450051575899124
  - 9.026325598359108
  - 7.601856112480164
  - 8.968414604663849
  - 8.03698280453682
  - 7.946758419275284
  - 8.076284110546112
  - 8.493160635232925
  - 7.366309255361557
  - 7.408260256052017
  - 7.52563539147377
  - 7.944911539554596
  validation_losses:
  - 1.0456199645996094
  - 1.4655206203460693
  - 0.6535854339599609
  - 0.8959289193153381
  - 1.0414835214614868
  - 0.601163923740387
  - 0.44598257541656494
  - 0.480090469121933
  - 0.6491084694862366
  - 0.4023883044719696
  - 0.48450329899787903
  - 0.38147103786468506
  - 0.5409987568855286
  - 0.5820656418800354
  - 0.8259711861610413
  - 0.937069833278656
  - 0.47308027744293213
  - 0.4885520040988922
  - 0.49538615345954895
  - 0.40814873576164246
  - 1.3685919046401978
  - 0.5282937288284302
  - 0.5707986950874329
  - 0.5732240676879883
  - 0.3974214196205139
  - 1.2262606620788574
  - 2.1491758823394775
  - 0.5167647004127502
  - 0.4868817925453186
  - 0.557225227355957
  - 0.4414537847042084
  - 0.4126528203487396
  - 0.4300459623336792
  - 0.40312445163726807
  - 0.38963252305984497
  - 0.38643354177474976
  - 0.46878936886787415
  - 0.4391727149486542
  - 0.4636463522911072
  - 0.5855658650398254
  - 0.4360770285129547
  - 0.4364445209503174
  - 0.4790227711200714
  - 0.37509313225746155
  - 0.38021039962768555
  - 0.37568560242652893
  - 0.37612009048461914
  - 0.39273738861083984
  - 0.4418174922466278
  - 0.3850004971027374
  - 0.5574681162834167
  - 0.6259310245513916
  - 0.3808372914791107
  - 0.381157249212265
  - 0.5601242780685425
  - 0.3732624053955078
  - 0.3805387020111084
  - 0.4181578755378723
  - 0.43962612748146057
  - 0.44440513849258423
  - 0.3703325390815735
  - 0.38347524404525757
  - 0.3739650249481201
  - 0.3781682252883911
  - 0.39553937315940857
  - 0.4410623013973236
  - 0.40399330854415894
  - 0.4826357662677765
  - 0.7060434818267822
  - 0.3879070580005646
  - 0.6102960109710693
  - 0.3719598352909088
  - 0.38002508878707886
  - 0.378248006105423
  - 0.39967554807662964
  - 0.38102462887763977
  - 0.37271952629089355
  - 0.3787582814693451
  - 0.38892000913619995
  - 0.3920835554599762
  - 0.43792980909347534
  - 0.4098615050315857
  - 0.5754346251487732
  - 0.6308073401451111
  - 0.3707672655582428
  - 0.37304261326789856
  - 0.47607240080833435
  - 0.8739078044891357
  - 0.4058282673358917
  - 0.4103682339191437
  - 0.4339311420917511
  - 0.44142138957977295
  - 0.37825527787208557
  - 0.3979213833808899
  - 0.6564723253250122
  - 0.378418892621994
  - 0.3755570650100708
  - 0.3966389298439026
  - 0.40068963170051575
  - 0.3741685450077057
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 31 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:02:32.934037'
