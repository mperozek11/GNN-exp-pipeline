config:
  aggregation: mean
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:05:24.424130'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_115fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_115fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_115fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_115fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_115fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_115fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_115fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_115fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_115fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_115fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.0232742428779602
  - 0.8312709689140321
  - 0.8348777234554291
  - 0.8012851178646088
  - 0.9154674530029298
  - 0.8101653277873994
  - 0.7930137157440186
  - 0.7471071749925614
  - 0.7703256905078888
  - 0.7595721125602722
  - 0.7864650070667267
  - 0.7758274734020234
  - 0.7780280947685242
  - 0.8691068589687347
  - 0.8182788729667664
  - 0.7737365245819092
  - 0.7851754188537599
  validation_losses:
  - 0.9647940397262573
  - 0.5300703644752502
  - 0.40070948004722595
  - 0.40428626537323
  - 0.3915858864784241
  - 0.3891644775867462
  - 0.39543235301971436
  - 0.39171120524406433
  - 0.38831713795661926
  - 0.3890420198440552
  - 0.4044542610645294
  - 0.38751211762428284
  - 0.3869820833206177
  - 0.38641801476478577
  - 0.3878856301307678
  - 0.3867548406124115
  - 0.3879217207431793
loss_records_fold1:
  train_losses:
  - 0.9219948410987855
  - 0.7716736137866974
  - 0.8122962892055512
  - 0.7948644101619721
  - 0.8325535774230958
  - 0.7731799602508546
  - 0.792168015241623
  - 0.7385297954082489
  - 0.7802498817443848
  - 0.7685494244098664
  - 0.7711397290229798
  - 0.7964782118797302
  - 0.7926769435405732
  - 0.7754588782787324
  - 0.7467402935028077
  validation_losses:
  - 0.3979479670524597
  - 0.38865211606025696
  - 0.39945703744888306
  - 0.38839638233184814
  - 0.38846758008003235
  - 0.38850876688957214
  - 0.3859306871891022
  - 0.39832213521003723
  - 0.4189029633998871
  - 0.3886072337627411
  - 0.38795778155326843
  - 0.395890474319458
  - 0.3918563723564148
  - 0.3938465714454651
  - 0.38890495896339417
loss_records_fold2:
  train_losses:
  - 0.7520134925842286
  - 0.7858101606369019
  - 0.8145624160766602
  - 0.7720540940761567
  - 0.7457096517086029
  - 0.7372205257415771
  - 0.8159104764461518
  - 0.7384369552135468
  - 0.7825419783592225
  - 0.7644454717636109
  - 0.7683894157409669
  - 0.8367843747138978
  - 0.8266991555690766
  - 0.7476035594940186
  - 0.7411683857440949
  - 0.7410382270812989
  - 0.7465633451938629
  - 0.7483744382858277
  - 0.7595955610275269
  - 0.7471394956111909
  - 0.7381929993629456
  validation_losses:
  - 0.38286879658699036
  - 0.38063952326774597
  - 0.384255975484848
  - 0.38146528601646423
  - 0.38177967071533203
  - 0.3847910463809967
  - 0.3897174000740051
  - 0.38327232003211975
  - 0.39434191584587097
  - 0.3797641098499298
  - 0.3898839056491852
  - 0.38098961114883423
  - 0.4126518964767456
  - 0.3916529715061188
  - 0.40380558371543884
  - 0.3819279074668884
  - 0.38128751516342163
  - 0.3811066150665283
  - 0.3861951529979706
  - 0.3769210875034332
  - 0.3807735741138458
loss_records_fold3:
  train_losses:
  - 0.7492154836654663
  - 0.7737757980823518
  - 0.7581733942031861
  - 0.8153270959854126
  - 0.79507355093956
  - 0.7823706626892091
  - 0.8309691011905671
  - 0.7762635350227356
  - 0.7570551931858063
  - 0.7903692424297333
  - 0.7863657355308533
  - 0.7729285299777985
  - 0.7658923149108887
  - 0.7558290958404541
  - 0.7787835896015167
  - 0.8110005140304566
  - 0.7547929704189301
  - 0.7782892405986787
  - 0.7563487768173218
  - 0.7731521368026734
  - 0.7363204836845398
  - 0.7406024098396302
  - 0.7364752650260926
  - 0.7819986879825592
  - 0.7845668852329255
  - 0.7620168209075928
  - 0.7861631929874421
  - 0.9315707445144654
  - 0.8034641206264497
  - 0.7474627852439881
  - 0.7430575489997864
  - 0.7539312183856964
  - 0.8486662209033966
  - 0.7539879262447358
  - 0.7514784872531891
  - 0.8501920402050018
  - 0.821751606464386
  - 0.7854518830776215
  - 0.7837396502494812
  - 0.8573934853076935
  - 0.7607690095901489
  - 0.7482513308525086
  - 0.7966770052909852
  - 0.7955931901931763
  - 0.7395873159170151
  - 0.7950231015682221
  - 0.8178532719612122
  - 0.7463644683361054
  - 0.759834349155426
  - 0.7580365002155305
  - 0.7778440654277802
  - 0.7451346635818482
  - 0.7186117917299271
  - 0.7254730820655824
  - 0.7246378719806672
  - 0.7395339131355286
  - 0.8077809870243073
  - 0.7372099757194519
  - 0.7554644525051117
  - 0.777683311700821
  - 0.7724899709224702
  - 0.7877159833908082
  - 0.7950383484363557
  - 0.7419582605361938
  - 0.7639815151691437
  - 0.7632951736450195
  - 0.7293388664722443
  - 0.7438032031059265
  - 0.7522796273231507
  - 0.7363956391811372
  - 0.7403165876865387
  - 0.7320960164070129
  - 0.7287228643894196
  - 0.7464534223079682
  - 0.7669899225234986
  - 0.8084833443164826
  - 0.7403222382068635
  - 0.7306520462036133
  - 0.7219412803649903
  - 0.7842742025852204
  - 0.7629526376724244
  - 0.7922413170337678
  - 0.7574766755104065
  - 0.7397385299205781
  - 0.7550244808197022
  - 0.7468725502490998
  - 0.780887657403946
  - 0.7603401720523835
  - 0.7470324337482452
  - 0.741607129573822
  - 0.7504239916801453
  - 0.7309182286262512
  - 0.7919014871120453
  - 0.7430130243301392
  - 0.7593077659606934
  - 0.779232394695282
  - 0.751267832517624
  - 0.768393224477768
  - 0.7543395102024079
  - 0.7898404777050019
  validation_losses:
  - 0.37993544340133667
  - 0.4140815734863281
  - 0.3794463574886322
  - 0.3764329254627228
  - 0.36454513669013977
  - 0.3672177791595459
  - 0.4035172760486603
  - 0.3704877495765686
  - 0.38777726888656616
  - 0.7018543481826782
  - 0.3716694414615631
  - 0.36890795826911926
  - 0.36747175455093384
  - 0.366710364818573
  - 0.4330657124519348
  - 0.37093913555145264
  - 0.36735203862190247
  - 0.3669531047344208
  - 0.3723132312297821
  - 0.41147562861442566
  - 0.36720404028892517
  - 0.37971165776252747
  - 0.38348811864852905
  - 0.37152335047721863
  - 0.5909897685050964
  - 0.3995126485824585
  - 0.49070706963539124
  - 0.36632540822029114
  - 0.3659053146839142
  - 0.39542847871780396
  - 0.36944660544395447
  - 0.3830569386482239
  - 0.38058704137802124
  - 1.0884811878204346
  - 0.7524636387825012
  - 0.37116336822509766
  - 0.3943811357021332
  - 0.4105690121650696
  - 0.3868471384048462
  - 0.459638386964798
  - 0.3687654137611389
  - 0.3963814079761505
  - 0.3758517801761627
  - 0.42117467522621155
  - 0.7301043272018433
  - 0.3832181394100189
  - 0.36536094546318054
  - 0.38004782795906067
  - 0.40028589963912964
  - 0.3757951557636261
  - 0.3801911771297455
  - 0.40611106157302856
  - 0.44840529561042786
  - 0.5617403388023376
  - 0.3964923620223999
  - 0.4940038025379181
  - 0.37898436188697815
  - 0.38596785068511963
  - 0.38951051235198975
  - 0.3947984278202057
  - 0.42932993173599243
  - 0.37931644916534424
  - 0.3804387152194977
  - 0.387395977973938
  - 0.3878195881843567
  - 0.43470698595046997
  - 0.8602472543716431
  - 0.5248982906341553
  - 0.6223993897438049
  - 0.4275631010532379
  - 0.36588960886001587
  - 0.37701794505119324
  - 0.37180864810943604
  - 0.3670085370540619
  - 0.38293027877807617
  - 0.41953033208847046
  - 0.40949746966362
  - 0.8306894302368164
  - 0.4904809296131134
  - 0.37926816940307617
  - 0.36523061990737915
  - 0.4852345287799835
  - 0.5377944707870483
  - 0.3799493610858917
  - 0.4210589826107025
  - 0.3785851001739502
  - 0.4656977951526642
  - 0.36438387632369995
  - 0.5098220705986023
  - 0.3725931942462921
  - 0.3844308853149414
  - 0.3922947943210602
  - 0.7006596326828003
  - 0.37045371532440186
  - 0.3873280882835388
  - 0.41895946860313416
  - 0.36710596084594727
  - 0.40877556800842285
  - 0.3881278932094574
  - 0.3999631702899933
loss_records_fold4:
  train_losses:
  - 0.7343183696269989
  - 0.7319215714931488
  - 0.7482632637023926
  - 0.755398851633072
  - 0.7450391232967377
  - 0.756391328573227
  - 0.7362765312194824
  - 0.7429807305335999
  - 0.7293474674224854
  - 0.7491989612579346
  - 0.7530592083930969
  - 0.7450123608112336
  - 0.7504657089710236
  - 0.7526707231998444
  - 0.7635700047016144
  - 0.7440027534961701
  - 0.739360672235489
  - 0.7466571390628816
  - 0.7677148759365082
  - 0.7348561704158784
  - 0.735459041595459
  - 0.752804058790207
  - 0.815697592496872
  - 0.7387297540903092
  - 0.8025825202465058
  - 0.8112832903862
  - 0.7773574948310853
  - 0.745863550901413
  - 0.7624438285827637
  - 0.735454136133194
  - 0.7473718762397766
  - 0.755892163515091
  - 0.7514510989189148
  validation_losses:
  - 0.3787042200565338
  - 0.3787137567996979
  - 0.412195086479187
  - 0.40536341071128845
  - 0.3856719136238098
  - 0.3762243092060089
  - 0.3832527697086334
  - 0.36899885535240173
  - 0.4038177728652954
  - 0.36726662516593933
  - 0.3719137907028198
  - 0.3983946442604065
  - 0.3749202787876129
  - 0.37401798367500305
  - 0.42379966378211975
  - 0.4006827473640442
  - 0.37727800011634827
  - 0.376473993062973
  - 0.42004942893981934
  - 0.37159258127212524
  - 0.3832319378852844
  - 0.3797279894351959
  - 0.37536317110061646
  - 0.38509783148765564
  - 0.5767354369163513
  - 0.36691102385520935
  - 0.4459308683872223
  - 0.3836492598056793
  - 0.37591060996055603
  - 0.3757372498512268
  - 0.3779614269733429
  - 0.3839818835258484
  - 0.37515249848365784
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 33 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.023809523809523808, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0047619047619047615
  total_train_time: '0:16:03.001191'
