config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 02:17:46.647894'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/109/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/109/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/109/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/109/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/109/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/109/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/109/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/109/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/109/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/109/fold_4_state_dict.pt
loss_records_fold4:
  train_losses:
  - 8.098127394914627
  - 8.175671696662903
  - 8.405579924583435
  - 8.30053037405014
  - 8.088122725486755
  - 8.066866993904114
  - 8.103552848100662
  - 8.182181805372238
  - 8.02072674036026
  - 8.277444303035736
  - 9.013180315494537
  - 8.599186480045319
  - 8.065282136201859
  - 8.510642409324646
  - 8.349150210618973
  - 8.304269701242447
  - 8.189491510391235
  - 8.413327604532242
  - 8.201788753271103
  - 8.008077263832092
  - 7.9739843010902405
  - 7.977589964866638
  - 8.230533450841904
  - 8.21448490023613
  - 8.225683182477951
  - 8.217008799314499
  - 8.393592804670334
  - 8.090048491954803
  - 8.055397868156433
  - 8.076410174369812
  - 8.371655911207199
  - 8.24924486875534
  - 8.188048303127289
  - 8.23197141289711
  - 8.425347626209259
  - 8.345955342054367
  - 8.103242456912994
  - 8.244646340608597
  - 8.35906058549881
  - 8.332419753074646
  - 8.284832090139389
  - 8.281999498605728
  - 8.133774667978287
  - 8.304141759872437
  - 8.058680415153503
  - 8.15419426560402
  - 8.228925198316574
  - 8.291067600250244
  - 8.247419834136963
  - 8.240994274616241
  - 8.351511746644974
  - 8.268916517496109
  - 8.416714489459991
  - 8.452674061059952
  - 8.336279183626175
  - 7.888089686632156
  - 8.496203124523163
  - 8.20172730088234
  - 8.023537456989288
  - 8.3194240629673
  - 8.597454190254211
  - 8.48820412158966
  - 8.286211401224136
  - 8.146676063537598
  - 8.077192187309265
  - 8.246412605047226
  - 8.15634423494339
  - 8.360215604305267
  - 8.252676516771317
  - 8.347144812345505
  - 8.345474988222122
  - 8.303848594427109
  - 8.159805923700333
  - 8.025626361370087
  - 8.274473935365677
  - 8.274806499481201
  - 8.150492042303085
  - 8.126511931419373
  - 8.279703140258789
  - 8.249898105859756
  - 8.5185045003891
  - 8.362697511911392
  - 8.220684736967087
  - 8.228281289339066
  - 8.114953935146332
  - 8.34731313586235
  - 8.25597694516182
  - 8.207048565149307
  - 8.032140076160431
  - 8.294420599937439
  - 8.310042291879654
  - 8.428423941135406
  - 8.324801743030548
  - 8.117860496044159
  - 8.34989321231842
  - 8.418559461832047
  - 8.160688608884811
  - 7.942750841379166
  - 8.561483561992645
  - 8.230691999197006
  validation_losses:
  - 0.4024372398853302
  - 0.41251927614212036
  - 0.4085167944431305
  - 0.4143159091472626
  - 0.41022878885269165
  - 0.41118019819259644
  - 0.4077989459037781
  - 0.4205595552921295
  - 0.41577228903770447
  - 0.4253440201282501
  - 0.43736010789871216
  - 0.4192322790622711
  - 0.40038177371025085
  - 0.4123323857784271
  - 0.4257631301879883
  - 0.4124487638473511
  - 0.40835845470428467
  - 1.749953031539917
  - 1089.5032958984375
  - 263691.0
  - 38039760.0
  - 0.42462244629859924
  - 0.41957974433898926
  - 2.4554452896118164
  - 1287.0377197265625
  - 473618.09375
  - 49642472.0
  - 3378254848.0
  - 122602823680.0
  - 176975937536.0
  - 440571330560.0
  - 294083887104.0
  - 436379975680.0
  - 307598426112.0
  - 138469556224.0
  - 116762640.0
  - 146370363392.0
  - 667667464192.0
  - 312866963456.0
  - 167780990976.0
  - 268215173120.0
  - 399680208896.0
  - 201853157376.0
  - 76134981632.0
  - 109568147456.0
  - 112208396288.0
  - 180166246400.0
  - 124843184.0
  - 302316683264.0
  - 360200306688.0
  - 221997285376.0
  - 6110870528.0
  - 186471104512.0
  - 366385987584.0
  - 361559818240.0
  - 396851085312.0
  - 230610944000.0
  - 376780062720.0
  - 249204064256.0
  - 559591325696.0
  - 152681185280.0
  - 120726056.0
  - 1472643203072.0
  - 1122000699392.0
  - 405366571008.0
  - 560599072768.0
  - 471604592640.0
  - 335261171712.0
  - 136400150528.0
  - 742214008832.0
  - 118978920448.0
  - 36017713152.0
  - 218258358272.0
  - 906560339968.0
  - 560191766528.0
  - 458208706560.0
  - 169834463232.0
  - 305758240768.0
  - 267681759232.0
  - 886023979008.0
  - 1659179106304.0
  - 1066545053696.0
  - 421846581248.0
  - 700008103936.0
  - 493201424384.0
  - 269937885184.0
  - 392184659968.0
  - 473813680128.0
  - 282100170752.0
  - 781704429568.0
  - 421577490432.0
  - 276153630720.0
  - 688143597568.0
  - 620952027136.0
  - 405017657344.0
  - 160487063552.0
  - 457971924992.0
  - 458977280000.0
  - 431397306368.0
  - 429742096384.0
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8593481989708405, 0.8593481989708405,
    0.140893470790378]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.24698795180722893]'
  mean_eval_accuracy: 0.7146280938150225
  mean_f1_accuracy: 0.04939759036144579
  total_train_time: '0:02:36.588287'
