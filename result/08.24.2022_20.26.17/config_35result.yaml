config:
  aggregation: mean
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:10:36.429294'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_35fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_35fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_35fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_35fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_35fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_35fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_35fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_35fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_35fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_35fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 0.9758900523185731
  - 0.8043659389019013
  - 0.9336596250534058
  - 0.8026983618736268
  - 0.7758995711803437
  - 0.8071689546108246
  - 0.8464185237884522
  - 0.7910864710807801
  - 0.7536370128393174
  - 0.7966257333755493
  - 0.8431528389453888
  validation_losses:
  - 0.40917062759399414
  - 0.4074161648750305
  - 0.42986589670181274
  - 0.4002905488014221
  - 0.41643479466438293
  - 0.3970961570739746
  - 0.3964466452598572
  - 0.39609864354133606
  - 0.3958122730255127
  - 0.3905191719532013
  - 0.3965245485305786
loss_records_fold1:
  train_losses:
  - 0.7989091038703919
  - 0.7712504506111145
  - 0.7678954482078553
  - 0.7409911185503006
  - 0.8448962509632111
  - 0.8115128815174103
  - 0.7703272581100464
  - 0.7766342222690583
  - 0.7230176687240601
  - 0.749444341659546
  - 0.7868117332458496
  - 0.8289762854576112
  - 0.7647723019123078
  - 0.798924446105957
  - 0.7707061111927033
  - 0.7476283729076386
  - 0.7540107309818268
  - 0.7211732327938081
  - 0.785623699426651
  - 0.7761421620845795
  - 0.8075745105743408
  - 0.7953174948692322
  - 0.7501233756542206
  - 0.7384876191616059
  - 0.7216981261968614
  - 0.731893938779831
  - 0.7282286465168
  - 0.7681173026561737
  - 0.7512057542800904
  - 0.7397890388965607
  - 0.7535100042819978
  - 0.8099297761917115
  - 0.7322287797927857
  - 0.8120460271835328
  - 0.7717050909996033
  - 0.7289865255355835
  - 0.739274138212204
  - 0.7587962031364441
  - 0.926227754354477
  - 0.7641828179359437
  - 0.7411165833473206
  - 0.7298855841159821
  - 0.7178776532411576
  - 0.7418798029422761
  - 0.716430538892746
  - 0.71607306599617
  - 0.7552614986896515
  - 0.7397027850151062
  - 0.7313361644744873
  - 0.7185130715370178
  - 0.738133454322815
  - 0.748969042301178
  - 0.7273624002933503
  - 0.7670612871646881
  - 0.8450500905513764
  - 0.7441331684589386
  - 0.7839894652366639
  - 0.7767739713191987
  - 0.7362089812755586
  - 0.7499768078327179
  - 0.7467041432857514
  validation_losses:
  - 0.4025377929210663
  - 0.3939594328403473
  - 0.3941606283187866
  - 0.3981316089630127
  - 0.4074964225292206
  - 0.39237144589424133
  - 0.4014734625816345
  - 0.4201723039150238
  - 0.4795311391353607
  - 0.5136082172393799
  - 0.5161643028259277
  - 0.4182394742965698
  - 0.39397695660591125
  - 0.4232287108898163
  - 0.3925209641456604
  - 0.6218556761741638
  - 0.7404793500900269
  - 0.4525774121284485
  - 0.6685878038406372
  - 0.39747753739356995
  - 0.4023779034614563
  - 0.38787323236465454
  - 0.38799917697906494
  - 0.3874191343784332
  - 0.4506162405014038
  - 0.527601420879364
  - 0.3856206238269806
  - 0.40877893567085266
  - 0.3873952627182007
  - 0.4034685790538788
  - 0.4005579650402069
  - 0.4113588035106659
  - 0.4033149182796478
  - 0.5193583369255066
  - 0.4370449185371399
  - 0.4104553163051605
  - 0.38244712352752686
  - 0.38193440437316895
  - 0.3863227367401123
  - 0.4117802381515503
  - 0.39075392484664917
  - 0.40051522850990295
  - 0.4106522500514984
  - 0.4907437562942505
  - 0.464614599943161
  - 0.6687479019165039
  - 0.7740584015846252
  - 1.130427360534668
  - 1.242624044418335
  - 0.39648541808128357
  - 0.3923562169075012
  - 0.3884010910987854
  - 0.3929080069065094
  - 0.3910413086414337
  - 0.4071584939956665
  - 0.3865527808666229
  - 0.38745054602622986
  - 0.38582801818847656
  - 0.3871246874332428
  - 0.38801029324531555
  - 0.38771939277648926
loss_records_fold2:
  train_losses:
  - 0.758563792705536
  - 0.7594700038433075
  - 0.7989639699459077
  - 0.7374810695648194
  - 0.76022647023201
  - 0.7397032618522644
  - 0.7724872589111329
  - 0.7252675592899323
  - 0.7580056369304657
  - 0.816769027709961
  - 0.7433333396911621
  - 0.7522128403186799
  - 0.7406388342380524
  - 0.7962567806243896
  - 0.7397932887077332
  - 0.7323852837085725
  - 0.7596338629722595
  - 0.7517565190792084
  - 0.7270345091819763
  - 0.7463924407958985
  - 0.7735090732574463
  - 0.7395922303199769
  - 0.7532431900501252
  validation_losses:
  - 0.38073334097862244
  - 0.38491833209991455
  - 0.388639360666275
  - 0.39012208580970764
  - 0.39018863439559937
  - 0.38234925270080566
  - 0.38985776901245117
  - 0.3805427849292755
  - 0.3983588218688965
  - 0.3793988823890686
  - 0.4973733127117157
  - 0.5620976686477661
  - 0.5020957589149475
  - 0.38310325145721436
  - 0.41638419032096863
  - 0.38364994525909424
  - 0.5076894164085388
  - 0.44271335005760193
  - 0.39857161045074463
  - 0.40036338567733765
  - 0.38847941160202026
  - 0.3917030096054077
  - 0.3964275121688843
loss_records_fold3:
  train_losses:
  - 0.7380451440811158
  - 0.7436772167682648
  - 0.7772563755512238
  - 0.7699992418289185
  - 0.7278677105903626
  - 0.78082132935524
  - 0.7277433097362519
  - 0.7565261423587799
  - 0.7473129272460938
  - 0.7239192068576813
  - 0.7413286924362184
  - 0.7973712146282197
  - 0.7832019448280335
  - 0.7395408093929291
  - 0.7524462163448334
  - 0.7367917060852052
  - 0.7406291007995606
  - 0.7180967628955841
  - 0.7286413311958313
  - 0.7340043067932129
  - 0.7369541525840759
  - 0.7034004420042038
  - 0.7329969286918641
  - 0.741845589876175
  - 0.7524725735187531
  - 0.7211482584476472
  - 0.7676322638988495
  - 0.7343865036964417
  - 0.7305325686931611
  - 0.7426193714141847
  - 0.737712597846985
  - 0.7591456472873688
  - 0.7114925235509872
  - 0.7285780638456345
  - 0.6983107298612595
  - 0.8153307616710663
  - 0.7559238016605377
  - 0.7998747885227204
  - 0.758532202243805
  - 0.7496212482452393
  - 0.7534995496273041
  - 0.7505922496318818
  - 0.7588997185230255
  - 0.7382511675357819
  - 0.7613615989685059
  - 0.7538716197013855
  - 0.8704014360904694
  - 0.7583917915821076
  - 0.7267887175083161
  - 0.7751060962677002
  - 0.7362998545169831
  - 0.724584773182869
  - 0.8021726191043854
  - 0.763792234659195
  - 0.7353717386722565
  - 0.7700590074062348
  - 0.7359683930873872
  - 0.7496378779411317
  - 0.7420876801013947
  - 0.7135018795728684
  - 0.7535646498203278
  - 0.7599382936954499
  - 0.7950751125812531
  - 0.735585367679596
  - 0.7615072309970856
  - 0.7284841656684876
  - 0.7258467853069306
  - 0.7217555284500122
  - 0.7381946384906769
  - 0.7453504621982575
  - 0.7155361503362656
  - 0.744492906332016
  - 0.7512532889842988
  - 0.7653809010982514
  - 0.7217558532953263
  - 0.718375289440155
  - 0.7445967137813568
  - 0.7313139140605927
  - 0.746262675523758
  - 0.7601626515388489
  - 0.7421364963054657
  - 0.7303288161754609
  - 0.8137760639190674
  - 0.7689635872840882
  - 0.744820272922516
  - 0.7294273912906647
  - 0.8475357830524445
  - 0.7317713081836701
  - 0.7993479371070862
  - 0.7544259548187257
  - 0.8284371674060822
  - 0.7806781053543091
  - 0.729079043865204
  - 0.771411144733429
  - 0.7241692245006561
  - 0.7587630987167359
  - 0.7744353353977204
  - 0.7239920794963837
  - 0.7191620171070099
  - 0.7785039901733399
  validation_losses:
  - 0.4768437147140503
  - 0.42582836747169495
  - 0.5452407002449036
  - 0.3981480598449707
  - 0.45698389410972595
  - 0.5253897905349731
  - 0.5447559952735901
  - 0.40742576122283936
  - 0.369214802980423
  - 0.5347094535827637
  - 0.5353121757507324
  - 0.5081740617752075
  - 0.675238847732544
  - 0.8434162735939026
  - 0.9513394236564636
  - 1.361211895942688
  - 1.0562121868133545
  - 0.598484992980957
  - 0.377168744802475
  - 0.3699299991130829
  - 0.9652609825134277
  - 0.757331907749176
  - 1.1656428575515747
  - 0.42478883266448975
  - 0.4290279448032379
  - 0.5102662444114685
  - 0.39153990149497986
  - 0.4484666585922241
  - 0.36680030822753906
  - 0.3589724600315094
  - 0.46929243206977844
  - 0.9844253659248352
  - 1.2520744800567627
  - 0.8584203124046326
  - 1.249943733215332
  - 0.6600239872932434
  - 0.37548986077308655
  - 0.4601074159145355
  - 0.38933590054512024
  - 0.381866991519928
  - 0.37465375661849976
  - 0.37571981549263
  - 0.36786335706710815
  - 0.3799753785133362
  - 0.3831263482570648
  - 0.3737284243106842
  - 0.37646248936653137
  - 0.3648267090320587
  - 0.36735799908638
  - 0.40396881103515625
  - 0.36317959427833557
  - 0.5082342028617859
  - 0.9558886885643005
  - 0.7368834018707275
  - 0.9470906257629395
  - 0.5162104368209839
  - 0.7615647912025452
  - 0.8444077372550964
  - 0.6757298707962036
  - 0.7362946271896362
  - 0.6867173314094543
  - 0.5113791823387146
  - 0.3928448259830475
  - 0.37273016571998596
  - 0.5911582708358765
  - 0.5634506940841675
  - 0.6778470277786255
  - 0.5676922798156738
  - 0.9770312309265137
  - 0.8112212419509888
  - 0.8354664444923401
  - 0.3781670033931732
  - 0.3839976191520691
  - 0.39971667528152466
  - 0.3693293333053589
  - 0.4160400331020355
  - 0.40248996019363403
  - 0.39994052052497864
  - 0.3808693289756775
  - 0.382125586271286
  - 0.4227040708065033
  - 0.5215176939964294
  - 0.5099486112594604
  - 0.39176878333091736
  - 0.3961276113986969
  - 0.4827291667461395
  - 0.4290332496166229
  - 0.49762049317359924
  - 0.8386006951332092
  - 0.7103250026702881
  - 0.9589075446128845
  - 0.6514039635658264
  - 0.6632115244865417
  - 0.4857783913612366
  - 0.49050021171569824
  - 0.5350496768951416
  - 0.45236557722091675
  - 0.5131964087486267
  - 0.43101054430007935
  - 0.43241021037101746
loss_records_fold4:
  train_losses:
  - 0.7749692440032959
  - 0.7582324206829072
  - 0.7025118440389634
  - 0.747256886959076
  - 0.7221138954162598
  - 0.6945513755083085
  - 0.7404649555683136
  - 0.7482625246047974
  - 0.7764389455318451
  - 0.7596680581569673
  - 0.7165960967540741
  - 0.6988899320363999
  - 0.7254149496555329
  - 0.728450357913971
  - 0.7332595050334931
  - 0.7141977012157441
  - 0.7435158073902131
  - 0.7433484971523285
  - 0.7646268069744111
  - 0.7792706727981568
  - 0.7425291001796723
  - 0.7438349783420564
  - 0.7813289284706116
  - 0.8131609439849854
  - 0.7884110450744629
  - 0.7291426777839661
  - 0.7718102395534516
  - 0.7275708198547364
  validation_losses:
  - 0.37304162979125977
  - 0.4003332257270813
  - 0.44874584674835205
  - 0.4508519768714905
  - 0.37633562088012695
  - 0.36335745453834534
  - 0.3928495943546295
  - 0.3579745292663574
  - 0.36308085918426514
  - 0.3690362870693207
  - 0.38572120666503906
  - 0.3592529594898224
  - 0.41122961044311523
  - 0.36205220222473145
  - 0.38278403878211975
  - 0.3683241009712219
  - 0.3648509681224823
  - 0.368401437997818
  - 0.36316365003585815
  - 0.37762001156806946
  - 0.4002310633659363
  - 0.41450151801109314
  - 0.38112354278564453
  - 0.3809698522090912
  - 0.3801240622997284
  - 0.37565454840660095
  - 0.37130945920944214
  - 0.37512722611427307
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 61 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8542024013722127, 0.8473413379073756, 0.8490566037735849,
    0.8556701030927835]'
  fold_eval_f1: '[0.023255813953488372, 0.0, 0.10101010101010101, 0.08333333333333334,
    0.023255813953488372]'
  mean_eval_accuracy: 0.8524376226768757
  mean_f1_accuracy: 0.04617101245008222
  total_train_time: '0:18:26.833257'
