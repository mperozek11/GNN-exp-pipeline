config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:59:13.254510'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_80fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_80fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_80fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_80fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_80fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_80fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_80fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_80fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_80fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_80fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.09216514825821
  - 6.183399111032486
  - 6.079228302836419
  - 6.064831992983819
  - 6.238422152400017
  - 5.884661424160004
  - 5.81622876226902
  - 5.731455609202385
  - 6.015980684757233
  - 6.0385212957859045
  - 5.733414658904076
  - 6.014839957654477
  - 5.760712563991547
  - 5.807518845796586
  - 5.657779029011727
  - 5.739402145147324
  - 5.733966487646104
  - 5.633085487782956
  - 5.630138143897057
  - 5.611846309900284
  - 5.660181140899659
  - 5.5161096334457405
  - 5.528842133283615
  - 5.571206799149514
  - 5.643066689372063
  - 5.607409045100212
  - 5.695271268486977
  - 5.638142210245133
  - 5.612964996695519
  - 5.56043441593647
  - 5.558264988660813
  - 5.502406430244446
  - 5.55627980530262
  - 5.602257120609284
  - 5.579005780816079
  - 5.56033060848713
  - 5.517998024821281
  - 5.483442482352257
  - 5.5724843651056295
  - 5.4735621333122255
  - 5.470038789510728
  - 5.5216632932424545
  - 5.5339488118886955
  - 5.593120342493058
  - 5.496483245491982
  - 5.663064554333687
  - 5.536081439256669
  - 5.557413819432259
  - 5.509813204407692
  - 5.578210958838463
  - 5.481135639548302
  - 5.564973312616349
  - 5.491228178143501
  - 5.514457669854164
  - 5.545886707305908
  - 5.600833252072334
  - 5.528776925802231
  - 5.572010758519173
  - 5.6128149211406715
  - 5.482796227931977
  - 5.502891734242439
  - 5.497399109601975
  - 5.500687441229821
  - 5.562594869732857
  - 5.518220978975297
  - 5.4931240558624275
  - 5.538714483380318
  - 5.4769605696201324
  - 5.489927843213081
  - 5.439608624577523
  - 5.514186656475068
  - 5.53377041220665
  - 5.50752743780613
  - 5.442854273319245
  - 5.511070382595062
  - 5.481646150350571
  - 5.521390113234521
  - 5.453767824172974
  - 5.484930938482285
  - 5.559891164302826
  - 5.4643714964389805
  - 5.49528047144413
  - 5.448229125142098
  - 5.481636789441109
  - 5.489374339580536
  - 5.43741212785244
  - 5.583870026469231
  - 5.57823315858841
  - 5.516648867726326
  - 5.421493871510029
  - 5.506930860877038
  - 5.429802677035332
  - 5.4924103856086735
  - 5.550871658325196
  - 5.457657930254936
  - 5.506181359291077
  - 5.488212296366692
  - 5.474034655094147
  - 5.556538602709771
  - 5.527815410494805
  validation_losses:
  - 0.4767089784145355
  - 0.4709870219230652
  - 0.39697185158729553
  - 0.3995204269886017
  - 0.39566293358802795
  - 0.3918972611427307
  - 0.4119561016559601
  - 0.3867563009262085
  - 0.3898031413555145
  - 0.4298321008682251
  - 0.4391835033893585
  - 0.4095155596733093
  - 0.44105491042137146
  - 0.43809157609939575
  - 0.46519726514816284
  - 0.7856423258781433
  - 0.3975290060043335
  - 0.3869873583316803
  - 0.5094358325004578
  - 0.41415122151374817
  - 0.3823564648628235
  - 0.7712133526802063
  - 0.46518149971961975
  - 0.3822750151157379
  - 0.4176589846611023
  - 0.5676800608634949
  - 0.3808121979236603
  - 0.6283619999885559
  - 0.38686060905456543
  - 0.38207516074180603
  - 0.4012903571128845
  - 0.401445597410202
  - 0.40786001086235046
  - 0.4183436632156372
  - 0.39917877316474915
  - 0.4058593511581421
  - 0.3972454369068146
  - 0.40109166502952576
  - 0.38107946515083313
  - 0.4663217067718506
  - 0.640566885471344
  - 0.4889051616191864
  - 0.7470395565032959
  - 0.4547964036464691
  - 0.4808318316936493
  - 0.39373278617858887
  - 0.4324665069580078
  - 0.3871747851371765
  - 0.6643550395965576
  - 0.37802553176879883
  - 0.5128155946731567
  - 0.45966675877571106
  - 0.5337022542953491
  - 0.7450049519538879
  - 0.5057224631309509
  - 0.4178551733493805
  - 0.4084259569644928
  - 0.3782861828804016
  - 0.5086970329284668
  - 0.509855329990387
  - 0.49590322375297546
  - 0.6497375965118408
  - 0.3921426832675934
  - 0.4471469819545746
  - 0.3821752667427063
  - 0.3996884226799011
  - 0.43035241961479187
  - 0.5424612760543823
  - 0.3867252469062805
  - 0.6200100779533386
  - 0.4123665690422058
  - 0.4407571852207184
  - 0.6028885841369629
  - 0.5486912727355957
  - 0.3875522017478943
  - 0.4697636365890503
  - 0.43071573972702026
  - 0.39514124393463135
  - 0.38153067231178284
  - 0.41786471009254456
  - 0.5386497378349304
  - 0.4423942267894745
  - 0.45031100511550903
  - 0.37831008434295654
  - 0.3971082866191864
  - 0.4279078543186188
  - 0.4448998272418976
  - 0.5112665891647339
  - 0.3881798982620239
  - 0.43555948138237
  - 0.5024465322494507
  - 0.5216579437255859
  - 0.6929230093955994
  - 0.3734767735004425
  - 0.40005964040756226
  - 0.40388011932373047
  - 0.4228491485118866
  - 0.40567728877067566
  - 0.3943532407283783
  - 0.3919501006603241
loss_records_fold1:
  train_losses:
  - 5.438570520281792
  - 5.463340395689011
  - 5.373407778143883
  - 5.4740633279085165
  - 5.46855853497982
  - 5.437262579798698
  - 5.502093923091889
  - 5.468521437048913
  - 5.528898486495018
  - 5.387267658114434
  - 5.502703142166138
  - 5.445256933569908
  - 5.5150405913591385
  - 5.438579279184342
  - 5.485151869058609
  - 5.520612993836403
  - 5.481536948680878
  - 5.428206875920296
  - 5.431737872958184
  - 5.4114697098732
  - 5.401939815282822
  - 5.419147282838821
  - 5.375610458850861
  - 5.475660598278046
  - 5.382753330469132
  - 5.498626843094826
  - 5.452761945128441
  - 5.421818163990975
  - 5.428499890863896
  - 5.432354320585728
  - 5.377313360571861
  - 5.47862575352192
  - 5.391068208217622
  - 5.41932153403759
  - 5.397704684734345
  - 5.399509999155999
  - 5.42680116891861
  - 5.397432011365891
  - 5.3996750801801685
  - 5.3963148415088655
  - 5.391783618927002
  - 5.397064453363419
  - 5.415056571364403
  - 5.379048112034798
  - 5.349294903874398
  - 5.368712568283081
  - 5.372326067090035
  - 5.380292871594429
  - 5.386107656359673
  - 5.4051421850919725
  - 5.3637742638587955
  - 5.342352172732354
  - 5.373330077528954
  - 5.463811415433884
  - 5.4464743643999105
  - 5.274183067679406
  - 5.4600319325923925
  - 5.349527418613434
  - 5.422243192791939
  - 5.405869415402413
  - 5.381925365328789
  - 5.325193248689175
  - 5.326191940903664
  - 5.371868118643761
  - 5.341741132736207
  - 5.348767557740212
  - 5.370466585457326
  - 5.3389058977365496
  - 5.277232536673546
  - 5.42088427245617
  - 5.40709579885006
  - 5.356411918997765
  - 5.328852736949921
  - 5.3838944166898735
  - 5.364858880639076
  - 5.385025036334992
  - 5.342050385475159
  - 5.360624077916146
  - 5.331263011693955
  - 5.349384835362435
  - 5.316990745067597
  - 5.2893136024475105
  - 5.27714610695839
  - 5.349902930855752
  - 5.338990408182145
  - 5.489135050773621
  - 5.401203382015229
  - 5.34925186932087
  - 5.36765912771225
  - 5.363794289529324
  - 5.26547127366066
  - 5.394601750373841
  - 5.326431104540825
  - 5.224611687660218
  - 5.379121232032777
  - 5.238365182280541
  - 5.385488140583039
  - 5.4828819870948795
  - 5.414305663108826
  - 5.419002017378808
  validation_losses:
  - 0.47065430879592896
  - 0.48561057448387146
  - 0.47146549820899963
  - 0.39051753282546997
  - 0.4105907380580902
  - 0.5581102967262268
  - 0.42273467779159546
  - 0.45215484499931335
  - 0.4685261845588684
  - 0.6074020266532898
  - 0.43108314275741577
  - 0.4698358178138733
  - 0.39168840646743774
  - 0.4672571122646332
  - 0.41734442114830017
  - 0.5134718418121338
  - 0.4231509268283844
  - 0.428103506565094
  - 0.5378019213676453
  - 0.6196655035018921
  - 0.5580044388771057
  - 0.7479619383811951
  - 0.4203435480594635
  - 0.4376488924026489
  - 0.402887225151062
  - 0.4949638247489929
  - 0.39907675981521606
  - 0.4087975323200226
  - 0.3912913203239441
  - 0.5065857172012329
  - 0.5127376317977905
  - 0.7635794878005981
  - 0.5457843542098999
  - 0.4328925907611847
  - 0.4936727285385132
  - 0.7387803196907043
  - 0.45766788721084595
  - 0.434793621301651
  - 0.46114814281463623
  - 0.6430570483207703
  - 0.4430394470691681
  - 0.5413691997528076
  - 0.5543559789657593
  - 0.5224608182907104
  - 0.880966067314148
  - 0.6684186458587646
  - 0.711652398109436
  - 0.5239773988723755
  - 0.802073061466217
  - 0.6003645062446594
  - 0.9189847111701965
  - 0.525165319442749
  - 2.5622169971466064
  - 0.531038761138916
  - 0.46982017159461975
  - 0.5182809233665466
  - 0.622019350528717
  - 0.7112011313438416
  - 0.7067174911499023
  - 0.5748783349990845
  - 0.6210435032844543
  - 0.5392943024635315
  - 0.49421942234039307
  - 0.8329809308052063
  - 3.565106153488159
  - 1.1397830247879028
  - 0.9897875785827637
  - 1.455888271331787
  - 0.48665308952331543
  - 0.5027908682823181
  - 0.4903104901313782
  - 0.4727458655834198
  - 0.48838552832603455
  - 1.2674108743667603
  - 1.1637908220291138
  - 4.296383380889893
  - 0.47593656182289124
  - 0.5394206643104553
  - 3.6458773612976074
  - 5.352376937866211
  - 5.436143398284912
  - 1.9070775508880615
  - 0.49591562151908875
  - 12.105911254882812
  - 1.9956053495407104
  - 9.71580982208252
  - 5.413946151733398
  - 3.4344305992126465
  - 4.8390960693359375
  - 4.699796676635742
  - 10.899614334106445
  - 5.730093002319336
  - 15.684510231018066
  - 17.659896850585938
  - 0.953957736492157
  - 23.812623977661133
  - 122.3267593383789
  - 0.40989622473716736
  - 0.7098531126976013
  - 0.8827736973762512
loss_records_fold2:
  train_losses:
  - 5.376438662409782
  - 5.406824094057083
  - 5.409290182590485
  - 5.406167116761208
  - 5.3171845316886905
  - 5.389135280251503
  - 5.313344578444958
  - 5.298981994390488
  - 5.383099260926247
  - 5.314346399903298
  - 5.34241361618042
  - 5.32868404686451
  - 5.374878185987473
  - 5.309874197840691
  - 5.32403516471386
  - 5.356924626231194
  - 5.293513709306717
  - 5.276859226822854
  - 5.260204187035561
  - 5.357645964622498
  - 5.343379727005959
  - 5.312247657775879
  - 5.428596004843712
  - 5.3376326918601995
  - 5.391845372319222
  - 5.3343986004590995
  - 5.417322021722794
  - 5.296337229013443
  - 5.3059512913227085
  - 5.298983737826347
  - 5.25417098402977
  - 5.325342690944672
  - 5.2874906539917
  - 5.23201148211956
  - 5.318152844905853
  - 5.309675034880638
  - 5.329133746027947
  - 5.340315815806389
  - 5.376534268260002
  - 5.302527385950089
  - 5.258801129460335
  - 5.372875276207925
  - 5.339753216505051
  - 5.357718232274056
  - 5.358671653270722
  - 5.317451184988022
  - 5.295957019925118
  - 5.317175301909447
  - 5.339574903249741
  - 5.406422209739685
  - 5.386376076936722
  - 5.472314229607583
  - 5.296834334731102
  - 5.305621168017388
  - 5.3442226350307465
  - 5.360017096996308
  - 5.30872232913971
  - 5.375675460696221
  - 5.31807356774807
  - 5.263520416617394
  - 5.259881365299226
  - 5.247149094939232
  - 5.301331382989884
  - 5.2861312389373785
  - 5.240238600969315
  - 5.351377725601196
  - 5.263957285881043
  - 5.319526818394661
  - 5.271899941563607
  - 5.2909464806318285
  - 5.204926836490632
  - 5.210750195384026
  - 5.341014769673348
  - 5.232164558768273
  - 5.2745292574167255
  - 5.279105836153031
  - 5.2523564249277115
  - 5.236018545925617
  - 5.282421958446503
  - 5.320994713902474
  - 5.223957642912865
  - 5.218132016062737
  - 5.249418160319329
  - 5.299480211734772
  - 5.390745848417282
  - 5.2097710341215135
  - 5.305847644805908
  - 5.293794402480126
  - 5.318917992711068
  - 5.181715792417527
  - 5.260105940699578
  - 5.386387124657631
  - 5.29729996919632
  - 5.2530065178871155
  - 5.171214163303375
  - 5.288597175478936
  - 5.243457961082459
  - 5.259289690852166
  - 5.190176737308502
  - 5.233943247795105
  validation_losses:
  - 0.5759299397468567
  - 0.43307387828826904
  - 0.5488598346710205
  - 1.0846070051193237
  - 1.815751552581787
  - 3.9845597743988037
  - 0.7097386121749878
  - 2.258319139480591
  - 3.5238802433013916
  - 5.515723705291748
  - 5.523842811584473
  - 23.662471771240234
  - 8.349318504333496
  - 1.0178704261779785
  - 11.244641304016113
  - 0.8313173055648804
  - 5.070568084716797
  - 4.983016014099121
  - 12.15360164642334
  - 9.407431602478027
  - 13.094633102416992
  - 1.8114575147628784
  - 25.698394775390625
  - 15.934226989746094
  - 0.6876049637794495
  - 3.231154680252075
  - 1.9453938007354736
  - 2.0318877696990967
  - 7.656368255615234
  - 18.49028205871582
  - 5.200037002563477
  - 2.2959251403808594
  - 4.141216278076172
  - 0.4534927010536194
  - 3.1522505283355713
  - 4.988266944885254
  - 4.790653705596924
  - 2.20324444770813
  - 21.12000274658203
  - 16.132814407348633
  - 4.8610920906066895
  - 1.3830101490020752
  - 7.121771335601807
  - 5.2991251945495605
  - 6.483884811401367
  - 9.956169128417969
  - 8.099056243896484
  - 13.597831726074219
  - 9.891995429992676
  - 4.023571014404297
  - 3.5829391479492188
  - 2.310948133468628
  - 4.626791000366211
  - 4.890552520751953
  - 16.78127098083496
  - 6.913230895996094
  - 9.724170684814453
  - 2.562504291534424
  - 6.2713303565979
  - 2.836592435836792
  - 9.49150276184082
  - 11.641703605651855
  - 35.85378646850586
  - 4.109698295593262
  - 6.6570329666137695
  - 5.312231063842773
  - 7.916331768035889
  - 5.9749908447265625
  - 5.37308406829834
  - 0.5163911581039429
  - 8.163517951965332
  - 5.047347545623779
  - 11.208648681640625
  - 16.73989486694336
  - 9.806087493896484
  - 7.5646491050720215
  - 9.561607360839844
  - 12.945962905883789
  - 7.830741882324219
  - 11.21367359161377
  - 29.79328727722168
  - 2.2907185554504395
  - 7.9283552169799805
  - 10.926100730895996
  - 18.716188430786133
  - 1.3425613641738892
  - 0.6124950051307678
  - 2.5632643699645996
  - 10.419751167297363
  - 7.10402250289917
  - 20.96193504333496
  - 8.853857040405273
  - 14.026703834533691
  - 10.098305702209473
  - 5.182104587554932
  - 6.424262046813965
  - 3.653872013092041
  - 6.829861164093018
  - 7.6703996658325195
  - 3.8548929691314697
loss_records_fold3:
  train_losses:
  - 5.403959119319916
  - 5.384855738282204
  - 5.374467241764069
  - 5.341504868865013
  - 5.41497628390789
  - 5.322697731852532
  - 5.372581896185875
  - 5.445706909894944
  - 5.30010586977005
  - 5.312797549366952
  - 5.488104696571828
  - 5.363708567619324
  - 5.4968992382287984
  - 5.34164097905159
  - 5.339974462985992
  - 5.2848229050636295
  - 5.350224089622498
  - 5.390083369612694
  - 5.443031784892082
  - 5.252487766742707
  - 5.363950157165528
  - 5.3286946505308155
  - 5.426621061563492
  - 5.369306042790413
  - 5.421400132775307
  - 5.376300942897797
  - 5.296648615598679
  - 5.394553647935391
  - 5.360483574867249
  - 5.290768724679947
  - 5.390973207354546
  - 5.35646505355835
  - 5.331959414482117
  - 5.38883700966835
  - 5.353121972084046
  - 5.305617561936379
  - 5.274672177433968
  - 5.285454842448235
  - 5.401980409026146
  - 5.239802357554436
  - 5.32609481215477
  - 5.433844742178917
  - 5.336760178208351
  - 5.332653507590294
  - 5.366524800658226
  - 5.330830746889115
  - 5.347434294223786
  - 5.314203724265099
  - 5.278339391946793
  - 5.314898896217347
  - 5.359581100940705
  - 5.332374238967896
  - 5.376422727108002
  - 5.301941937208176
  - 5.331665179133416
  - 5.326817086338997
  - 5.347519391775132
  - 5.3410930156707765
  - 5.320670595765114
  - 5.248683366179467
  - 5.29826067686081
  - 5.305348487198353
  - 5.337265706062317
  - 5.441623210906982
  - 5.339543277025223
  - 5.3369838327169425
  - 5.289703753590584
  - 5.405177491903306
  - 5.297191873192787
  - 5.367352697253228
  - 5.282644373178482
  - 5.422128602862358
  - 5.286615931987763
  - 5.371752133965493
  - 5.256358945369721
  - 5.350577020645142
  - 5.395780834555627
  - 5.320491155982018
  - 5.356591632962227
  - 5.406278583407403
  - 5.383189845085145
  - 5.283973807096482
  - 5.313568508625031
  - 5.296320077776909
  - 5.260282839834691
  - 5.354389193654061
  - 5.336491301655769
  - 5.264393597841263
  - 5.333301523327828
  - 5.3451699167490005
  - 5.220559918880463
  - 5.274117714166642
  - 5.327836021780968
  - 5.373621714115143
  - 5.278725272417069
  - 5.466810122132301
  - 5.296818280220032
  - 5.420300817489625
  - 5.320441207289696
  - 5.342906445264816
  validation_losses:
  - 23.11536979675293
  - 6.405231952667236
  - 14.214301109313965
  - 15.711393356323242
  - 0.5163547396659851
  - 0.48648393154144287
  - 5.674259185791016
  - 6.492166996002197
  - 21.352956771850586
  - 21.03711700439453
  - 15.600510597229004
  - 9.205242156982422
  - 22.458101272583008
  - 15.480599403381348
  - 44.17961120605469
  - 62.16034698486328
  - 48.316463470458984
  - 32.1707878112793
  - 36.94010925292969
  - 42.451622009277344
  - 51.2088623046875
  - 16.57423973083496
  - 63.41652297973633
  - 35.84623718261719
  - 48.948577880859375
  - 49.593265533447266
  - 102.16415405273438
  - 125.32728576660156
  - 60.85943603515625
  - 27.25739288330078
  - 38.446800231933594
  - 31.364356994628906
  - 73.56116485595703
  - 32.669517517089844
  - 38.053470611572266
  - 31.901824951171875
  - 31.8060359954834
  - 18.30471420288086
  - 25.483036041259766
  - 12.862607955932617
  - 29.966495513916016
  - 17.599897384643555
  - 20.2888240814209
  - 10.839323043823242
  - 69.58380889892578
  - 26.159748077392578
  - 17.491979598999023
  - 37.39218521118164
  - 5.256216526031494
  - 7.93975830078125
  - 10.6176118850708
  - 1.4684778451919556
  - 33.17343521118164
  - 21.05721092224121
  - 11.780220985412598
  - 26.975723266601562
  - 14.52987003326416
  - 54.9294548034668
  - 40.08720779418945
  - 107.12920379638672
  - 46.843780517578125
  - 24.547819137573242
  - 2.0920565128326416
  - 33.636600494384766
  - 23.97554588317871
  - 11.563857078552246
  - 4.006074905395508
  - 5.190784931182861
  - 0.9156257510185242
  - 0.5576088428497314
  - 30.81917381286621
  - 14.178925514221191
  - 0.665743887424469
  - 3.130384683609009
  - 1.1498593091964722
  - 0.900799572467804
  - 1.8075535297393799
  - 16.152006149291992
  - 2.9683568477630615
  - 2.5686678886413574
  - 2.9082705974578857
  - 2.0614891052246094
  - 0.6134147047996521
  - 1.7928004264831543
  - 3.253032684326172
  - 6.481959819793701
  - 45.32583999633789
  - 4.235884666442871
  - 13.013001441955566
  - 6.489175319671631
  - 26.7630672454834
  - 24.4769229888916
  - 18.865314483642578
  - 25.649646759033203
  - 45.95109558105469
  - 24.676124572753906
  - 30.501718521118164
  - 0.4755841791629791
  - 7.755496025085449
  - 12.38527774810791
loss_records_fold4:
  train_losses:
  - 5.285286790132523
  - 5.43690140247345
  - 5.317243546247482
  - 5.32010717689991
  - 5.35144163966179
  - 5.334207603335381
  - 5.435208493471146
  - 5.281791779398919
  - 5.306768679618836
  - 5.269773611426354
  - 5.3328357338905334
  - 5.4157268241047865
  - 5.331279104948044
  - 5.373085421323776
  - 5.347149735689164
  - 5.3392463475465775
  - 5.31842124760151
  - 5.222956362366677
  - 5.321629288792611
  - 5.3108247280120855
  - 5.282410502433777
  - 5.4418379187583925
  - 5.394209834933282
  - 5.282615581154824
  - 5.36779170036316
  - 5.425982341170311
  - 5.397483783960343
  - 5.391192322969437
  - 5.393184953927994
  - 5.348189780116082
  - 5.3517834261059765
  - 5.313660477101803
  - 5.316792097687721
  - 5.2998480051755905
  - 5.229344090819359
  - 5.327368649840356
  - 5.343976670503617
  - 5.319835603237152
  - 5.320654657483101
  - 5.331757211685181
  - 5.321652480959893
  - 5.280799281597138
  - 5.3435210466384895
  - 5.258428707718849
  - 5.259166339039803
  - 5.298445156216622
  - 5.327120015025139
  - 5.340380051732064
  - 5.3500431239604955
  - 5.298807495832444
  - 5.241447070240975
  - 5.394112552702428
  - 5.334434938430786
  - 5.368526229262352
  - 5.352774062752724
  - 5.341522943973541
  - 5.36131266951561
  - 5.347990998625756
  - 5.245545122027398
  - 5.311980003118515
  - 5.299311390519143
  - 5.234018352627754
  - 5.366531890630722
  - 5.306424549221993
  - 5.257644912600518
  - 5.362293294072152
  - 5.325880047678948
  - 5.295154479146004
  - 5.3286483764648445
  - 5.302433887124062
  - 5.3311502844095235
  - 5.385849487781525
  - 5.364316546916962
  - 5.313551580905915
  - 5.232342517375947
  - 5.401242032647133
  - 5.336901879310608
  - 5.3035342574119575
  - 5.324498704075814
  - 5.272138157486916
  - 5.358912861347199
  - 5.244225162267686
  - 5.268458518385888
  - 5.322115021944047
  - 5.349478921294213
  - 5.306714370846748
  - 5.329856330156327
  - 5.368534120917321
  - 5.255417361855507
  - 5.26165209710598
  - 5.403053098917008
  - 5.3034171760082245
  - 5.2650309801101685
  - 5.381367176771164
  - 5.202767264842987
  - 5.334213438630105
  - 5.383961158990861
  - 5.34361335337162
  - 5.323934257030487
  - 5.262552832067013
  validation_losses:
  - 0.9933969974517822
  - 0.8208009004592896
  - 0.6041697263717651
  - 0.9271923303604126
  - 1.146588683128357
  - 1.290186882019043
  - 0.8020602464675903
  - 4.271052360534668
  - 1.7267972230911255
  - 4.748517036437988
  - 3.2479748725891113
  - 2.837573766708374
  - 1.4712228775024414
  - 0.9696900844573975
  - 1.5651662349700928
  - 0.4771616458892822
  - 0.48931995034217834
  - 2.0911331176757812
  - 1.590562105178833
  - 3.7735793590545654
  - 1.2295790910720825
  - 4.239529609680176
  - 3.0320639610290527
  - 2.7511730194091797
  - 2.6085758209228516
  - 0.48593616485595703
  - 0.9023488163948059
  - 3.503307580947876
  - 2.2248263359069824
  - 2.7173266410827637
  - 0.9456568360328674
  - 2.6024703979492188
  - 2.7960500717163086
  - 3.438533306121826
  - 0.851138710975647
  - 2.6969568729400635
  - 0.9057970643043518
  - 4.237862586975098
  - 2.0533945560455322
  - 0.7284051179885864
  - 0.958069384098053
  - 0.5181367993354797
  - 0.8480727076530457
  - 0.5976505875587463
  - 2.1580803394317627
  - 1.4667940139770508
  - 0.5095532536506653
  - 1.0444929599761963
  - 0.5333904027938843
  - 0.8435332179069519
  - 3.1277782917022705
  - 0.9378892779350281
  - 2.734360694885254
  - 1.2237269878387451
  - 4.958098411560059
  - 2.5054819583892822
  - 2.2581722736358643
  - 2.4881932735443115
  - 0.6932855844497681
  - 2.8170642852783203
  - 3.8386318683624268
  - 1.5785373449325562
  - 1.3610799312591553
  - 2.0841150283813477
  - 4.124499797821045
  - 2.0815746784210205
  - 2.974410057067871
  - 3.6560864448547363
  - 5.037662982940674
  - 4.613844871520996
  - 4.434194087982178
  - 3.8559491634368896
  - 1.1611521244049072
  - 1.3187499046325684
  - 2.0475234985351562
  - 1.090096354484558
  - 2.466627836227417
  - 1.6913217306137085
  - 2.229614496231079
  - 2.3198177814483643
  - 1.7986891269683838
  - 0.7264112830162048
  - 0.46847856044769287
  - 0.5717943906784058
  - 0.4595087766647339
  - 1.5650811195373535
  - 0.5347445607185364
  - 0.6882874369621277
  - 0.44154489040374756
  - 0.8150231242179871
  - 1.696494460105896
  - 1.2609068155288696
  - 0.8142462968826294
  - 1.672273874282837
  - 1.5584338903427124
  - 1.490225076675415
  - 2.1564900875091553
  - 0.6036490201950073
  - 1.8116296529769897
  - 1.0072176456451416
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8524871355060034, 0.8439108061749572, 0.8198970840480274, 0.7753001715265866,
    0.8436426116838488]'
  fold_eval_f1: '[0.044444444444444446, 0.11650485436893204, 0.2553191489361702, 0.28415300546448086,
    0.2222222222222222]'
  mean_eval_accuracy: 0.8270475617878846
  mean_f1_accuracy: 0.18452873508724996
  total_train_time: '0:51:58.956873'
