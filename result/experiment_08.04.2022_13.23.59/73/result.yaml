config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 21:08:27.631861'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/73/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/73/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/73/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/73/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/73/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/73/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/73/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/73/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/73/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/73/fold_4_state_dict.pt
loss_records_fold3:
  train_losses:
  - 34.5487674921751
  - 35.89928825944662
  - 34.121936812996864
  - 33.59684205055237
  - 33.59290659427643
  - 33.24342130124569
  - 34.01171123981476
  - 33.44318951666355
  - 33.01627041399479
  - 33.39272801578045
  - 32.20739437639713
  - 33.490159913897514
  - 32.449367836117744
  - 32.28256191313267
  - 33.19423706829548
  - 32.02018168568611
  - 32.24971581995487
  - 35.73376652598381
  - 38.44220757484436
  - 32.8743010610342
  - 33.24350579082966
  - 33.423013389110565
  - 32.64594426751137
  - 32.14220820367336
  - 31.849780023097992
  - 31.924836575984955
  - 42.79694280028343
  - 41.04778651893139
  - 37.59655523300171
  - 34.772547006607056
  - 32.2551284134388
  - 32.406241089105606
  - 32.47592870891094
  - 32.054842218756676
  - 32.571801632642746
  - 32.712303310632706
  - 32.6640198379755
  - 33.16972656548023
  - 32.493872582912445
  - 31.622542068362236
  - 33.51814445853233
  - 31.859798699617386
  - 33.394508838653564
  - 31.886021971702576
  - 32.529091864824295
  - 32.89482045173645
  - 32.6792157292366
  - 32.644813030958176
  - 32.35712982714176
  - 32.50758683681488
  - 32.272517547011375
  - 32.17214737832546
  - 32.58994236588478
  - 34.32654716074467
  - 36.84558293223381
  - 39.55266961455345
  - 37.50614346563816
  - 36.9901225566864
  - 35.515070125460625
  - 32.73854859173298
  - 31.845879033207893
  - 32.11371320486069
  - 32.87328168749809
  - 32.479962557554245
  - 32.447948917746544
  - 32.09131437540054
  - 32.12064501643181
  - 33.10374277830124
  - 32.31677857041359
  - 33.078743532299995
  - 34.29605929553509
  - 33.66571298241615
  - 33.86639246344566
  - 32.86170473694801
  - 31.718101784586906
  - 32.85781706869602
  - 34.11439447104931
  - 33.1071390658617
  - 32.38960158824921
  - 32.42299751192331
  - 31.61840558052063
  - 32.90142120420933
  - 32.868524730205536
  - 32.67017602920532
  - 32.149041295051575
  - 32.83180224895477
  - 32.69100384414196
  - 31.696982353925705
  - 32.348945409059525
  - 32.1078115478158
  - 32.764355808496475
  - 32.23711630702019
  - 32.17788141965866
  - 32.91036197543144
  - 31.806754171848297
  - 31.663674369454384
  - 32.722535878419876
  - 32.73826891928911
  - 33.722596526145935
  - 32.53863933682442
  validation_losses:
  - 0.4207286834716797
  - 0.4095112979412079
  - 0.4136417806148529
  - 0.48277366161346436
  - 0.4040941298007965
  - 0.41407763957977295
  - 0.4104536771774292
  - 0.4157090485095978
  - 0.44530561566352844
  - 1876823.25
  - 0.4508296549320221
  - 0.4343956410884857
  - 443858496258048.0
  - 0.4104546904563904
  - 40786955599872.0
  - 26707960.0
  - 0.4053746461868286
  - 0.5854554176330566
  - 7222325.0
  - 8.401565916332032e+16
  - 3.0633769658788033e+20
  - 480802464.0
  - 3.317188372971127e+18
  - 432038304.0
  - 0.41306307911872864
  - 0.40707653760910034
  - 0.4072543680667877
  - 0.4176561236381531
  - 0.4144807457923889
  - 0.41021865606307983
  - 6089.6123046875
  - 0.4295050799846649
  - 5286216220016640.0
  - 7.87964320850903e+16
  - 4.491002608852778e+22
  - 1.3611935318223422e+23
  - 0.4180164933204651
  - 0.43378308415412903
  - 0.4124720096588135
  - 0.4345163106918335
  - 3.2285311222076416
  - 0.43074315786361694
  - 0.42566969990730286
  - 0.4132612943649292
  - 37155.265625
  - 0.4229404032230377
  - 0.4300518333911896
  - 0.40846192836761475
  - 0.4216926097869873
  - 0.41064897179603577
  - 0.4084787964820862
  - 0.40761151909828186
  - 0.4812884032726288
  - 0.4325779974460602
  - 0.5636668801307678
  - 280.2013244628906
  - 0.40358299016952515
  - 0.41367071866989136
  - 0.4200229048728943
  - 0.4058692455291748
  - 8448.6748046875
  - 82503096.0
  - 59750076.0
  - 37480096.0
  - 44559208.0
  - 39636408.0
  - 126389.765625
  - 17815396.0
  - 77233816.0
  - 13678387.0
  - 9799026.0
  - 86102016.0
  - 149758.109375
  - 96379512.0
  - 113494.9140625
  - 28704804.0
  - 72662336.0
  - 19878348.0
  - 53971004.0
  - 57851588.0
  - 3.5111714255035736e+24
  - 17487524.0
  - 1780509.0
  - 137820.4375
  - 4.7075808410338147e+24
  - 6.791462825719846e+24
  - 6.791463402180598e+24
  - 6.787457576412842e+24
  - 5.659186110035242e+24
  - 5.877849202598976e+24
  - 5.57067286736206e+24
  - 7.368151856481182e+24
  - 6.368736662065966e+24
  - 6.022904598161839e+24
  - 5.601596527958625e+24
  - 5.654730068419936e+24
  - 7.738944094499294e+24
  - 6.166322268727408e+24
  - 5.977052333462872e+24
  - 5.540232857336678e+24
loss_records_fold4:
  train_losses:
  - 33.18509443104267
  - 32.671807914972305
  - 32.50945325195789
  - 32.152004688978195
  - 31.702474124729633
  - 32.50129842758179
  - 32.20073340833187
  - 32.17229461669922
  - 33.2808930426836
  - 31.9304850846529
  - 32.51125632226467
  - 33.357819229364395
  - 32.91714830696583
  - 32.254124619066715
  - 32.89990070462227
  - 32.381424993276596
  - 32.37450248003006
  - 32.130106031894684
  - 32.38339310884476
  - 32.40424093604088
  - 33.633181035518646
  - 32.53431096673012
  - 32.500895611941814
  - 33.218739703297615
  - 32.646692499518394
  - 32.95277290046215
  - 32.570804715156555
  - 32.62026020884514
  - 33.41537319123745
  - 32.9453881829977
  - 32.85987235605717
  - 31.74119159579277
  - 32.174977883696556
  - 32.50102046132088
  - 32.87079246342182
  - 32.28878392279148
  - 32.842046931385994
  - 32.38029000163078
  - 31.865148574113846
  - 32.2053257972002
  - 32.68097898364067
  - 32.340026423335075
  - 33.24926154315472
  - 32.24001793563366
  - 31.9423955231905
  - 32.32394666969776
  - 32.885931223630905
  - 32.28597962856293
  - 32.52971124649048
  - 32.232422322034836
  - 32.42819558084011
  - 32.240088403224945
  - 32.69984449446201
  - 32.02147641777992
  - 32.988887414336205
  - 32.423328295350075
  - 34.057216465473175
  - 32.894886150956154
  - 32.4690206348896
  - 32.66805300116539
  - 32.480029091238976
  - 32.821284264326096
  - 31.68663950264454
  - 32.99778114259243
  - 32.27516332268715
  - 32.136879444122314
  - 32.526637211441994
  - 32.811887204647064
  - 32.67555384337902
  - 33.35226744413376
  - 33.168564185500145
  - 32.23667564988136
  - 32.71811221539974
  - 32.2666092813015
  - 32.984349980950356
  - 33.61563202738762
  - 32.94775901734829
  - 32.817166805267334
  - 32.37445184588432
  - 33.019047498703
  - 32.144899904727936
  - 32.50269393622875
  - 33.5463654845953
  - 32.69301135838032
  - 32.290223866701126
  - 32.998003259301186
  - 32.57214218378067
  - 33.001213759183884
  - 32.30590496957302
  - 31.913047462701797
  - 34.16125564277172
  - 32.42300474643707
  - 33.385762110352516
  - 33.65707239508629
  - 32.46947152912617
  - 32.2253365367651
  - 32.16592064499855
  - 31.37872152030468
  - 33.33201241493225
  - 33.98696377873421
  validation_losses:
  - 1.6193582371496987e+24
  - 1.5816474717707014e+24
  - 1.5371336048233953e+24
  - 1.430365003046269e+24
  - 1.374178526440759e+24
  - 1.4498356855312575e+24
  - 1.2465029902142801e+24
  - 1.341571600447468e+24
  - 1.1684437912265272e+24
  - 1.300722582732931e+24
  - 1.547603429121918e+24
  - 1.1776800615727148e+24
  - 1.3624779584360682e+24
  - 1.783953321099264e+17
  - 1.6732123771952872e+21
  - 83318.5625
  - 3.939843781817464e+24
  - 4.509205996417023e+24
  - 77088.796875
  - 66859.21875
  - 107185.2109375
  - 47514493386752.0
  - 1.6749774594952397e+17
  - 2.7768344830122394e+17
  - 3.7440662720321946e+17
  - 2.686407554367488e+17
  - 2.8100921327720858e+17
  - 2.2998725236359168e+17
  - 2.0256350822150963e+17
  - 1.54698279550976e+17
  - 1.4478499681560166e+17
  - 1.0556023732227277e+17
  - 1.684308018247762e+17
  - 9.138147653635277e+16
  - 5.364283993309152e+23
  - 2.6419289952938148e+23
  - 6.251204529237018e+23
  - 2.086858359636565e+23
  - 9.593768966911996e+23
  - 1.0077328745526081e+24
  - 4.767504800926884e+23
  - 8.683359817055587e+22
  - 4.9931780165265984e+23
  - 9.337553059215396e+23
  - 1.078331374368862e+24
  - 4.2608354313288956e+23
  - 1.0869653873440806e+23
  - 2.0332122374800652e+24
  - 1.8106848402632646e+24
  - 1.8607761091042944e+24
  - 1.9351569840891932e+24
  - 2.1171810959276507e+24
  - 1.7058504084253043e+24
  - 2.1804295049792538e+24
  - 1.816673979249321e+24
  - 4.950530729495251e+23
  - 1.492110578916617e+24
  - 3.3694035856110973e+24
  - 3.085201517196482e+24
  - 6.483025770817643e+24
  - 9.568943540498861e+24
  - 7.70139862924102e+24
  - 1.5042777919002974e+25
  - 1.2918315979658543e+25
  - 1.6058889146187154e+25
  - 1.5074529377239847e+25
  - 1.1357418212667003e+25
  - 1.2283240698060908e+25
  - 1.4392260406772634e+25
  - 1.6704376461892894e+25
  - 1.8864841833983188e+25
  - 1.9427854909856877e+25
  - 1.7369047238513788e+25
  - 1.7603966524292479e+25
  - 1.897489280328393e+25
  - 1.9395803692028807e+25
  - 1.846906232819573e+25
  - 1.6473497013066345e+25
  - 1.6851594161737647e+25
  - 2.1938113207680374e+25
  - 1.671560476442626e+25
  - 1.8141387048606905e+25
  - 1.5322736083359132e+25
  - 1.7426380872016381e+25
  - 1.6773269286400676e+25
  - 1.6973982544059187e+25
  - 2.1363372614261824e+25
  - 1.7457787606723377e+25
  - 1.9198323226269214e+25
  - 2.069878715383227e+25
  - 1.8598263323687993e+25
  - 1.999783854914742e+25
  - 2.155514035396609e+25
  - 1.6950369558723334e+25
  - 2.007020743199159e+25
  - 2.084046276000438e+25
  - 1.7904282976576984e+25
  - 1.949787875036068e+25
  - 1.7538504794182407e+25
  - 1.7281379087303483e+25
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 82 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 38 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 54 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:18:38.471463'
