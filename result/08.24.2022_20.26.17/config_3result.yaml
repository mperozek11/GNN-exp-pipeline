config:
  aggregation: mean
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.838585'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_3fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_3fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_3fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_3fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_3fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_3fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_3fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_3fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_3fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_3fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.018041580915451
  - 0.814688527584076
  - 0.8129013776779175
  - 0.7999238669872284
  - 0.805063682794571
  - 0.7620233267545701
  - 0.7781633019447327
  - 0.8246226251125336
  - 0.765836650133133
  - 0.7520047068595886
  - 0.7484762847423554
  - 0.7652573525905609
  - 0.8097374141216278
  - 0.784470671415329
  - 0.8141953706741334
  - 0.7984749376773834
  - 0.7822990536689759
  - 0.7590577185153962
  - 0.7534585237503052
  - 0.7524856388568879
  - 0.7769809007644654
  - 0.7559323012828827
  - 0.7720223069190979
  - 0.8135254085063934
  - 0.7858654618263246
  - 0.8296596348285675
  - 0.8399402439594269
  - 0.7564130604267121
  - 0.7796212434768677
  - 0.7424860119819642
  - 0.7344360679388047
  - 0.7540652632713318
  - 0.7746379435062409
  - 0.7617119073867799
  - 0.7730569005012513
  - 0.7434656023979187
  - 0.7679112493991852
  - 0.7615745544433594
  - 0.7565669596195221
  - 0.764115196466446
  - 0.7620101928710938
  - 0.7616325438022614
  - 0.7432405889034271
  - 0.7959163784980774
  - 0.7735908687114716
  - 0.7276983052492142
  - 0.7328932881355286
  - 0.7224726289510728
  - 0.7667542576789856
  - 0.7353872656822205
  - 0.7735163390636445
  - 0.7511281371116638
  - 0.7079477459192276
  - 0.7458771824836732
  - 0.7992260932922364
  - 0.727853274345398
  - 0.7465966880321503
  - 0.7144474714994431
  - 0.7434323191642762
  - 0.7088129848241806
  - 0.8062325358390808
  - 0.738034325838089
  - 0.7304706543684006
  - 0.7268232971429825
  - 0.7559219479560852
  - 0.7918102025985718
  validation_losses:
  - 0.4111192226409912
  - 0.4044771194458008
  - 0.39603322744369507
  - 0.3971349895000458
  - 0.39961257576942444
  - 0.38823920488357544
  - 0.3961904048919678
  - 0.3905673921108246
  - 0.38526785373687744
  - 0.3853614628314972
  - 0.39622482657432556
  - 0.38198089599609375
  - 0.40051913261413574
  - 0.3821193277835846
  - 0.4649042785167694
  - 0.3856664299964905
  - 0.3862241506576538
  - 0.3879775404930115
  - 0.3876707851886749
  - 0.419197142124176
  - 0.4950680434703827
  - 0.4251498579978943
  - 0.43506190180778503
  - 0.6738343238830566
  - 0.42792126536369324
  - 0.39078599214553833
  - 0.3961659073829651
  - 0.4140675663948059
  - 0.41454946994781494
  - 0.4036802649497986
  - 0.37254199385643005
  - 0.42750218510627747
  - 0.38538143038749695
  - 0.4903481900691986
  - 0.41625893115997314
  - 0.4042936861515045
  - 0.42383474111557007
  - 0.5124980211257935
  - 0.5023557543754578
  - 0.5682185888290405
  - 0.6168405413627625
  - 0.8939248919487
  - 0.3754989206790924
  - 0.37432652711868286
  - 0.38371485471725464
  - 0.397805392742157
  - 0.38303080201148987
  - 0.5509974360466003
  - 0.5076376795768738
  - 0.3750852942466736
  - 0.3746890127658844
  - 0.39414358139038086
  - 0.3749118447303772
  - 0.3788948953151703
  - 0.3766407370567322
  - 0.3770422041416168
  - 0.5312885642051697
  - 0.5798001885414124
  - 0.46635329723358154
  - 0.8614449501037598
  - 0.42109405994415283
  - 0.3864744007587433
  - 0.3745516836643219
  - 0.3723912239074707
  - 0.3766101598739624
  - 0.3719070851802826
loss_records_fold1:
  train_losses:
  - 0.7117535710334778
  - 0.7591575026512146
  - 0.7449091851711274
  - 0.778329622745514
  - 0.7498031556606293
  - 0.7263218343257904
  - 0.7317526340484619
  - 0.7637194156646729
  - 0.7213343024253845
  - 0.7515766561031342
  - 0.7164372384548188
  - 0.765724015235901
  - 0.7274603843688965
  - 0.7202590018510819
  - 0.7425545454025269
  - 0.7843415796756745
  - 0.7284228861331941
  - 0.7146784067153931
  - 0.75316321849823
  - 0.7527350246906281
  - 0.7753360033035279
  - 0.7801260113716126
  - 0.7563665211200714
  - 0.7118334323167801
  - 0.7569194674491883
  - 0.7539394557476045
  - 0.7288401246070862
  - 0.7374329626560212
  - 0.7288343727588654
  - 0.7289361953735352
  - 0.7206927657127381
  - 0.782224178314209
  - 0.7155300855636597
  - 0.7308440744876862
  - 0.7121354639530182
  - 0.7963690757751465
  - 0.7908463597297669
  - 0.7520457267761231
  - 0.7416805922985077
  - 0.7517007768154145
  - 0.7582244575023651
  - 0.7465008020401002
  - 0.7275862365961075
  - 0.7418757736682893
  - 0.7434021294116975
  - 0.8144058167934418
  - 0.7499221026897431
  validation_losses:
  - 0.4632761776447296
  - 0.4648929238319397
  - 0.3981742560863495
  - 0.40424883365631104
  - 0.39425337314605713
  - 0.6095077395439148
  - 0.5555003881454468
  - 0.5711923241615295
  - 0.40691402554512024
  - 0.3980826735496521
  - 0.4304273724555969
  - 0.42424240708351135
  - 0.4672660529613495
  - 0.5542463064193726
  - 0.7910373210906982
  - 0.6076174378395081
  - 0.38401612639427185
  - 0.38632190227508545
  - 0.43300482630729675
  - 0.5627579689025879
  - 0.3848363161087036
  - 0.3920115530490875
  - 0.38132044672966003
  - 0.40640679001808167
  - 0.40323755145072937
  - 0.4399498999118805
  - 0.455433189868927
  - 0.5131844282150269
  - 0.45173922181129456
  - 0.4073938727378845
  - 0.39138513803482056
  - 0.4350029230117798
  - 0.4802692234516144
  - 0.5560222268104553
  - 0.4845942258834839
  - 0.42632466554641724
  - 0.4185638427734375
  - 0.41436767578125
  - 0.5228442549705505
  - 0.4272654354572296
  - 0.46714016795158386
  - 0.4093550443649292
  - 0.3933103084564209
  - 0.3880937099456787
  - 0.39381781220436096
  - 0.396105021238327
  - 0.3953358232975006
loss_records_fold2:
  train_losses:
  - 0.7436328500509263
  - 0.7457945406436921
  - 0.7136698007583618
  - 0.7477046191692353
  - 0.7910582840442658
  - 0.7518549859523773
  - 0.7489746868610383
  - 0.7280831873416901
  - 0.7623803913593292
  - 0.7361552894115448
  - 0.7327506065368653
  validation_losses:
  - 0.4345995783805847
  - 0.38649874925613403
  - 0.39379623532295227
  - 0.3883020281791687
  - 0.38398176431655884
  - 0.38703015446662903
  - 0.3850102424621582
  - 0.38146936893463135
  - 0.38501596450805664
  - 0.38017523288726807
  - 0.383767306804657
loss_records_fold3:
  train_losses:
  - 0.7279644072055818
  - 0.7153278559446336
  - 0.7833442091941833
  - 0.7198162615299225
  - 0.7460133850574494
  - 0.749754935503006
  - 0.7681252598762512
  - 0.8302625298500061
  - 0.7730546593666077
  - 0.7425135970115662
  - 0.7281775295734406
  - 0.7514178991317749
  - 0.743141269683838
  - 0.7406319797039033
  - 0.733557802438736
  - 0.7557425737380982
  - 0.7575933158397675
  - 0.7132314682006836
  - 0.7243965387344361
  - 0.7438519239425659
  - 0.7471525728702546
  - 0.7740800797939301
  - 0.7414630174636841
  - 0.7175706028938293
  - 0.7375777244567872
  - 0.7773385524749756
  - 0.756789129972458
  - 0.795458722114563
  - 0.7478067100048066
  - 0.7749232351779938
  - 0.7332591921091081
  - 0.7663303852081299
  - 0.7497450470924378
  - 0.7237846732139588
  - 0.7571398913860321
  - 0.7280189752578736
  - 0.7360282123088837
  - 0.7446762025356293
  - 0.7680650711059571
  - 0.7229662269353867
  - 0.7370203375816345
  - 0.7593902528285981
  - 0.7106151580810547
  - 0.7366725385189057
  - 0.7381223738193512
  - 0.7842484593391419
  - 0.7705783843994141
  - 0.7173395216464997
  - 0.7265957832336426
  - 0.7569477140903473
  - 0.7385407447814942
  - 0.7736046075820924
  - 0.7639122009277344
  - 0.7436802327632904
  - 0.7377832710742951
  - 0.7745758831501007
  - 0.7427466213703156
  - 0.7144826471805573
  - 0.759552824497223
  - 0.7052129566669465
  - 0.7430156469345093
  - 0.7493248462677002
  - 0.821959811449051
  - 0.7649826526641846
  - 0.7508921384811402
  - 0.7145373463630676
  - 0.7895428955554963
  - 0.7672487974166871
  - 0.7275485098361969
  - 0.7935932159423829
  - 0.7457651853561402
  - 0.7910471796989441
  - 0.7450018405914307
  - 0.7569955706596375
  - 0.7903246223926544
  - 0.7569371104240418
  - 0.7397003233432771
  - 0.7429808676242828
  - 0.7307909727096558
  - 0.7303088128566743
  - 0.7494153857231141
  - 0.7626199901103974
  - 0.7366741180419922
  - 0.7645952880382538
  - 0.7943452060222627
  - 0.7476200699806214
  - 0.7541841208934784
  - 0.7234390914440155
  - 0.7703403234481812
  validation_losses:
  - 0.47044146060943604
  - 0.5930355787277222
  - 0.5316049456596375
  - 0.6338467597961426
  - 0.39426442980766296
  - 0.3629685044288635
  - 0.3659878969192505
  - 0.39557960629463196
  - 0.49043571949005127
  - 0.748781144618988
  - 0.5786803960800171
  - 0.463554710149765
  - 0.4184304475784302
  - 0.5435522198677063
  - 0.5994058847427368
  - 0.4725366532802582
  - 0.5283058285713196
  - 0.45671963691711426
  - 0.6205902099609375
  - 0.7917707562446594
  - 0.6855446696281433
  - 0.7535340189933777
  - 0.5366460680961609
  - 0.37429285049438477
  - 0.3914974629878998
  - 0.4389948546886444
  - 0.38061439990997314
  - 0.3772262632846832
  - 0.3701806664466858
  - 0.37726882100105286
  - 0.4132653474807739
  - 0.49281764030456543
  - 0.37474340200424194
  - 0.4227410852909088
  - 0.5067652463912964
  - 0.533890962600708
  - 0.5466920137405396
  - 0.45993730425834656
  - 0.49991458654403687
  - 0.3948427736759186
  - 0.385538250207901
  - 0.4217933416366577
  - 0.37704023718833923
  - 0.6275457143783569
  - 0.5713979601860046
  - 0.7197194695472717
  - 0.9662730097770691
  - 0.5274277329444885
  - 0.6750764846801758
  - 0.7501081824302673
  - 0.5308104157447815
  - 1.200645923614502
  - 1.1504026651382446
  - 1.0936962366104126
  - 1.2011263370513916
  - 0.4231172204017639
  - 0.41574135422706604
  - 0.5024998188018799
  - 0.4283730685710907
  - 0.4820980727672577
  - 0.450436532497406
  - 0.4624786674976349
  - 0.45733821392059326
  - 0.39767760038375854
  - 0.40022578835487366
  - 0.3923729360103607
  - 0.3928605318069458
  - 0.4953491985797882
  - 0.4820551574230194
  - 0.5619354844093323
  - 0.3808958828449249
  - 0.37025317549705505
  - 0.39007794857025146
  - 0.3775867521762848
  - 0.37572866678237915
  - 0.44152554869651794
  - 0.36306872963905334
  - 0.4131532907485962
  - 0.36386406421661377
  - 0.38998541235923767
  - 0.3676949441432953
  - 0.3690398037433624
  - 0.4029041826725006
  - 0.3774057924747467
  - 0.37426814436912537
  - 0.370795339345932
  - 0.37138110399246216
  - 0.36673614382743835
  - 0.37141698598861694
loss_records_fold4:
  train_losses:
  - 0.7548702299594879
  - 0.7363893330097199
  - 0.7370971202850343
  - 0.7560177028179169
  - 0.7493817806243896
  - 0.8252459108829499
  - 0.780560314655304
  - 0.7519241154193879
  - 0.7253755331039429
  - 0.7641680777072907
  - 0.7626315414905549
  validation_losses:
  - 0.3807450830936432
  - 0.37014201283454895
  - 0.37221089005470276
  - 0.37655109167099
  - 0.3879927098751068
  - 0.3937076926231384
  - 0.3809037208557129
  - 0.3770724833011627
  - 0.37164023518562317
  - 0.37269890308380127
  - 0.3701692521572113
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 66 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 47 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 89 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.855917667238422, 0.8593481989708405,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8572409565407038
  mean_f1_accuracy: 0.0
  total_train_time: '0:17:59.651735'
