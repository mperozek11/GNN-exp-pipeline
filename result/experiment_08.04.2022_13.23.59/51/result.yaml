config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 18:39:23.784399'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/51/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/51/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/51/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/51/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/51/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/51/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/51/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/51/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/51/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/51/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 280.826328933239
  - 54.382577270269394
  - 82.59646786749363
  - 36.82304768264294
  - 41.549697533249855
  - 23.20039141178131
  - 68.50448688864708
  - 51.67742122709751
  - 39.83637824654579
  - 38.04542088508606
  - 62.94250513613224
  - 17.989948868751526
  - 16.519839078187943
  - 27.90043216943741
  - 17.477199345827103
  - 17.863619685173035
  - 22.67853769659996
  - 31.888090789318085
  - 26.757368952035904
  - 15.911141186952591
  - 17.20834544301033
  - 17.41738548874855
  - 16.734342381358147
  - 21.51894897222519
  - 16.692049652338028
  - 15.605845898389816
  - 16.054436922073364
  - 15.330004006624222
  - 18.68356315791607
  - 19.01682958006859
  - 15.675617545843124
  - 18.651426047086716
  - 22.30024391412735
  - 20.00454095005989
  - 15.91381986439228
  - 15.756503745913506
  - 19.85167720913887
  - 18.52067321538925
  - 16.303078025579453
  - 16.58328866958618
  - 30.46793895959854
  - 17.22199621796608
  - 17.366637259721756
  - 26.3249249458313
  - 16.655777752399445
  - 15.450846761465073
  - 16.01473292708397
  - 22.155977234244347
  - 16.75544574856758
  - 17.837694466114044
  - 17.81574410200119
  - 21.47167533636093
  - 27.300819903612137
  - 177.81946861743927
  - 128.16891703009605
  - 77.27687382698059
  - 67.59793657064438
  - 103.5012344121933
  - 33.620503664016724
  - 33.60969948768616
  - 24.415874123573303
  - 21.142681390047073
  - 22.983867898583412
  - 30.168903410434723
  - 24.105913788080215
  - 18.463969588279724
  - 16.142684400081635
  - 18.40377414226532
  - 18.558198302984238
  - 16.652231097221375
  - 23.938876032829285
  - 26.66710177063942
  - 19.865421295166016
  - 16.152555242180824
  - 18.257594227790833
  - 17.99681942164898
  - 18.678771659731865
  - 17.170622751116753
  - 17.931171968579292
  - 18.359359741210938
  - 16.8126313239336
  - 19.61799082159996
  - 15.302143976092339
  - 16.78275464475155
  - 15.578181192278862
  - 16.37475034594536
  - 18.15619769692421
  - 26.062044724822044
  - 121.72464361786842
  - 26.408533722162247
  - 22.770576685667038
  - 790.8186506032944
  - 1440.7916257977486
  - 558.5948008298874
  - 1005.5672859847546
  - 232.70612755417824
  - 97.3915826678276
  - 154.53924672305584
  - 157.12348198890686
  - 60.76335267722607
  validation_losses:
  - 0.43758368492126465
  - 5.713531970977783
  - 7.026375770568848
  - 0.5314692258834839
  - 1.0295872688293457
  - 0.4460884928703308
  - 0.4477917551994324
  - 7.7473368644714355
  - 0.48592737317085266
  - 0.4493352174758911
  - 0.5129387378692627
  - 0.41234466433525085
  - 1.5205795764923096
  - 0.4152776896953583
  - 0.5549037456512451
  - 15.41634464263916
  - 0.6323963403701782
  - 0.5236957669258118
  - 6.230113983154297
  - 519.765380859375
  - 0.4573286771774292
  - 0.4132952094078064
  - 0.541885256767273
  - 0.8101325631141663
  - 2262415.75
  - 2556743.25
  - 1591089.0
  - 0.475886732339859
  - 0.7539322376251221
  - 0.45291656255722046
  - 1307796.125
  - 0.7465498447418213
  - 0.39291465282440186
  - 0.4034033417701721
  - 0.40171706676483154
  - 0.4723934829235077
  - 1433618.625
  - 2184090.75
  - 0.53033047914505
  - 0.5963777899742126
  - 0.6728370189666748
  - 937361.375
  - 1716112.125
  - 1953403.875
  - 1039636.4375
  - 721624.25
  - 1371005.0
  - 0.5568127632141113
  - 934351.3125
  - 0.531686007976532
  - 0.7961791753768921
  - 545099.1875
  - 0.42436474561691284
  - 7.542994022369385
  - 0.38713783025741577
  - 0.4068029522895813
  - 0.4154822528362274
  - 0.40396440029144287
  - 0.4918452203273773
  - 0.6925559639930725
  - 0.6593687534332275
  - 0.67667555809021
  - 0.39488092064857483
  - 0.48652735352516174
  - 0.3896839916706085
  - 0.39724797010421753
  - 0.6663057208061218
  - 0.3889220058917999
  - 2567191.0
  - 16574140.0
  - 17982990.0
  - 21051482.0
  - 16411742.0
  - 19513510.0
  - 16184308.0
  - 16593402.0
  - 18040502.0
  - 14917409.0
  - 0.764858067035675
  - 0.42266297340393066
  - 126422.3984375
  - 362056.59375
  - 3650329.5
  - 4143283.25
  - 4575335.0
  - 2490453.5
  - 4477309.5
  - 0.4032759368419647
  - 4548.9365234375
  - 1693.0853271484375
  - 1859840.875
  - 951.4224243164062
  - 7.466770172119141
  - 16.91191291809082
  - 15.976322174072266
  - 35.85628890991211
  - 4.3646240234375
  - 1.0233255624771118
  - 0.9292957782745361
  - 0.543961226940155
loss_records_fold1:
  train_losses:
  - 44.95167472958565
  - 54.88630431890488
  - 54.12080901861191
  - 131.9725520312786
  - 494.7911489903927
  - 156.47103133797646
  - 56.82026615738869
  - 66.58450129628181
  - 90.68380528688431
  - 40.8191035091877
  - 67.66056582331657
  - 41.93359808623791
  - 90.24594074487686
  - 75.02616101503372
  - 76.02349352836609
  - 49.786792144179344
  - 46.394638389348984
  - 40.67129799723625
  - 43.15065082907677
  - 33.90867383778095
  - 26.783414110541344
  - 32.158167123794556
  - 39.75126540660858
  - 29.035430818796158
  - 37.6806525439024
  - 31.315931499004364
  - 28.06004072725773
  - 24.904008969664574
  - 26.457161590456963
  - 29.28138867020607
  - 21.617406249046326
  - 20.27916820347309
  - 30.410548985004425
  - 29.631038278341293
  - 47.22904786467552
  - 33.329158425331116
  - 60.334485590457916
  - 35.45128920674324
  - 23.634595781564713
  - 25.135377198457718
  - 23.005171984434128
  - 20.165133625268936
  - 57.36853560805321
  - 27.30986911058426
  - 27.021561712026596
  - 26.62741556763649
  - 19.606959581375122
  - 28.71730411052704
  - 36.136814922094345
  - 28.504768133163452
  - 26.53398685157299
  - 22.293554097414017
  - 32.42839789390564
  - 24.585026055574417
  - 22.256631672382355
  - 41.308474123477936
  - 20.245312869548798
  - 24.382660686969757
  - 22.24451169371605
  - 18.5063433945179
  - 26.9302476644516
  - 30.47089684009552
  - 20.20609974861145
  - 20.132350265979767
  - 20.665381997823715
  - 24.41763824224472
  - 19.178480207920074
  - 19.309875771403313
  - 18.06013560295105
  - 20.70599789917469
  - 20.59374898672104
  - 20.86648142337799
  - 22.322900146245956
  - 58.279148519039154
  - 40.58015835285187
  - 19.50274494290352
  - 17.634203359484673
  - 17.32853877544403
  - 23.541529715061188
  - 15.893792122602463
  - 21.412015080451965
  - 26.140192463994026
  - 17.4150670170784
  - 15.416158810257912
  - 16.004013776779175
  - 20.237170666456223
  - 19.521687507629395
  - 16.558838352560997
  - 17.881933480501175
  - 15.720264613628387
  - 17.81582011282444
  - 18.648277133703232
  - 15.589398995041847
  - 19.114245533943176
  - 18.96711128950119
  - 16.018881231546402
  - 17.82476833462715
  - 17.933987125754356
  - 20.691791772842407
  - 28.03293204307556
  validation_losses:
  - 0.8349341154098511
  - 440.9285583496094
  - 1.798745036125183
  - 2.0037784576416016
  - 15.363040924072266
  - 0.40919655561447144
  - 3.9873244762420654
  - 1.0719997882843018
  - 1.381527304649353
  - 0.786496639251709
  - 0.4405716061592102
  - 1.0801591873168945
  - 2.0608572959899902
  - 1.1211644411087036
  - 1.3196032047271729
  - 0.8852411508560181
  - 0.45756807923316956
  - 0.7261736989021301
  - 0.9480912685394287
  - 0.664542019367218
  - 0.7328490614891052
  - 1.0076230764389038
  - 0.6457729339599609
  - 0.6080560088157654
  - 0.5876994729042053
  - 0.6066440343856812
  - 0.7511655688285828
  - 0.5330140590667725
  - 0.645203709602356
  - 0.5608630776405334
  - 0.4615004360675812
  - 0.4960412085056305
  - 0.4208015501499176
  - 0.7336782813072205
  - 0.6671442985534668
  - 2.2196481227874756
  - 1.4514635801315308
  - 0.45989540219306946
  - 1.0124298334121704
  - 0.5461664795875549
  - 0.5593605041503906
  - 0.6091198325157166
  - 0.8722423911094666
  - 0.9022796750068665
  - 1.280905842781067
  - 0.6063901782035828
  - 1.0308500528335571
  - 0.623284101486206
  - 0.8257988691329956
  - 0.48247313499450684
  - 0.4779577851295471
  - 0.494297057390213
  - 14.823254585266113
  - 0.7874106168746948
  - 0.44503268599510193
  - 0.5241511464118958
  - 0.8702512979507446
  - 1.1034427881240845
  - 0.4539684057235718
  - 0.6155136823654175
  - 0.48829853534698486
  - 1.062159538269043
  - 0.40760499238967896
  - 0.8463828563690186
  - 0.5274839401245117
  - 0.5083338022232056
  - 0.4942159056663513
  - 0.442545086145401
  - 0.4118667542934418
  - 0.5105273723602295
  - 0.44820889830589294
  - 0.7375879883766174
  - 0.4098150134086609
  - 7.464017391204834
  - 11.876346588134766
  - 0.5235814452171326
  - 0.6128374934196472
  - 0.5062946081161499
  - 0.40399467945098877
  - 0.6575424075126648
  - 1.3008930683135986
  - 0.5259619355201721
  - 0.48127657175064087
  - 0.40243130922317505
  - 0.4672030508518219
  - 0.8414855599403381
  - 0.4267021417617798
  - 0.6718786358833313
  - 0.41985443234443665
  - 0.42387232184410095
  - 0.5132565498352051
  - 0.41539466381073
  - 0.4169156551361084
  - 0.420955091714859
  - 0.4086886942386627
  - 1.031199336051941
  - 0.45324262976646423
  - 0.6602945923805237
  - 0.7140547037124634
  - 17379952640.0
loss_records_fold2:
  train_losses:
  - 19.540355384349823
  - 17.475652158260345
  - 24.859251260757446
  - 19.932324558496475
  - 16.849315881729126
  - 15.31373442709446
  - 17.33877569437027
  - 17.78355175256729
  - 17.54818457365036
  - 21.227893203496933
  - 17.473230436444283
  - 16.85859414935112
  - 19.72327959537506
  - 28.967205435037613
  - 21.2660451233387
  - 25.640572637319565
  - 16.796901285648346
  - 20.95618672668934
  - 16.489194601774216
  - 22.472641319036484
  - 24.417486771941185
  - 16.06909967958927
  - 16.7393856048584
  - 15.40920965373516
  - 16.649312645196915
  - 15.767482191324234
  - 15.798976346850395
  - 15.003990978002548
  - 16.25457574427128
  - 19.160419166088104
  - 20.058615416288376
  - 22.90713334083557
  - 23.83289670944214
  - 16.124031335115433
  - 16.17558777332306
  - 19.631833881139755
  - 20.861344948410988
  - 15.760551452636719
  - 17.110870629549026
  - 16.913150936365128
  - 15.632911503314972
  - 17.718970865011215
  - 17.206043675541878
  - 17.89219108223915
  - 17.38166704773903
  - 21.212778478860855
  - 16.72367963194847
  - 15.365873351693153
  - 15.382416844367981
  - 19.2545555382967
  - 24.73143720626831
  - 17.036680966615677
  - 15.99661323428154
  - 16.824671909213066
  - 16.12434095144272
  - 23.721357613801956
  - 18.8690467774868
  - 17.78794500231743
  - 16.13669303059578
  - 18.932430490851402
  - 19.659157037734985
  - 23.991891235113144
  - 15.835154414176941
  - 16.185808926820755
  - 21.999630458652973
  - 18.47200071811676
  - 15.580493956804276
  - 15.554175168275833
  - 24.591602653265
  - 18.944035977125168
  - 17.04192066192627
  - 24.04916599392891
  - 19.263130083680153
  - 21.919393748044968
  - 16.093674063682556
  - 18.282525569200516
  - 16.543102711439133
  - 21.682629331946373
  - 19.968227803707123
  - 15.415803998708725
  - 18.68187664449215
  - 15.379043906927109
  - 15.513563685119152
  - 17.8817707747221
  - 15.497653231024742
  - 16.526681438088417
  - 19.147384822368622
  - 21.89435812830925
  - 17.638047769665718
  - 24.29248809814453
  - 16.996637225151062
  - 20.494636982679367
  - 15.977798730134964
  - 16.990280464291573
  - 16.983382001519203
  - 16.365469127893448
  - 15.883101716637611
  - 15.651077598333359
  - 17.50755962729454
  - 16.943338751792908
  validation_losses:
  - 1720417408.0
  - 2255557632.0
  - 2358946560.0
  - 2425224192.0
  - 1832367744.0
  - 1746770048.0
  - 1375284096.0
  - 7373008384.0
  - 1382841984.0
  - 1295489792.0
  - 1106702208.0
  - 1270470528.0
  - 1223486336.0
  - 627756544.0
  - 1008114560.0
  - 1111928576.0
  - 995789504.0
  - 1000788224.0
  - 1192224000.0
  - 1100716416.0
  - 1574700160.0
  - 961972480.0
  - 2153463552.0
  - 900989760.0
  - 1152365952.0
  - 1297711744.0
  - 4447833600.0
  - 1375371136.0
  - 2399600128.0
  - 2139687296.0
  - 2007492480.0
  - 3114914816.0
  - 1862022144.0
  - 2403553024.0
  - 2268265984.0
  - 2914831616.0
  - 2458229760.0
  - 2598323200.0
  - 2750344704.0
  - 2609359616.0
  - 3247766784.0
  - 2695333632.0
  - 2614273536.0
  - 2467394304.0
  - 3065590272.0
  - 3172105216.0
  - 2961208576.0
  - 2473164032.0
  - 2728196864.0
  - 3006746112.0
  - 2554473216.0
  - 2843647744.0
  - 2337131520.0
  - 2390661632.0
  - 2279070208.0
  - 2897451264.0
  - 2201857792.0
  - 2628601600.0
  - 3005024000.0
  - 2106448896.0
  - 2757898240.0
  - 2595888128.0
  - 2426648576.0
  - 2784737280.0
  - 3800199680.0
  - 2120301184.0
  - 2514963200.0
  - 2799574016.0
  - 2108313600.0
  - 2618334720.0
  - 3504345600.0
  - 2871481344.0
  - 1747073408.0
  - 3438750720.0
  - 2716092672.0
  - 2384953856.0
  - 3597708800.0
  - 2460775936.0
  - 2283055872.0
  - 2151987712.0
  - 2660932096.0
  - 2498887424.0
  - 2764867328.0
  - 2739149824.0
  - 2030159232.0
  - 2300396032.0
  - 2861215744.0
  - 2270330880.0
  - 2179776000.0
  - 1497319808.0
  - 2506525184.0
  - 2119602048.0
  - 0.5888329148292542
  - 0.38421177864074707
  - 0.3998069167137146
  - 0.4589637517929077
  - 0.3811631500720978
  - 0.4006781578063965
  - 0.6139280796051025
  - 10290848768.0
loss_records_fold3:
  train_losses:
  - 16.591788232326508
  - 15.798729658126831
  - 15.005635201931
  - 21.923819214105606
  - 18.85545127093792
  - 16.0342555642128
  - 17.249667659401894
  - 15.785032853484154
  - 22.169838786125183
  - 17.932795509696007
  - 16.743947058916092
  - 16.40103754401207
  - 16.804223880171776
  - 16.525625497102737
  - 19.829567775130272
  - 16.251440346240997
  - 19.875827968120575
  - 26.42161723971367
  - 19.651512563228607
  - 19.479989111423492
  - 20.75970897078514
  - 15.86440497636795
  - 15.371936485171318
  - 16.487575098872185
  - 16.36384615302086
  - 15.592921316623688
  - 16.80734872817993
  - 16.341841399669647
  - 16.26180273294449
  - 17.932157963514328
  - 22.80603951215744
  - 16.57157751917839
  - 16.437396436929703
  - 15.272929102182388
  - 15.872281551361084
  - 15.242197632789612
  - 16.580009669065475
  - 16.230665177106857
  - 18.050839245319366
  - 15.667202353477478
  - 17.21443834900856
  - 19.422703370451927
  - 21.621566265821457
  - 19.29219552874565
  - 16.19987055659294
  - 27.083153277635574
  - 22.553793907165527
  - 19.522335425019264
  - 15.517858743667603
  - 15.950802713632584
  - 16.863392114639282
  - 18.622425973415375
  - 17.79922966659069
  - 16.56388658285141
  - 29.053083956241608
  - 17.80582921206951
  - 16.763178393244743
  - 21.517903372645378
  - 17.587863117456436
  - 17.698446810245514
  - 15.713772267103195
  - 15.944519937038422
  - 16.293855100870132
  - 18.137431621551514
  - 20.765308380126953
  - 17.174217849969864
  - 17.61035990715027
  - 16.52275186777115
  - 18.384805232286453
  - 27.6066035926342
  - 30.634975850582123
  - 19.91277527809143
  - 22.228049650788307
  - 28.18945810198784
  - 24.655665278434753
  - 16.25103424489498
  - 15.520301163196564
  - 18.487880766391754
  - 17.07695883512497
  - 15.33223381638527
  - 26.543206304311752
  - 23.484383195638657
  - 15.404293656349182
  - 17.57908035814762
  - 17.79000359773636
  - 22.350060284137726
  - 16.2535363137722
  - 17.586225405335426
  - 21.35758861899376
  - 18.586534827947617
  - 16.43398106098175
  - 17.62621122598648
  - 15.724747568368912
  - 16.425808936357498
  - 15.273787066340446
  - 16.244854778051376
  - 16.532441824674606
  - 15.922870606184006
  - 15.321175634860992
  - 17.677602410316467
  validation_losses:
  - 2806947072.0
  - 3150954496.0
  - 4141504768.0
  - 3438529280.0
  - 3208528640.0
  - 0.5920993089675903
  - 0.6589487791061401
  - 0.47043856978416443
  - 0.4582185745239258
  - 0.638839066028595
  - 895254.875
  - 1170638848.0
  - 4575593984.0
  - 4503606272.0
  - 4.73748851392512e+16
  - 3.144804191189336e+18
  - 3.636369176811012e+18
  - 4.632335946308649e+18
  - 3.894169744030302e+18
  - 4.718470587716993e+18
  - 4.1466574710641787e+18
  - 4.81918090501895e+18
  - 3.821749860911022e+18
  - 3.9006626350702264e+18
  - 4.238351243262558e+18
  - 3.87501212830584e+18
  - 4.0904562090883154e+18
  - 3.7033151412914094e+18
  - 3.903943302889603e+18
  - 3.956539815993803e+18
  - 4.591129823912395e+18
  - 3.870019520882016e+18
  - 3.8054361320118026e+18
  - 4.325553510461473e+18
  - 4.143321827663413e+18
  - 4.140462272797475e+18
  - 3.5958367801646776e+18
  - 3.3069651138560655e+18
  - 4.5552307692655084e+18
  - 4.2155157611431854e+18
  - 3.6525949447800095e+18
  - 4.727627320553112e+18
  - 3.6874252741246976e+18
  - 4.160503895870669e+18
  - 4.22584127483963e+18
  - 4.700247831754047e+18
  - 5.019386029518356e+18
  - 4.1681092177999954e+18
  - 3.5237418027314053e+18
  - 4.425940571344863e+18
  - 4.735131487412683e+18
  - 4.5466455075979264e+18
  - 3.950416910616625e+18
  - 3.766533486475739e+18
  - 4.5154457656481546e+18
  - 4.079615299316351e+18
  - 4.309016580701815e+18
  - 4.797965278405198e+18
  - 5.090480451370353e+18
  - 4.1348786778737213e+18
  - 4.390757298767659e+18
  - 3.848407520326451e+18
  - 3.818148135696335e+18
  - 4.313506711311745e+18
  - 4.2885736358844826e+18
  - 3.742344505542574e+18
  - 4.3180883762646876e+18
  - 4.542744440342577e+18
  - 4.0379839407980544e+18
  - 3.9637861473766605e+18
  - 4.724503058262786e+18
  - 3.6288768297014395e+18
  - 4.370064215055008e+18
  - 4.20155196347043e+18
  - 3.730970882386952e+18
  - 4.10074571377895e+18
  - 4.249362302458921e+18
  - 4.021516555148853e+18
  - 4.0589620729002066e+18
  - 4.1923814867389645e+18
  - 3.8252388861838623e+18
  - 3.9634329292662374e+18
  - 4.2701955739041137e+18
  - 3.8158270666500997e+18
  - 3.934287899670872e+18
  - 4.107995068818784e+18
  - 4.2766362381417185e+18
  - 3.755503460703797e+18
  - 4.0492720769246167e+18
  - 3.8722707709398876e+18
  - 4.1914034711460577e+18
  - 3.7586002352034284e+18
  - 4.5856292421165384e+18
  - 4.1169670837014364e+18
  - 4.866807900443509e+18
  - 4.406895930440155e+18
  - 3.894984482146484e+18
  - 4.930634550435906e+18
  - 4.213496508038775e+18
  - 4.0246169030612746e+18
loss_records_fold4:
  train_losses:
  - 22.03164142370224
  - 22.71701891720295
  - 34.51275786757469
  - 54.32405139505863
  - 18.673150926828384
  - 16.565039783716202
  - 16.225591897964478
  - 15.64395210146904
  - 17.40192538499832
  - 16.91886693239212
  - 17.1925807595253
  - 20.042436107993126
  - 16.407201528549194
  - 16.990536093711853
  - 22.176582157611847
  - 21.639990597963333
  - 17.33300307393074
  - 20.522403687238693
  - 20.93430794775486
  - 17.936765491962433
  - 18.08949339389801
  - 16.053583547472954
  - 21.56703272461891
  - 19.801690191030502
  - 18.217050194740295
  - 16.36378389596939
  - 15.085717871785164
  - 17.61870840191841
  - 16.333374366164207
  - 18.050309270620346
  - 15.33712187409401
  - 15.890387684106827
  - 17.687435030937195
  - 20.597896233201027
  - 16.77551507949829
  - 15.862678527832031
  - 23.167722284793854
  - 28.947338819503784
  - 19.085885763168335
  - 17.221236556768417
  - 16.593919336795807
  - 22.643459141254425
  - 16.267884612083435
  - 15.412194103002548
  - 17.02856332063675
  - 15.56663852930069
  - 16.6566521525383
  - 17.12785053253174
  - 16.409045472741127
  - 15.880635231733322
  - 24.63974779844284
  - 18.40059518069029
  - 20.62985000014305
  - 15.549102172255516
  - 16.671421229839325
  - 22.873665690422058
  - 18.354330390691757
  - 18.759142369031906
  - 17.80294731259346
  - 16.571366742253304
  - 15.730956345796585
  - 15.059373199939728
  - 17.017185747623444
  - 18.81559480726719
  - 23.622772186994553
  - 23.310374319553375
  - 19.853249043226242
  - 18.940427899360657
  - 33.69263079762459
  - 17.629415959119797
  - 17.212629303336143
  - 22.111244410276413
  - 16.967635571956635
  - 17.879438370466232
  - 16.254923820495605
  - 14.983038529753685
  - 16.021778479218483
  - 15.79565717279911
  - 16.43559241294861
  - 15.94562116265297
  - 18.008935689926147
  - 15.37087069451809
  - 17.469350963830948
  - 16.13652166724205
  - 22.499900728464127
  - 19.69602137990296
  - 22.492870330810547
  - 21.449274346232414
  - 16.807030111551285
  - 15.797937333583832
  - 16.726590782403946
  - 15.731613963842392
  - 18.507336497306824
  - 21.227476179599762
  - 16.011225014925003
  - 16.39298242330551
  - 16.44239643216133
  - 22.532407373189926
  - 24.791145831346512
  - 18.51421570777893
  validation_losses:
  - 1.81703559875448e+18
  - 1.6197040478942659e+18
  - 1.6661804044003574e+18
  - 1.7161033175426662e+18
  - 1.6927733300687012e+18
  - 1.8941115013005312e+18
  - 1.6994832372161577e+18
  - 1.851312873750397e+18
  - 1.6211422091033969e+18
  - 1.8002287388902031e+18
  - 1.8634734723535995e+18
  - 1.9859574184204042e+18
  - 0.39242950081825256
  - 0.6450632810592651
  - 0.5163583755493164
  - 4436515727867904.0
  - 5.859052823081124e+17
  - 1.6452215137516913e+18
  - 1.680446018014937e+18
  - 1.818280383356076e+18
  - 1.420014869226193e+18
  - 1.6603962860434883e+18
  - 1.607557605503271e+18
  - 1.7178967584465224e+18
  - 1.83702362063582e+18
  - 1.6968912759926292e+18
  - 1.7343439405695631e+18
  - 1.7689604149026488e+18
  - 1.6456350675626885e+18
  - 1.695407347611992e+18
  - 1.750989447102464e+18
  - 1.772631134471979e+18
  - 1.9085805245662495e+18
  - 1.5853974984012595e+18
  - 1.8091198022292603e+18
  - 1.7947306309955092e+18
  - 1.742368038990119e+18
  - 1.5164758489647022e+18
  - 1.5089980703841976e+18
  - 1.8802171102992794e+18
  - 1.8967198177595228e+18
  - 1.7229363699924337e+18
  - 1.75329828408184e+18
  - 1.4815285590706094e+18
  - 1.6656923586765783e+18
  - 1.6418075301474468e+18
  - 1.7069823188344504e+18
  - 1.8307959867760968e+18
  - 1.5647489449095332e+18
  - 1.8939310439546225e+18
  - 1.6170807505893458e+18
  - 1.546639301205295e+18
  - 1.7538326467329393e+18
  - 1.7128292467930563e+18
  - 1.8229053415793623e+18
  - 1.6423303479264543e+18
  - 1.5932888307927613e+18
  - 1.6783247852070502e+18
  - 1.6783324817884447e+18
  - 1.725708238806057e+18
  - 1.6529553411025142e+18
  - 1.7604295790606418e+18
  - 1.4896337464737137e+18
  - 1.6321936753520804e+18
  - 1.542541421368574e+18
  - 1.6513473053468918e+18
  - 1.6577679034972897e+18
  - 1.9396277092776673e+18
  - 1.5820729875557253e+18
  - 1.754150817910227e+18
  - 1.764189084193915e+18
  - 1.6450811885801964e+18
  - 1.6290652898931507e+18
  - 1.6148495666186813e+18
  - 1.6653391405661553e+18
  - 1.6137427707263713e+18
  - 1.5661071166477435e+18
  - 1.5247241103183708e+18
  - 1.636394496964952e+18
  - 1.7570746942063903e+18
  - 1.9325054852697948e+18
  - 1.7390428409498173e+18
  - 1.833300536825217e+18
  - 1.6757482171463107e+18
  - 1.66804971160653e+18
  - 1.5830725810643272e+18
  - 1.674121764570923e+18
  - 1.6404062025778463e+18
  - 1.8691689425854792e+18
  - 1.7249126047044076e+18
  - 1.7876720412230943e+18
  - 1.579450514884526e+18
  - 1.348916186766639e+18
  - 1.6601517821452616e+18
  - 1.444012535136125e+18
  - 1.6259961406231675e+18
  - 1.71456084016785e+18
  - 1.532728829846487e+18
  - 1.7082338379447665e+18
  - 1.4795394050970092e+18
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.14408233276157806, 0.8576329331046312, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.24962406015037594, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.7155605854302606
  mean_f1_accuracy: 0.04992481203007519
  total_train_time: '0:11:26.570666'
