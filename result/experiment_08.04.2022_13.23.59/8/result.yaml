config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 14:12:32.789820'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/8/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/8/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/8/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/8/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/8/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/8/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/8/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/8/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/8/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/8/fold_4_state_dict.pt
loss_records_fold3:
  train_losses:
  - 29.8963912576437
  - 30.173577055335045
  - 30.553974330425262
  - 30.082559123635292
  - 30.18080846965313
  - 29.847676277160645
  - 30.161802023649216
  - 29.427370384335518
  - 30.854669719934464
  - 30.29897826910019
  - 29.882830813527107
  - 30.481530636548996
  - 30.63524840772152
  - 29.89160504937172
  - 30.186124682426453
  - 29.625287294387817
  - 29.90208189189434
  - 30.14042204618454
  - 30.16984149813652
  - 30.56455224752426
  - 29.414814069867134
  - 30.111126109957695
  - 29.89915180206299
  - 29.905993789434433
  - 29.458399012684822
  - 29.45491473376751
  - 29.88111187517643
  - 30.324365124106407
  - 29.320174172520638
  - 29.719475284218788
  - 30.400974854826927
  - 29.639324948191643
  - 30.784845992922783
  - 30.593034476041794
  - 30.400093615055084
  - 29.861247956752777
  - 30.203809276223183
  - 30.484011188149452
  - 29.307879745960236
  - 29.461125403642654
  - 30.026850774884224
  - 29.813688784837723
  - 29.364276349544525
  - 29.513125985860825
  - 30.04760892689228
  - 29.481922507286072
  - 30.53647328913212
  - 29.529742553830147
  - 30.177711367607117
  - 29.40314155817032
  - 30.29264724254608
  - 29.68544030189514
  - 29.86212219297886
  - 30.175230652093887
  - 29.983138740062714
  - 30.464034125208855
  - 30.31767624616623
  - 29.839466527104378
  - 29.99015963077545
  - 30.17969235777855
  - 29.61039336025715
  - 30.012115702033043
  - 29.578834235668182
  - 29.19133371114731
  - 29.768465608358383
  - 28.8092320561409
  - 29.6235990524292
  - 29.576166316866875
  - 29.53478229045868
  - 30.293971732258797
  - 29.84343510866165
  - 29.8435148447752
  - 29.53845450282097
  - 28.888110175728798
  - 29.085435658693314
  - 29.252303421497345
  - 30.020632699131966
  - 29.917832493782043
  - 29.79262824356556
  - 29.295540660619736
  - 30.839453876018524
  - 29.580039978027344
  - 30.251717939972878
  - 30.35939632356167
  - 29.97830781340599
  - 30.69434282183647
  - 30.188368886709213
  - 30.198432847857475
  - 30.205132991075516
  - 29.24009495973587
  - 29.867681249976158
  - 29.658505871891975
  - 29.584276542067528
  - 29.980015411973
  - 29.240575164556503
  - 30.51399652659893
  - 29.5652247518301
  - 29.968534886837006
  - 29.71967524290085
  - 29.149170130491257
  validation_losses:
  - 0.39259207248687744
  - 0.4267627000808716
  - 0.42421114444732666
  - 0.4290773868560791
  - 0.5024198293685913
  - 0.39936062693595886
  - 0.39656174182891846
  - 0.42899152636528015
  - 0.39420875906944275
  - 0.4023783504962921
  - 0.46707046031951904
  - 0.5045825242996216
  - 0.4375840127468109
  - 0.6331438422203064
  - 0.40943488478660583
  - 0.4109962582588196
  - 0.8890655636787415
  - 0.6600279211997986
  - 0.5719281435012817
  - 0.5763299465179443
  - 0.6797341704368591
  - 1.339446783065796
  - 1.547827124595642
  - 0.6724505424499512
  - 1.0929129123687744
  - 0.4417969286441803
  - 0.507810115814209
  - 0.41280117630958557
  - 0.39561644196510315
  - 0.5308740139007568
  - 0.42032286524772644
  - 0.6455079317092896
  - 0.40090110898017883
  - 0.3996790051460266
  - 0.4036046266555786
  - 0.37858065962791443
  - 0.3960471451282501
  - 0.48194313049316406
  - 0.5489879250526428
  - 0.5246967673301697
  - 0.4847610592842102
  - 0.5208682417869568
  - 0.6398043632507324
  - 0.5231572985649109
  - 0.45183035731315613
  - 0.5794320106506348
  - 1.473019003868103
  - 6.351622581481934
  - 0.43985751271247864
  - 0.5165849328041077
  - 2.7732858657836914
  - 0.4984097480773926
  - 0.4777485728263855
  - 0.5850619673728943
  - 0.4413982629776001
  - 0.4467799663543701
  - 0.4747050702571869
  - 0.45477548241615295
  - 0.49363765120506287
  - 0.4226228892803192
  - 1.4707221984863281
  - 0.453495055437088
  - 0.6513324975967407
  - 0.964080810546875
  - 0.5572113990783691
  - 0.5434285402297974
  - 0.5149660110473633
  - 0.5465241074562073
  - 1.9807732105255127
  - 0.46831685304641724
  - 1.705880880355835
  - 1.0286072492599487
  - 0.4410474896430969
  - 0.5448659658432007
  - 0.6583117842674255
  - 0.6207829117774963
  - 0.5621137022972107
  - 0.6235724687576294
  - 0.5658997297286987
  - 0.5508426427841187
  - 0.5266962051391602
  - 0.9547114968299866
  - 0.7551332712173462
  - 0.3982924520969391
  - 0.7136337161064148
  - 1.0820224285125732
  - 1.0383325815200806
  - 0.3976137340068817
  - 0.8375458717346191
  - 0.3780865967273712
  - 0.39819827675819397
  - 0.3974323272705078
  - 0.40597447752952576
  - 0.40217190980911255
  - 0.5140285491943359
  - 0.3865091800689697
  - 0.3910348117351532
  - 0.4158220589160919
  - 0.4031151831150055
  - 0.5110270380973816
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 42 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:05:49.016586'
