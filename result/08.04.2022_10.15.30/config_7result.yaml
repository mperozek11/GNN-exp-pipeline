config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 10:15:54.090323'
fold_0_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_7fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_7fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_7fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_7fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_7fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_7fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_7fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_7fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_7fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_7fold_4_state_dict.pt
loss_records:
  train_losses:
  - 15.387362226843834
  - 15.180053323507309
  - 15.200834274291992
  - 15.228732585906982
  - 15.565605372190475
  - 15.504932165145874
  - 15.662779718637466
  - 16.33608204126358
  - 15.282571092247963
  - 15.135465562343597
  - 15.393711671233177
  - 15.7609244287014
  - 15.261157155036926
  - 15.28102833032608
  - 15.747786819934845
  - 15.309174448251724
  - 15.235997557640076
  - 15.27598387002945
  - 15.316767573356628
  - 15.245925784111023
  - 15.218333542346954
  - 15.322069212794304
  - 15.41064304113388
  - 15.311310917139053
  - 15.336376786231995
  - 15.275994181632996
  - 15.288900673389435
  - 15.314972281455994
  - 15.325612753629684
  - 15.528868734836578
  - 15.92444209754467
  - 15.69022387266159
  - 15.536680847406387
  - 15.594665288925171
  - 15.735756933689117
  - 15.26595276594162
  - 15.516921043395996
  - 15.66497665643692
  - 16.09000214934349
  - 15.278361171483994
  - 15.31068629026413
  - 15.237901613116264
  - 15.257390692830086
  - 15.277786403894424
  - 15.239383339881897
  - 15.1221823990345
  - 15.309233039617538
  - 15.439806163311005
  - 15.280375361442566
  - 15.187014728784561
  - 15.400075823068619
  - 15.99641638994217
  - 15.33496841788292
  - 15.1882826089859
  - 15.287845581769943
  - 15.491454362869263
  - 15.363250076770782
  - 15.18034291267395
  - 15.286501318216324
  - 15.40106438100338
  - 16.05838133394718
  - 15.642197579145432
  - 15.70618286728859
  - 15.360289692878723
  - 15.219280898571014
  - 15.4509616792202
  - 15.18997436761856
  - 15.536736696958542
  - 15.651008158922195
  - 16.005698293447495
  - 15.710821568965912
  - 15.572548285126686
  - 15.378767937421799
  - 15.342603385448456
  - 15.136288851499557
  - 15.512973487377167
  - 15.453470289707184
  - 15.259634375572205
  - 15.311535209417343
  - 15.389325648546219
  - 15.279153823852539
  - 15.395303130149841
  - 15.314703822135925
  - 15.349121510982513
  - 15.349683612585068
  - 15.551935985684395
  - 15.355151355266571
  - 15.160008937120438
  - 15.835847198963165
  - 16.419124335050583
  - 15.32382270693779
  - 15.419064432382584
  - 15.469806224107742
  - 15.394025340676308
  - 15.335308760404587
  - 15.475345462560654
  - 15.262420564889908
  - 15.298609405755997
  - 15.200430065393448
  - 15.331570357084274
  validation_losses:
  - 6535743799296.0
  - 10903974051840.0
  - 9418729062400.0
  - 12581239521280.0
  - 9880662441984.0
  - 11685204066304.0
  - 14911316951040.0
  - 13982710628352.0
  - 13597307568128.0
  - 13063765884928.0
  - 13006627930112.0
  - 7076009476096.0
  - 12288365953024.0
  - 8819285426176.0
  - 15027944816640.0
  - 9315958128640.0
  - 13383996801024.0
  - 14625727840256.0
  - 11673238765568.0
  - 11938186657792.0
  - 11115255824384.0
  - 8839288061952.0
  - 13304039735296.0
  - 13521015275520.0
  - 12416310050816.0
  - 8961408368640.0
  - 13041159634944.0
  - 13262256078848.0
  - 13631291916288.0
  - 11853269827584.0
  - 12010097999872.0
  - 10210051620864.0
  - 12083197378560.0
  - 11219972915200.0
  - 9636237279232.0
  - 7540542799872.0
  - 11289036324864.0
  - 11539037814784.0
  - 10729551822848.0
  - 8826753384448.0
  - 10905835274240.0
  - 12144216113152.0
  - 13239387684864.0
  - 12741574131712.0
  - 5640627945472.0
  - 10969906413568.0
  - 14283256627200.0
  - 8144244178944.0
  - 11875503833088.0
  - 14050621652992.0
  - 12662850191360.0
  - 12134100500480.0
  - 8831940689920.0
  - 10314611425280.0
  - 11433744007168.0
  - 9380569284608.0
  - 12103877394432.0
  - 8293650006016.0
  - 9833613885440.0
  - 11039824412672.0
  - 11709985062912.0
  - 10018794504192.0
  - 12997695111168.0
  - 9478815612928.0
  - 7536753770496.0
  - 11405559332864.0
  - 10506340401152.0
  - 10087094550528.0
  - 7339679678464.0
  - 9183611060224.0
  - 11034082410496.0
  - 12959145263104.0
  - 10120733917184.0
  - 11008845283328.0
  - 14649617547264.0
  - 10608800956416.0
  - 8396664733696.0
  - 12306013487104.0
  - 12456715878400.0
  - 11976125186048.0
  - 9181566337024.0
  - 11931504082944.0
  - 9271466000384.0
  - 12478558765056.0
  - 13501287366656.0
  - 11961233309696.0
  - 14075895480320.0
  - 8862514020352.0
  - 10814200217600.0
  - 7923288768512.0
  - 11280818634752.0
  - 13892402020352.0
  - 10565665685504.0
  - 12497303109632.0
  - 9976256921600.0
  - 9102306574336.0
  - 15105073872896.0
  - 14872219746304.0
  - 11589265653760.0
  - 9716715487232.0
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 53 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 60 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.1423670668953688, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.2492492492492493, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.7155605854302606
  mean_f1_accuracy: 0.04984984984984986
  total_train_time: '0:17:53.725659'
