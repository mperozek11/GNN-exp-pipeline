config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:30:57.428753'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_90fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_90fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_90fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_90fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_90fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_90fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_90fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_90fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_90fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_90fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 19.931332230567932
  - 6.073579174280167
  - 3.5267030179500582
  - 3.5170155793428424
  - 3.882928240299225
  - 2.4805010855197906
  - 3.795758563280106
  - 3.2859476268291474
  - 5.35859860777855
  - 2.011694180965424
  - 2.014467918872833
  - 2.8723635554313662
  - 1.7445931434631348
  - 2.1712138295173644
  - 2.6084302902221683
  - 4.30837881565094
  - 10.353374058008194
  - 6.390565168857575
  - 1.734771865606308
  - 1.6251271426677705
  - 1.7908758878707887
  - 1.6315420031547547
  - 4.125219136476517
  - 3.849448722600937
  - 1.9085511267185211
  - 1.7154608368873596
  - 1.889686954021454
  - 1.9529099941253663
  - 2.88136659860611
  - 2.9135510563850406
  - 2.126632460951805
  - 2.1990131735801697
  - 1.738519984483719
  - 2.732070130109787
  - 2.4872456729412082
  - 2.600101256370545
  - 2.262343603372574
  - 1.5890576720237732
  - 1.8802360832691194
  - 2.23090980052948
  - 2.3154741406440738
  - 1.7090802520513535
  - 1.806731787323952
  - 5.35006611943245
  - 1.6636263608932496
  validation_losses:
  - 8.168265342712402
  - 1.2577524185180664
  - 0.677644670009613
  - 0.8486409187316895
  - 0.9046499133110046
  - 0.7471129298210144
  - 0.5293465852737427
  - 0.5489591360092163
  - 0.5723534822463989
  - 0.5463635921478271
  - 0.6754871010780334
  - 0.6006430983543396
  - 0.520720899105072
  - 0.42897269129753113
  - 0.4405646026134491
  - 4.31060791015625
  - 0.9605292081832886
  - 0.6752657890319824
  - 0.4139079749584198
  - 0.550640881061554
  - 0.39848971366882324
  - 0.4507228136062622
  - 0.42566782236099243
  - 0.5357847213745117
  - 0.38759249448776245
  - 0.44524410367012024
  - 0.42430341243743896
  - 0.3825695216655731
  - 0.5775468945503235
  - 0.4997694790363312
  - 0.9036063551902771
  - 0.4116230010986328
  - 0.4216385781764984
  - 0.3887062668800354
  - 0.40450987219810486
  - 0.4006408751010895
  - 0.3865812420845032
  - 0.3857294023036957
  - 0.4096716046333313
  - 0.4006141126155853
  - 0.39365559816360474
  - 0.3985573649406433
  - 0.3986186385154724
  - 0.39176860451698303
  - 0.39144036173820496
loss_records_fold1:
  train_losses:
  - 1.68600332736969
  - 1.5644476294517518
  - 1.5034837007522583
  - 1.543703407049179
  - 2.3889293015003203
  - 3.5429801642894745
  - 2.314889806509018
  - 3.2949572145938877
  - 4.5444385528564455
  - 1.565271717309952
  - 1.5883528649806977
  - 1.530462521314621
  - 2.19196789264679
  validation_losses:
  - 0.40036478638648987
  - 0.39949750900268555
  - 0.4050061106681824
  - 0.4241047203540802
  - 0.4184941053390503
  - 0.43949416279792786
  - 0.47260385751724243
  - 0.4217943549156189
  - 0.40424469113349915
  - 0.40662306547164917
  - 0.4092257618904114
  - 0.4050184190273285
  - 0.4032098054885864
loss_records_fold2:
  train_losses:
  - 1.5595729380846024
  - 1.9428931891918184
  - 1.692649030685425
  - 2.523700171709061
  - 1.7629196226596833
  - 2.8209459424018863
  - 1.6857980966567994
  - 1.577205616235733
  - 2.675200986862183
  - 2.683418416976929
  - 4.53211162686348
  - 3.7119074285030367
  - 3.429650127887726
  - 1.8218276858329774
  - 4.853345829248429
  - 1.8547437191009521
  - 1.6769486010074617
  - 1.6052506089210512
  - 1.5521843492984773
  - 1.6275452494621279
  - 1.6952238261699677
  - 1.5808268070220948
  validation_losses:
  - 0.37966296076774597
  - 0.37981656193733215
  - 0.41275346279144287
  - 0.3898472189903259
  - 0.382006973028183
  - 0.38257306814193726
  - 0.42271625995635986
  - 0.38930076360702515
  - 2.182521343231201
  - 0.4000067710876465
  - 0.38944941759109497
  - 0.38614127039909363
  - 3.0736374855041504
  - 0.3802480399608612
  - 0.3823295831680298
  - 0.5350086688995361
  - 0.3835223615169525
  - 0.3835063576698303
  - 0.38424885272979736
  - 0.39285653829574585
  - 0.4022853970527649
  - 0.3814229369163513
loss_records_fold3:
  train_losses:
  - 1.8243475317955018
  - 1.7579314410686493
  - 1.6311218798160554
  - 2.204510027170181
  - 1.6519802689552308
  - 1.6114828884601593
  - 1.7356508016586305
  - 1.6551381289958955
  - 1.5305712908506395
  - 1.494686409831047
  - 1.676934587955475
  - 1.493015056848526
  - 1.7338436603546143
  - 2.256676137447357
  - 1.579623454809189
  - 2.16919783949852
  - 2.402289664745331
  - 1.5531935036182405
  - 1.5267592906951906
  - 1.55262770652771
  validation_losses:
  - 0.4040444791316986
  - 0.3937286138534546
  - 0.4061868190765381
  - 0.4022490084171295
  - 0.39570939540863037
  - 0.4011906385421753
  - 0.39297330379486084
  - 0.40889808535575867
  - 0.3957201838493347
  - 0.40235841274261475
  - 0.3999035954475403
  - 0.40126705169677734
  - 0.42530885338783264
  - 0.450069785118103
  - 0.3948017954826355
  - 0.39370855689048767
  - 0.3934333026409149
  - 0.394726037979126
  - 0.4035658836364746
  - 0.39810240268707275
loss_records_fold4:
  train_losses:
  - 3.3800879150629046
  - 1.5523489356040956
  - 1.526430606842041
  - 1.4903523564338685
  - 1.616947513818741
  - 1.5525254011154175
  - 1.5248890161514284
  - 1.5148028135299683
  - 1.5309874594211579
  - 1.504918956756592
  - 1.534343945980072
  - 1.5100805878639223
  validation_losses:
  - 0.43302980065345764
  - 0.39138221740722656
  - 0.3942866325378418
  - 0.40336164832115173
  - 0.39244934916496277
  - 0.40321388840675354
  - 0.39856669306755066
  - 0.398307740688324
  - 0.39536648988723755
  - 0.3963523805141449
  - 0.39290544390678406
  - 0.39113888144493103
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:09:34.261662'
