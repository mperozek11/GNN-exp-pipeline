config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:25:30.923694'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_44fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_44fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_44fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_44fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_44fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_44fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_44fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_44fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_44fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_44fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 75.89762443304062
  - 49.048025923967366
  - 27.28996427059174
  - 31.493347477167845
  - 28.72614419311285
  - 32.05372323393822
  - 21.573778980970385
  - 12.909563976526261
  - 20.39877614378929
  - 19.251695585250857
  - 21.12177148163319
  - 28.623028340935708
  - 12.801673775911333
  - 12.488739058375359
  - 10.93786642253399
  - 14.863830471038819
  - 20.712269985675814
  - 9.619971424341202
  - 9.600198018550874
  - 11.688927844166756
  - 10.128446498513222
  - 11.82570344209671
  - 12.05794097185135
  - 11.669739046692849
  - 7.145631417632103
  - 9.168969053030015
  - 9.739047658443452
  - 8.639586883783341
  - 9.729187998175622
  - 8.758947038650513
  - 7.118500593304635
  - 6.645356312394142
  - 8.001949852705001
  - 7.891200551390648
  - 6.792425093054772
  - 6.7614685058593755
  - 6.476346817612648
  - 6.761435145139695
  - 6.91562087237835
  - 6.770891338586807
  - 6.7021456390619285
  - 6.595476111769677
  - 6.307333183288574
  - 6.379309514164925
  - 6.759653139114381
  - 6.317814195156098
  - 6.927788332104683
  - 6.653522250056267
  - 6.562606880068779
  - 6.348534116148949
  - 6.368562769889832
  - 6.394608640670777
  - 6.504640156030655
  - 6.446429768204689
  - 6.597898650169373
  - 6.655001348257065
  - 6.6332552462816246
  - 6.857807931303978
  - 6.479378142952919
  - 7.962435385584832
  - 9.164848822355271
  - 7.09907446205616
  - 6.779513344168663
  - 6.437031322717667
  - 6.638701102137566
  - 6.640539509057999
  - 6.366401094198228
  - 6.596041023731232
  - 6.201569804549218
  - 6.540050143003464
  - 6.535378780961037
  - 6.459075602889062
  - 6.752459594607354
  - 6.554365774989129
  - 6.5395070105791095
  - 6.709691554307938
  - 6.696058595180512
  - 6.391367110610009
  - 6.510836219787598
  - 6.359444895386696
  - 6.400878819823266
  - 6.445493450760842
  - 6.407616522908211
  - 6.508684754371643
  - 6.9470667302608495
  - 6.467069518566132
  - 6.301877853274346
  - 6.460002920031548
  - 6.220371478796006
  validation_losses:
  - 1.6233091354370117
  - 0.7159344553947449
  - 0.6645156145095825
  - 0.881936252117157
  - 0.6461952328681946
  - 0.6880130171775818
  - 0.5290314555168152
  - 0.4985599219799042
  - 0.4635676145553589
  - 0.4003749191761017
  - 0.44138213992118835
  - 0.504652202129364
  - 0.5079014897346497
  - 0.45066946744918823
  - 0.5278040170669556
  - 0.42416420578956604
  - 0.4780905842781067
  - 0.46668681502342224
  - 0.4021161198616028
  - 0.38841769099235535
  - 0.43604522943496704
  - 0.42405688762664795
  - 0.4193098545074463
  - 0.48378539085388184
  - 0.4698127508163452
  - 0.4392443299293518
  - 0.4184366762638092
  - 0.4489874541759491
  - 0.6909213662147522
  - 0.4212973117828369
  - 0.40848711133003235
  - 0.4496120810508728
  - 0.4493064880371094
  - 0.40509143471717834
  - 0.42332038283348083
  - 0.4187222421169281
  - 0.4013248383998871
  - 0.42266419529914856
  - 0.40020284056663513
  - 0.399223268032074
  - 0.4027291238307953
  - 0.41584891080856323
  - 0.4000718593597412
  - 0.3917219042778015
  - 0.42129987478256226
  - 1.5530399084091187
  - 0.40396901965141296
  - 0.45141303539276123
  - 0.45524072647094727
  - 0.43717411160469055
  - 0.401611328125
  - 0.659476637840271
  - 0.3991525173187256
  - 0.41202011704444885
  - 0.425704687833786
  - 0.4158512055873871
  - 0.9523054957389832
  - 1.1998437643051147
  - 0.4447370171546936
  - 0.4515019655227661
  - 0.41525375843048096
  - 0.4264981746673584
  - 0.4105639159679413
  - 0.40770041942596436
  - 0.4348585307598114
  - 0.42998218536376953
  - 0.4076385796070099
  - 0.6023545861244202
  - 0.41046807169914246
  - 0.9915928840637207
  - 4.818294048309326
  - 0.4626002311706543
  - 0.4116082787513733
  - 0.43492045998573303
  - 0.4021104574203491
  - 0.40474116802215576
  - 0.4019653797149658
  - 0.42958688735961914
  - 0.4122459292411804
  - 0.42305636405944824
  - 0.41410166025161743
  - 0.40368491411209106
  - 0.4253489673137665
  - 0.4142343997955322
  - 0.4202585816383362
  - 0.4092566967010498
  - 0.40967902541160583
  - 0.4122212529182434
  - 0.4079514741897583
loss_records_fold1:
  train_losses:
  - 6.4055236190557485
  - 6.542179599404335
  - 6.685450664162636
  - 6.206354123353958
  - 6.35914820432663
  - 6.783589550852776
  - 6.537941867113114
  - 6.346940186619759
  - 6.488948798179627
  - 6.5223379850387575
  - 6.324672454595566
  - 6.332170072197915
  - 6.4163426041603095
  - 6.335172468423844
  - 6.491763740777969
  - 6.40137224495411
  - 6.401005178689957
  - 6.458403077721596
  - 6.33027663230896
  - 6.560388019680977
  - 6.5849115490913395
  - 6.4680743947625166
  - 6.390315738320351
  - 6.211950573325158
  - 6.646704316139221
  - 6.3750130355358126
  - 6.395598882436753
  - 6.3039424359798435
  - 6.192775696516037
  - 6.680661705136299
  - 6.50660574734211
  - 6.478615704178811
  - 6.206076672673226
  - 6.52237223982811
  - 6.391641858220101
  - 6.413218358159066
  - 6.525946581363678
  - 6.377052623033524
  - 6.563901028037072
  - 6.4311596184968955
  - 6.551586988568307
  - 6.46633150279522
  - 6.315806767344475
  - 6.442235404253006
  - 6.496592736244202
  - 6.418101862072945
  - 6.236871957778931
  - 6.688808706402779
  - 6.530846101045609
  - 6.682592579722405
  - 6.509086704254151
  - 6.301067957282067
  - 6.27657571732998
  - 6.523606869578362
  - 6.267564192414284
  - 6.404756346344948
  - 6.895889270305634
  - 6.3035896718502045
  - 6.444915929436684
  - 6.5536265015602115
  - 6.486271154880524
  - 6.652266839146614
  - 6.367488089203835
  - 6.586512324213982
  - 6.368027046322823
  - 6.258418872952461
  - 6.3102667987346654
  - 6.432891795039177
  - 6.539352011680603
  - 6.701056322455407
  - 6.542515778541565
  - 6.518402528762818
  - 6.333611544966698
  - 6.461213117837906
  - 6.358583024144173
  - 6.300769370794296
  - 6.475231117010117
  - 6.375929763913155
  - 6.589075866341592
  - 6.5051659822464
  - 6.696315479278565
  - 6.351631769537926
  - 6.755909579992295
  - 6.237424635887146
  - 6.5715809822082525
  - 6.371120864152909
  - 6.557657995820046
  - 6.263521683216095
  - 6.392880567908287
  - 6.338674238324166
  - 6.360120195150376
  - 6.281519258022309
  - 6.224912184476853
  - 6.1765389263629915
  - 6.386296656727791
  - 6.4968760281801226
  - 6.507410606741906
  - 6.298117870092392
  - 6.373099625110626
  - 6.384736889600754
  validation_losses:
  - 0.4201578199863434
  - 0.4061780273914337
  - 0.4459335505962372
  - 1065801154560.0
  - 0.43552666902542114
  - 14655461376.0
  - 0.42814168334007263
  - 0.42609965801239014
  - 0.4281090497970581
  - 0.5218570232391357
  - 0.4225095808506012
  - 0.43921276926994324
  - 0.41742509603500366
  - 0.4880468547344208
  - 0.40790432691574097
  - 0.44272083044052124
  - 0.4351838231086731
  - 0.4312615394592285
  - 0.5083698630332947
  - 0.5084832310676575
  - 0.43459227681159973
  - 0.43319249153137207
  - 0.4182911217212677
  - 0.418220192193985
  - 0.4468560814857483
  - 0.42395246028900146
  - 0.4443260431289673
  - 0.42699378728866577
  - 0.4391331374645233
  - 0.4629734456539154
  - 0.43188074231147766
  - 0.41864681243896484
  - 0.40841928124427795
  - 0.4490360617637634
  - 0.4589996337890625
  - 0.4694034457206726
  - 0.43555426597595215
  - 0.42139917612075806
  - 0.43625617027282715
  - 0.42240646481513977
  - 0.5687277317047119
  - 0.4147448241710663
  - 0.4285447597503662
  - 0.46628302335739136
  - 0.42079922556877136
  - 0.47804754972457886
  - 0.4936524033546448
  - 0.5110008716583252
  - 0.4559040367603302
  - 0.43019986152648926
  - 0.44570332765579224
  - 0.40109625458717346
  - 0.4528557360172272
  - 0.4578036665916443
  - 0.48309949040412903
  - 0.4994891285896301
  - 0.43752598762512207
  - 0.4281933903694153
  - 0.42888587713241577
  - 0.4277792274951935
  - 0.443185031414032
  - 0.4261009991168976
  - 0.43682554364204407
  - 0.40534481406211853
  - 0.47115015983581543
  - 0.417314350605011
  - 0.4360826015472412
  - 0.4503723680973053
  - 0.42681440711021423
  - 0.4587089419364929
  - 0.41965872049331665
  - 0.4474818706512451
  - 0.4143514633178711
  - 0.4582268297672272
  - 0.4369450509548187
  - 0.43564754724502563
  - 0.41354817152023315
  - 0.45312899351119995
  - 0.4716791808605194
  - 0.46680182218551636
  - 0.4324962794780731
  - 0.40595704317092896
  - 0.42668449878692627
  - 0.426983118057251
  - 0.4202927052974701
  - 0.40993791818618774
  - 0.4829709529876709
  - 0.4267876148223877
  - 0.5161547660827637
  - 0.4189915359020233
  - 0.4213690459728241
  - 0.457049697637558
  - 0.4292849898338318
  - 0.47004321217536926
  - 0.41005390882492065
  - 0.43736907839775085
  - 0.41773608326911926
  - 0.41951900720596313
  - 0.4274807572364807
  - 0.4395870864391327
loss_records_fold2:
  train_losses:
  - 6.513385072350502
  - 6.296009308099747
  - 6.589410042762757
  - 6.480203753709794
  - 6.230850940942765
  - 11.897945269942284
  - 8.705168393254281
  - 6.952431291341782
  - 6.729846966266632
  - 7.101610779762268
  - 6.605750167369843
  - 6.834695705771447
  - 6.325815400481225
  - 6.371863400936127
  - 6.455017691850663
  - 6.394915327429771
  - 6.660480189323426
  - 6.338388040661812
  - 6.340149593353272
  - 6.353556665778161
  - 6.30176401436329
  - 6.856916534900666
  - 6.269391077756882
  - 6.460626637935639
  - 6.351826459169388
  - 6.785329595208168
  - 6.540599474310875
  - 6.354065951704979
  - 6.567155793309212
  - 6.537420827150346
  - 6.614395523071289
  - 6.906637659668923
  - 7.464163440465928
  - 6.486645126342774
  - 6.679978784918785
  - 10.312800794839859
  - 7.946984097361565
  - 6.4581133961677555
  - 6.3196917355060584
  - 6.27606018781662
  - 6.411013421416283
  - 6.32074910402298
  - 6.328128358721734
  - 6.761760941147805
  - 6.207826799154282
  - 6.269992119073868
  - 6.450182318687439
  - 6.448360735177994
  - 6.5559661239385605
  - 6.597521805763245
  - 6.267005395889282
  - 6.302390235662461
  - 6.460923874378205
  - 6.600014799833298
  - 6.237509983778001
  - 6.50796954035759
  - 6.483302885293961
  - 6.5963398039340975
  - 6.413043197989464
  - 6.441929230093956
  - 6.391155511140823
  - 6.436398729681969
  - 6.367011517286301
  - 6.386797085404396
  - 6.211437693238259
  - 6.520490723848344
  - 6.553487354516983
  - 6.53617856502533
  - 6.450524890422821
  - 6.26201448738575
  - 6.550540274381638
  - 6.2733082503080375
  - 6.479799491167069
  - 6.416138309240342
  - 6.7784680247306826
  - 6.405858492851258
  - 6.486893805861474
  - 6.508791473507881
  - 6.431934046745301
  - 6.39621017575264
  - 6.439532911777497
  - 6.409099814295769
  - 6.5157999962568285
  - 6.430148851871491
  - 6.367937043309212
  - 6.51833253800869
  - 6.412115931510925
  - 6.4965566188097
  - 6.5256252795457845
  - 6.356679156422615
  - 6.903471070528031
  - 6.50069329738617
  - 6.521849028766155
  - 6.414187449216843
  - 6.415908405184746
  - 6.50766530930996
  - 6.7520818471908575
  - 6.307515734434128
  - 6.447696083784104
  - 6.341753578186036
  validation_losses:
  - 0.40440133213996887
  - 0.42243492603302
  - 0.39838874340057373
  - 0.40338602662086487
  - 4313.34765625
  - 0.4039398431777954
  - 29339.55859375
  - 0.41297581791877747
  - 0.39263251423835754
  - 0.4361666440963745
  - 12348.6474609375
  - 0.4276348650455475
  - 0.3978220224380493
  - 0.39418840408325195
  - 0.4076389968395233
  - 0.5121945738792419
  - 0.405850350856781
  - 0.4019891619682312
  - 0.4586177170276642
  - 0.4082939922809601
  - 0.4889237582683563
  - 0.3928524851799011
  - 0.42683279514312744
  - 0.4069216549396515
  - 0.3971360921859741
  - 4.0266571044921875
  - 41278.5703125
  - 436391.71875
  - 56826.5625
  - 1489.831787109375
  - 55448.77734375
  - 0.45397326350212097
  - 1.3399964570999146
  - 10.94849681854248
  - 17.962120056152344
  - 1810.9013671875
  - 190269088.0
  - 163353591808.0
  - 343.02703857421875
  - 775.3793334960938
  - 351.50225830078125
  - 487.9566955566406
  - 173881.9375
  - 1830.3116455078125
  - 805.6381225585938
  - 338.93621826171875
  - 1316.842041015625
  - 334.3768615722656
  - 49.78380584716797
  - 406.2746887207031
  - 233.78152465820312
  - 345.1472473144531
  - 136.9755096435547
  - 135.70001220703125
  - 43.864723205566406
  - 27.87657928466797
  - 396.74346923828125
  - 23.386234283447266
  - 280.2142639160156
  - 59.3056526184082
  - 66.89473724365234
  - 2121.95458984375
  - 141.9530487060547
  - 1289.68408203125
  - 321.3678894042969
  - 232.51051330566406
  - 263.0444030761719
  - 103.69215393066406
  - 167.84873962402344
  - 744.053955078125
  - 535.9853515625
  - 223.85121154785156
  - 58.363075256347656
  - 184.5286865234375
  - 3480.784423828125
  - 418.7274169921875
  - 2220.74365234375
  - 632.4425659179688
  - 1960.2843017578125
  - 1313.4747314453125
  - 900.6904296875
  - 375.8855895996094
  - 69.55537414550781
  - 48.5384635925293
  - 3640.75390625
  - 120.06349182128906
  - 258.5407409667969
  - 114.0934829711914
  - 803.6902465820312
  - 30818.875
  - 186.8848114013672
  - 349.57904052734375
  - 0.41708698868751526
  - 345.8164367675781
  - 0.4224245846271515
  - 239.54556274414062
  - 0.4122438430786133
  - 0.43916404247283936
  - 107.94930267333984
  - 0.399442583322525
loss_records_fold3:
  train_losses:
  - 6.33735072016716
  - 6.324159926176072
  - 6.451382726430893
  - 6.542165058851243
  - 6.295177233219147
  - 6.541126394271851
  - 6.3152001976966865
  - 6.536299371719361
  - 6.398614156246186
  - 6.3724613904953005
  - 6.462060436606407
  - 6.543738734722138
  - 6.492112940549851
  - 6.294995361566544
  - 6.459231388568878
  - 6.496503302454949
  - 6.6629614561796195
  - 6.452914637327194
  - 6.512290272116662
  - 6.485400938987732
  - 6.6734301269054415
  - 6.524755755066872
  - 6.56198450922966
  - 6.5910715341568
  - 6.472529241442681
  - 6.324876585602761
  - 6.5250710368156435
  - 6.403495699167252
  - 6.693191415071488
  - 6.704582181572914
  - 6.309725224971771
  - 6.357395538687706
  - 6.479990470409394
  - 6.291135132312775
  - 6.695423057675362
  - 6.359801870584488
  - 6.467004349827767
  - 6.39773295223713
  - 6.427180859446526
  - 6.485735546052457
  - 6.298619744181633
  - 6.638499173521996
  - 6.269175890088082
  - 6.363570052385331
  - 6.387978535890579
  - 6.382032886147499
  - 6.318529716134072
  - 6.367414009571076
  - 6.396061420440674
  - 6.421472045779229
  - 6.465996903181076
  - 6.3255477309226995
  - 6.689988315105438
  - 6.525175547599793
  - 6.48048224747181
  - 6.29230569601059
  - 6.240043127536774
  - 6.338728627562523
  - 6.2013783931732185
  - 6.535832011699677
  - 6.735340279340744
  - 6.481356269121171
  - 6.533387023210526
  - 6.3597440958023075
  - 6.362461876869202
  - 6.592828381061555
  - 6.386345759034157
  - 6.178149840235711
  - 6.472905814647675
  - 6.549018013477326
  - 6.438091823458672
  - 6.95672407746315
  - 6.5820105373859406
  - 6.498102369904519
  - 6.550276476144791
  - 6.356044504046441
  - 6.39898037314415
  - 6.234170511364937
  - 6.616596895456315
  - 6.566150215268135
  - 6.367848357558251
  - 6.306136336922646
  - 6.351403212547303
  - 6.5061355322599415
  - 6.560740949213505
  - 6.292990833520889
  - 6.347678801417351
  - 6.459652146697045
  - 6.405931785702705
  - 6.360654595494271
  - 6.345752942562104
  - 6.330211856961251
  - 6.183395001292229
  - 6.794810962677002
  - 6.455009549856186
  - 6.5043842732906345
  - 6.52209909260273
  - 6.604304176568985
  - 6.295416775345803
  - 6.267872703075409
  validation_losses:
  - 0.4173440635204315
  - 0.4321838319301605
  - 0.4265184998512268
  - 0.40374135971069336
  - 0.41552284359931946
  - 0.41584888100624084
  - 306.50543212890625
  - 19.65736198425293
  - 0.44038116931915283
  - 0.43810051679611206
  - 0.40612077713012695
  - 0.417029470205307
  - 0.434751033782959
  - 0.4119461178779602
  - 0.4149110019207001
  - 0.44361236691474915
  - 0.4937829077243805
  - 0.42104998230934143
  - 0.4193819463253021
  - 0.45587876439094543
  - 0.40790173411369324
  - 0.41868025064468384
  - 0.40732541680336
  - 108.3217544555664
  - 19.450830459594727
  - 0.43258237838745117
  - 0.3979022204875946
  - 0.47316431999206543
  - 0.40973928570747375
  - 0.4659197926521301
  - 0.41467103362083435
  - 0.40627744793891907
  - 0.4438806474208832
  - 0.41890937089920044
  - 0.41047537326812744
  - 0.40932297706604004
  - 0.43939560651779175
  - 0.451965868473053
  - 179.95419311523438
  - 0.4026944041252136
  - 0.40750548243522644
  - 0.43723583221435547
  - 0.4228792190551758
  - 0.41526591777801514
  - 0.418965607881546
  - 179.2222442626953
  - 0.4154258966445923
  - 0.42503127455711365
  - 0.4991404414176941
  - 299.2315673828125
  - 208.0341033935547
  - 0.4201255142688751
  - 0.5431251525878906
  - 0.41464662551879883
  - 0.40170708298683167
  - 0.41822949051856995
  - 0.4277615547180176
  - 0.4914088249206543
  - 0.40232348442077637
  - 0.45485368371009827
  - 0.4501804709434509
  - 0.4244688153266907
  - 0.41029268503189087
  - 0.42316049337387085
  - 0.40719664096832275
  - 0.42076367139816284
  - 0.4066108763217926
  - 363.45025634765625
  - 0.47085002064704895
  - 0.4684762954711914
  - 0.3980526626110077
  - 0.43092262744903564
  - 0.4356729984283447
  - 0.41437259316444397
  - 0.4371523857116699
  - 0.40889111161231995
  - 374.9914855957031
  - 0.4409717917442322
  - 0.4249046742916107
  - 0.42417868971824646
  - 0.42205846309661865
  - 315.083740234375
  - 0.4723667502403259
  - 0.48785218596458435
  - 0.4168511927127838
  - 0.4181370139122009
  - 0.4255102574825287
  - 3.3598408699035645
  - 0.4371051788330078
  - 57.89988708496094
  - 0.40843844413757324
  - 0.4201839864253998
  - 0.4211310148239136
  - 0.4085964560508728
  - 0.44596409797668457
  - 0.48654705286026
  - 0.4167977273464203
  - 0.4298498034477234
  - 0.4225096106529236
  - 394.9482727050781
loss_records_fold4:
  train_losses:
  - 6.1985858410596855
  - 6.424691990017891
  - 6.5613142430782325
  - 6.397329092025757
  - 6.417021271586418
  - 9.316449165344238
  - 6.586162030696869
  - 6.596577847003937
  - 6.510535997152329
  - 6.724990537762642
  - 6.4910872131586075
  - 6.638798820972443
  - 6.293959590792657
  - 6.202938336133958
  - 6.380510726571083
  - 6.519754627346993
  - 6.441382634639741
  - 6.5160068601369865
  - 6.673765459656716
  - 6.6741820037364965
  - 6.4949478328228
  - 6.393051087856293
  - 6.539509624242783
  - 6.392994964122773
  - 6.394439172744751
  - 6.381796935200692
  - 6.451482418179513
  - 6.308894917368889
  - 6.391181141138077
  validation_losses:
  - 214.61428833007812
  - 0.41099411249160767
  - 74.5751724243164
  - 1.2622514819694592e+16
  - 2764986.0
  - 10210519.0
  - 43065569968128.0
  - 0.4324488937854767
  - 0.4308156371116638
  - 0.4074576497077942
  - 0.4115510880947113
  - 0.403993159532547
  - 0.422016441822052
  - 0.41771385073661804
  - 0.41179922223091125
  - 0.4066146910190582
  - 0.4421066343784332
  - 0.4077744781970978
  - 0.42858147621154785
  - 0.4049627482891083
  - 0.41448745131492615
  - 0.40354692935943604
  - 0.44654515385627747
  - 0.40870338678359985
  - 0.410395085811615
  - 0.40198302268981934
  - 0.410939484834671
  - 0.41739967465400696
  - 0.40582311153411865
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 89 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.14065180102915953,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.24661654135338348, 0.0]'
  mean_eval_accuracy: 0.7148744790837769
  mean_f1_accuracy: 0.0493233082706767
  total_train_time: '0:45:15.826167'
