config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.218125'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_2fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_2fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_2fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_2fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_2fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_2fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_2fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_2fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_2fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_2fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.7775311827659608
  - 1.501747688651085
  - 1.5057663142681124
  - 1.5157953202724457
  - 1.4941527962684633
  - 1.4675666868686676
  - 1.4645968139171601
  - 1.507068145275116
  - 1.4630617558956147
  - 1.44853632748127
  - 1.4378188133239747
  - 1.452568554878235
  validation_losses:
  - 0.4267653524875641
  - 0.40674281120300293
  - 0.4059668183326721
  - 0.43086937069892883
  - 0.3946108818054199
  - 0.410453736782074
  - 0.3989179730415344
  - 0.408241331577301
  - 0.3884871006011963
  - 0.386881560087204
  - 0.38554972410202026
  - 0.39061206579208374
loss_records_fold1:
  train_losses:
  - 1.4887621104717255
  - 1.433061382174492
  - 1.4992416679859162
  - 1.4503145694732666
  - 1.4359453558921815
  - 1.471153050661087
  - 1.4204416990280153
  - 1.4220148593187334
  - 1.4205310642719269
  - 1.5108264863491059
  - 1.4325320184230805
  - 1.4304221808910371
  - 1.504733207821846
  - 1.462545168399811
  - 1.441754972934723
  - 1.4264273911714556
  validation_losses:
  - 0.40329253673553467
  - 0.38727429509162903
  - 0.38832834362983704
  - 0.38989195227622986
  - 0.38982081413269043
  - 0.4058241844177246
  - 0.3857766389846802
  - 0.3839130401611328
  - 0.386575847864151
  - 0.40321922302246094
  - 0.3958591818809509
  - 0.39431220293045044
  - 0.3929736018180847
  - 0.38968178629875183
  - 0.3915555477142334
  - 0.39359724521636963
loss_records_fold2:
  train_losses:
  - 1.434444645047188
  - 1.4051228791475296
  - 1.4192161679267885
  - 1.436539649963379
  - 1.4173953711986542
  - 1.43485124707222
  - 1.4373654007911683
  - 1.4434930235147476
  - 1.4230503857135774
  - 1.4333646655082704
  - 1.4435477614402772
  - 1.4183585524559021
  - 1.400863665342331
  - 1.4161144316196443
  - 1.4646847486495973
  - 1.4189803659915925
  - 1.4412844598293306
  - 1.411580139398575
  - 1.3887326419353485
  - 1.4191903471946716
  - 1.3872692912817002
  - 1.4431643128395082
  - 1.4282103002071382
  - 1.4188926458358766
  - 1.4182284951210022
  - 1.3752821236848831
  - 1.4175275146961213
  - 1.4133940398693086
  - 1.3992800265550613
  - 1.4050245881080627
  - 1.4406362295150759
  - 1.4310611546039582
  - 1.4165994107723237
  - 1.4481554359197617
  - 1.4207592546939851
  - 1.3925077319145203
  - 1.3705161333084108
  - 1.3727224975824357
  - 1.3948128759860994
  - 1.405346179008484
  - 1.371393471956253
  - 1.3543435513973237
  - 1.4514699161052704
  - 1.3915839850902558
  - 1.4001602590084077
  - 1.4380166172981264
  - 1.3791526794433595
  - 1.3984442114830018
  - 1.4306769311428071
  - 1.411429512500763
  - 1.4016317307949067
  - 1.4259167432785036
  - 1.3690703779459001
  - 1.4317364633083345
  - 1.4135654211044313
  - 1.42341365814209
  - 1.4062662839889528
  - 1.368836295604706
  - 1.443136352300644
  - 1.464896458387375
  - 1.4109053879976274
  - 1.4610258400440217
  - 1.4275850832462311
  - 1.3711612701416016
  - 1.3890958607196808
  - 1.3959743976593018
  - 1.4113509476184847
  - 1.3961175382137299
  - 1.387304198741913
  - 1.373729395866394
  - 1.3883540570735933
  - 1.397066128253937
  - 1.4049773395061493
  - 1.393159157037735
  - 1.3925151765346528
  - 1.377566707134247
  - 1.3979794800281526
  - 1.4504603683948518
  - 1.3805448889732361
  - 1.374817967414856
  - 1.3909299910068513
  - 1.394409203529358
  - 1.4437984168529512
  - 1.4606870353221895
  - 1.3822974085807802
  - 1.3927519351243973
  - 1.4107052803039553
  - 1.3920008957386019
  - 1.4116164088249208
  - 1.444884604215622
  - 1.4049582779407501
  - 1.3907252073287966
  - 1.3771591424942018
  - 1.437277388572693
  - 1.4079640328884127
  - 1.4150591671466828
  - 1.4347399353981019
  - 1.3803884804248812
  - 1.3463298916816713
  - 1.4138862252235413
  validation_losses:
  - 0.40156444907188416
  - 0.387405127286911
  - 0.4862081706523895
  - 0.5019317865371704
  - 0.3932875394821167
  - 0.5065918564796448
  - 0.4029977321624756
  - 0.6595961451530457
  - 0.3831087052822113
  - 0.40059390664100647
  - 0.4603319764137268
  - 0.4944632053375244
  - 0.4647543430328369
  - 0.405111700296402
  - 0.45226985216140747
  - 0.386555552482605
  - 0.4776184856891632
  - 0.4623011648654938
  - 0.4999803304672241
  - 0.46387872099876404
  - 0.4076712131500244
  - 0.5700606107711792
  - 0.3891525864601135
  - 0.47033604979515076
  - 0.47111058235168457
  - 0.5763798952102661
  - 0.42054134607315063
  - 0.4858604669570923
  - 0.9329296350479126
  - 0.4587312340736389
  - 0.6531319618225098
  - 0.4726586639881134
  - 0.6010127067565918
  - 0.4236820340156555
  - 0.6491208672523499
  - 0.8661678433418274
  - 0.7187911868095398
  - 0.5569108128547668
  - 0.434151291847229
  - 0.4155007600784302
  - 0.4708983898162842
  - 0.5236554145812988
  - 0.5158126354217529
  - 0.3852032423019409
  - 0.4199003577232361
  - 0.5284913778305054
  - 0.4714522063732147
  - 0.4629780650138855
  - 0.5129778981208801
  - 0.5417836904525757
  - 0.4205515384674072
  - 0.4862479567527771
  - 0.46726685762405396
  - 0.402066707611084
  - 0.4155101776123047
  - 0.42811110615730286
  - 0.40866371989250183
  - 0.3984098732471466
  - 0.4116418957710266
  - 0.39923062920570374
  - 0.5673750042915344
  - 0.3899754285812378
  - 0.4949977993965149
  - 0.46475541591644287
  - 0.3987482786178589
  - 0.41009387373924255
  - 0.4498574733734131
  - 0.38939693570137024
  - 0.3939956724643707
  - 0.4672124981880188
  - 0.5000676512718201
  - 0.3937132656574249
  - 0.4935801327228546
  - 0.3934290111064911
  - 0.440864235162735
  - 0.38565099239349365
  - 0.45718756318092346
  - 0.42072242498397827
  - 0.3999219834804535
  - 0.4707236588001251
  - 0.39055174589157104
  - 0.4243314266204834
  - 0.39728304743766785
  - 0.4662817120552063
  - 0.3797839283943176
  - 0.38487890362739563
  - 0.3838053047657013
  - 0.47693365812301636
  - 0.45448020100593567
  - 0.41498133540153503
  - 0.44568511843681335
  - 0.5407796502113342
  - 0.3823882043361664
  - 0.3841419517993927
  - 0.38446903228759766
  - 0.38360288739204407
  - 0.41902831196784973
  - 0.4501427710056305
  - 0.416486531496048
  - 0.4732454717159271
loss_records_fold3:
  train_losses:
  - 1.4080999910831453
  - 1.3958977341651917
  - 1.4303626596927643
  - 1.4826628088951113
  - 1.418224424123764
  - 1.3962446212768556
  - 1.402080911397934
  - 1.420694637298584
  - 1.4109559923410417
  - 1.4230314254760743
  - 1.4115011334419252
  - 1.4195584475994112
  - 1.4222715914249422
  - 1.3755610406398775
  - 1.397143691778183
  - 1.4078038036823273
  - 1.4125265777111053
  - 1.4135243773460389
  - 1.4085528552532196
  - 1.4678679138422013
  - 1.4246101438999177
  - 1.4555185198783875
  - 1.462196183204651
  - 1.4680619537830353
  - 1.4139322221279145
  - 1.4573715329170227
  - 1.433603397011757
  - 1.4236209273338318
  - 1.4353999078273774
  - 1.4110525816679003
  - 1.4670192718505861
  - 1.4277756273746491
  - 1.4210688084363938
  - 1.4391958296298981
  - 1.4442571699619293
  - 1.4022864639759065
  - 1.427466720342636
  - 1.4162365317344667
  - 1.390102791786194
  - 1.4269525706768036
  - 1.4176107347011566
  - 1.448621153831482
  - 1.4359795570373537
  - 1.4098745107650759
  - 1.3871473759412767
  - 1.4551911056041718
  - 1.4207959294319155
  - 1.4435445725917817
  - 1.4213111400604248
  - 1.4018813371658325
  - 1.416991835832596
  - 1.4326341211795808
  - 1.4324564933776855
  - 1.392637974023819
  - 1.4341439664363862
  - 1.3624195128679277
  - 1.4028476417064668
  - 1.39430188536644
  - 1.438695776462555
  - 1.3912557005882265
  - 1.3882642030715944
  - 1.4024011373519898
  - 1.5020103573799135
  - 1.4352600514888765
  - 1.4176896929740908
  - 1.4403234779834748
  - 1.4123737871646882
  - 1.4183973133563996
  - 1.4052588284015657
  - 1.4029790818691255
  - 1.4473628818988802
  - 1.4312566161155702
  - 1.413358736038208
  - 1.3880655437707903
  - 1.4056898653507233
  - 1.4102770805358888
  - 1.425518098473549
  - 1.4002480566501618
  - 1.410773891210556
  - 1.3954336106777192
  - 1.3956243574619294
  - 1.4216339319944382
  - 1.405467450618744
  - 1.3942474067211152
  - 1.4098107337951662
  - 1.4136007070541383
  - 1.3951807916164398
  - 1.4145161092281342
  - 1.4321097910404206
  - 1.4585226237773896
  - 1.4142249286174775
  - 1.3966521203517914
  - 1.3945111870765687
  - 1.3725137531757357
  - 1.3934766948223114
  - 1.3952448070049286
  - 1.3883454978466034
  - 1.3767227709293366
  - 1.3328277111053468
  - 1.3862383604049684
  validation_losses:
  - 0.4429541230201721
  - 0.37956491112709045
  - 0.46646982431411743
  - 0.4415203332901001
  - 1.2465001344680786
  - 0.571986734867096
  - 0.45577919483184814
  - 0.4428471326828003
  - 0.5331740379333496
  - 0.3812209963798523
  - 0.3683737814426422
  - 0.3995817303657532
  - 0.38582825660705566
  - 0.4025875926017761
  - 0.3771257996559143
  - 0.421596884727478
  - 0.4060477912425995
  - 0.5245622396469116
  - 0.3979640305042267
  - 0.43341657519340515
  - 0.43600520491600037
  - 0.44264623522758484
  - 0.37059611082077026
  - 0.36663901805877686
  - 0.36849692463874817
  - 0.37890738248825073
  - 0.3625129163265228
  - 0.3708947002887726
  - 0.36796504259109497
  - 0.36707696318626404
  - 0.3666561543941498
  - 0.3847002685070038
  - 0.3898259401321411
  - 0.3691672384738922
  - 0.3779403865337372
  - 0.40358874201774597
  - 0.3846319913864136
  - 0.4082260727882385
  - 0.397927850484848
  - 0.3841598629951477
  - 0.3682594895362854
  - 0.37722718715667725
  - 0.4543517827987671
  - 0.37011390924453735
  - 0.3692534267902374
  - 0.3697260618209839
  - 0.37339693307876587
  - 0.41723260283470154
  - 0.4536478817462921
  - 0.5108570456504822
  - 0.3813023269176483
  - 0.42238518595695496
  - 0.41133326292037964
  - 0.440372109413147
  - 0.4519813060760498
  - 0.4630225896835327
  - 0.436301052570343
  - 0.4723997116088867
  - 0.5184862613677979
  - 0.515216588973999
  - 0.6714873313903809
  - 0.3870429992675781
  - 0.37497571110725403
  - 0.3818637728691101
  - 0.36597940325737
  - 0.3818441927433014
  - 0.38885676860809326
  - 0.36740243434906006
  - 0.3822433054447174
  - 0.3717123568058014
  - 0.3822510540485382
  - 0.3901788294315338
  - 0.40643200278282166
  - 0.4529988169670105
  - 0.3715045154094696
  - 0.40281733870506287
  - 0.3920089900493622
  - 0.4150334596633911
  - 0.3897862136363983
  - 0.4001047611236572
  - 0.3728445768356323
  - 0.3778149485588074
  - 0.36185234785079956
  - 0.36586400866508484
  - 0.38278356194496155
  - 0.3773502707481384
  - 0.3780358135700226
  - 0.40349674224853516
  - 0.39968055486679077
  - 0.5550957322120667
  - 0.6986786723136902
  - 0.5712036490440369
  - 0.48856863379478455
  - 0.5924462676048279
  - 0.44342517852783203
  - 0.5436787009239197
  - 0.5457358956336975
  - 0.5747847557067871
  - 0.46740105748176575
  - 0.6813956499099731
loss_records_fold4:
  train_losses:
  - 1.4290952742099763
  - 1.4261566221714022
  - 1.4339468300342562
  - 1.4082791805267334
  - 1.3884336054325104
  - 1.370245373249054
  - 1.3967003405094147
  - 1.360367587208748
  - 1.3835159122943879
  - 1.4017047405242922
  - 1.4101754248142244
  - 1.3793260514736176
  - 1.39491782784462
  - 1.4125661730766297
  - 1.4035320937633515
  - 1.3923043608665466
  - 1.3924030721187592
  - 1.3872932434082033
  - 1.3792419999837877
  - 1.3996069014072419
  - 1.4221426367759706
  - 1.4198885262012482
  - 1.3799910455942155
  - 1.4194536089897156
  - 1.4019774675369263
  - 1.423152142763138
  - 1.405642807483673
  - 1.3953441619873048
  - 1.367620235681534
  - 1.3992981910705566
  - 1.3790150940418244
  - 1.4172270357608796
  - 1.3757433950901032
  - 1.389103651046753
  - 1.350582867860794
  - 1.3957350552082062
  - 1.4284386813640595
  - 1.5642537474632263
  - 1.4747797310352326
  - 1.4039461612701416
  - 1.4298314571380617
  - 1.3878533691167831
  - 1.4162732005119325
  - 1.4194741487503053
  - 1.4313960552215577
  - 1.406813269853592
  - 1.4081675469875337
  validation_losses:
  - 0.37460997700691223
  - 0.38883185386657715
  - 0.36796700954437256
  - 0.371797114610672
  - 0.36655592918395996
  - 0.3632875978946686
  - 0.3613043427467346
  - 0.3717583119869232
  - 0.3843138515949249
  - 0.3601328730583191
  - 0.4113592505455017
  - 0.36944684386253357
  - 0.3741273880004883
  - 0.3781934380531311
  - 0.3957582414150238
  - 0.37149879336357117
  - 0.3739239275455475
  - 0.3917888104915619
  - 0.371426522731781
  - 0.38172200322151184
  - 0.37383851408958435
  - 0.37272390723228455
  - 0.37623661756515503
  - 0.3715229034423828
  - 0.39662259817123413
  - 0.3905459940433502
  - 0.4076494574546814
  - 0.3875219523906708
  - 0.3800022304058075
  - 0.6734803915023804
  - 0.3723840117454529
  - 0.4499933421611786
  - 0.5799618363380432
  - 0.368662565946579
  - 0.3720192313194275
  - 0.4241325259208679
  - 0.37699419260025024
  - 0.38498082756996155
  - 0.374789834022522
  - 0.3757176697254181
  - 0.41181060671806335
  - 0.4005977511405945
  - 0.3851616382598877
  - 0.3762126863002777
  - 0.3713013529777527
  - 0.3749500811100006
  - 0.37268170714378357
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 47 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8353344768439108, 0.8370497427101201,
    0.8608247422680413]'
  fold_eval_f1: '[0.023529411764705882, 0.0, 0.21311475409836067, 0.18803418803418806,
    0.024096385542168676]'
  mean_eval_accuracy: 0.8496949656062668
  mean_f1_accuracy: 0.08975494788788466
  total_train_time: '0:22:43.517282'
