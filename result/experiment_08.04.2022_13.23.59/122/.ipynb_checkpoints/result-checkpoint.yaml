config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 04:12:33.418083'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/122/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/122/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/122/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/122/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/122/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/122/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/122/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/122/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/122/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/122/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 282.38318169116974
  - 102.72293210029602
  - 77.94017258286476
  - 107.83469441533089
  - 66.03182628750801
  - 35.83006101846695
  - 46.36905747652054
  - 27.073748409748077
  - 26.58408984541893
  - 34.719954907894135
  - 28.71311104297638
  - 37.70728421211243
  - 41.00790059566498
  - 32.56096628308296
  - 29.680300429463387
  - 40.46869656443596
  - 50.05953586101532
  - 58.7989504635334
  - 48.13242897391319
  - 39.74458095431328
  - 31.842525511980057
  - 60.154820412397385
  - 32.74051249027252
  - 31.42668977379799
  - 38.15575996041298
  - 31.201018571853638
  - 32.48391279578209
  - 39.237959295511246
  - 46.670591562986374
  - 38.88929259777069
  - 24.47856393456459
  - 28.843973264098167
  - 22.51414641737938
  - 33.52299311757088
  - 24.152949631214142
  - 23.907602071762085
  - 22.87845778465271
  - 21.519274979829788
  - 21.78255718946457
  - 21.276299595832825
  - 28.660407334566116
  - 25.99663332104683
  - 19.97643581032753
  - 17.560231417417526
  - 16.64696717262268
  - 24.01363694667816
  - 31.318191796541214
  - 26.730564311146736
  - 25.490260750055313
  - 31.41555482149124
  - 18.222515866160393
  - 21.59967178106308
  - 20.82222220301628
  - 21.731736809015274
  - 19.068953067064285
  - 20.98544415831566
  - 19.22219504415989
  - 16.22814241051674
  - 20.093617290258408
  - 15.811850562691689
  - 19.228975385427475
  - 18.275943130254745
  - 18.083039462566376
  - 19.81252434849739
  - 17.337860122323036
  - 24.6686352789402
  - 16.584345787763596
  - 16.75273448228836
  - 16.11803749203682
  - 15.619715049862862
  - 15.933177888393402
  - 18.226137652993202
  - 19.298286467790604
  - 18.35150757431984
  - 16.014796435832977
  - 17.74288173019886
  - 16.80001676082611
  - 16.17302441596985
  - 16.26601055264473
  - 16.73280420899391
  - 16.371409714221954
  - 15.829829081892967
  - 16.177630752325058
  - 18.880095183849335
  - 16.466896146535873
  - 16.31076517701149
  - 16.657701015472412
  - 16.76552787423134
  - 16.576777577400208
  - 17.67515268921852
  - 17.111697524785995
  - 18.216388911008835
  - 19.012292742729187
  - 16.358149766921997
  - 16.478451132774353
  - 18.24940922856331
  - 29.04724931716919
  - 18.063932955265045
  - 16.352024763822556
  - 15.723088383674622
  validation_losses:
  - 2.2134783267974854
  - 1.1078932285308838
  - 1.0108100175857544
  - 1.7823323011398315
  - 0.49460098147392273
  - 0.47651195526123047
  - 0.5037010908126831
  - 0.49585020542144775
  - 0.5332768559455872
  - 1.7543245553970337
  - 0.49074500799179077
  - 0.4163956344127655
  - 0.404652863740921
  - 0.43617406487464905
  - 0.4528733193874359
  - 0.7073613405227661
  - 0.6620757579803467
  - 0.40092402696609497
  - 0.4555584490299225
  - 0.6055868268013
  - 0.41464897990226746
  - 0.5199577212333679
  - 0.45589399337768555
  - 0.45210129022598267
  - 0.45127013325691223
  - 1.0421814918518066
  - 0.3994942307472229
  - 0.5237999558448792
  - 0.5259785056114197
  - 0.5987396240234375
  - 0.7305847406387329
  - 0.3973822295665741
  - 0.400008887052536
  - 0.5746715664863586
  - 0.433713436126709
  - 0.42825570702552795
  - 0.40399813652038574
  - 0.49100565910339355
  - 0.520129919052124
  - 0.5839043259620667
  - 0.43226608633995056
  - 0.3945310115814209
  - 0.5116931200027466
  - 0.4050675630569458
  - 0.3938927948474884
  - 0.7294274568557739
  - 0.4534725844860077
  - 0.3976101279258728
  - 0.4453430771827698
  - 0.3890010416507721
  - 0.46010202169418335
  - 0.4445413053035736
  - 0.41020461916923523
  - 0.4013930559158325
  - 0.4310202896595001
  - 0.4828992784023285
  - 0.41730406880378723
  - 0.42230990529060364
  - 0.4332938492298126
  - 0.4042797088623047
  - 0.4347042739391327
  - 0.4502952992916107
  - 0.4087698757648468
  - 0.40322020649909973
  - 0.4127422273159027
  - 0.6112118363380432
  - 0.43509936332702637
  - 0.4145537316799164
  - 0.39945188164711
  - 0.40124911069869995
  - 0.42544785141944885
  - 0.47169214487075806
  - 0.40378838777542114
  - 0.40022727847099304
  - 0.39468249678611755
  - 0.43048399686813354
  - 0.45358502864837646
  - 0.4110215902328491
  - 0.44052377343177795
  - 0.4300055205821991
  - 0.42874598503112793
  - 0.39921167492866516
  - 0.42706987261772156
  - 0.4184305667877197
  - 0.4070945680141449
  - 0.40796971321105957
  - 0.42425626516342163
  - 0.4375332295894623
  - 0.4247407019138336
  - 0.41050100326538086
  - 0.4346538782119751
  - 0.3954322636127472
  - 0.40825527906417847
  - 0.4168952703475952
  - 0.408521443605423
  - 0.4233577847480774
  - 0.4110613465309143
  - 0.4074987769126892
  - 0.41249051690101624
  - 0.40548041462898254
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 83 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 97 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:08:24.165861'
