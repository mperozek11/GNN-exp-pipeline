config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:21:57.678564'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_126fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_126fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_126fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_126fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_126fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_126fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_126fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_126fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_126fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_126fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 28.5758246421814
  - 15.166621601581575
  - 11.21095577478409
  - 6.194217002391816
  - 5.040617328882218
  - 6.859323108196259
  - 5.448331916332245
  - 3.6903983771800997
  - 2.475640630722046
  - 2.2888123035430907
  - 2.5445426166057588
  - 2.662469005584717
  - 2.223282587528229
  - 5.527567112445832
  - 9.839428544044495
  - 5.652758723497391
  - 3.7191957950592043
  - 2.195332431793213
  - 3.7608346462249758
  - 9.234486162662506
  - 5.374319851398468
  - 8.737878292798996
  - 6.5695484519004825
  - 4.9607454419136054
  - 7.2641251444816595
  - 2.523017865419388
  - 3.0374826431274418
  - 15.3501038312912
  - 3.3096678972244264
  - 2.6035765290260318
  - 2.6178652286529545
  - 3.972512209415436
  - 2.249747759103775
  - 1.9701347529888154
  - 3.102538061141968
  - 8.075456631183625
  - 4.665617209672928
  - 4.932634001970292
  - 2.860542094707489
  - 3.0326647937297824
  - 1.9911804378032685
  - 19.435477435588837
  - 3.7667801916599277
  - 1.9003073751926423
  - 1.971802145242691
  - 2.394281524419785
  - 3.231558704376221
  - 3.2485911369323732
  - 1.774632203578949
  validation_losses:
  - 9.846593856811523
  - 2.433931827545166
  - 0.6547484397888184
  - 0.9957093596458435
  - 0.5228002667427063
  - 0.8907312750816345
  - 0.5586532950401306
  - 0.5706308484077454
  - 0.6858269572257996
  - 0.6432852745056152
  - 0.5865095853805542
  - 0.727762758731842
  - 0.40080755949020386
  - 1.0891752243041992
  - 1.079592227935791
  - 0.601347804069519
  - 0.4962179362773895
  - 0.7311131358146667
  - 0.7149003148078918
  - 0.7157642841339111
  - 0.7323238849639893
  - 0.45946869254112244
  - 1.2312333583831787
  - 0.5186268091201782
  - 0.43061596155166626
  - 0.41509920358657837
  - 0.4750033915042877
  - 0.39244794845581055
  - 0.4748515486717224
  - 0.46356144547462463
  - 0.43393778800964355
  - 0.4032091498374939
  - 0.40502020716667175
  - 0.3881646692752838
  - 0.398478627204895
  - 0.40535181760787964
  - 0.4214998781681061
  - 0.4214094877243042
  - 0.396214097738266
  - 0.39385154843330383
  - 0.41841650009155273
  - 0.39345985651016235
  - 0.40669742226600647
  - 0.4066750407218933
  - 0.4157334864139557
  - 0.4037053883075714
  - 0.3955842852592468
  - 0.4002513289451599
  - 0.39165207743644714
loss_records_fold1:
  train_losses:
  - 2.455161595344544
  - 1.6939442336559296
  - 5.618253087997437
  - 1.903762173652649
  - 1.7756372809410097
  - 1.8548936128616333
  - 2.808473598957062
  - 2.0609488785266876
  - 2.187972104549408
  - 1.6626617074012757
  - 1.7641604721546174
  - 1.866397100687027
  - 2.7941587150096896
  - 1.739454585313797
  - 1.6271860361099244
  - 1.5613389670848847
  - 1.617008137702942
  - 1.5865542352199555
  - 1.5986055791378022
  validation_losses:
  - 0.40422523021698
  - 0.4047391414642334
  - 0.454506516456604
  - 0.5331408381462097
  - 0.44600337743759155
  - 0.41943255066871643
  - 0.4076712131500244
  - 0.4934596121311188
  - 0.4054667055606842
  - 0.41378918290138245
  - 0.4067823588848114
  - 0.43324846029281616
  - 0.4533861577510834
  - 0.43227699398994446
  - 0.4163592457771301
  - 0.40742382407188416
  - 0.4111337661743164
  - 0.40596991777420044
  - 0.40894678235054016
loss_records_fold2:
  train_losses:
  - 1.6555636346340181
  - 4.296145522594452
  - 2.0903512954711916
  - 1.6842824578285218
  - 1.7976603448390962
  - 1.6557336866855623
  - 2.6617843180894853
  - 1.7083254992961885
  - 1.8034310519695282
  - 2.860674148797989
  - 2.8272794365882876
  - 3.5516900658607486
  - 2.1678845018148425
  - 2.878586304187775
  - 1.8076908886432648
  - 1.753282070159912
  validation_losses:
  - 0.38891440629959106
  - 0.39474302530288696
  - 0.381116658449173
  - 0.3911362886428833
  - 0.3938474655151367
  - 0.390308678150177
  - 0.4115111231803894
  - 0.3868021070957184
  - 0.3883972764015198
  - 0.4352232813835144
  - 0.4073474407196045
  - 0.40169796347618103
  - 0.39813432097435
  - 0.4050905108451843
  - 0.4024885594844818
  - 0.398680716753006
loss_records_fold3:
  train_losses:
  - 3.0052457213401795
  - 2.364941829442978
  - 3.358651965856552
  - 4.144868403673172
  - 9.766481459140778
  - 1.7745715975761414
  - 2.20194154381752
  - 1.7418826401233674
  - 3.7619757592678074
  - 1.773980289697647
  - 1.6992176055908204
  - 2.0829473972320556
  - 1.6438111364841461
  - 1.6744412243366242
  - 1.6821105122566224
  - 1.7241960525512696
  - 1.6400127410888672
  - 1.5722950816154482
  - 1.618126553297043
  - 1.8060255587100984
  - 1.6190613567829133
  - 1.5760040849447252
  - 1.622427296638489
  validation_losses:
  - 0.40584737062454224
  - 0.4003581702709198
  - 0.47394904494285583
  - 0.4532756507396698
  - 0.43432021141052246
  - 0.41025128960609436
  - 0.41211065649986267
  - 0.40871739387512207
  - 0.4072823226451874
  - 0.42416614294052124
  - 0.4110240638256073
  - 0.42038118839263916
  - 0.409160315990448
  - 0.4403626024723053
  - 0.4284404218196869
  - 0.40261244773864746
  - 0.4214271008968353
  - 0.414671927690506
  - 0.40574654936790466
  - 0.4093317687511444
  - 0.40701037645339966
  - 0.40512216091156006
  - 0.39847713708877563
loss_records_fold4:
  train_losses:
  - 1.760596388578415
  - 1.6214305996894838
  - 2.0717549443244936
  - 1.6076538741588593
  - 1.662909519672394
  - 1.6796782791614533
  - 1.789310771226883
  - 2.338690036535263
  - 1.6852222204208376
  - 1.7522392868995667
  - 1.5966449081897736
  - 1.6175226986408235
  - 1.7689502716064454
  - 1.9303090155124665
  - 2.4027335882186893
  - 1.6746002554893495
  - 1.588936659693718
  - 1.6161942183971405
  - 1.6422147035598755
  - 1.610460025072098
  - 1.5926780462265016
  - 1.7140004992485047
  - 1.6141778349876406
  - 1.5702082693576813
  - 1.5875235676765442
  - 1.7622985243797302
  - 1.628682255744934
  - 1.6067360043525696
  - 1.566936868429184
  validation_losses:
  - 0.4335469901561737
  - 0.4078628122806549
  - 0.40059328079223633
  - 0.41426166892051697
  - 0.4315457344055176
  - 0.4300380349159241
  - 0.42106732726097107
  - 0.4274612367153168
  - 0.41274043917655945
  - 0.3977625072002411
  - 0.4125950038433075
  - 0.41273513436317444
  - 0.44158831238746643
  - 0.4178002178668976
  - 0.5004828572273254
  - 0.44194817543029785
  - 0.45296770334243774
  - 0.4167179763317108
  - 0.40283554792404175
  - 0.4469161927700043
  - 0.4153621196746826
  - 0.4122702479362488
  - 0.43341806530952454
  - 0.41966089606285095
  - 0.41934600472450256
  - 0.42406201362609863
  - 0.40259864926338196
  - 0.40421414375305176
  - 0.40172526240348816
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 49 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:10:29.039355'
