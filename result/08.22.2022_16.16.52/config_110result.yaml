config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:45:49.240583'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_110fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_110fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_110fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_110fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_110fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_110fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_110fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_110fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_110fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_110fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 22.741759538650513
  - 24.678531360626224
  - 11.38476837873459
  - 4.448735249042511
  - 5.536139750480652
  - 7.663727873563767
  - 5.7874583363533025
  - 3.5266869843006137
  - 2.78973251581192
  - 2.307006984949112
  - 3.713966518640518
  - 3.1702063977718353
  - 2.8462340354919435
  - 7.407269430160523
  - 9.54176452755928
  - 6.888259637355805
  - 2.9930767476558686
  - 2.332269656658173
  - 3.275405704975128
  - 9.371953105926513
  - 6.941985321044922
  - 6.267061734199524
  - 14.072380310297014
  - 3.2454828500747683
  - 6.294093436002732
  - 3.3685118049383167
  - 2.089161252975464
  - 16.69533772468567
  - 2.432066684961319
  - 2.861451029777527
  - 2.1531764149665835
  - 2.9938530921936035
  - 1.8013542711734774
  - 1.8594758152961732
  - 2.801469135284424
  - 6.9032676935195925
  - 4.341755539178848
  - 6.820335334539414
  - 2.7343221306800842
  - 3.1908784031867983
  - 2.0814647495746614
  - 20.41945037841797
  - 4.008885383605957
  - 2.5904134571552277
  - 2.682189494371414
  - 3.3666632711887363
  - 5.931755012273789
  - 4.573747253417969
  - 1.7904743432998658
  - 1.7099047243595125
  validation_losses:
  - 31.573362350463867
  - 1.3541052341461182
  - 0.6707025170326233
  - 0.5664404034614563
  - 0.6158861517906189
  - 0.8802844285964966
  - 0.8987357020378113
  - 0.5468775033950806
  - 0.5264779329299927
  - 0.9052122235298157
  - 1.3886417150497437
  - 0.6894672513008118
  - 0.48788774013519287
  - 1.1817412376403809
  - 0.7358701229095459
  - 0.6204858422279358
  - 0.48965972661972046
  - 0.42291563749313354
  - 0.6821615099906921
  - 0.4565514028072357
  - 0.420424222946167
  - 0.4335527718067169
  - 0.5864261388778687
  - 0.4979091286659241
  - 0.39222753047943115
  - 0.41062164306640625
  - 0.3981460630893707
  - 0.3947179317474365
  - 0.5278313159942627
  - 0.4684160351753235
  - 0.42540526390075684
  - 0.39274492859840393
  - 0.4325374364852905
  - 0.38856416940689087
  - 0.4682181775569916
  - 0.47290489077568054
  - 0.426116406917572
  - 0.5341308116912842
  - 0.389889121055603
  - 0.40093958377838135
  - 0.5029600858688354
  - 0.39813342690467834
  - 0.3956584930419922
  - 0.4341723322868347
  - 0.411001592874527
  - 0.39232221245765686
  - 0.3878463804721832
  - 0.3917635381221771
  - 0.3861199915409088
  - 0.39045923948287964
loss_records_fold1:
  train_losses:
  - 1.7566961467266085
  - 7.46036251783371
  - 1.6400611311197282
  - 1.59419464468956
  - 1.6694919347763062
  - 6.100628632307053
  - 1.9492969393730164
  - 1.623070079088211
  - 2.064144605398178
  - 2.0082216739654544
  - 2.156832551956177
  - 2.458236598968506
  - 2.623141086101532
  - 1.6287844002246858
  - 1.6853395223617555
  - 1.7498172998428345
  - 1.6933831334114076
  validation_losses:
  - 0.40895581245422363
  - 0.45235174894332886
  - 0.4192792773246765
  - 0.4268348217010498
  - 0.4286150634288788
  - 0.40865078568458557
  - 0.40522903203964233
  - 0.4070262014865875
  - 0.4203833043575287
  - 0.4036106765270233
  - 0.47635114192962646
  - 0.4280921220779419
  - 0.41688063740730286
  - 0.402840256690979
  - 0.40268486738204956
  - 0.4084744453430176
  - 0.4026993215084076
loss_records_fold2:
  train_losses:
  - 1.8449386060237885
  - 1.8089752912521364
  - 3.287711799144745
  - 3.5735043346881867
  - 2.302906441688538
  - 2.0254062294960025
  - 1.8055674374103547
  - 2.72313661724329
  - 1.7670237839221956
  - 1.6945597589015962
  - 2.7448400616645814
  - 1.6065526902675629
  - 3.005681145191193
  - 2.0390288531780243
  - 3.9353661715984347
  - 2.136065512895584
  - 2.424549376964569
  - 2.5094076216220857
  - 5.167462551593781
  - 1.73529309630394
  - 2.4118230521678927
  - 1.9034800231456757
  - 2.686329418420792
  - 3.1316595256328585
  - 1.8613610029220582
  - 2.089582198858261
  - 1.8692307949066163
  - 1.7974497377872467
  - 1.6263442933559418
  - 3.238043314218521
  - 1.7106149792671204
  - 2.3961244463920592
  - 1.627890557050705
  - 2.0425036281347277
  - 2.7759872615337375
  - 4.85511236190796
  - 3.3074811875820163
  - 1.7953201174736024
  - 2.363643389940262
  - 1.8916559278964997
  - 1.7036193430423738
  validation_losses:
  - 0.42679524421691895
  - 0.38985151052474976
  - 0.39920395612716675
  - 0.4317670166492462
  - 0.49775007367134094
  - 0.4086152911186218
  - 0.4013281762599945
  - 0.40731409192085266
  - 0.3830034136772156
  - 0.390442430973053
  - 0.4053114950656891
  - 0.40290123224258423
  - 0.3868350684642792
  - 0.3945401608943939
  - 0.48329102993011475
  - 0.4056338667869568
  - 0.443439781665802
  - 0.41013339161872864
  - 0.3896089196205139
  - 0.3962802588939667
  - 0.420325368642807
  - 0.41075918078422546
  - 0.38499516248703003
  - 0.3986995816230774
  - 0.44130292534828186
  - 0.42364978790283203
  - 0.38618168234825134
  - 0.39398056268692017
  - 0.3910287618637085
  - 0.38214927911758423
  - 0.39838796854019165
  - 0.39244982600212097
  - 0.3924895226955414
  - 0.3916131854057312
  - 0.40972787141799927
  - 0.4053378999233246
  - 0.4091714322566986
  - 0.39716067910194397
  - 0.39986592531204224
  - 0.3999560475349426
  - 0.3871539235115051
loss_records_fold3:
  train_losses:
  - 1.6658684611320496
  - 1.8450846254825592
  - 2.353959023952484
  - 1.6360448420047762
  - 1.870243376493454
  - 1.7493771970272065
  - 1.6387465417385103
  - 1.7221555888652802
  - 1.5846959769725801
  - 1.719383180141449
  - 1.6753354966640472
  - 1.6197615325450898
  - 1.5776222109794618
  - 2.436663204431534
  - 2.2513202250003816
  - 2.830753153562546
  - 1.6895274937152864
  - 1.9772952139377595
  - 2.1867294371128083
  - 1.6321376681327822
  - 1.668660753965378
  validation_losses:
  - 0.39252179861068726
  - 0.43405067920684814
  - 0.4210148751735687
  - 0.39832571148872375
  - 0.4257846772670746
  - 0.39976516366004944
  - 0.39419132471084595
  - 0.3974705636501312
  - 0.3930853009223938
  - 0.41988104581832886
  - 0.4065261781215668
  - 0.4015742242336273
  - 0.3953632414340973
  - 0.3923603892326355
  - 0.7756869196891785
  - 0.3937269449234009
  - 0.40263527631759644
  - 0.40850719809532166
  - 0.4134942591190338
  - 0.39982515573501587
  - 0.40088722109794617
loss_records_fold4:
  train_losses:
  - 1.5549897968769075
  - 1.560057157278061
  - 1.5734563916921616
  - 1.6902398645877839
  - 1.602179229259491
  - 1.5730757594108582
  - 1.5485460460186005
  - 1.680001896619797
  - 1.7045173823833466
  - 1.537013903260231
  - 1.6106715321540834
  validation_losses:
  - 0.39621293544769287
  - 0.40611135959625244
  - 0.4065568149089813
  - 0.42786774039268494
  - 0.39925894141197205
  - 0.39231815934181213
  - 0.39554592967033386
  - 0.3960498571395874
  - 0.40103980898857117
  - 0.40622130036354065
  - 0.3992898762226105
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 50 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 41 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:12:50.860767'
