config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:43:08.437987'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_105fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_105fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_105fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_105fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_105fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_105fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_105fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_105fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_105fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_105fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 33.949903428554535
  - 22.031441229581834
  - 16.873679518699646
  - 11.621262466907503
  - 13.444873970746995
  - 8.389766895771027
  - 9.828650724887849
  - 9.238716217875481
  - 10.366808700561524
  - 17.83468397855759
  - 13.48909177184105
  - 7.893510288000107
  - 10.00870969593525
  - 10.989077106118202
  - 16.635424184799195
  - 8.658920848369599
  - 6.385806369781495
  - 5.155764994025231
  - 8.138289374113084
  - 7.806818354129792
  - 5.859099811315537
  - 10.017179322242738
  - 8.945849478244781
  - 3.88334721326828
  - 5.721922671794892
  - 9.394833543896675
  - 7.184623867273331
  - 8.396708035469056
  - 7.418053704500199
  - 4.242667812108993
  - 5.68769334256649
  - 9.041209930181504
  - 6.64287348985672
  - 5.102984637022018
  - 5.995355018973351
  - 6.285970783233643
  - 10.682646453380585
  - 5.176874306797981
  - 3.070135658979416
  - 3.714712911844254
  validation_losses:
  - 2.0094306468963623
  - 2.7932705879211426
  - 0.6053264141082764
  - 1.6706011295318604
  - 0.6140131950378418
  - 1.1569581031799316
  - 0.4268931448459625
  - 0.5152005553245544
  - 0.6323332190513611
  - 1.2749240398406982
  - 0.41170889139175415
  - 0.9809376001358032
  - 0.6648610830307007
  - 5.993046283721924
  - 0.5659979581832886
  - 0.47149378061294556
  - 0.5117461681365967
  - 0.9506763815879822
  - 0.39584264159202576
  - 0.5159462094306946
  - 0.4057612717151642
  - 0.499627947807312
  - 0.618199348449707
  - 0.38668185472488403
  - 0.41225895285606384
  - 0.4368801414966583
  - 0.38100191950798035
  - 0.4411643147468567
  - 0.40649786591529846
  - 0.40489768981933594
  - 0.46441933512687683
  - 0.8502394556999207
  - 0.43945077061653137
  - 0.5719214081764221
  - 0.41619259119033813
  - 0.4084685742855072
  - 0.3798678517341614
  - 0.38327398896217346
  - 0.3816892206668854
  - 0.38031068444252014
loss_records_fold1:
  train_losses:
  - 4.381508421897888
  - 4.026994472742081
  - 4.109212589263916
  - 4.03059213757515
  - 3.826301208138466
  - 5.6126272797584535
  - 3.3050292134284973
  - 3.5009770512580873
  - 4.088205116987228
  - 3.399521619081497
  - 3.6677965044975283
  - 3.2723350822925568
  - 3.9732391238212585
  - 3.4895433604717256
  - 3.354449132084847
  - 5.214718943834305
  - 4.631194084882736
  - 4.212906280159951
  - 3.6093171685934067
  - 3.9200981706380844
  - 3.437619960308075
  - 4.242340105772018
  - 3.073575419187546
  - 3.1034124553203584
  - 2.850763437151909
  - 3.230113595724106
  - 3.850577059388161
  - 3.0802009254693985
  - 3.319372329115868
  - 4.194082361459732
  - 3.121305447816849
  - 3.5326566368341448
  - 2.8908028423786165
  - 2.8958761543035507
  - 3.212047904729843
  - 3.5833818137645723
  - 3.680597233772278
  - 4.05418036878109
  - 3.53139328956604
  - 3.576853409409523
  - 3.716040086746216
  - 5.506771749258042
  - 4.406954610347748
  - 3.6327911555767063
  - 3.6010608673095703
  - 3.022877982258797
  - 2.9728430807590485
  - 3.0504143238067627
  validation_losses:
  - 0.39437586069107056
  - 0.5496094226837158
  - 0.5533447265625
  - 0.4669506847858429
  - 0.4296535849571228
  - 0.39819028973579407
  - 0.40745097398757935
  - 0.5076417326927185
  - 0.4035247564315796
  - 0.475134938955307
  - 0.3978325128555298
  - 0.3960057199001312
  - 1.0352303981781006
  - 0.5494989156723022
  - 0.43067002296447754
  - 0.517555296421051
  - 0.4217159152030945
  - 0.44280776381492615
  - 0.4286502003669739
  - 0.4530315101146698
  - 0.403330534696579
  - 0.41941019892692566
  - 0.4288008213043213
  - 0.40527772903442383
  - 0.3922947347164154
  - 0.46842432022094727
  - 0.39038360118865967
  - 0.5264140367507935
  - 0.4147685468196869
  - 0.4167622923851013
  - 0.7369260191917419
  - 0.39678773283958435
  - 0.40685880184173584
  - 0.4716997444629669
  - 0.4095238745212555
  - 0.3920053541660309
  - 0.42646968364715576
  - 0.3962746262550354
  - 0.4206918179988861
  - 0.5146540999412537
  - 0.40442079305648804
  - 0.541182816028595
  - 0.41197505593299866
  - 0.4145345687866211
  - 0.40621453523635864
  - 0.4068630337715149
  - 0.40351545810699463
  - 0.4024529457092285
loss_records_fold2:
  train_losses:
  - 3.020103931427002
  - 3.342210811376572
  - 3.802316576242447
  - 3.023260569572449
  - 3.4468016028404236
  - 3.1316316366195682
  - 3.2191654801368714
  - 3.7162081003189087
  - 3.0742892265319828
  - 3.2698767960071566
  - 3.4038763642311096
  - 2.9843195855617526
  - 3.2644197136163715
  - 3.9603613376617433
  - 3.313080900907517
  - 3.4006558060646057
  - 2.9811339318752292
  - 3.0085807085037235
  - 3.161008143424988
  - 3.246128755807877
  - 3.334274059534073
  - 3.065031808614731
  - 3.05752229988575
  - 3.302443188428879
  - 3.1521083205938343
  - 3.020489102602005
  - 3.2626340568065646
  validation_losses:
  - 0.38122954964637756
  - 0.3835577964782715
  - 0.4865349531173706
  - 0.42687585949897766
  - 0.5686845779418945
  - 0.41351068019866943
  - 0.47596681118011475
  - 0.38327276706695557
  - 0.39007094502449036
  - 0.38000285625457764
  - 0.4069114625453949
  - 0.42394959926605225
  - 0.49116232991218567
  - 0.3847511410713196
  - 0.42723479866981506
  - 0.4912113845348358
  - 0.43922045826911926
  - 0.3782144784927368
  - 0.4741804599761963
  - 0.3878922462463379
  - 0.4360480010509491
  - 0.38255658745765686
  - 0.38761985301971436
  - 0.39181792736053467
  - 0.3798447549343109
  - 0.37965109944343567
  - 0.3836557865142822
loss_records_fold3:
  train_losses:
  - 3.1857915580272675
  - 3.0797483652830127
  - 3.0303960472345355
  - 2.9735716849565508
  - 2.9532716423273087
  - 2.918745565414429
  - 3.2053804248571396
  - 3.420974624156952
  - 2.9627027779817583
  - 3.4367850691080095
  - 3.1367779940366747
  - 3.130214050412178
  - 2.9795102149248125
  - 2.9887106716632843
  - 3.010729330778122
  - 3.010418027639389
  - 2.856627434492111
  - 2.9349843561649323
  - 3.045965075492859
  - 3.231689929962158
  - 3.1950600624084475
  - 3.1762864828109745
  - 2.877711194753647
  - 2.9870617389678955
  - 3.5924037575721743
  - 3.167719948291779
  - 3.1516943991184236
  - 4.019857901334762
  - 3.1167444348335267
  validation_losses:
  - 0.40306174755096436
  - 0.3882156014442444
  - 0.39452606439590454
  - 0.3984009325504303
  - 0.39149051904678345
  - 0.38876083493232727
  - 0.38355231285095215
  - 0.39115601778030396
  - 0.39750128984451294
  - 0.3933858871459961
  - 0.4038543403148651
  - 0.4086245894432068
  - 0.42161664366722107
  - 0.40336617827415466
  - 0.4092184007167816
  - 0.400629460811615
  - 0.38545235991477966
  - 0.3915340006351471
  - 0.40498483180999756
  - 0.5760796666145325
  - 0.3929479122161865
  - 0.3902016580104828
  - 3.373600482940674
  - 0.3980085253715515
  - 0.4000857174396515
  - 0.39501550793647766
  - 0.4034326672554016
  - 0.3973577320575714
  - 0.40654054284095764
loss_records_fold4:
  train_losses:
  - 2.986555403470993
  - 3.0801421999931335
  - 2.9512627482414246
  - 2.945129442214966
  - 3.019005274772644
  - 3.086157606542111
  - 2.9640641868114472
  - 3.2142379581928253
  - 3.143692043423653
  - 2.9616484463214876
  - 2.9894108176231384
  - 3.0896703183650973
  - 2.95011730492115
  - 2.9948776900768284
  - 2.951014941930771
  - 2.944710946083069
  - 2.982043743133545
  - 3.006271278858185
  validation_losses:
  - 0.3910236656665802
  - 0.3942157030105591
  - 0.4400899112224579
  - 0.3928926885128021
  - 0.41202816367149353
  - 0.4003741145133972
  - 0.4370054006576538
  - 0.43703967332839966
  - 0.4104728102684021
  - 0.393135666847229
  - 0.3898751139640808
  - 0.43377387523651123
  - 0.40739449858665466
  - 0.4040476679801941
  - 0.39260849356651306
  - 0.39081671833992004
  - 0.3908177614212036
  - 0.38972967863082886
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 40 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 48 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:15:59.922770'
