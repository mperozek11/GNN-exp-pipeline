config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:46:20.729531'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_113fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_113fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_113fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_113fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_113fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_113fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_113fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_113fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_113fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_113fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.328400576114655
  - 3.1415283739566804
  - 3.06987664103508
  - 3.0597677528858185
  - 3.078421595692635
  - 3.0024660527706146
  - 2.988790971040726
  - 3.0034417599439625
  - 3.1358946919441224
  - 3.385757398605347
  - 3.068728366494179
  - 2.913846212625504
  - 2.9874130845069886
  - 2.9764568239450457
  - 3.0174321413040164
  validation_losses:
  - 0.45928841829299927
  - 0.4026581943035126
  - 0.40620365738868713
  - 0.39411085844039917
  - 0.4004978537559509
  - 0.4067497253417969
  - 0.4602898955345154
  - 0.4195376932621002
  - 0.5079649090766907
  - 0.3911198377609253
  - 0.39956027269363403
  - 0.3909042179584503
  - 0.3925114870071411
  - 0.3952745199203491
  - 0.4022287428379059
loss_records_fold1:
  train_losses:
  - 2.855902111530304
  - 2.8413363426923754
  - 2.8832760334014895
  - 2.9146490395069122
  - 2.910836136341095
  - 3.241889733076096
  - 2.889172595739365
  - 3.046417135000229
  - 3.097422727942467
  - 2.9424443900585175
  - 2.8253755152225497
  - 2.9302970975637437
  - 2.8557871341705323
  - 2.9168635666370393
  - 2.8425562977790833
  - 2.859764751791954
  - 2.8155654639005663
  - 2.8890418082475664
  - 3.0322956979274753
  - 2.9679823219776154
  - 3.009211188554764
  validation_losses:
  - 0.3990667164325714
  - 0.42810896039009094
  - 0.4070204794406891
  - 0.4179101586341858
  - 0.40251651406288147
  - 0.3964134454727173
  - 0.40310990810394287
  - 0.3965974450111389
  - 0.4078075885772705
  - 0.39002007246017456
  - 0.4138505756855011
  - 0.3914523422718048
  - 0.40430158376693726
  - 0.3889361321926117
  - 0.41376399993896484
  - 0.3874240219593048
  - 0.3894749879837036
  - 0.3864344656467438
  - 0.3948410451412201
  - 0.39761075377464294
  - 0.38623231649398804
loss_records_fold2:
  train_losses:
  - 2.860630834102631
  - 2.8848801463842393
  - 2.8803326785564423
  - 2.810796132683754
  - 2.915289229154587
  - 2.8215369164943698
  - 2.817728984355927
  - 2.95479898750782
  - 2.9015848457813265
  - 2.844647300243378
  - 2.870516285300255
  - 2.7962103426456455
  - 2.9592559665441516
  - 2.9164574563503267
  - 2.8629374682903292
  - 2.8089536249637606
  - 2.8139758199453357
  - 2.8117665827274325
  - 2.8740682482719424
  - 2.8173973470926286
  - 2.7698718398809437
  - 2.7820340394973755
  - 2.7961507678031925
  - 2.797564470767975
  - 2.8242254704236984
  - 2.791636580228806
  - 2.8063847035169602
  - 2.784063619375229
  - 2.852737066149712
  - 2.7998336225748064
  - 2.7139102756977085
  - 2.841205632686615
  - 2.77475346326828
  - 2.8072397112846375
  - 2.797933077812195
  - 2.7340868920087815
  - 2.7988986730575562
  - 2.736206102371216
  - 2.758825436234474
  - 2.820560419559479
  - 2.783652365207672
  - 2.760276383161545
  - 2.795173317193985
  - 2.784219777584076
  - 2.747165259718895
  - 2.8089827328920367
  - 2.823359596729279
  - 2.810176867246628
  - 2.8291322320699694
  - 2.8559263169765474
  - 2.8036049216985703
  - 2.7809739232063295
  - 2.7797108441591263
  - 2.769627952575684
  - 2.7765209257602694
  - 2.7495046257972717
  - 2.7608618378639225
  - 2.7638873755931854
  - 2.759118601679802
  - 2.733879786729813
  - 2.7679361641407016
  - 2.7688905864953997
  - 2.7423629760742188
  - 2.837965497374535
  - 2.8119745314121247
  - 2.787740570306778
  - 2.726564821600914
  - 2.7758640497922897
  - 2.742364874482155
  - 2.7650551319122316
  - 2.7161650836467746
  - 2.8128722071647645
  - 2.789341735839844
  - 2.7283433347940447
  - 2.753453314304352
  - 2.7860137045383455
  - 2.751627951860428
  - 2.767626702785492
  - 2.783521354198456
  - 2.7055224597454073
  - 2.7425836205482486
  - 2.740693470835686
  - 2.756684732437134
  - 2.736231300234795
  - 2.7418890804052354
  - 2.7555871397256855
  - 2.8445349961519244
  - 2.7546465694904327
  - 2.763911610841751
  - 2.76215261220932
  - 2.767689347267151
  - 2.729922515153885
  - 2.717954057455063
  - 2.7286623537540438
  - 2.7446474164724353
  - 2.786133569478989
  - 2.804752826690674
  - 2.805398413538933
  - 2.766033035516739
  - 2.7168990612030033
  validation_losses:
  - 0.38675010204315186
  - 0.40153753757476807
  - 0.39098241925239563
  - 0.38824713230133057
  - 0.4000369906425476
  - 0.38910961151123047
  - 0.404552161693573
  - 0.41433557868003845
  - 0.40437638759613037
  - 0.3957277834415436
  - 0.4078839421272278
  - 0.41297298669815063
  - 0.38974812626838684
  - 0.4456842839717865
  - 0.3844761550426483
  - 0.5191529989242554
  - 0.48240235447883606
  - 0.5549378395080566
  - 0.38807040452957153
  - 0.38214483857154846
  - 0.4262601435184479
  - 0.5125904083251953
  - 1.26688551902771
  - 0.745301365852356
  - 0.8361172676086426
  - 0.4298742413520813
  - 0.3840422034263611
  - 1.3194892406463623
  - 0.7332050204277039
  - 0.42246532440185547
  - 1.2554681301116943
  - 1.0191614627838135
  - 0.3964175581932068
  - 0.4844147562980652
  - 0.8463348150253296
  - 1.1487400531768799
  - 0.5226043462753296
  - 0.3904128968715668
  - 0.38468244671821594
  - 0.39152494072914124
  - 0.7952022552490234
  - 0.49624744057655334
  - 0.6026994585990906
  - 0.5083305239677429
  - 0.412853866815567
  - 0.6493738889694214
  - 0.42652222514152527
  - 0.42161035537719727
  - 0.39500176906585693
  - 0.4213244915008545
  - 0.38521555066108704
  - 0.4414447247982025
  - 0.38751810789108276
  - 0.4295603036880493
  - 0.38958874344825745
  - 0.40086519718170166
  - 0.4098084270954132
  - 0.43223822116851807
  - 0.4296051263809204
  - 0.44438859820365906
  - 0.38178306818008423
  - 0.3820253908634186
  - 0.4872376322746277
  - 0.45467430353164673
  - 0.5542182326316833
  - 0.38847091794013977
  - 0.4443427622318268
  - 1.0128512382507324
  - 0.7144616842269897
  - 0.46831193566322327
  - 0.6074075102806091
  - 0.962098240852356
  - 1.7914310693740845
  - 0.4127809703350067
  - 0.44884631037712097
  - 0.42135727405548096
  - 0.4773080348968506
  - 0.39848560094833374
  - 0.5952230095863342
  - 0.6166732907295227
  - 0.8445699214935303
  - 0.4871647357940674
  - 0.6068284511566162
  - 0.821615993976593
  - 0.5602226853370667
  - 0.6416906714439392
  - 0.46638378500938416
  - 0.7215189933776855
  - 0.9642823934555054
  - 0.38743898272514343
  - 0.6203227639198303
  - 0.537848174571991
  - 0.519845187664032
  - 0.650130569934845
  - 0.39119577407836914
  - 0.5665755271911621
  - 0.3943336009979248
  - 0.4045191705226898
  - 0.39451807737350464
  - 0.4372366666793823
loss_records_fold3:
  train_losses:
  - 2.7631451129913334
  - 2.7537305474281313
  - 2.750879639387131
  - 2.780178260803223
  - 2.833653783798218
  - 2.7672960042953494
  - 2.7888362944126133
  - 2.7798257887363436
  - 2.7481139391660694
  - 2.7749036848545074
  - 2.7807712584733966
  - 2.773889654874802
  - 2.7644464641809465
  - 2.762596392631531
  - 2.7481976956129075
  - 2.7513166636228563
  - 2.795962625741959
  - 2.73474298119545
  - 2.766486120223999
  - 2.747496345639229
  - 2.7771364003419876
  - 2.7663305550813675
  - 2.7559780538082124
  - 2.713125517964363
  - 2.7906776428222657
  - 2.7147392660379412
  - 2.714756900072098
  - 2.770720931887627
  - 2.772044163942337
  - 2.778984135389328
  - 2.788189333677292
  - 2.7619580745697023
  - 2.7118277639150623
  - 2.742200303077698
  - 2.734012407064438
  - 2.777415904402733
  - 2.726118579506874
  - 2.7317713737487797
  - 2.734327441453934
  - 2.791269060969353
  - 2.7864357411861422
  - 2.7460091531276705
  - 2.6804763674736023
  - 2.837422087788582
  - 2.773056736588478
  - 2.785355979204178
  - 2.7664570808410645
  - 2.730025124549866
  - 2.782834365963936
  - 2.7351314008235934
  - 2.7630023777484896
  - 2.699220204353333
  - 2.717971792817116
  - 2.715702563524246
  - 2.6618852227926255
  - 2.7753485053777696
  - 2.7271871060132984
  - 2.760542365908623
  - 2.839932453632355
  - 2.752792948484421
  - 2.7745555579662327
  - 2.774525034427643
  - 2.767361742258072
  - 2.735759708285332
  - 2.723997908830643
  - 2.7328615307807924
  - 2.708831986784935
  - 2.7600175321102145
  - 2.763214474916458
  - 2.690863013267517
  - 2.8686263859272003
  - 2.917799630761147
  - 2.7588145583868027
  - 2.7315115958452227
  - 2.691707962751389
  - 2.7463995397090915
  - 2.691134309768677
  - 2.6944447785615924
  - 2.6716804385185244
  - 2.7258612722158433
  - 2.754023098945618
  - 2.7301054418087007
  - 2.727957209944725
  - 2.6817778021097185
  - 2.7301030069589616
  - 2.6959884762763977
  - 2.6817673683166507
  - 2.7156141936779026
  - 2.773720860481262
  - 2.8061308920383454
  - 2.7497768700122833
  - 2.7586300373077393
  - 2.715187758207321
  - 2.694621601700783
  - 2.910224109888077
  - 2.7124029994010925
  - 2.7386845856904984
  - 2.7125960826873783
  - 2.7550413072109223
  - 2.756707647442818
  validation_losses:
  - 0.4278850853443146
  - 0.6073759198188782
  - 0.5836350321769714
  - 1.1487535238265991
  - 1.034715175628662
  - 0.6128804087638855
  - 0.5824416279792786
  - 1.9737979173660278
  - 0.8253086805343628
  - 3.2424967288970947
  - 0.8019176721572876
  - 0.7088329792022705
  - 0.5546409487724304
  - 0.6084469556808472
  - 1.3139644861221313
  - 1.0023430585861206
  - 0.4242217540740967
  - 1.4243258237838745
  - 0.8665079474449158
  - 1.2398715019226074
  - 0.8728085160255432
  - 4.138513565063477
  - 1.227257490158081
  - 0.6074602007865906
  - 0.9041708707809448
  - 1.1166818141937256
  - 1.125159740447998
  - 0.48918527364730835
  - 0.5773470997810364
  - 0.49695196747779846
  - 0.975464940071106
  - 0.7334174513816833
  - 0.694327175617218
  - 0.6104503273963928
  - 0.5613359212875366
  - 0.5967152118682861
  - 0.4367303252220154
  - 0.7547636032104492
  - 0.45362186431884766
  - 0.5828540325164795
  - 1.0417170524597168
  - 0.4082101881504059
  - 0.6560751795768738
  - 1.2266603708267212
  - 0.897188127040863
  - 0.52988201379776
  - 0.49267810583114624
  - 0.5946850180625916
  - 0.6231299638748169
  - 0.7084534764289856
  - 0.7548345327377319
  - 0.5711436867713928
  - 0.5230903625488281
  - 0.5120196342468262
  - 0.903378963470459
  - 0.5039604902267456
  - 0.536090612411499
  - 0.6079414486885071
  - 0.5958971381187439
  - 0.4351866841316223
  - 0.4149555563926697
  - 0.5312022566795349
  - 0.4055021405220032
  - 0.628784716129303
  - 0.6906861662864685
  - 0.6079832315444946
  - 0.6070384979248047
  - 0.4550495445728302
  - 0.6456215381622314
  - 0.7698376774787903
  - 0.8108254671096802
  - 0.3953167200088501
  - 0.601189136505127
  - 0.4731491506099701
  - 0.4720715582370758
  - 0.6304106712341309
  - 0.614434003829956
  - 0.5043506026268005
  - 0.4742507040500641
  - 0.8947882056236267
  - 0.8667742609977722
  - 1.3331992626190186
  - 1.0069048404693604
  - 1.0385499000549316
  - 1.1309077739715576
  - 0.7071340084075928
  - 1.849702000617981
  - 1.262983798980713
  - 0.5769547820091248
  - 0.5916363000869751
  - 0.5543211102485657
  - 0.9611567258834839
  - 1.4459267854690552
  - 1.2802355289459229
  - 1.2749258279800415
  - 1.456194281578064
  - 1.400496006011963
  - 1.0344325304031372
  - 1.6294671297073364
  - 1.6331870555877686
loss_records_fold4:
  train_losses:
  - 2.708941507339478
  - 2.736416551470757
  - 2.6706948906183245
  - 2.73741991519928
  - 2.7399127691984178
  - 2.6965071320533753
  - 2.745372399687767
  - 2.692722421884537
  - 2.7114163398742677
  - 2.738205409049988
  - 2.735374391078949
  - 2.685162276029587
  - 2.7001826584339144
  - 2.8874373495578767
  - 2.8340697169303897
  - 2.754629644751549
  - 2.7372428834438325
  - 2.705733522772789
  - 2.7310504555702213
  - 2.7726761132478717
  - 2.7823436200618747
  - 2.724149024486542
  - 2.747513318061829
  - 2.7361355274915695
  - 2.7447968959808353
  - 2.7320228219032288
  - 2.751656448841095
  - 2.7015008568763736
  - 2.7074553817510605
  - 2.758633625507355
  - 2.7313541859388355
  - 2.672675603628159
  - 2.702275183796883
  - 2.761971750855446
  - 2.678979840874672
  - 2.6824861019849777
  - 2.7173430919647217
  - 2.6952419877052307
  - 2.7838573515415193
  - 2.6846965938806537
  - 2.7239780604839328
  - 2.6951914191246034
  - 2.706416675448418
  - 2.69669326543808
  - 2.7162748128175735
  - 2.7246445268392563
  - 2.7781051993370056
  - 2.713773483037949
  - 2.72789743244648
  - 2.679400283098221
  - 2.6849450379610063
  - 2.7138798236846924
  - 2.7295820623636247
  - 2.68387568295002
  - 2.7199649661779404
  - 2.7059098809957507
  - 2.7457169741392136
  - 2.686878937482834
  - 2.6584942907094957
  - 2.7396426618099214
  - 2.693947485089302
  - 2.643997859954834
  - 2.71746983230114
  - 2.653354322910309
  - 2.6815186172723773
  - 2.6980485737323763
  - 2.6903240203857424
  - 2.7094930469989777
  - 2.740793135762215
  - 2.7431975811719895
  - 2.7178106099367145
  - 2.7579352855682373
  - 2.7138689517974854
  - 2.7082607954740525
  - 2.814601308107376
  - 2.740531948208809
  - 2.689326524734497
  - 2.7375582784414294
  - 2.7380941301584247
  - 2.7270099848508838
  - 2.6928572297096256
  - 2.686079773306847
  - 2.718517988920212
  - 2.7052068889141085
  - 2.651863247156143
  - 2.635632482171059
  - 2.71416709125042
  - 2.6972685754299164
  - 2.7416084796190265
  - 2.6939930737018587
  - 2.7612499535083774
  - 2.6802779108285906
  - 2.727450042963028
  - 2.726443439722061
  - 2.7089041382074357
  - 2.6459958106279373
  - 2.6946153342723846
  - 2.6417360305786133
  - 2.676311406493187
  - 2.68923434317112
  validation_losses:
  - 0.5597213506698608
  - 0.5029338598251343
  - 0.5383651852607727
  - 0.48976582288742065
  - 0.5411558747291565
  - 0.5959190726280212
  - 0.48522597551345825
  - 0.5151738524436951
  - 0.49582093954086304
  - 0.5418538451194763
  - 0.7174830436706543
  - 0.5632981061935425
  - 0.647680401802063
  - 0.38294899463653564
  - 0.38015252351760864
  - 0.38654616475105286
  - 0.4487672746181488
  - 0.4159852862358093
  - 0.4809667766094208
  - 0.40216341614723206
  - 0.45240139961242676
  - 0.4045950174331665
  - 0.42364266514778137
  - 0.5490070581436157
  - 0.4585930109024048
  - 0.4952276349067688
  - 0.4331285059452057
  - 0.4418398439884186
  - 0.466366708278656
  - 0.4214952290058136
  - 0.40916645526885986
  - 0.43236324191093445
  - 0.4661376178264618
  - 0.5055091977119446
  - 0.4968241751194
  - 0.486263632774353
  - 0.7147212624549866
  - 0.4105980694293976
  - 0.5011312961578369
  - 0.677808940410614
  - 1.1409879922866821
  - 1.0492686033248901
  - 0.9946958422660828
  - 0.6213418841362
  - 1.175766110420227
  - 1.3200454711914062
  - 1.4549927711486816
  - 0.7042437195777893
  - 0.5365705490112305
  - 2.386204242706299
  - 1.3140991926193237
  - 0.5278488993644714
  - 0.4515277147293091
  - 0.3879619538784027
  - 0.42629534006118774
  - 0.4169555604457855
  - 0.5332666635513306
  - 0.5180580615997314
  - 0.6220405101776123
  - 2.6963841915130615
  - 0.6472339034080505
  - 1.5139031410217285
  - 0.8074605464935303
  - 0.3894449770450592
  - 0.4237753450870514
  - 0.6632798314094543
  - 0.3747653365135193
  - 11.310126304626465
  - 0.37227824330329895
  - 0.36413872241973877
  - 0.3660777509212494
  - 0.37045392394065857
  - 0.3985443711280823
  - 0.4219355285167694
  - 0.3768033981323242
  - 0.5084861516952515
  - 0.4572398364543915
  - 0.4474375545978546
  - 0.4388391077518463
  - 0.48918387293815613
  - 0.46265363693237305
  - 0.809613049030304
  - 1.1367357969284058
  - 0.4181956350803375
  - 0.4120214879512787
  - 0.5005695819854736
  - 0.46151793003082275
  - 0.4383212625980377
  - 0.4887116551399231
  - 0.42984551191329956
  - 0.48237165808677673
  - 0.46810075640678406
  - 0.5378502607345581
  - 0.4543313980102539
  - 1.5273460149765015
  - 1.0132670402526855
  - 1.11544930934906
  - 1.6266180276870728
  - 2.4458911418914795
  - 0.6701017022132874
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8387650085763293, 0.8507718696397941,
    0.7938144329896907]'
  fold_eval_f1: '[0.0, 0.0, 0.2033898305084746, 0.20183486238532114, 0.23076923076923075]'
  mean_eval_accuracy: 0.8397234354830152
  mean_f1_accuracy: 0.1271987847326053
  total_train_time: '0:33:04.969971'
