config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:19:35.512379'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_55fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_55fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_55fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_55fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_55fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_55fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_55fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_55fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_55fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_55fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.1300479888916015
  - 0.8860730588436128
  - 0.9877835869789124
  - 0.874405574798584
  - 0.8461472392082214
  - 0.8853341579437256
  - 0.9011203467845917
  - 0.8415182173252106
  - 0.7895268440246582
  - 0.8593252122402192
  - 0.869143581390381
  - 0.916207093000412
  - 0.9804875671863557
  - 0.8806041121482849
  - 0.8083233088254929
  - 0.8473689675331116
  - 0.8325454533100128
  - 0.8215572893619538
  - 0.8073291718959809
  - 0.8163063883781434
  validation_losses:
  - 0.40392112731933594
  - 0.418841153383255
  - 0.4085114002227783
  - 0.42728129029273987
  - 0.44026365876197815
  - 0.4137577414512634
  - 0.42979204654693604
  - 0.41011831164360046
  - 0.39931121468544006
  - 0.406724750995636
  - 0.42754223942756653
  - 0.44354552030563354
  - 0.40149927139282227
  - 0.44486233592033386
  - 0.39273130893707275
  - 0.3904346525669098
  - 0.39142072200775146
  - 0.3963046967983246
  - 0.3903075158596039
  - 0.38976016640663147
loss_records_fold1:
  train_losses:
  - 0.7924879431724549
  - 0.825847589969635
  - 0.8426551997661591
  - 0.8243248641490937
  - 0.8670326292514802
  - 0.7975264966487885
  - 0.8185427546501161
  - 0.8014191567897797
  - 0.7916590631008149
  - 0.8201633036136627
  - 0.8133202731609345
  validation_losses:
  - 0.3932263255119324
  - 0.3905973732471466
  - 0.4111839234828949
  - 0.38974231481552124
  - 0.40064671635627747
  - 0.3924589455127716
  - 0.4016094207763672
  - 0.3958556056022644
  - 0.4022620916366577
  - 0.3991212546825409
  - 0.39985203742980957
loss_records_fold2:
  train_losses:
  - 0.7602839678525926
  - 0.8087791681289673
  - 0.7956817924976349
  - 0.7976806581020356
  - 0.7874001115560532
  - 0.8658197700977326
  - 0.7821957349777222
  - 0.8166748821735382
  - 0.7972119092941284
  - 0.8056693077087402
  - 0.7871327757835389
  validation_losses:
  - 0.3899765908718109
  - 0.38645097613334656
  - 0.3906768560409546
  - 0.3915160298347473
  - 0.39197051525115967
  - 0.39769646525382996
  - 0.3874582052230835
  - 0.3903801441192627
  - 0.38837820291519165
  - 0.38967615365982056
  - 0.3907528519630432
loss_records_fold3:
  train_losses:
  - 0.8290505766868592
  - 0.8353102803230286
  - 0.807935357093811
  - 0.8293434381484985
  - 0.7858555734157563
  - 0.8137579739093781
  - 0.8105437397956848
  - 0.8229601383209229
  - 0.8365532219409944
  - 0.826680487394333
  - 0.7983300507068635
  validation_losses:
  - 0.38852202892303467
  - 0.41643184423446655
  - 0.37484923005104065
  - 0.3787579834461212
  - 0.3942839801311493
  - 0.3723810017108917
  - 0.3777422606945038
  - 0.3724241852760315
  - 0.38150596618652344
  - 0.377776175737381
  - 0.37841492891311646
loss_records_fold4:
  train_losses:
  - 0.7949449360370636
  - 0.8383104026317597
  - 0.7687389105558395
  - 0.7901301920413971
  - 0.8150927126407623
  - 0.7603007644414902
  - 0.7909663856029511
  - 0.8185647964477539
  - 0.8291414499282838
  - 0.7780591309070588
  - 0.772711318731308
  - 0.7988159656524658
  - 0.8216542601585388
  - 0.7846369624137879
  - 0.8093980133533478
  - 0.97992422580719
  - 0.872380256652832
  validation_losses:
  - 0.3816449046134949
  - 0.3818912208080292
  - 0.39346539974212646
  - 0.37164306640625
  - 0.39888909459114075
  - 0.38022473454475403
  - 0.38046810030937195
  - 0.4213453531265259
  - 0.3849812150001526
  - 0.3950175344944
  - 0.445772260427475
  - 0.44010281562805176
  - 0.3893083333969116
  - 0.387042760848999
  - 0.38958925008773804
  - 0.38541746139526367
  - 0.3761920928955078
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:05:56.508255'
