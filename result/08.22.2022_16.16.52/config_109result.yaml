config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:45:06.135873'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_109fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_109fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_109fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_109fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_109fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_109fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_109fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_109fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_109fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_109fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 55.686771321296696
  - 29.824448931217194
  - 19.50512615442276
  - 7.978212767839432
  - 14.990418845415116
  - 18.937745118141176
  - 10.546635240316391
  - 11.512617594003679
  - 15.861542350053789
  - 13.971892398595811
  - 10.83033347427845
  - 11.43260304927826
  - 19.611685526371005
  - 30.96318719983101
  - 18.360735476017
  - 10.148726600408555
  - 8.355716633796693
  - 4.700103896856308
  - 8.487048125267028
  - 11.100724065303803
  - 9.544431388378143
  - 9.997092235088349
  - 11.418875372409822
  - 8.58225857615471
  - 8.987773901224136
  - 8.923393523693084
  - 10.95525627732277
  - 7.562160724401474
  - 15.99197228550911
  - 5.747091162204743
  - 11.544782656431199
  - 7.58741216659546
  - 7.536110210418702
  - 10.7664235830307
  - 7.153714787960053
  - 6.974453204870224
  - 6.63373689353466
  - 5.183184975385666
  - 3.848789191246033
  - 6.3438698053359985
  - 4.377920335531235
  - 6.2089259147644045
  - 6.655673885345459
  - 5.125685334205627
  - 7.749201348423958
  - 4.364803129434586
  - 4.7383586227893835
  - 3.5705293118953705
  - 3.985842299461365
  validation_losses:
  - 14.68349838256836
  - 0.91382896900177
  - 0.7588346004486084
  - 0.7150890827178955
  - 1.0262237787246704
  - 0.9694531559944153
  - 0.6228291988372803
  - 0.5662458539009094
  - 1.2876520156860352
  - 0.5710194706916809
  - 0.8900238871574402
  - 0.49457836151123047
  - 1.0773617029190063
  - 1.1342315673828125
  - 0.4215695858001709
  - 0.4901469647884369
  - 0.5549336075782776
  - 0.47021999955177307
  - 0.419484943151474
  - 0.4810653030872345
  - 0.5155066251754761
  - 0.4241753816604614
  - 0.5957114696502686
  - 0.42503032088279724
  - 0.4329977035522461
  - 0.6237431168556213
  - 0.43098631501197815
  - 0.4376341998577118
  - 0.42123302817344666
  - 0.42877739667892456
  - 0.44451045989990234
  - 0.42018193006515503
  - 0.5021161437034607
  - 0.4916673004627228
  - 0.4081759750843048
  - 0.4483600854873657
  - 0.40498727560043335
  - 0.5478096604347229
  - 0.40244409441947937
  - 0.42237675189971924
  - 0.5816245079040527
  - 0.3867872953414917
  - 0.42315012216567993
  - 0.39081045985221863
  - 0.392180472612381
  - 0.3871335983276367
  - 0.37645235657691956
  - 0.38468319177627563
  - 0.39378973841667175
loss_records_fold1:
  train_losses:
  - 4.792444598674774
  - 5.124803173542023
  - 4.74863760471344
  - 3.5437064945697787
  - 4.5384290575981145
  - 3.2758179157972336
  - 5.660891601443291
  - 5.232771307229996
  - 4.170732110738754
  - 4.095589888095856
  - 5.135100144147874
  - 3.929880052804947
  - 4.37017577290535
  - 3.3944216132164002
  - 3.237619322538376
  - 3.0786080300807956
  - 3.27135551571846
  - 3.4686646431684496
  - 3.705954670906067
  - 3.205718269944191
  - 4.120449954271317
  - 3.840774911642075
  - 4.361138641834259
  - 3.2748653292655945
  - 3.9112602114677433
  - 3.4308027923107147
  - 3.564839386940003
  - 3.7177681744098665
  - 3.638384622335434
  - 3.1473955452442173
  - 3.8298714220523835
  - 3.681290477514267
  - 4.793557131290436
  - 4.587415879964829
  - 4.531624436378479
  - 4.338216531276703
  - 3.237029302120209
  - 3.78945826292038
  - 3.2940999031066895
  - 3.708014088869095
  - 3.519177252054215
  - 4.131814014911652
  - 3.967307409644127
  - 3.7661545068025593
  - 3.2621713101863863
  - 3.725776821374893
  - 3.110619390010834
  - 3.65715611577034
  - 3.4943848669528963
  - 3.3433331787586216
  - 3.2342987418174745
  - 3.2955182909965517
  - 3.3812998950481417
  - 3.4370712608098986
  - 3.343106019496918
  - 3.6870456993579865
  - 3.505836778879166
  - 3.3909447968006137
  - 3.598021107912064
  - 3.4078758984804156
  - 3.5177749902009965
  - 3.221418881416321
  - 3.400676041841507
  - 3.39101579785347
  - 3.241725206375122
  - 3.361281743645668
  - 3.2323599219322205
  - 3.201298898458481
  - 3.153938388824463
  - 3.2734494119882585
  - 3.365822118520737
  - 3.4018398076295853
  - 3.3584174454212192
  - 3.6744027197360993
  - 3.736948895454407
  - 3.4895833611488345
  - 4.644577693939209
  - 7.572418498992921
  - 3.370164057612419
  - 3.373279976844788
  - 3.2618108034133915
  - 3.341853028535843
  - 4.544788682460785
  - 3.6455921292304994
  - 3.419582813978195
  - 3.227452012896538
  - 3.2555641293525697
  - 3.4942793011665345
  - 3.229315823316574
  - 3.3787581622600555
  - 3.3041599273681643
  - 3.271099579334259
  - 3.357115089893341
  - 3.3372885644435883
  - 3.1223793506622317
  - 3.1969334065914157
  - 3.3360026597976686
  - 3.032009756565094
  - 3.2079398334026337
  - 3.1214675545692447
  validation_losses:
  - 1.0247993469238281
  - 0.41951003670692444
  - 0.4086075723171234
  - 0.5536442399024963
  - 0.4452938139438629
  - 0.417087584733963
  - 0.43384841084480286
  - 0.5316119194030762
  - 0.4098392426967621
  - 0.43728938698768616
  - 0.5818577408790588
  - 0.4019758105278015
  - 0.4222102761268616
  - 0.45268893241882324
  - 0.46061280369758606
  - 0.4267490804195404
  - 0.43510815501213074
  - 0.42226913571357727
  - 0.4156108498573303
  - 0.4624827206134796
  - 0.41676753759384155
  - 0.45340684056282043
  - 0.44897523522377014
  - 0.3937290608882904
  - 0.4029044806957245
  - 0.40997177362442017
  - 0.4708414375782013
  - 0.4203064739704132
  - 0.4153428375720978
  - 0.47002342343330383
  - 0.44166579842567444
  - 0.40153953433036804
  - 0.4293019771575928
  - 0.41457095742225647
  - 0.3969505727291107
  - 0.4093230068683624
  - 0.4243042767047882
  - 0.5405451655387878
  - 0.42436373233795166
  - 0.40854910016059875
  - 0.4689945578575134
  - 0.42393577098846436
  - 0.42768794298171997
  - 0.44129619002342224
  - 0.5141398310661316
  - 0.43495064973831177
  - 0.4232504963874817
  - 0.4818374514579773
  - 0.487606942653656
  - 0.39587700366973877
  - 0.404297411441803
  - 0.5204728841781616
  - 0.41101446747779846
  - 0.43648457527160645
  - 0.45005369186401367
  - 1.4181400537490845
  - 0.4128594398498535
  - 0.42712873220443726
  - 0.4155362546443939
  - 0.4204062223434448
  - 0.40399548411369324
  - 0.4319517910480499
  - 0.46912050247192383
  - 0.48255985975265503
  - 0.41365131735801697
  - 0.5081421732902527
  - 0.4285549521446228
  - 0.42634108662605286
  - 0.44649815559387207
  - 0.44302865862846375
  - 0.43336185812950134
  - 0.4277253746986389
  - 0.4580775499343872
  - 1.0708584785461426
  - 0.40549585223197937
  - 0.41143491864204407
  - 0.5495697855949402
  - 0.4739324450492859
  - 0.45501092076301575
  - 0.44070541858673096
  - 0.4261411428451538
  - 24.564441680908203
  - 0.490501344203949
  - 0.43186789751052856
  - 0.41540712118148804
  - 0.4109211266040802
  - 0.42236611247062683
  - 0.5906006097793579
  - 0.4351848065853119
  - 0.42335978150367737
  - 0.42192205786705017
  - 0.43297868967056274
  - 0.40858426690101624
  - 0.4472479820251465
  - 0.4071089029312134
  - 0.47100281715393066
  - 0.5119640827178955
  - 0.403620183467865
  - 0.46395143866539
  - 0.5488498210906982
loss_records_fold2:
  train_losses:
  - 3.3134807467460634
  - 3.124418878555298
  - 3.239638900756836
  - 3.1382954329252244
  - 3.1516679465770725
  - 3.165988063812256
  - 3.253924638032913
  - 3.217688021063805
  - 3.278497588634491
  - 3.3684261918067935
  - 3.2624885320663455
  - 3.312954241037369
  - 3.201574295759201
  - 3.290328875184059
  - 3.218313767015934
  - 3.2986381828784945
  - 3.238927185535431
  - 3.3231130212545397
  - 3.182577884197235
  - 3.157338112592697
  - 3.203094905614853
  - 3.1741263568401337
  - 3.2430917203426364
  - 3.1397110342979433
  - 3.2521790802478794
  - 3.2013300478458406
  - 3.1523294657468797
  - 3.2470011711120605
  - 3.238471353054047
  - 3.5171090066432953
  - 3.137193810939789
  - 3.1730007171630863
  - 3.2486120849847797
  - 3.155146872997284
  - 3.292797213792801
  - 3.189344930648804
  - 3.1873540312051776
  - 4.4741658687591555
  - 4.066933804750443
  - 7.132791304588318
  - 4.582731807231903
  - 3.684798461198807
  - 3.474140176177025
  - 3.4609520316123965
  - 3.206457591056824
  - 3.320202568173409
  - 3.2781136453151705
  - 3.4408717811107636
  - 3.4140355408191683
  - 3.3883078277111056
  - 3.1087484002113346
  - 3.15562681555748
  - 3.098634567856789
  - 3.096386927366257
  - 3.145985531806946
  validation_losses:
  - 0.3910161256790161
  - 0.39450308680534363
  - 0.41882702708244324
  - 0.3911289572715759
  - 0.39022573828697205
  - 0.39347437024116516
  - 0.39312097430229187
  - 0.42698201537132263
  - 0.41956907510757446
  - 0.4507986605167389
  - 3.3147928714752197
  - 0.41841039061546326
  - 0.3985103666782379
  - 0.3942982852458954
  - 0.4060821235179901
  - 0.40910807251930237
  - 0.41849154233932495
  - 5.047175407409668
  - 0.4037107527256012
  - 0.7347407341003418
  - 0.4152417480945587
  - 0.3982451260089874
  - 0.41865962743759155
  - 0.3891982436180115
  - 0.39415720105171204
  - 0.38971611857414246
  - 0.4142109155654907
  - 0.39690467715263367
  - 0.42119625210762024
  - 0.4105747938156128
  - 0.39568954706192017
  - 0.4273838698863983
  - 0.4079516530036926
  - 0.40725216269493103
  - 0.4034077525138855
  - 6.137088298797607
  - 33.515525817871094
  - 0.40570783615112305
  - 0.41477930545806885
  - 0.4017969071865082
  - 0.3887617588043213
  - 0.40725964307785034
  - 0.4063185453414917
  - 0.42808160185813904
  - 0.39914387464523315
  - 0.3976882994174957
  - 0.40845805406570435
  - 0.40361320972442627
  - 0.44442513585090637
  - 0.4104328155517578
  - 0.3884947896003723
  - 0.39593055844306946
  - 0.3906891942024231
  - 0.4003945291042328
  - 0.3950749635696411
loss_records_fold3:
  train_losses:
  - 3.199694645404816
  - 3.1609001934528353
  - 3.1686263978481293
  - 3.3923265725374225
  - 3.3451840698719026
  - 3.1386748611927033
  - 3.250653564929962
  - 3.192647099494934
  - 3.172191390395165
  - 3.233070078492165
  - 3.1400754749774933
  - 3.6454900205135345
  - 3.22822662293911
  - 3.279434615373612
  - 3.2163977265357975
  - 3.348078241944313
  - 3.4900992929935457
  - 3.2019016683101658
  - 3.271444630622864
  - 3.6979647755622866
  - 3.4624564290046695
  - 6.891957232356072
  - 4.470312041044235
  - 3.4927199959754947
  - 5.891437929868698
  - 9.519818556308747
  - 7.267881584167481
  - 6.029828369617462
  - 7.558935469388962
  - 3.4169226408004763
  - 4.182781523466111
  - 4.231351405382156
  - 3.748775315284729
  - 4.17521699666977
  - 3.496328377723694
  - 3.90940300822258
  - 4.142061963677406
  - 3.259593087434769
  - 4.072182655334473
  - 3.2791810572147373
  - 3.194038248062134
  - 3.663674384355545
  - 3.369905960559845
  - 3.439983540773392
  - 3.253142213821411
  - 3.1827258050441745
  - 3.416757929325104
  - 3.2719720602035522
  - 3.285974246263504
  - 3.380460649728775
  - 3.2624033927917484
  - 3.444110244512558
  - 3.2692017167806626
  - 3.6172580361366276
  - 4.690183559060097
  - 3.324491995573044
  - 3.287575787305832
  - 3.56485897898674
  - 3.578280916810036
  - 3.733257967233658
  - 3.453527569770813
  - 3.4093394041061402
  - 3.126066166162491
  - 3.231840318441391
  - 3.4963806331157685
  - 3.3203446298837664
  - 3.4191106081008913
  - 3.366461157798767
  - 3.219174176454544
  - 3.460085773468018
  - 3.269459876418114
  - 3.3477537989616395
  - 3.2887217700481415
  - 3.4030250906944275
  - 3.307300689816475
  - 3.169858026504517
  - 3.422025889158249
  - 3.3803616881370546
  - 3.2073656857013706
  - 3.329914703965187
  - 3.2067332684993746
  - 3.1932450175285343
  - 3.2534503221511843
  - 3.3246712744235993
  - 3.2244102120399476
  - 3.2336352646350863
  - 3.2585539758205417
  - 3.2167990267276765
  - 3.0810386776924137
  - 3.085240250825882
  - 3.2517019450664524
  - 3.3031576037406922
  - 3.218705815076828
  - 3.1794562101364137
  - 3.227283710241318
  - 3.2558339029550556
  - 3.180694055557251
  - 3.1544973194599155
  - 3.1640586376190187
  - 3.2423363626003265
  validation_losses:
  - 0.4353988468647003
  - 0.40490958094596863
  - 0.4185795187950134
  - 0.428640216588974
  - 0.42410045862197876
  - 0.42437222599983215
  - 0.4105875492095947
  - 0.4784490466117859
  - 0.47543078660964966
  - 0.42605292797088623
  - 0.418490469455719
  - 0.4053742289543152
  - 0.4206322133541107
  - 0.4067757725715637
  - 0.4087240695953369
  - 0.4453575313091278
  - 0.410619854927063
  - 0.4253664016723633
  - 1.2642468214035034
  - 0.4185214936733246
  - 0.41245993971824646
  - 0.8080602288246155
  - 0.4343757927417755
  - 0.43479791283607483
  - 24.750925064086914
  - 0.4801582396030426
  - 0.466067910194397
  - 0.5049750804901123
  - 0.42581263184547424
  - 0.4164468050003052
  - 0.47664782404899597
  - 0.497146338224411
  - 0.46016278862953186
  - 0.40731677412986755
  - 0.4143357276916504
  - 0.4318733811378479
  - 0.4106456935405731
  - 0.4219798147678375
  - 0.40874865651130676
  - 0.4106871783733368
  - 0.41246291995048523
  - 0.5121062397956848
  - 0.4175513684749603
  - 0.4118247926235199
  - 0.41254037618637085
  - 0.4195308983325958
  - 0.44117432832717896
  - 0.40674421191215515
  - 0.5866888761520386
  - 0.4051092267036438
  - 0.40623098611831665
  - 0.4126015305519104
  - 0.5122894644737244
  - 0.42497920989990234
  - 0.40794748067855835
  - 0.44285351037979126
  - 0.40007519721984863
  - 0.41805770993232727
  - 0.4089665412902832
  - 0.43267524242401123
  - 0.40532100200653076
  - 0.39538314938545227
  - 0.426425576210022
  - 0.40773117542266846
  - 0.4733259677886963
  - 0.40720757842063904
  - 0.4096376597881317
  - 0.44891390204429626
  - 0.4191576838493347
  - 0.5145465135574341
  - 0.4055778980255127
  - 0.4113486111164093
  - 0.4393463432788849
  - 0.41797390580177307
  - 0.4104108512401581
  - 0.4106486737728119
  - 0.4391702711582184
  - 0.40043628215789795
  - 209.21026611328125
  - 1040579.1875
  - 5736559.5
  - 0.44098347425460815
  - 0.4418767988681793
  - 0.4255058765411377
  - 0.39957889914512634
  - 0.44925352931022644
  - 0.4353318512439728
  - 0.4031881093978882
  - 0.413215309381485
  - 0.41331082582473755
  - 0.41412481665611267
  - 0.4236694574356079
  - 0.39884325861930847
  - 0.42188945412635803
  - 0.4087439775466919
  - 0.4247928857803345
  - 0.4065501093864441
  - 0.4293300211429596
  - 0.4414847791194916
  - 0.39860254526138306
loss_records_fold4:
  train_losses:
  - 3.2098890006542207
  - 3.198152279853821
  - 3.1561804533004763
  - 3.3049206882715225
  - 3.254189294576645
  - 3.1422235101461413
  - 3.246161550283432
  - 3.321428868174553
  - 3.1834497511386872
  - 3.315121641755104
  - 3.3764555335044864
  - 3.1920707464218143
  - 3.2285140335559848
  - 3.136518007516861
  - 3.268539345264435
  - 3.2237300455570224
  - 3.228007706999779
  - 3.1577662587165833
  - 3.1177944242954254
  - 3.197903662919998
  - 3.154355299472809
  - 3.2244172513484957
  - 3.2066255629062654
  - 3.254294964671135
  - 3.2351298451423647
  - 3.2047543585300446
  - 3.3330720722675324
  - 3.161524099111557
  - 3.212834519147873
  - 3.187866634130478
  - 3.3104734838008882
  - 3.1165377974510196
  - 3.249130564928055
  - 3.337723419070244
  validation_losses:
  - 0.400633305311203
  - 0.41129857301712036
  - 0.406243234872818
  - 0.4820098876953125
  - 0.40906423330307007
  - 0.41737204790115356
  - 0.4023796319961548
  - 0.41117140650749207
  - 1.486354947090149
  - 0.397966206073761
  - 0.3987259864807129
  - 0.41092371940612793
  - 0.4061933755874634
  - 0.40825125575065613
  - 0.4492652714252472
  - 0.4080454707145691
  - 0.42426368594169617
  - 0.4058349132537842
  - 0.4145115613937378
  - 0.40537145733833313
  - 0.43996039032936096
  - 0.42289605736732483
  - 0.4159817397594452
  - 0.4014132022857666
  - 0.40744203329086304
  - 0.3934265971183777
  - 0.4195204973220825
  - 0.4330320954322815
  - 0.43149539828300476
  - 0.4078158438205719
  - 0.4034591317176819
  - 0.4123653471469879
  - 0.41115304827690125
  - 0.41679009795188904
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 49 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 55 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:33:41.539290'
