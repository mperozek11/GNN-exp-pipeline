config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:28:42.252923'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_63fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_63fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_63fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_63fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_63fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_63fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_63fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_63fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_63fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_63fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 24.544122791290285
  - 13.250511360168458
  - 13.883460235595704
  - 5.155578362941743
  - 4.984703087806702
  - 3.5412821531295777
  - 4.241004073619843
  - 3.37422171831131
  - 2.448450517654419
  - 2.897307014465332
  - 2.0806236267089844
  - 1.9186735033988953
  - 1.6584576785564424
  - 1.7041584730148316
  - 1.1769651174545288
  - 1.6440026044845581
  - 1.7577203065156937
  - 1.7140887260437012
  - 1.2100552737712862
  - 1.0511552274227143
  - 1.174170082807541
  - 0.9580966413021088
  - 2.452457541227341
  - 3.4107375621795657
  - 10.882059526443483
  - 2.9105790972709658
  - 1.7635885298252107
  - 1.7607374846935273
  - 1.709973329305649
  - 1.230967926979065
  - 1.2695557951927186
  - 2.2236670136451724
  - 0.9686917901039124
  - 1.3047990024089815
  - 1.1452206611633302
  - 1.5492712497711183
  - 2.241574412584305
  - 2.1353963255882262
  - 1.5387438893318177
  - 1.3533206343650819
  - 1.1369277238845825
  - 1.124060893058777
  - 2.28684264421463
  - 1.160397356748581
  - 0.879975038766861
  - 0.9933572173118592
  - 1.0858457922935487
  - 0.9731703042984009
  - 1.062112718820572
  - 0.9749126732349396
  - 1.0126393377780916
  - 0.8673814296722413
  - 0.9381972014904023
  - 0.8601104557514191
  - 1.03983034491539
  - 1.4907917559146882
  - 1.0785535156726838
  - 1.3511290073394777
  - 1.0947429537773132
  - 1.0892279982566835
  - 0.9088520884513855
  - 1.109765362739563
  - 0.8247057735919953
  - 1.154207444190979
  - 1.0235055267810822
  - 1.0818114459514618
  - 1.2545033395290375
  - 1.1361854076385498
  - 2.38296474814415
  - 1.056652018427849
  - 0.9860203087329865
  - 0.8417967975139619
  - 1.6194600105285646
  - 0.9968633174896241
  - 0.8831703186035157
  - 0.9429429262876511
  - 0.8721140086650849
  - 0.8533815681934357
  - 0.8779844105243684
  - 0.8654649376869202
  - 0.9261714637279511
  - 0.9310935199260713
  - 1.0097147047519683
  - 0.8313124358654023
  - 0.8847314834594727
  - 0.8905018746852875
  - 0.9042410790920258
  - 0.8935051739215851
  - 0.948328596353531
  - 0.988096046447754
  - 1.139982908964157
  - 0.9783938229084015
  - 2.770137506723404
  - 2.193587926030159
  - 0.9830803036689759
  - 0.9638389170169831
  - 1.7163759797811509
  - 1.1496629595756531
  - 1.0776179134845734
  - 0.9700064361095428
  validation_losses:
  - 5.982273578643799
  - 1.664068579673767
  - 2.3400979042053223
  - 2.041865348815918
  - 1.8331037759780884
  - 1.3329898118972778
  - 1.4514739513397217
  - 2.046797275543213
  - 0.8512977361679077
  - 0.6593493819236755
  - 0.7261862754821777
  - 0.6299652457237244
  - 0.5491578578948975
  - 0.4379854202270508
  - 0.6080036759376526
  - 0.4417193531990051
  - 0.7103257775306702
  - 0.7210241556167603
  - 0.4678448438644409
  - 0.4251730144023895
  - 0.4640539586544037
  - 0.39853712916374207
  - 0.5097451210021973
  - 2.5418365001678467
  - 1.012087106704712
  - 1.3277969360351562
  - 0.5014510154724121
  - 0.48374199867248535
  - 0.6298558712005615
  - 0.49735164642333984
  - 0.5585372447967529
  - 0.4282133877277374
  - 0.43287619948387146
  - 0.4530719220638275
  - 0.4978695511817932
  - 0.6784263253211975
  - 0.6473500728607178
  - 0.7088761925697327
  - 0.43896710872650146
  - 0.4169384837150574
  - 0.5030683875083923
  - 0.48067668080329895
  - 0.49413976073265076
  - 0.46117863059043884
  - 0.42694976925849915
  - 0.3880394399166107
  - 0.4118305444717407
  - 0.3976669907569885
  - 0.3896099627017975
  - 0.42724770307540894
  - 0.42497703433036804
  - 0.41240572929382324
  - 0.4059499502182007
  - 0.4197198450565338
  - 0.526749312877655
  - 0.5675599575042725
  - 0.6695926189422607
  - 0.609639048576355
  - 0.5384471416473389
  - 0.40845486521720886
  - 0.40105965733528137
  - 0.3928312659263611
  - 0.45151233673095703
  - 0.4312525987625122
  - 0.4238475561141968
  - 0.4583723545074463
  - 0.39133548736572266
  - 0.42371463775634766
  - 0.3855336606502533
  - 0.4371557831764221
  - 0.394308477640152
  - 0.39979851245880127
  - 0.5254422426223755
  - 0.41433975100517273
  - 0.3868744969367981
  - 0.3927212953567505
  - 0.3878336250782013
  - 0.3857971429824829
  - 0.4218968152999878
  - 0.39756521582603455
  - 0.40887969732284546
  - 0.417143315076828
  - 0.40694212913513184
  - 0.4108940362930298
  - 0.4471887946128845
  - 0.5730697512626648
  - 0.4451792538166046
  - 0.4189857840538025
  - 0.3981451690196991
  - 0.4436182379722595
  - 0.48879534006118774
  - 0.46254032850265503
  - 0.5147858262062073
  - 0.42557859420776367
  - 0.4270360767841339
  - 0.4224696755409241
  - 0.4119972884654999
  - 0.4446660280227661
  - 0.38494759798049927
  - 0.3949263095855713
loss_records_fold1:
  train_losses:
  - 1.2492855072021485
  - 1.1316569387912752
  - 0.8620373904705048
  - 0.9711698651313783
  - 0.9009259223937989
  - 0.8187103629112245
  - 0.8049632549285889
  - 1.0583427667617797
  - 0.8258973449468613
  - 0.9226576745510102
  - 0.814583295583725
  - 3.5897394478321076
  - 2.455084264278412
  - 9.133002585172653
  - 1.3367477774620058
  - 1.6737714827060701
  - 1.0485576450824738
  - 0.9673673450946808
  - 0.9545523345470429
  - 0.9768037378787995
  - 0.9105972766876221
  - 0.8561074495315553
  - 0.9173197388648987
  - 9.24786331653595
  - 0.8059686958789826
  - 0.9952308416366578
  - 0.8724896013736725
  - 1.3701333820819857
  - 4.309940999746323
  - 0.9403856933116913
  - 3.7792530417442323
  - 0.8923561334609986
  - 0.8818653047084809
  - 0.8620834320783616
  - 0.8037427812814713
  - 1.9450636923313143
  - 1.0161524772644044
  - 1.2024620592594149
  - 1.7910807132720947
  - 1.0540306746959687
  - 0.9127293825149536
  - 0.982353150844574
  - 0.9291416287422181
  - 0.8976896643638611
  - 0.8256048291921616
  - 0.8436649918556214
  - 0.8388031542301179
  - 0.8200571060180665
  - 0.8629831552505494
  - 1.1401719927787781
  - 0.8880358815193177
  - 0.8765930652618409
  - 0.960013473033905
  - 0.802936115860939
  - 1.0239212930202484
  - 0.8478067457675934
  - 0.9139710962772369
  - 0.9968407273292542
  - 0.9046922862529755
  - 1.3725234538316728
  validation_losses:
  - 0.5796099305152893
  - 0.4082322418689728
  - 0.41306376457214355
  - 0.4304925799369812
  - 0.40292710065841675
  - 0.4209720492362976
  - 0.3935476243495941
  - 0.4046340584754944
  - 0.4255717992782593
  - 0.39789238572120667
  - 0.43199631571769714
  - 0.47268882393836975
  - 0.4440387189388275
  - 0.4992067515850067
  - 1.4137260913848877
  - 0.3972075581550598
  - 0.43575286865234375
  - 0.4031464457511902
  - 0.40628913044929504
  - 0.4586905241012573
  - 0.4846724271774292
  - 0.4203142523765564
  - 0.40703171491622925
  - 0.4319058358669281
  - 0.4338962435722351
  - 0.40318185091018677
  - 0.4019883871078491
  - 0.9413802027702332
  - 0.4694044291973114
  - 0.44629138708114624
  - 0.40429672598838806
  - 0.4012828469276428
  - 0.42034223675727844
  - 0.3973565101623535
  - 0.4160221815109253
  - 0.5607141256332397
  - 0.5639699697494507
  - 0.6135382056236267
  - 0.7218456268310547
  - 0.470304936170578
  - 0.4306020140647888
  - 0.4485851228237152
  - 0.41291382908821106
  - 0.4103028178215027
  - 0.4582364857196808
  - 0.4475734233856201
  - 0.4032815396785736
  - 0.4118749797344208
  - 0.414050430059433
  - 0.4343286454677582
  - 0.44700464606285095
  - 0.41641324758529663
  - 0.4165860414505005
  - 0.4732013940811157
  - 0.4213793873786926
  - 0.4100225269794464
  - 0.4100031852722168
  - 0.41507387161254883
  - 0.41826868057250977
  - 0.4039686918258667
loss_records_fold2:
  train_losses:
  - 0.9054511547088624
  - 0.8251220762729645
  - 0.864525842666626
  - 2.9557565927505496
  - 1.8152071058750154
  - 1.4927148997783661
  - 2.2263048946857453
  - 1.0622047364711762
  - 1.061556899547577
  - 1.0052715808153152
  - 0.9213860929012299
  - 0.9818996369838715
  - 0.9340650022029877
  - 1.171130520105362
  - 0.8654049515724183
  - 0.8767750978469849
  - 0.8692669689655305
  - 1.4313985526561739
  - 0.9521794229745866
  - 1.1782763600349426
  - 1.1828372597694397
  - 0.8915837109088898
  - 1.2269842803478241
  - 0.8970369517803193
  - 3.081351763010025
  - 1.7584877729415895
  - 0.9764542162418366
  - 0.8795997321605683
  - 1.287544095516205
  - 0.8868324041366578
  - 1.0306786835193635
  - 0.8365271091461182
  - 0.9846907556056976
  - 2.854599177837372
  - 1.0121677160263063
  - 0.8905691206455231
  - 0.9522368371486665
  - 1.0101252645254135
  - 0.8734760999679566
  - 2.5468046069145203
  - 3.116394352912903
  - 0.896877783536911
  - 1.013859784603119
  - 0.9601300835609436
  - 0.9708441078662873
  - 0.8608516752719879
  - 0.8377088367938996
  - 0.8373505175113678
  validation_losses:
  - 0.4168558418750763
  - 0.4080951511859894
  - 0.39066949486732483
  - 0.4269389808177948
  - 0.4059808552265167
  - 0.388696551322937
  - 0.39528724551200867
  - 0.40281131863594055
  - 0.43665367364883423
  - 0.42001232504844666
  - 0.3923477232456207
  - 0.44129231572151184
  - 0.39592039585113525
  - 0.3912939727306366
  - 0.39734137058258057
  - 0.39553046226501465
  - 0.4136503338813782
  - 0.40375760197639465
  - 0.4146115779876709
  - 0.39630576968193054
  - 0.4352337121963501
  - 0.42629745602607727
  - 0.40049615502357483
  - 0.39854758977890015
  - 0.41857409477233887
  - 0.4928174912929535
  - 0.40334266424179077
  - 0.4126335382461548
  - 0.40017378330230713
  - 0.412338525056839
  - 0.390703946352005
  - 0.4037204384803772
  - 0.38872042298316956
  - 0.4047333598136902
  - 0.3876274824142456
  - 0.41021043062210083
  - 0.4027656614780426
  - 0.401528924703598
  - 0.42382875084877014
  - 0.5947006940841675
  - 0.4000750780105591
  - 0.4239523410797119
  - 0.39540430903434753
  - 0.39822918176651
  - 0.3961774706840515
  - 0.40162011981010437
  - 0.40532365441322327
  - 0.3938673138618469
loss_records_fold3:
  train_losses:
  - 0.8618911743164063
  - 0.97111434340477
  - 0.8211797833442689
  - 0.8613754570484162
  - 2.4779816627502442
  - 2.781296318769455
  - 1.322790837287903
  - 1.2714466154575348
  - 1.0794484734535217
  - 1.6686321556568147
  - 2.135409426689148
  - 0.8499311804771423
  - 0.9757150709629059
  - 0.9087411403656006
  - 0.9778091132640839
  - 0.9077500522136689
  validation_losses:
  - 0.3932308554649353
  - 0.3975584805011749
  - 0.4064851403236389
  - 0.4080105721950531
  - 0.41440635919570923
  - 0.5534740090370178
  - 0.40953779220581055
  - 0.4061954915523529
  - 0.39272135496139526
  - 0.4116208255290985
  - 0.4098628759384155
  - 0.4009896218776703
  - 0.4058845341205597
  - 0.4040338099002838
  - 0.4039788544178009
  - 0.41129404306411743
loss_records_fold4:
  train_losses:
  - 0.9630020797252655
  - 0.876038271188736
  - 0.8267079412937165
  - 0.8029224961996079
  - 2.909216409921646
  - 1.056024956703186
  - 0.9153892040252686
  - 0.8353715479373932
  - 0.7852971643209458
  - 0.8673184454441071
  - 0.831902676820755
  - 0.8734502613544465
  - 0.925254112482071
  - 0.9685393691062928
  - 0.9029032170772553
  - 0.8196153968572617
  - 0.86650710105896
  - 0.8611058354377747
  - 0.9803369283676148
  - 0.9317308843135834
  - 0.9224947750568391
  - 0.8604608058929444
  - 0.87613645195961
  - 0.8194827795028687
  - 0.8590958654880524
  - 0.80909321308136
  - 0.8444792449474335
  - 0.8218154132366181
  - 0.8391776859760285
  - 0.8919025301933289
  - 0.8596873164176941
  - 0.8464791655540467
  - 0.8551038146018982
  - 0.9430474400520326
  - 0.8584028005599976
  - 0.8460503637790681
  - 0.8251950979232788
  - 0.8949820339679718
  - 0.836634486913681
  - 1.093176817893982
  - 2.035311144590378
  - 1.2774859368801117
  - 1.2973348379135132
  - 2.2016841769218445
  - 0.8269169270992279
  - 0.8638087868690492
  - 0.8166411459445954
  validation_losses:
  - 0.4430028200149536
  - 0.425927996635437
  - 0.45336809754371643
  - 0.4363395869731903
  - 0.4139682948589325
  - 0.41742613911628723
  - 0.427151620388031
  - 0.46559661626815796
  - 0.4311459958553314
  - 0.4066812992095947
  - 0.42342066764831543
  - 0.4329187870025635
  - 0.4464085102081299
  - 0.42468786239624023
  - 0.4497280716896057
  - 0.4454056918621063
  - 0.40047580003738403
  - 0.4320709705352783
  - 0.47426435351371765
  - 0.48134538531303406
  - 0.4911591410636902
  - 0.4174400866031647
  - 0.46543705463409424
  - 0.4601391851902008
  - 0.5276331901550293
  - 0.44397199153900146
  - 0.4099266529083252
  - 0.4447655975818634
  - 0.42057156562805176
  - 0.47313714027404785
  - 0.4392130672931671
  - 0.4502551555633545
  - 0.43251392245292664
  - 0.4508153200149536
  - 0.44678762555122375
  - 0.46291065216064453
  - 0.40726372599601746
  - 0.4327912926673889
  - 0.4068721830844879
  - 0.42429104447364807
  - 0.45580175518989563
  - 0.4276083707809448
  - 0.4333057701587677
  - 0.4375482499599457
  - 0.41309282183647156
  - 0.41750526428222656
  - 0.41365328431129456
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 60 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 48 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 47 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8576329331046312,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.0
  total_train_time: '0:22:52.456074'
