config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cpu
  dropout: 0.0
  epochs: 30
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.05
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 5
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-09 14:54:34.045518'
fold_0_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_3fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_3fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_3fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_3fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_3fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_3fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_3fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_3fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_3fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_3fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.28127011358738
  - 5.863368177413941
  - 5.918599477410317
  - 5.806149598956108
  - 5.83640820235014
  - 5.804991173744202
  - 5.83681464791298
  - 5.798174265027047
  - 5.804821085929871
  - 5.670075324177742
  - 5.783427974581719
  - 5.657070308923721
  - 5.727851435542107
  - 5.590650737285614
  validation_losses:
  - 0.4396403133869171
  - 0.40555045008659363
  - 0.388392835855484
  - 0.40999293327331543
  - 0.48659321665763855
  - 0.39336147904396057
  - 0.3851320147514343
  - 0.3955990970134735
  - 0.38739076256752014
  - 0.444517582654953
  - 0.38342657685279846
  - 0.38579216599464417
  - 0.3895469605922699
  - 0.3899330496788025
loss_records_fold1:
  train_losses:
  - 5.6848683089017875
  - 5.611518377065659
  - 5.669740292429925
  - 5.598610150814057
  - 5.606061840057373
  - 5.602907627820969
  - 5.61472858786583
  - 5.64050757586956
  - 5.60562344789505
  - 5.497013884782792
  - 5.586087794601918
  - 5.553832015395165
  - 5.488997679948807
  - 5.554957139492036
  - 5.589716404676437
  - 5.5856588125228885
  validation_losses:
  - 0.40872281789779663
  - 0.392447829246521
  - 0.3859506845474243
  - 0.38877934217453003
  - 0.3902527391910553
  - 0.39277154207229614
  - 0.38633933663368225
  - 0.40176981687545776
  - 0.38916558027267456
  - 0.3917853832244873
  - 0.4500649571418762
  - 0.5706184506416321
  - 0.589569628238678
  - 0.3855440616607666
  - 0.40990039706230164
  - 0.3946376144886017
loss_records_fold2:
  train_losses:
  - 5.531444573402405
  - 5.61843658387661
  - 5.569334167242051
  - 5.610159534215928
  - 5.480912402272224
  - 5.523977494239808
  - 5.55347630083561
  - 5.521386942267418
  - 5.5896951228380205
  - 5.5744113475084305
  - 5.493341499567032
  validation_losses:
  - 0.8650213479995728
  - 0.6403182744979858
  - 0.5373175740242004
  - 0.6256182789802551
  - 0.41389569640159607
  - 0.4004446864128113
  - 0.3866683840751648
  - 0.41293418407440186
  - 0.3998242914676666
  - 0.4212918281555176
  - 0.4457310140132904
loss_records_fold3:
  train_losses:
  - 5.596900594234467
  - 5.563281205296517
  - 5.6121891200542455
  - 5.551126322150231
  - 5.475825077295304
  - 5.466088718175889
  - 5.488929405808449
  - 5.557776206731797
  - 5.550323733687401
  - 5.599897477030755
  - 5.579713129997254
  - 5.650550642609597
  - 5.569923904538155
  - 5.497398626804352
  - 5.611505311727524
  - 5.53231243789196
  - 5.551803556084633
  - 5.521418145298958
  - 5.50219673216343
  - 5.5049766302108765
  - 5.606651875376702
  - 5.505457392334939
  - 5.480189913511277
  - 5.5285308957099915
  - 5.451300486922264
  - 5.503340962529183
  - 5.448149532079697
  - 5.542641046643258
  - 5.4677957713603975
  - 5.514782792329789
  validation_losses:
  - 0.5199550986289978
  - 0.6946932673454285
  - 0.5293473601341248
  - 0.5987247824668884
  - 0.5159077048301697
  - 0.7031728625297546
  - 0.8062930107116699
  - 0.5093556642532349
  - 0.6286832690238953
  - 0.823923647403717
  - 0.7854190468788147
  - 0.4860968589782715
  - 0.4070712924003601
  - 0.5102095007896423
  - 0.42989248037338257
  - 0.5094314217567444
  - 0.4825102984905243
  - 0.4230637550354004
  - 0.5145814418792725
  - 0.4407452344894409
  - 0.4276376962661743
  - 0.5444068908691406
  - 0.9245923757553101
  - 0.5248530507087708
  - 0.6455582976341248
  - 0.5417487025260925
  - 0.5882866978645325
  - 1.6130791902542114
  - 1.0949547290802002
  - 1.5242559909820557
loss_records_fold4:
  train_losses:
  - 5.511756467819215
  - 5.5783541887998584
  - 5.460464850068092
  - 5.484973976016045
  - 5.476778763532639
  - 5.509437930583954
  - 5.57943586409092
  - 5.521513852477074
  - 5.447620442509652
  - 5.580814394354821
  - 5.51850318312645
  - 5.53933721780777
  - 5.518041023612023
  - 5.51792553961277
  - 5.477418631315231
  - 5.508709043264389
  - 5.438183715939522
  validation_losses:
  - 0.4293380677700043
  - 0.3760477602481842
  - 0.37699946761131287
  - 0.3999366760253906
  - 0.40703195333480835
  - 0.374711811542511
  - 0.3945009410381317
  - 0.3739987909793854
  - 0.4946092963218689
  - 0.3999990224838257
  - 0.38817980885505676
  - 0.3910861015319824
  - 0.44484367966651917
  - 0.3778068423271179
  - 0.37519389390945435
  - 0.41340699791908264
  - 0.37994804978370667
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: completed 30 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8421955403087479, 0.8507718696397941,
    0.8556701030927835]'
  fold_eval_f1: '[0.0, 0.0, 0.06122448979591836, 0.02247191011235955, 0.04545454545454545]'
  mean_eval_accuracy: 0.8527806758501175
  mean_f1_accuracy: 0.02583018907256467
  total_train_time: '0:03:11.218493'
