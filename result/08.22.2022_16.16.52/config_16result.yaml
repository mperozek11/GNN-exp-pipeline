config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:27:49.404987'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_16fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_16fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_16fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_16fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_16fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_16fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_16fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_16fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_16fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_16fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.318605688214302
  - 5.799046759307385
  - 5.847617200016976
  - 5.72476589679718
  - 5.856071074306965
  - 5.920639923214913
  - 5.763532894849778
  - 5.764864477515221
  - 5.825533011555672
  - 5.706715828180314
  - 5.736696076393128
  - 5.802195289731026
  - 5.643995222449303
  - 5.579815647006035
  - 5.59912887364626
  - 5.691199716925621
  - 5.626288440823555
  - 5.678463578224182
  - 5.6440391719341285
  - 5.606681174039841
  - 5.633032748103142
  validation_losses:
  - 0.4685795307159424
  - 0.3941042721271515
  - 0.38728100061416626
  - 0.3952091932296753
  - 0.4142824113368988
  - 0.38995006680488586
  - 0.38821908831596375
  - 0.4105568528175354
  - 0.38589468598365784
  - 0.4438590705394745
  - 0.38614678382873535
  - 0.3875804841518402
  - 0.39818453788757324
  - 0.3860255479812622
  - 0.4133559465408325
  - 0.3925288915634155
  - 0.3902704417705536
  - 0.3881242573261261
  - 0.3903517723083496
  - 0.3878915011882782
  - 0.3936441242694855
loss_records_fold1:
  train_losses:
  - 5.6416507244110115
  - 5.649593958258629
  - 5.533884489536286
  - 5.734563204646111
  - 5.6332512348890305
  - 5.590028557181359
  - 5.605196791887284
  - 5.614449545741081
  - 5.641245117783547
  - 5.577667716145516
  - 5.57593666613102
  validation_losses:
  - 0.38590002059936523
  - 0.38642069697380066
  - 0.3853776454925537
  - 0.39410385489463806
  - 0.3870982527732849
  - 0.38880953192710876
  - 0.3848705589771271
  - 0.39181771874427795
  - 0.3889734447002411
  - 0.3849065899848938
  - 0.3870866894721985
loss_records_fold2:
  train_losses:
  - 5.630293560028076
  - 5.677818474173546
  - 5.560524272918702
  - 5.626690596342087
  - 5.670132473111153
  - 5.615982645750046
  - 5.622216704487801
  - 5.603748318552971
  - 5.535416701436043
  - 5.609545081853867
  - 5.59313232600689
  validation_losses:
  - 0.38616126775741577
  - 0.3865831196308136
  - 0.3924538195133209
  - 0.3888828456401825
  - 0.39302051067352295
  - 0.38808780908584595
  - 0.38547244668006897
  - 0.38875511288642883
  - 0.39175328612327576
  - 0.3897338807582855
  - 0.3862004280090332
loss_records_fold3:
  train_losses:
  - 5.69210998415947
  - 5.692080649733544
  - 5.621643993258477
  - 5.635856518149376
  - 5.67689523100853
  - 5.664726284146309
  - 5.657282018661499
  - 5.6909810483455665
  - 5.639106342196465
  - 5.680684849619865
  - 5.627866268157959
  - 5.617790454626084
  - 5.678142672777176
  - 5.691558706760407
  - 5.660968211293221
  - 5.643838852643967
  - 5.614333227276802
  validation_losses:
  - 0.37650159001350403
  - 0.37633100152015686
  - 0.3731270134449005
  - 0.3776964843273163
  - 0.40197551250457764
  - 0.3877282738685608
  - 0.37452760338783264
  - 0.37370163202285767
  - 0.37579086422920227
  - 0.37037986516952515
  - 0.38276731967926025
  - 0.38814079761505127
  - 0.3735766112804413
  - 0.3784558176994324
  - 0.37331876158714294
  - 0.37723836302757263
  - 0.3745826184749603
loss_records_fold4:
  train_losses:
  - 5.624560382962227
  - 5.614983883500099
  - 5.582992514967919
  - 5.587663620710373
  - 5.609898468852044
  - 5.5801987767219545
  - 5.604604253172875
  - 5.5960314810276035
  - 5.607681906223298
  - 5.619575899839401
  - 5.617128843069077
  validation_losses:
  - 0.3785988390445709
  - 0.3793907165527344
  - 0.38150885701179504
  - 0.37934523820877075
  - 0.3779703974723816
  - 0.37782609462738037
  - 0.3807506263256073
  - 0.38118091225624084
  - 0.38035446405410767
  - 0.37995627522468567
  - 0.37854763865470886
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:07:00.134304'
