config:
  aggregation: mean
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.840485'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_7fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_7fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_7fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_7fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_7fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_7fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_7fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_7fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_7fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_7fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.0918465912342072
  - 0.8861382186412812
  - 0.8704276025295258
  - 0.8769704461097718
  - 0.8815741240978241
  - 0.8321284830570221
  - 0.8514392137527467
  - 0.9046299934387207
  - 0.8740393340587617
  - 0.8295077741146089
  - 0.8232053697109223
  - 0.8189224123954774
  - 0.8745710313320161
  validation_losses:
  - 0.4479294419288635
  - 0.4674643576145172
  - 0.4110735356807709
  - 0.43366673588752747
  - 0.4241315722465515
  - 0.41421112418174744
  - 0.4294970631599426
  - 0.40627190470695496
  - 0.40557536482810974
  - 0.39878758788108826
  - 0.4020041227340698
  - 0.40777164697647095
  - 0.3912677764892578
loss_records_fold1:
  train_losses:
  - 0.8023114502429962
  - 0.8658641159534455
  - 0.8354771494865418
  - 0.8161878824234009
  - 0.8404682576656342
  - 0.82997624874115
  - 0.7975701719522477
  - 0.8002300381660462
  - 0.8413491487503052
  - 0.8114122927188874
  - 0.8093441843986512
  validation_losses:
  - 0.4021799564361572
  - 0.4010791480541229
  - 0.39866912364959717
  - 0.39743664860725403
  - 0.3987995386123657
  - 0.3977826237678528
  - 0.4039098024368286
  - 0.41114291548728943
  - 0.40121200680732727
  - 0.3988852798938751
  - 0.40400001406669617
loss_records_fold2:
  train_losses:
  - 0.8568874835968018
  - 0.8441459655761719
  - 0.806876540184021
  - 0.825535398721695
  - 0.8009197354316712
  - 0.7751136779785157
  - 0.7935880482196809
  - 0.8048233807086945
  - 0.7982384145259858
  - 0.811665952205658
  - 0.8130015432834625
  - 0.8385535299777985
  - 0.7899133145809174
  - 0.8574632763862611
  - 0.8303457319736481
  - 0.8241116881370545
  - 0.8314285635948182
  - 0.8101549923419953
  - 0.8423402488231659
  - 0.8011395514011384
  - 0.8155838310718537
  validation_losses:
  - 0.39384138584136963
  - 0.41930121183395386
  - 0.398692786693573
  - 0.39408817887306213
  - 0.39275574684143066
  - 0.3893396854400635
  - 0.41523194313049316
  - 0.39363548159599304
  - 0.3830242156982422
  - 0.3984159529209137
  - 0.3897644579410553
  - 0.39164620637893677
  - 0.39087462425231934
  - 0.3974960446357727
  - 0.428752064704895
  - 0.3981998562812805
  - 0.40075939893722534
  - 0.39166349172592163
  - 0.38943880796432495
  - 0.3925415575504303
  - 0.40240004658699036
loss_records_fold3:
  train_losses:
  - 0.8306399106979371
  - 0.8151732623577118
  - 0.8658345758914948
  - 0.8444847702980042
  - 0.8218204259872437
  - 0.8437692642211915
  - 0.7986446499824524
  - 0.8365604758262635
  - 0.8667323768138886
  - 0.8347540855407716
  - 0.8408209383487701
  - 0.851426112651825
  - 0.8154585182666779
  - 0.8197196781635285
  - 0.841287362575531
  - 0.7838959068059922
  - 0.7821710616350175
  - 0.8227217137813568
  - 0.8332626760005951
  - 0.8214890360832214
  - 0.7799660861492157
  - 0.8009122967720033
  - 0.8178819358348847
  - 0.8012424528598786
  - 0.798000693321228
  - 0.8107999682426453
  - 0.8070701062679291
  - 0.8199808180332184
  - 0.8015348613262177
  - 0.7877552568912507
  - 0.7939046442508698
  - 0.8146169364452363
  - 0.7911078870296478
  - 0.7866105020046235
  - 0.7768888354301453
  - 0.8114430904388428
  - 0.7928359091281891
  - 0.7631099343299866
  - 0.7885444104671478
  - 0.8060336112976074
  - 0.7952689826488495
  - 0.8396425485610962
  - 0.8279468655586243
  - 0.819932508468628
  - 0.8785535991191864
  - 0.8814458250999451
  - 0.838352644443512
  - 0.8064855933189392
  - 0.8647038578987122
  - 0.8280964434146881
  - 0.8205293774604798
  - 0.8289677858352662
  - 0.8256110608577729
  - 0.8369181275367737
  - 0.8839914381504059
  - 0.8265372037887574
  - 0.8189634919166565
  - 0.8543751835823059
  - 0.8486010074615479
  - 0.7862764298915863
  - 0.8306109249591828
  - 0.8782140135765076
  - 0.86692733168602
  - 0.829026848077774
  - 0.8199190080165863
  - 0.7934734106063843
  - 0.7862228572368622
  - 0.7600370138883591
  - 0.8392784535884857
  - 0.8118859589099885
  - 0.7674927532672883
  - 0.8232467114925385
  - 0.8482017397880555
  - 0.7748956233263016
  - 0.8168382465839387
  - 0.8000740647315979
  - 0.7818817347288132
  - 0.784874838590622
  - 0.7971315264701844
  - 0.7602984547615051
  - 0.7596087485551835
  - 0.8304156005382538
  - 0.7615328490734101
  - 0.7829970836639405
  - 0.8068365573883057
  - 0.8362983405590058
  - 0.8765330612659454
  - 0.8162021160125733
  - 0.8050418555736543
  - 0.7808866411447526
  - 0.8050548017024994
  - 0.8047834098339082
  - 0.8054969191551209
  - 0.784117740392685
  - 0.8092676341533661
  - 0.8157893359661103
  - 0.7736310929059983
  - 0.8024048745632172
  - 0.7826223731040955
  - 0.7976018369197846
  validation_losses:
  - 0.4908093512058258
  - 0.3812798261642456
  - 0.3739769756793976
  - 0.3766132593154907
  - 0.3745995759963989
  - 0.3771725296974182
  - 0.37552952766418457
  - 0.3789750635623932
  - 0.38943594694137573
  - 0.38713863492012024
  - 0.4136962592601776
  - 0.3854798376560211
  - 0.5152143836021423
  - 0.4471896290779114
  - 0.38264626264572144
  - 0.38462844491004944
  - 0.38560521602630615
  - 0.3901055157184601
  - 0.5456392168998718
  - 0.7656335234642029
  - 0.9647176265716553
  - 0.6014146208763123
  - 0.4930576980113983
  - 0.451816588640213
  - 1.3354252576828003
  - 1.9599437713623047
  - 0.39430302381515503
  - 0.38643455505371094
  - 0.3841845393180847
  - 0.37725406885147095
  - 0.3868173360824585
  - 0.4929637908935547
  - 0.5383655428886414
  - 0.5811335444450378
  - 0.6113815307617188
  - 0.7214555740356445
  - 0.5017052888870239
  - 0.6152939200401306
  - 0.37697839736938477
  - 0.4316019117832184
  - 0.7041382193565369
  - 0.7095625996589661
  - 0.5093481540679932
  - 0.6654499173164368
  - 0.39989879727363586
  - 0.4350326359272003
  - 0.41389212012290955
  - 0.47008007764816284
  - 0.6243541836738586
  - 0.6697633266448975
  - 0.8619358539581299
  - 0.5011679530143738
  - 1.366775631904602
  - 0.7840524315834045
  - 0.5415415167808533
  - 0.4599156677722931
  - 0.9080321788787842
  - 0.43579578399658203
  - 0.408847838640213
  - 1.5390368700027466
  - 1.540937900543213
  - 1.5580511093139648
  - 1.6431148052215576
  - 0.404618501663208
  - 0.399947851896286
  - 0.3731388449668884
  - 0.6437414884567261
  - 1.0334256887435913
  - 1.0847142934799194
  - 1.0788706541061401
  - 0.3797241747379303
  - 1.0210254192352295
  - 2.43579363822937
  - 2.0864648818969727
  - 1.9656976461410522
  - 0.503315269947052
  - 0.5979960560798645
  - 0.6903151869773865
  - 0.8120946288108826
  - 1.3417878150939941
  - 0.8967024683952332
  - 0.4981442391872406
  - 0.4165864884853363
  - 0.6630849242210388
  - 0.3749319911003113
  - 0.37881603837013245
  - 0.46723052859306335
  - 0.40794244408607483
  - 0.39551517367362976
  - 0.3877171576023102
  - 0.3803592324256897
  - 0.39000463485717773
  - 0.4008331596851349
  - 0.4452613592147827
  - 0.4870651066303253
  - 0.6098573207855225
  - 0.529231607913971
  - 0.7593168616294861
  - 0.8992887735366821
  - 0.7712649703025818
loss_records_fold4:
  train_losses:
  - 0.7948021650314332
  - 0.7966158390045166
  - 0.8150193870067597
  - 0.8354235112667084
  - 0.7985710501670837
  - 0.8346911132335664
  - 0.8177064061164856
  - 0.8036730468273163
  - 0.8234330415725708
  - 0.8118099212646485
  - 0.8434047460556031
  - 0.7949136734008789
  - 0.7836275815963746
  - 0.7830975711345673
  - 0.7910734355449677
  - 0.799871414899826
  - 0.813755214214325
  - 0.7819547414779664
  - 0.7813771277666093
  - 0.765745535492897
  - 0.8235260367393494
  - 0.8004795730113984
  - 0.8333224534988404
  validation_losses:
  - 0.5041044354438782
  - 0.4556368887424469
  - 0.5337679982185364
  - 0.43254557251930237
  - 0.391778826713562
  - 0.3821925222873688
  - 0.3975555896759033
  - 0.5007514357566833
  - 0.36983680725097656
  - 0.36997199058532715
  - 0.3833180069923401
  - 0.4156813621520996
  - 0.36827442049980164
  - 0.378273606300354
  - 0.384475976228714
  - 0.37131696939468384
  - 0.3959927558898926
  - 0.37803739309310913
  - 0.38736653327941895
  - 0.3830863833427429
  - 0.3910292387008667
  - 0.3905767500400543
  - 0.38353753089904785
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8130360205831904,
    0.8539518900343642]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.11382113821138214, 0.0449438202247191]'
  mean_eval_accuracy: 0.8483203951595314
  mean_f1_accuracy: 0.03175299168722025
  total_train_time: '0:13:30.398411'
