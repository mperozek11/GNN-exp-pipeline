config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 07:35:43.152539'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/149/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/149/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/149/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/149/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/149/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/149/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/149/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/149/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/149/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/149/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 595.5905823111534
  - 37.742167949676514
  - 29.007169127464294
  - 26.863391429185867
  - 38.12157762050629
  - 30.992032885551453
  - 19.503880500793457
  - 9.032845333218575
  - 8.92591968178749
  - 8.827319949865341
  - 8.752154141664505
  - 48.3982672393322
  - 12.093155890703201
  - 22.341722190380096
  - 104.56227487325668
  - 79.41681671142578
  - 169.69142198562622
  - 48.044445753097534
  - 14.288133800029755
  - 11.023614168167114
  - 11.019771784543991
  - 8.956084191799164
  - 10.409532934427261
  - 15.04399859905243
  - 9.257237493991852
  - 11.25611811876297
  - 12.382485419511795
  - 16.37660577893257
  - 16.343587428331375
  - 9.239801913499832
  - 9.76935338973999
  - 10.265339225530624
  - 8.478098630905151
  - 14.91771912574768
  - 14.647046983242035
  - 14.57482585310936
  - 16.092065006494522
  - 7.618994474411011
  - 11.593508064746857
  - 9.792672514915466
  - 35.494335025548935
  - 10.88106381893158
  - 8.409475535154343
  - 27.25424912571907
  - 10.646848410367966
  - 8.710884511470795
  - 11.859528452157974
  - 46.26682645082474
  - 62.51415795087814
  - 73.17540043592453
  - 12.484118551015854
  - 54.086720019578934
  - 12.949402526021004
  - 10.235906928777695
  - 9.749889075756073
  - 11.965358763933182
  - 19.528745263814926
  - 32.21767047047615
  - 23.51550006866455
  - 59.169892847537994
  - 68.5040502846241
  - 16.817752212285995
  - 14.310610383749008
  - 12.806098490953445
  - 8.194859355688095
  - 8.157386809587479
  - 8.357483476400375
  - 8.992864191532135
  - 8.99854952096939
  - 11.305874228477478
  - 11.641067177057266
  - 11.495370328426361
  - 15.768550008535385
  - 9.143920198082924
  - 11.414298206567764
  - 9.030030995607376
  - 7.757010579109192
  - 10.374719381332397
  - 11.751679211854935
  - 14.461549207568169
  - 9.56884080171585
  - 9.204180926084518
  - 8.122857868671417
  - 9.284890472888947
  - 8.174150288105011
  - 8.155651211738586
  - 8.33538880944252
  - 10.252107679843903
  - 9.22742772102356
  - 8.328565806150436
  - 9.529801934957504
  - 11.857131958007812
  - 8.928969651460648
  - 9.10288280248642
  - 7.52223014831543
  - 7.907695442438126
  - 8.955323606729507
  - 8.0581414103508
  - 8.750440418720245
  - 9.735921382904053
  validation_losses:
  - 20.132328033447266
  - 9.757807731628418
  - 0.7566673159599304
  - 0.878493070602417
  - 1.139386773109436
  - 2.4165592193603516
  - 0.4258538782596588
  - 0.471296489238739
  - 0.388385146856308
  - 0.43729883432388306
  - 0.548628568649292
  - 5.7365241050720215
  - 0.4317515790462494
  - 0.4002791941165924
  - 125.73190307617188
  - 326.0785827636719
  - 10.267373085021973
  - 0.666321873664856
  - 0.48077479004859924
  - 0.41831067204475403
  - 0.4538700580596924
  - 0.39609718322753906
  - 0.45945650339126587
  - 0.4608624577522278
  - 0.40856415033340454
  - 0.5508357882499695
  - 0.41747143864631653
  - 0.45293688774108887
  - 0.5453983545303345
  - 0.3883174657821655
  - 0.4022214710712433
  - 0.47357189655303955
  - 0.4480629563331604
  - 7.934173583984375
  - 0.3950235843658447
  - 0.9105079174041748
  - 0.40504899621009827
  - 0.3928637206554413
  - 0.4219551980495453
  - 0.5179996490478516
  - 0.4320400059223175
  - 0.45171764492988586
  - 0.41001689434051514
  - 0.6115193963050842
  - 0.42268842458724976
  - 0.4145570397377014
  - 0.5798118710517883
  - 0.59201979637146
  - 7.210752487182617
  - 4.676950454711914
  - 0.4569956660270691
  - 0.7504147291183472
  - 109.6364974975586
  - 1.2105761766433716
  - 0.49085479974746704
  - 0.3875981867313385
  - 0.39256396889686584
  - 0.40574759244918823
  - 0.7932412028312683
  - 2.1582753658294678
  - 0.7906343936920166
  - 0.5289106965065002
  - 0.41931599378585815
  - 0.40058133006095886
  - 0.38881194591522217
  - 0.48982295393943787
  - 0.4024713337421417
  - 0.3982010781764984
  - 0.9786362648010254
  - 0.9967717528343201
  - 0.5719254016876221
  - 0.39555031061172485
  - 0.42447710037231445
  - 0.43521931767463684
  - 0.6552969217300415
  - 0.3901605010032654
  - 0.39939963817596436
  - 0.421709805727005
  - 0.3921973407268524
  - 0.4548698663711548
  - 0.6120583415031433
  - 0.395827054977417
  - 0.502547025680542
  - 0.40822869539260864
  - 0.41303741931915283
  - 0.3973279893398285
  - 0.5500972867012024
  - 0.3943205177783966
  - 0.45759931206703186
  - 0.39695751667022705
  - 0.5007866621017456
  - 0.6148722767829895
  - 0.6152485609054565
  - 0.3992175757884979
  - 0.39036568999290466
  - 0.5979476571083069
  - 0.3994273543357849
  - 0.5005080699920654
  - 0.4118766188621521
  - 0.4581426680088043
loss_records_fold1:
  train_losses:
  - 8.774228870868683
  - 8.33935296535492
  - 7.690343469381332
  - 10.474465370178223
  - 12.994376093149185
  - 11.30600842833519
  - 8.455481052398682
  - 8.09780529141426
  - 9.051095604896545
  - 9.529158025979996
  - 9.185877799987793
  - 8.006899774074554
  - 7.691366404294968
  - 7.85398057103157
  - 9.351864635944366
  - 9.249978348612785
  - 7.4794042110443115
  - 7.996388137340546
  - 8.247298419475555
  - 7.706652939319611
  - 8.076379209756851
  - 8.814135640859604
  - 11.923010557889938
  - 12.94224464893341
  - 8.669247031211853
  - 8.735662698745728
  - 9.874184250831604
  - 8.805051743984222
  - 7.683297127485275
  - 7.837410181760788
  - 14.91726040840149
  - 11.947520703077316
  - 8.277007192373276
  - 8.365672528743744
  - 9.36918705701828
  - 8.913374096155167
  - 9.797964096069336
  - 9.02548897266388
  - 10.702737987041473
  - 9.84885349869728
  - 7.551099091768265
  - 7.7952582240104675
  - 9.664570450782776
  - 8.152383476495743
  - 7.973913669586182
  - 8.10419550538063
  - 9.1467964053154
  - 7.8764515817165375
  - 8.450330764055252
  - 8.154747605323792
  - 8.272399961948395
  - 7.665746718645096
  - 8.239093542098999
  - 7.623728722333908
  - 8.036294281482697
  - 7.512616991996765
  - 8.469096958637238
  - 8.547570139169693
  - 7.946201860904694
  - 7.8622740507125854
  - 7.659237831830978
  - 8.210054099559784
  - 7.806704252958298
  - 7.536982506513596
  - 8.568947494029999
  - 8.778168231248856
  - 8.65702274441719
  - 9.828969448804855
  - 13.570848733186722
  - 9.725557088851929
  - 8.261952728033066
  - 8.568108767271042
  - 7.730407178401947
  - 7.693923264741898
  - 7.675290912389755
  - 8.506012171506882
  - 11.810907632112503
  - 9.89161017537117
  - 14.761068791151047
  - 10.403563261032104
  - 8.410495668649673
  - 7.627765387296677
  - 7.62934222817421
  - 7.6456286907196045
  - 7.833913743495941
  - 11.97151306271553
  - 7.597656935453415
  - 8.864412069320679
  - 10.692167818546295
  - 9.077609062194824
  - 9.80662402510643
  - 9.568888694047928
  - 9.58900237083435
  - 9.747563868761063
  - 9.851646512746811
  - 8.324589490890503
  - 9.581007987260818
  - 9.80273699760437
  - 8.363518983125687
  - 7.601433023810387
  validation_losses:
  - 0.4554746448993683
  - 0.43771791458129883
  - 0.5059202909469604
  - 0.6978694200515747
  - 0.754607617855072
  - 0.6231030225753784
  - 0.402316153049469
  - 0.4195649325847626
  - 0.44076377153396606
  - 0.4084450304508209
  - 0.5277568697929382
  - 0.4044795632362366
  - 0.4096682369709015
  - 0.4470844268798828
  - 0.5494396686553955
  - 0.40640953183174133
  - 0.4696691036224365
  - 0.5940195918083191
  - 0.40538254380226135
  - 0.4164755344390869
  - 0.4062192738056183
  - 0.40891072154045105
  - 0.9093700647354126
  - 0.7161361575126648
  - 0.41061365604400635
  - 0.4497176706790924
  - 0.5190637707710266
  - 0.40338578820228577
  - 0.41376394033432007
  - 0.7741889357566833
  - 0.593629002571106
  - 0.6290395855903625
  - 0.4954012930393219
  - 0.48491501808166504
  - 0.51517254114151
  - 0.537814199924469
  - 0.47909510135650635
  - 0.8863816857337952
  - 0.917064368724823
  - 0.41295191645622253
  - 0.4024849236011505
  - 0.6628934144973755
  - 0.4416655898094177
  - 0.42202436923980713
  - 0.43275800347328186
  - 0.6057630181312561
  - 0.42278462648391724
  - 0.4244769215583801
  - 0.40902987122535706
  - 0.43602922558784485
  - 0.4113602638244629
  - 0.4805006980895996
  - 0.4204823970794678
  - 0.4049258828163147
  - 0.4634787440299988
  - 0.40561431646347046
  - 0.40526077151298523
  - 0.41240331530570984
  - 0.43130069971084595
  - 0.4479561448097229
  - 0.45930954813957214
  - 0.4059247374534607
  - 0.4209463596343994
  - 0.4116221070289612
  - 0.563894510269165
  - 0.4521523714065552
  - 0.4978019595146179
  - 0.7019922733306885
  - 0.7705845832824707
  - 0.4913809299468994
  - 0.4231063425540924
  - 0.42786905169487
  - 0.4596859812736511
  - 0.5450419783592224
  - 0.49266448616981506
  - 0.8517548441886902
  - 0.8343960046768188
  - 0.9630353450775146
  - 0.8040270209312439
  - 0.4713842570781708
  - 0.4367717206478119
  - 0.4084562361240387
  - 0.4044873118400574
  - 0.43081802129745483
  - 0.71275794506073
  - 0.4287685453891754
  - 0.4087464213371277
  - 0.6445712447166443
  - 0.4572126269340515
  - 0.46790075302124023
  - 0.48818331956863403
  - 0.47712188959121704
  - 0.5836960673332214
  - 0.7439338564872742
  - 0.42008405923843384
  - 0.4273906350135803
  - 0.5940163135528564
  - 0.46543756127357483
  - 0.4585961401462555
  - 0.5589353442192078
loss_records_fold3:
  train_losses:
  - 7.8109084367752075
  - 8.006048694252968
  - 8.240355253219604
  - 9.138674885034561
  - 7.935195714235306
  - 7.542707443237305
  - 8.63429656624794
  - 8.210630863904953
  - 8.841262847185135
  - 7.659092575311661
  - 7.707780420780182
  - 7.638395607471466
  - 8.700752049684525
  - 9.38821193575859
  - 7.939830720424652
  - 9.487629026174545
  - 9.038983210921288
  - 8.658991396427155
  - 9.675682038068771
  - 9.70236149430275
  - 7.8784763514995575
  - 7.5588358938694
  - 7.760489612817764
  - 9.645561009645462
  - 16.742040157318115
  - 23.70100724697113
  - 11.167611449956894
  - 8.961453914642334
  - 44.58055850863457
  - 23.50561434030533
  - 25.590370565652847
  - 12.544086262583733
  - 8.526238858699799
  - 23.111257761716843
  - 34.880868792533875
  - 39.578596383333206
  - 98.60079911351204
  - 240.33343935012817
  - 193.9533336162567
  - 30.596200346946716
  - 9.32088354229927
  - 10.425603061914444
  - 8.765680640935898
  - 7.736153841018677
  - 7.652546405792236
  - 7.665652543306351
  - 8.799807697534561
  - 8.658013761043549
  - 7.7406163811683655
  - 8.608889907598495
  - 7.80441552400589
  - 12.318671345710754
  - 70.26901459693909
  - 18.183966517448425
  - 12.236985713243484
  - 8.471017390489578
  - 7.364683881402016
  - 8.465755373239517
  - 13.376064777374268
  - 10.227865010499954
  - 9.374230414628983
  - 58.186902582645416
  - 10.82700863480568
  - 29.442207604646683
  - 30.709288746118546
  - 119.33070826530457
  - 25.73332543671131
  - 14.440169721841812
  - 56.91527286171913
  - 34.821245312690735
  - 60.33439485728741
  - 39.27580350637436
  - 39.54128611087799
  - 28.606100603938103
  - 15.836980879306793
  - 17.55348590016365
  - 37.4870448410511
  - 18.131657510995865
  - 18.54434807598591
  - 12.213248997926712
  - 8.49996680021286
  - 8.969700500369072
  - 17.43728595972061
  - 9.03495180606842
  - 50.15218758583069
  - 15.723710089921951
  - 9.998831868171692
  - 49.09726929664612
  - 11.651444345712662
  - 22.73427602648735
  - 24.68704903125763
  - 10.700659573078156
  - 10.428322941064835
  - 10.804582327604294
  - 9.844495564699173
  - 20.156397104263306
  - 20.964170306921005
  - 10.385558277368546
  - 20.29543173313141
  - 12.609538167715073
  validation_losses:
  - 0.4978577196598053
  - 0.5250856876373291
  - 0.5886908769607544
  - 0.4031596779823303
  - 0.403134822845459
  - 0.4037819504737854
  - 0.4262840151786804
  - 0.4018082618713379
  - 0.4286095201969147
  - 0.4321293830871582
  - 0.4106958508491516
  - 0.44641831517219543
  - 0.49666014313697815
  - 0.4034859836101532
  - 0.4496108293533325
  - 0.577721357345581
  - 0.5601963996887207
  - 0.467089980840683
  - 0.4791722595691681
  - 0.41394591331481934
  - 0.41654887795448303
  - 0.39742177724838257
  - 0.4058850109577179
  - 2.759937047958374
  - 2.6042816638946533
  - 0.41508978605270386
  - 0.43700146675109863
  - 0.48236319422721863
  - 0.6664793491363525
  - 2.777106285095215
  - 2.1231582164764404
  - 0.4020468294620514
  - 0.44472917914390564
  - 479.56396484375
  - 2.0874040126800537
  - 68.60716247558594
  - 1370.146240234375
  - 163.4460906982422
  - 1706.5999755859375
  - 8.544310569763184
  - 0.4024556875228882
  - 0.44113656878471375
  - 0.41175952553749084
  - 0.39739689230918884
  - 0.419827938079834
  - 0.5134152770042419
  - 0.5758612155914307
  - 0.39988937973976135
  - 0.5074777603149414
  - 0.4092806875705719
  - 0.39764875173568726
  - 0.4016573429107666
  - 1518.41015625
  - 5130.66064453125
  - 0.5664856433868408
  - 0.39796337485313416
  - 0.4209001362323761
  - 0.40232053399086
  - 0.4729880392551422
  - 0.7861215472221375
  - 0.4150577187538147
  - 0.4400339722633362
  - 0.41746702790260315
  - 391.2367248535156
  - 53.91200256347656
  - 112.26325988769531
  - 366.3796691894531
  - 0.49759191274642944
  - 1.121163010597229
  - 2.4728760719299316
  - 297.7371520996094
  - 0.5488603115081787
  - 0.5596233606338501
  - 0.49107834696769714
  - 0.9904828071594238
  - 2.6713249683380127
  - 1.3518805503845215
  - 0.47867533564567566
  - 0.3927795886993408
  - 0.552463948726654
  - 0.39979472756385803
  - 0.9405304193496704
  - 0.5742859244346619
  - 0.41476261615753174
  - 0.41506966948509216
  - 0.40034550428390503
  - 0.5959988236427307
  - 0.6242479085922241
  - 0.41053876280784607
  - 0.5271704792976379
  - 0.4482898414134979
  - 0.4154573082923889
  - 0.8962903022766113
  - 0.7356277108192444
  - 0.46109524369239807
  - 0.4039085805416107
  - 0.3990771174430847
  - 0.39878198504447937
  - 0.8343818187713623
  - 0.47275546193122864
loss_records_fold4:
  train_losses:
  - 7.932939112186432
  - 8.912736415863037
  - 8.388520359992981
  - 9.216748118400574
  - 10.51982095837593
  - 9.5300931930542
  - 7.917232155799866
  - 9.459486037492752
  - 7.815893650054932
  - 7.973884016275406
  - 8.347662091255188
  - 7.505397707223892
  - 8.118109732866287
  - 7.64349439740181
  - 8.5667023062706
  - 8.49500048160553
  - 8.046551629900932
  - 12.377905666828156
  - 12.126297354698181
  - 9.741345554590225
  - 8.114899337291718
  - 7.993740797042847
  - 7.570461302995682
  - 8.225677847862244
  - 9.966561108827591
  - 9.418068051338196
  - 9.13902835547924
  - 10.748619079589844
  - 10.281775265932083
  - 8.492254167795181
  - 8.79782721400261
  - 10.176153555512428
  - 9.500888407230377
  - 8.974999099969864
  - 7.958923697471619
  - 8.723636388778687
  - 12.057524114847183
  - 12.68668133020401
  - 7.975276559591293
  - 9.452680975198746
  - 8.170402318239212
  - 8.862752586603165
  - 9.594748497009277
  - 7.745342254638672
  - 8.576222702860832
  - 9.042186707258224
  - 7.9135395884513855
  - 7.659678936004639
  - 8.47533529996872
  - 8.061984539031982
  - 11.034177169203758
  - 13.134006410837173
  - 10.197691202163696
  - 8.574101567268372
  - 8.451026827096939
  - 9.482621103525162
  - 10.555118978023529
  - 9.979694366455078
  - 10.923446416854858
  - 11.679492741823196
  - 13.608596086502075
  - 9.180134177207947
  - 7.941797256469727
  - 7.902103126049042
  - 7.654795974493027
  - 7.966648668050766
  - 8.200814455747604
  - 9.563131749629974
  - 8.20594185590744
  - 8.406445741653442
  - 13.507854342460632
  - 12.021608144044876
  - 9.8415187895298
  - 7.750575631856918
  - 8.665387660264969
  - 17.97889843583107
  - 13.19229730963707
  - 8.127004325389862
  - 7.481048077344894
  - 8.128095090389252
  - 7.735389769077301
  - 8.949771046638489
  - 8.556924313306808
  - 9.68557059764862
  - 7.798142150044441
  - 8.097185343503952
  - 7.844264358282089
  - 8.46721875667572
  - 15.284144759178162
  - 12.234258741140366
  - 9.947077333927155
  - 8.800467103719711
  - 7.882720172405243
  - 13.160756811499596
  - 19.66517674922943
  - 10.268406063318253
  - 7.793554395437241
  - 8.129111588001251
  - 8.284804821014404
  - 7.503933757543564
  validation_losses:
  - 0.39743950963020325
  - 0.48604676127433777
  - 0.3987705111503601
  - 0.6513642072677612
  - 0.4312300682067871
  - 0.426008403301239
  - 0.43340635299682617
  - 0.39258089661598206
  - 0.4055953323841095
  - 0.39611679315567017
  - 0.398929238319397
  - 0.44783759117126465
  - 0.39797335863113403
  - 0.3922863304615021
  - 0.3914363384246826
  - 0.39842450618743896
  - 0.5799226760864258
  - 0.5452214479446411
  - 0.6083689332008362
  - 0.4292144179344177
  - 0.428752064704895
  - 0.39115726947784424
  - 0.38922107219696045
  - 0.39034363627433777
  - 0.47194910049438477
  - 0.38983070850372314
  - 0.6394872665405273
  - 0.4504407048225403
  - 0.40093639492988586
  - 0.3908907175064087
  - 0.6431771516799927
  - 0.45854833722114563
  - 0.4200410842895508
  - 0.41305118799209595
  - 0.39109644293785095
  - 0.4786142110824585
  - 0.7367158532142639
  - 0.41089800000190735
  - 0.4645636975765228
  - 0.42874541878700256
  - 0.5273556113243103
  - 0.4046434760093689
  - 0.40009739995002747
  - 0.5337421894073486
  - 0.495656818151474
  - 0.4082217514514923
  - 0.42403408885002136
  - 0.3958357274532318
  - 0.4471963047981262
  - 0.5497425198554993
  - 0.6436117887496948
  - 0.6124946475028992
  - 0.49179890751838684
  - 0.3916075527667999
  - 0.40439772605895996
  - 0.44927340745925903
  - 0.5149388313293457
  - 0.584160327911377
  - 0.601206362247467
  - 0.5511869788169861
  - 0.44300606846809387
  - 0.395875483751297
  - 0.39334818720817566
  - 0.39649105072021484
  - 0.4735648036003113
  - 0.4187883138656616
  - 0.6088504791259766
  - 0.3980163037776947
  - 0.4799719750881195
  - 0.6696406006813049
  - 564.5676879882812
  - 2.541119337081909
  - 0.46928536891937256
  - 0.3974076211452484
  - 0.3944416046142578
  - 0.5505191087722778
  - 0.44755634665489197
  - 0.3931742310523987
  - 0.5006839632987976
  - 0.39157360792160034
  - 0.4842698276042938
  - 0.4392378032207489
  - 0.7860426306724548
  - 0.4014185965061188
  - 0.42150604724884033
  - 0.3916654586791992
  - 0.4064997434616089
  - 0.5070353746414185
  - 0.8446388840675354
  - 0.3986266255378723
  - 0.5246122479438782
  - 0.45369771122932434
  - 0.6560754179954529
  - 0.8392067551612854
  - 0.45380961894989014
  - 0.39411768317222595
  - 0.3926037847995758
  - 0.4102068543434143
  - 0.39901629090309143
  - 0.40733426809310913
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 36 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:06:19.156245'
