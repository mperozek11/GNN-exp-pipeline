config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:20:28.018410'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_95fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_95fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_95fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_95fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_95fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_95fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_95fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_95fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_95fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_95fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 12.44157500267029
  - 10.600497388839722
  - 4.915913975238801
  - 2.7077743232250215
  - 2.710172951221466
  - 1.7854725420475006
  - 1.5321544706821442
  - 1.3352691799402239
  - 1.6494073152542115
  - 1.3929188489913942
  - 2.3287959456443788
  - 2.184200513362885
  - 1.540441119670868
  - 1.4136815190315248
  - 1.3576300859451296
  - 7.427490568161011
  - 9.61311388015747
  - 1.8795898199081422
  - 1.439406144618988
  - 1.407764768600464
  - 1.7179840683937073
  - 1.7273884117603302
  - 1.4828982532024384
  - 1.3796643793582917
  - 1.9099693775177002
  - 1.275219488143921
  - 1.6157264292240143
  - 1.3589219331741333
  - 1.246456927061081
  - 0.9592225790023804
  - 1.1451238155364991
  - 1.0237907350063324
  - 0.9799507379531861
  - 1.8413246989250185
  - 1.2479867339134216
  - 0.925511646270752
  - 1.0888717234134675
  - 1.0943302989006043
  - 0.9633697092533112
  - 1.0977324187755586
  - 1.2309852600097657
  - 0.8290969133377075
  - 0.9045144975185395
  - 0.9937117397785187
  - 0.9199931025505066
  - 0.846855366230011
  - 0.9474364817142487
  - 2.0266976535320285
  - 1.6312587499618532
  - 1.475014853477478
  - 1.1213772892951965
  - 1.0753845989704132
  - 1.5787354469299317
  - 1.0153047502040864
  - 1.087802028656006
  - 1.052111029624939
  - 1.2345187306404115
  - 1.7585675597190857
  - 1.785763555765152
  - 0.972877311706543
  - 2.228028154373169
  - 7.3528671085834505
  - 5.226269829273225
  - 2.8913158118724827
  - 1.3474549055099487
  - 2.4172055184841157
  - 1.1346020519733429
  - 1.60484858751297
  - 1.2615561664104462
  - 1.4503709137439729
  - 1.366112244129181
  - 4.735317760705948
  - 9.673169428110123
  - 1.195181840658188
  - 3.4647812247276306
  - 2.060913044214249
  - 1.2860261142253877
  - 1.099140414595604
  - 1.0225579619407654
  - 1.051743096113205
  - 0.9031659007072449
  - 0.9603636920452119
  - 1.0721721172332763
  - 0.9400634527206422
  - 1.6111593127250672
  - 2.4468119561672212
  - 8.296659445762634
  - 1.5117673456668854
  - 1.5764520823955537
  - 1.33876371383667
  - 1.3211718797683716
  - 1.0144990682601929
  - 1.1203528285026552
  - 1.0565667867660522
  - 0.8616673022508622
  - 1.0694555401802064
  - 0.9295199632644654
  - 0.9264341592788696
  - 1.1771338284015656
  - 0.847910362482071
  validation_losses:
  - 3.1753990650177
  - 6.802773475646973
  - 5.4472246170043945
  - 0.6750624775886536
  - 0.8102604746818542
  - 0.7063709497451782
  - 0.5802783370018005
  - 0.7033093571662903
  - 0.48576483130455017
  - 0.5891239643096924
  - 2.364342451095581
  - 0.8588023781776428
  - 1.1250215768814087
  - 0.7002524137496948
  - 1.0953383445739746
  - 0.7903662323951721
  - 1.846561074256897
  - 0.7119306325912476
  - 0.6873153448104858
  - 1.3993128538131714
  - 0.4293063282966614
  - 0.5643129348754883
  - 0.5054185390472412
  - 0.6856701374053955
  - 0.8888318538665771
  - 0.5984528660774231
  - 0.48482099175453186
  - 0.47074460983276367
  - 0.41613030433654785
  - 0.40613535046577454
  - 0.4968222677707672
  - 0.43723759055137634
  - 0.4663841426372528
  - 0.40682435035705566
  - 0.41574907302856445
  - 0.4179156422615051
  - 0.4695311188697815
  - 0.4289380609989166
  - 0.3945341110229492
  - 0.4168373644351959
  - 0.3814314603805542
  - 0.42719513177871704
  - 0.3933136463165283
  - 0.397198885679245
  - 0.4140741229057312
  - 0.38397490978240967
  - 0.39202097058296204
  - 0.45776692032814026
  - 0.41542595624923706
  - 0.413722425699234
  - 0.39035162329673767
  - 0.41442734003067017
  - 0.3954923152923584
  - 0.39053523540496826
  - 0.4146357476711273
  - 0.3852023184299469
  - 0.4116542935371399
  - 0.43652158975601196
  - 0.43080589175224304
  - 0.4426760673522949
  - 0.5001180768013
  - 0.5211010575294495
  - 0.8470565676689148
  - 0.6400056481361389
  - 0.4805571138858795
  - 0.47962474822998047
  - 0.6129011511802673
  - 0.49400830268859863
  - 0.8304430246353149
  - 0.5330807566642761
  - 0.6053152084350586
  - 0.42439761757850647
  - 0.4173584580421448
  - 0.4734918177127838
  - 0.5766162276268005
  - 0.8040896058082581
  - 0.42614030838012695
  - 0.461666464805603
  - 0.47289392352104187
  - 0.40260130167007446
  - 0.4412694275379181
  - 0.4520464241504669
  - 0.44736602902412415
  - 0.4228397607803345
  - 0.4064784049987793
  - 0.42641013860702515
  - 0.3972918689250946
  - 0.49613815546035767
  - 0.525194525718689
  - 0.6094966530799866
  - 0.42113426327705383
  - 0.39507627487182617
  - 0.5321975946426392
  - 0.4370670020580292
  - 0.39681655168533325
  - 0.4254050552845001
  - 0.41501012444496155
  - 0.39704811573028564
  - 0.3982219696044922
  - 0.39263612031936646
loss_records_fold1:
  train_losses:
  - 0.957762086391449
  - 0.8763325929641724
  - 0.8396068096160889
  - 0.9762134671211243
  - 0.9290519535541535
  - 1.1514402329921722
  - 0.8866411268711091
  - 0.883569073677063
  - 1.0205650866031648
  - 1.0323185622692108
  - 0.8219919323921204
  - 0.8773562371730805
  - 0.8719851136207581
  - 0.9154446601867676
  - 3.515559881925583
  - 1.158685427904129
  - 0.9772361516952515
  - 1.2329781830310822
  - 0.8983597040176392
  - 1.3534893810749056
  - 0.9225160181522369
  - 0.9164932310581207
  - 0.8760129272937776
  validation_losses:
  - 0.42119401693344116
  - 0.4194507300853729
  - 0.4024460017681122
  - 0.44232451915740967
  - 0.4197383224964142
  - 0.4316392242908478
  - 0.4571317732334137
  - 0.41931894421577454
  - 0.46561872959136963
  - 0.4078831970691681
  - 0.46731218695640564
  - 0.41918662190437317
  - 0.408454954624176
  - 0.43474724888801575
  - 0.43098756670951843
  - 0.4460124373435974
  - 0.5133004784584045
  - 0.419236421585083
  - 0.4163614809513092
  - 0.4178931415081024
  - 0.4130697250366211
  - 0.4187760651111603
  - 0.40961316227912903
loss_records_fold2:
  train_losses:
  - 0.9402450203895569
  - 0.9407925248146057
  - 0.8819761335849763
  - 0.8290170192718507
  - 0.8620839416980743
  - 0.8485233187675476
  - 0.9215283513069153
  - 0.8691928744316102
  - 2.9498724877834324
  - 1.6615220785140992
  - 1.4629474878311157
  - 4.40447251200676
  - 1.4011718034744263
  - 1.0155841708183289
  - 1.353539901971817
  - 1.0753072798252106
  validation_losses:
  - 0.38194525241851807
  - 0.39773210883140564
  - 0.40082934498786926
  - 0.39686110615730286
  - 0.39074552059173584
  - 0.3905316889286041
  - 0.40046778321266174
  - 0.3958541750907898
  - 0.42511552572250366
  - 0.5217217206954956
  - 0.44814378023147583
  - 0.41577965021133423
  - 0.4044855535030365
  - 0.408125102519989
  - 0.39567264914512634
  - 0.40140876173973083
loss_records_fold3:
  train_losses:
  - 7.218447268009186
  - 1.0613401323556901
  - 1.3140181481838227
  - 0.954115790128708
  - 1.9925614833831788
  - 4.157371598482132
  - 1.1526791870594024
  - 1.4793677031993866
  - 1.3019029676914216
  - 0.9514210522174835
  - 0.8274376809597016
  - 0.8464157164096833
  - 0.8491260290145874
  - 0.9991606295108796
  - 1.088656133413315
  - 0.9610362112522126
  - 0.9898393511772157
  - 0.9199013113975525
  - 1.1064648449420929
  - 0.9890482366085053
  - 0.9598824799060822
  - 0.8826387166976929
  - 0.8840013265609742
  - 1.1074502110481264
  - 2.8061577022075657
  - 0.9184083342552185
  - 1.0116822302341462
  - 0.8478678226470948
  - 1.2238848924636843
  - 0.8477764070034027
  - 1.3149368226528169
  - 0.9045011818408967
  - 0.9450505793094636
  - 0.8894912540912628
  - 0.8159441292285919
  - 2.3952756822109222
  - 1.0696384251117708
  - 0.9028248548507691
  - 1.0115664184093476
  - 0.8572468459606171
  - 0.9717053949832917
  - 1.0033040881156923
  - 0.9559296011924744
  - 0.8450822442770005
  - 0.9933775246143342
  - 1.015725463628769
  - 0.8813739657402039
  - 1.30660879611969
  - 0.9006904900074005
  - 0.917248970270157
  - 0.8812713444232941
  - 0.8493014276027679
  - 0.877820897102356
  - 0.877913773059845
  - 0.8759368538856507
  - 0.8337076246738434
  - 0.8433123826980591
  - 0.8299628078937531
  - 0.8400713682174683
  - 0.8501937448978425
  - 0.8582324504852296
  - 0.8333918809890748
  - 0.8021996170282364
  - 0.8490490913391113
  - 0.8225505590438843
  - 1.1149696826934814
  - 0.9283007323741913
  - 0.9118295609951019
  - 0.8893332600593568
  - 0.9107093334197999
  - 0.8687495648860932
  - 0.816497164964676
  - 0.8767060279846192
  - 0.8919535458087922
  - 0.8751072883605957
  - 1.0316591501235963
  - 0.8631045937538148
  - 0.8590315818786621
  - 0.8649410843849182
  - 0.836770236492157
  - 0.844164264202118
  - 0.8853379249572755
  - 1.0601647615432739
  - 0.9194331049919129
  - 0.8211079418659211
  - 0.8374086320400238
  - 0.8527287960052491
  - 0.8298666238784791
  - 0.8359163403511047
  - 0.8705579578876496
  - 0.8506616830825806
  - 0.8706103205680847
  - 0.8557618618011475
  - 0.886105352640152
  - 0.8217413067817688
  - 1.1564462959766388
  - 0.8893546223640443
  - 0.855737030506134
  - 0.8267839223146439
  - 0.9226898193359375
  validation_losses:
  - 0.4067997932434082
  - 0.5034113526344299
  - 0.4435403048992157
  - 0.4765453338623047
  - 0.45474767684936523
  - 0.42055240273475647
  - 0.6500800251960754
  - 0.4984765350818634
  - 0.4182036519050598
  - 0.402646005153656
  - 0.4009911119937897
  - 0.4164242744445801
  - 0.40101051330566406
  - 0.389565110206604
  - 0.3956111967563629
  - 0.4485776424407959
  - 0.47197890281677246
  - 0.41886311769485474
  - 0.3893814980983734
  - 0.40115123987197876
  - 0.4103568494319916
  - 0.41215619444847107
  - 0.40402910113334656
  - 0.39313820004463196
  - 0.40026944875717163
  - 0.55064457654953
  - 0.4018039107322693
  - 0.4208180904388428
  - 0.42798900604248047
  - 0.413085013628006
  - 0.432417094707489
  - 0.40236353874206543
  - 0.4044148325920105
  - 0.4069342613220215
  - 0.4238050878047943
  - 0.41215357184410095
  - 0.4189889132976532
  - 0.40907275676727295
  - 0.40794166922569275
  - 0.4196247160434723
  - 0.40968814492225647
  - 0.40748873353004456
  - 0.39594435691833496
  - 0.400747150182724
  - 0.39068084955215454
  - 0.41687995195388794
  - 0.41261446475982666
  - 0.41272345185279846
  - 0.45421913266181946
  - 0.4176449179649353
  - 0.42162391543388367
  - 0.4168075621128082
  - 0.41480615735054016
  - 0.4028864800930023
  - 0.4189970791339874
  - 0.4147529900074005
  - 0.40720176696777344
  - 0.4054197669029236
  - 0.4060271084308624
  - 0.4023967385292053
  - 0.4138607084751129
  - 0.39249861240386963
  - 0.3975394666194916
  - 0.3954028785228729
  - 0.42133453488349915
  - 0.413097083568573
  - 0.4038793742656708
  - 0.43984195590019226
  - 0.43148988485336304
  - 0.3995055556297302
  - 0.4071577787399292
  - 0.41352710127830505
  - 0.41271743178367615
  - 0.42551523447036743
  - 0.39590737223625183
  - 0.4220178723335266
  - 0.3969271183013916
  - 0.4204333424568176
  - 0.4003959894180298
  - 0.4095308482646942
  - 0.42204543948173523
  - 0.3945971727371216
  - 0.40625157952308655
  - 0.41057589650154114
  - 0.4402433931827545
  - 0.4153223931789398
  - 0.4024229645729065
  - 0.42047119140625
  - 0.4063218832015991
  - 0.39963990449905396
  - 0.4761877655982971
  - 0.4176895022392273
  - 0.4038102924823761
  - 0.3924179673194885
  - 0.40718507766723633
  - 0.4154544770717621
  - 0.39874231815338135
  - 0.424236923456192
  - 0.42974892258644104
  - 0.5664561986923218
loss_records_fold4:
  train_losses:
  - 1.1041330933570863
  - 0.8826500713825226
  - 0.9463361024856568
  - 0.8245536148548127
  - 0.8627344369888306
  - 0.818684309720993
  - 0.8235268473625184
  - 0.8842019021511078
  - 0.9333592712879182
  - 0.8580431461334229
  - 0.8845172226428986
  - 0.9022887647151947
  - 0.8327609777450562
  - 0.7958215862512589
  - 0.8434042513370514
  - 0.8539346873760224
  - 1.3023688554763795
  - 1.226978588104248
  - 0.8677172422409059
  - 0.8653370618820191
  - 0.852528339624405
  - 0.8338979899883271
  - 0.830030119419098
  - 0.847973906993866
  - 0.8473639965057373
  - 0.8706957638263703
  - 0.8213484764099122
  - 0.8217948257923127
  - 0.8554703056812287
  validation_losses:
  - 0.4744355082511902
  - 0.46584025025367737
  - 0.4344831109046936
  - 0.3902198076248169
  - 0.4061400294303894
  - 0.4095345735549927
  - 0.41332435607910156
  - 0.4161939322948456
  - 0.4041168689727783
  - 0.4016571044921875
  - 0.4752572774887085
  - 0.4047292172908783
  - 0.3993658721446991
  - 0.3866649568080902
  - 0.40655332803726196
  - 0.39527052640914917
  - 0.40328705310821533
  - 0.4039493501186371
  - 0.4018620252609253
  - 0.41438403725624084
  - 0.4335153102874756
  - 0.4089513123035431
  - 0.4204162359237671
  - 0.41588065028190613
  - 0.40281954407691956
  - 0.3961201608181
  - 0.3982265591621399
  - 0.4016898274421692
  - 0.39925503730773926
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
training_metrics:
  fold_eval_accs: '[0.8593481989708405, 0.8576329331046312, 0.8593481989708405, 0.6843910806174958,
    0.8591065292096219]'
  fold_eval_f1: '[0.02380952380952381, 0.0, 0.0, 0.1238095238095238, 0.0]'
  mean_eval_accuracy: 0.8239653881746859
  mean_f1_accuracy: 0.029523809523809525
  total_train_time: '0:22:31.210254'
