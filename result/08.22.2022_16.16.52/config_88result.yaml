config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:11:38.846514'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_88fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_88fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_88fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_88fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_88fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_88fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_88fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_88fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_88fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_88fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 49.180477628111845
  - 25.71944900751114
  - 27.31208591423929
  - 19.852326074242594
  - 22.629882341623308
  - 26.943604087084534
  - 17.550253811478616
  - 11.512886494398117
  - 12.509874247759582
  - 19.24628873169422
  - 7.395591485500336
  - 11.629556702822448
  - 6.885617765784264
  - 7.786559185385705
  - 8.066371086239815
  - 7.764134964346886
  - 6.745168107748032
  - 6.3138284444808965
  - 9.135040345788003
  - 6.7909890741109855
  - 7.614769619703293
  - 6.588641890883446
  - 6.132528975605965
  - 5.977828046679497
  - 6.6452585577964784
  - 6.051728869974614
  - 6.392522156238556
  - 6.214117777347565
  - 6.3520791888237005
  - 6.133110547065735
  - 6.112354075908661
  - 5.907503512501717
  - 5.94729286134243
  - 5.898780816793442
  - 6.313973662257195
  - 6.374032053351403
  - 6.009568281471729
  - 5.971499219536781
  - 6.248129239678383
  - 6.012151464819908
  - 5.929316776990891
  - 5.9888631999492645
  - 5.889739874005318
  - 5.907800051569939
  - 5.822187086939812
  - 5.95377279818058
  - 5.881384044885635
  - 6.094857412576676
  - 5.82766199707985
  - 5.88219043314457
  - 6.100394329428673
  - 6.012484440207482
  - 5.949597436189652
  - 6.119974732398987
  - 5.907363869249821
  - 6.160035356879234
  - 5.953162810206414
  - 5.9622987896204
  - 6.033811630308628
  - 6.419875741004944
  - 6.215325579047203
  - 7.024231386184693
  - 7.834638687968255
  - 6.603352293372154
  - 7.299364852905274
  - 6.304408413171768
  - 6.706521272659302
  - 6.445150765776635
  - 6.021051344275475
  - 6.082536694407463
  - 6.221685156226158
  validation_losses:
  - 3.7796847820281982
  - 0.9175541996955872
  - 0.576876163482666
  - 0.6029468178749084
  - 0.5891836881637573
  - 0.5838235020637512
  - 0.8752332925796509
  - 0.5163214206695557
  - 0.4071595072746277
  - 0.3875828981399536
  - 0.5146440863609314
  - 0.40634390711784363
  - 0.3973712921142578
  - 0.3852717876434326
  - 0.3993685841560364
  - 0.3849462866783142
  - 1.4568867683410645
  - 0.4022146761417389
  - 0.4126739203929901
  - 0.38808882236480713
  - 0.7803300619125366
  - 0.4055447578430176
  - 0.40336117148399353
  - 0.4438556134700775
  - 0.39764416217803955
  - 0.3901967704296112
  - 0.38966044783592224
  - 0.3916637897491455
  - 0.42813965678215027
  - 0.4278062582015991
  - 0.39826706051826477
  - 0.39116889238357544
  - 0.3945840895175934
  - 0.4381881058216095
  - 0.4632607102394104
  - 0.4716511070728302
  - 0.59009850025177
  - 0.3900853991508484
  - 0.4091709554195404
  - 0.3965948224067688
  - 0.5636059641838074
  - 0.39130762219429016
  - 0.3911048173904419
  - 0.3907053470611572
  - 0.40071219205856323
  - 0.4071958661079407
  - 0.3943849503993988
  - 0.6519038677215576
  - 0.39151498675346375
  - 0.3955918252468109
  - 0.4235133230686188
  - 0.3906448781490326
  - 0.4067659378051758
  - 0.4293866753578186
  - 0.3934265971183777
  - 0.4009653329849243
  - 0.39029911160469055
  - 0.5012562274932861
  - 2.752065420150757
  - 0.49386677145957947
  - 4.081367015838623
  - 0.9277984499931335
  - 0.7406967282295227
  - 0.43337181210517883
  - 0.5832008123397827
  - 0.440647155046463
  - 0.41335344314575195
  - 0.41603201627731323
  - 0.41197481751441956
  - 0.41769537329673767
  - 0.38634929060935974
loss_records_fold1:
  train_losses:
  - 6.133671590685845
  - 5.795618715882302
  - 6.106510454416275
  - 6.090566921234132
  - 5.963612207770348
  - 6.0739137709140785
  - 5.931743296980859
  - 5.935395221412183
  - 6.157481542229653
  - 5.8150502979755405
  - 5.928739327192307
  - 5.836316925287247
  - 5.878441110253334
  - 5.815635648369789
  - 5.982114350795746
  - 5.8579141572117805
  - 5.846109679341317
  - 5.827962094545365
  - 5.867312902212143
  - 5.976052552461624
  - 5.792076140642166
  - 5.902760466933251
  - 5.930929097533227
  - 5.879684865474701
  - 5.838522720336915
  - 5.931909346580506
  - 6.075241680443288
  - 5.908644841611386
  - 5.8345623254776005
  - 6.040943247079849
  - 5.864478251338006
  - 5.909121257066727
  - 5.870014524459839
  - 5.817652900516987
  - 5.81233951151371
  - 5.728520134091378
  - 5.7887661710381515
  - 5.694130524992943
  - 5.757116651535035
  - 5.7615373790264135
  - 5.822013404965401
  - 5.764908188581467
  - 5.809333026409149
  - 6.134942331910134
  - 6.4803433448076255
  - 6.262454545497895
  - 6.198820930719376
  - 7.279076626896859
  - 5.993553107976914
  - 5.990100648999214
  - 5.851265667378903
  - 6.048892012238503
  - 5.863293534517289
  - 5.761750981211662
  validation_losses:
  - 0.4332524240016937
  - 0.4203331172466278
  - 0.42110833525657654
  - 0.47214454412460327
  - 0.5016903877258301
  - 0.42152923345565796
  - 0.3987956643104553
  - 0.478450208902359
  - 0.4477842152118683
  - 0.4034258723258972
  - 0.43538111448287964
  - 0.4314383864402771
  - 0.41197195649147034
  - 0.4038918912410736
  - 0.40106359124183655
  - 0.404798686504364
  - 0.42414847016334534
  - 0.4501451253890991
  - 0.40368789434432983
  - 0.4228728711605072
  - 0.4651462137699127
  - 0.4146069586277008
  - 0.40545085072517395
  - 0.4234975278377533
  - 0.4010617733001709
  - 0.4056808054447174
  - 0.43325871229171753
  - 0.4207637906074524
  - 0.42287155985832214
  - 0.40169546008110046
  - 0.4343581199645996
  - 0.4149574339389801
  - 0.4059751033782959
  - 0.4032043516635895
  - 0.4355563521385193
  - 0.4012543261051178
  - 0.4338030219078064
  - 0.40113958716392517
  - 0.42117562890052795
  - 0.4346928894519806
  - 0.39936718344688416
  - 0.46535658836364746
  - 0.4122777581214905
  - 18634.6875
  - 0.42074674367904663
  - 0.4532790184020996
  - 0.42445477843284607
  - 0.4405581057071686
  - 0.44497257471084595
  - 0.44946154952049255
  - 0.41936028003692627
  - 0.4251498281955719
  - 0.4103580415248871
  - 0.4054417312145233
loss_records_fold2:
  train_losses:
  - 5.88938229084015
  - 5.959582102298737
  - 5.879173293709755
  - 6.060409405827523
  - 5.9693589001894
  - 6.015000608563423
  - 5.94730787575245
  - 5.8844521135091785
  - 5.817069950699807
  - 6.004211223125458
  - 5.916800674796105
  - 6.053033700585366
  - 6.063963252305985
  - 6.31627191901207
  - 5.90993085205555
  - 5.9388555735349655
  - 6.137168657779694
  - 5.903150191903115
  - 5.856100004911423
  - 6.012744796276093
  - 6.014472222328187
  - 5.958687555789948
  validation_losses:
  - 0.3816658556461334
  - 0.38328656554222107
  - 0.38236087560653687
  - 0.4149820804595947
  - 14.393640518188477
  - 0.384542852640152
  - 0.40232953429222107
  - 0.3832523822784424
  - 0.3821842074394226
  - 0.3843788504600525
  - 0.39253124594688416
  - 529.4451293945312
  - 412.9642639160156
  - 0.38323280215263367
  - 0.382609099149704
  - 0.43296682834625244
  - 0.39723578095436096
  - 0.390139639377594
  - 0.3876291811466217
  - 0.3825601637363434
  - 0.3827935457229614
  - 0.39231976866722107
loss_records_fold3:
  train_losses:
  - 5.8660632401704795
  - 5.841915878653527
  - 5.911135670542717
  - 5.933536872267723
  - 5.928732851147652
  - 5.81905522942543
  - 5.884360066056252
  - 6.4097780585289
  - 6.121706464886666
  - 6.132869014143944
  - 6.009608495235444
  - 5.891986072063446
  - 5.885742741823197
  - 5.899948567152023
  - 5.976050302386284
  - 6.008159506320954
  - 5.983106929063798
  - 5.880667138099671
  - 5.991882938146592
  - 5.958456408977509
  - 6.280358693003655
  - 5.959133252501488
  - 5.847525957226754
  - 6.00740881562233
  - 5.9194852918386465
  - 6.038681888580323
  - 6.4199527412652975
  - 5.971706771850586
  - 5.861224254965783
  - 5.8923035889863975
  - 5.949846252799034
  - 5.915306603908539
  - 5.795958617329598
  - 5.890821740031242
  - 5.9674986660480505
  - 6.363729658722878
  - 5.88617888391018
  - 5.897700989246369
  - 6.078831964731217
  - 5.8194910228252414
  - 5.920417156815529
  - 5.821610370278359
  - 5.887591141462327
  - 5.981093814969063
  - 6.013341811299324
  - 5.828973650932312
  - 5.950500565767289
  - 5.890634900331498
  - 6.026426845788956
  - 5.953186196088791
  - 5.877300244569779
  - 6.067812392115593
  - 5.834038941562176
  - 5.910791748762131
  - 5.943277096748353
  - 5.875491291284561
  - 6.027871629595757
  - 6.080089989304543
  - 5.997014659643174
  - 5.887934124469758
  - 5.909155780076981
  - 5.937708485126496
  - 5.909066399931908
  - 6.44537253677845
  - 5.915122970938683
  - 6.03035007417202
  - 5.935780459642411
  - 5.871146655082703
  - 5.864514544606209
  - 5.927677863836289
  - 5.903202232718468
  - 5.900112691521645
  - 5.8610264509916306
  - 6.029988391697407
  - 5.9848726242780685
  - 5.905679607391358
  - 6.489810624718666
  - 9.735135157406331
  - 7.159035310149193
  - 6.4477509111166
  - 5.8693173944950106
  - 6.131674283742905
  - 5.782109946012497
  - 5.9864955097436905
  - 5.827092432975769
  - 6.115569430589677
  - 6.10393962264061
  - 6.255722132325173
  - 6.1003967225551605
  - 5.9382745891809465
  - 5.8510078459978105
  - 5.94532313644886
  - 5.920069390535355
  - 6.397912019491196
  - 5.942470619082451
  - 6.209718257188797
  - 5.964952111244202
  - 5.864739108085633
  - 6.021646395325661
  - 5.910689187049866
  validation_losses:
  - 0.3971188962459564
  - 0.40584060549736023
  - 0.4047342836856842
  - 0.3978692591190338
  - 0.40133118629455566
  - 0.4491031765937805
  - 0.39879298210144043
  - 0.439573734998703
  - 0.4041660726070404
  - 0.4580983817577362
  - 0.40144455432891846
  - 0.40332069993019104
  - 0.41093572974205017
  - 0.3985554575920105
  - 0.41916966438293457
  - 0.40056583285331726
  - 0.40071558952331543
  - 0.4458343982696533
  - 0.40527087450027466
  - 0.3974299728870392
  - 0.4488773047924042
  - 0.4113342761993408
  - 0.4516856074333191
  - 0.3995247781276703
  - 0.39831820130348206
  - 0.39916709065437317
  - 0.41463151574134827
  - 0.39759179949760437
  - 0.4003428518772125
  - 0.3991076946258545
  - 0.425178200006485
  - 0.4035111367702484
  - 0.41496115922927856
  - 0.39896076917648315
  - 0.42380383610725403
  - 0.402635395526886
  - 0.3983539640903473
  - 0.41536444425582886
  - 0.45081159472465515
  - 0.39861080050468445
  - 0.3978525400161743
  - 0.3990054726600647
  - 0.40065687894821167
  - 0.4408968389034271
  - 0.4005114436149597
  - 0.4282872676849365
  - 0.39852374792099
  - 0.40074339509010315
  - 0.4290381669998169
  - 0.40053850412368774
  - 0.39994746446609497
  - 0.398234099149704
  - 0.4339737892150879
  - 0.4107414782047272
  - 0.4240662157535553
  - 0.3983383774757385
  - 0.39892175793647766
  - 0.47112950682640076
  - 0.4095056653022766
  - 0.39892125129699707
  - 0.40585437417030334
  - 0.39894598722457886
  - 0.39886730909347534
  - 0.41654399037361145
  - 0.4126019775867462
  - 0.3993719220161438
  - 0.39862918853759766
  - 0.4261655807495117
  - 0.3997294008731842
  - 0.41789525747299194
  - 0.4493105113506317
  - 0.4084620773792267
  - 0.39918455481529236
  - 0.47280409932136536
  - 0.4015369117259979
  - 0.3976200520992279
  - 0.4202665090560913
  - 0.5362264513969421
  - 0.4574914276599884
  - 0.4067750573158264
  - 0.4641869366168976
  - 0.394806832075119
  - 0.43171390891075134
  - 0.39664754271507263
  - 0.40555667877197266
  - 0.4319109618663788
  - 0.40099433064460754
  - 0.4450337290763855
  - 0.4091700613498688
  - 0.40306538343429565
  - 0.4685118496417999
  - 0.3964666426181793
  - 0.4176146984100342
  - 9.342620849609375
  - 0.396857351064682
  - 0.4399692118167877
  - 0.5050582885742188
  - 10.033507347106934
  - 110.04386138916016
  - 1.215415596961975
loss_records_fold4:
  train_losses:
  - 5.976730141043664
  - 5.960246872901917
  - 5.933183273673058
  - 5.93679766356945
  - 5.976554334163666
  - 6.866896569728851
  - 5.960174033045769
  - 5.968592429161072
  - 6.087385058403015
  - 5.90208278298378
  - 5.866877284646034
  validation_losses:
  - 0.4026987850666046
  - 0.40705105662345886
  - 0.39758995175361633
  - 0.3956671357154846
  - 0.3911657929420471
  - 0.3986586928367615
  - 0.39301300048828125
  - 0.39159780740737915
  - 0.39462584257125854
  - 0.3911873400211334
  - 0.39087212085723877
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 71 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 54 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:26:39.020134'
