config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 22:21:17.261303'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/82/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/82/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/82/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/82/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/82/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/82/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/82/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/82/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/82/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/82/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 171.6665323972702
  - 86.54952862858772
  - 108.03381451964378
  - 74.8214790225029
  - 42.20920467376709
  - 32.37539350986481
  - 38.202863067388535
  - 22.06787520647049
  - 21.18279656767845
  - 22.50635528564453
  - 23.020558416843414
  - 27.77810165286064
  - 32.735600233078
  - 23.47477900981903
  - 17.85374954342842
  - 31.27754119038582
  - 60.71244567632675
  - 35.612795516848564
  - 25.62001544237137
  - 28.174621984362602
  - 21.484844997525215
  - 39.27552901208401
  - 42.24803802371025
  - 32.213123857975006
  - 22.689551815390587
  - 31.079245403409004
  - 18.69082024693489
  - 25.684443041682243
  - 29.753936737775803
  - 20.931714743375778
  - 21.517687261104584
  - 16.56480646133423
  - 19.64824929833412
  - 40.71723699569702
  - 20.713241428136826
  - 16.75826631486416
  - 17.64831766486168
  - 23.991408616304398
  - 17.061954826116562
  - 15.148470759391785
  - 21.282408744096756
  - 19.12285941839218
  - 16.030333682894707
  - 18.456073135137558
  - 20.345152378082275
  - 26.630701899528503
  - 16.95514327287674
  - 17.431529253721237
  - 16.672187864780426
  - 17.55196350812912
  - 15.16411206126213
  - 15.96256735920906
  - 15.599426567554474
  - 23.656688570976257
  - 15.477647736668587
  - 15.986134707927704
  - 18.682797461748123
  - 14.958421379327774
  - 16.687615275382996
  - 14.363809153437614
  - 16.205021634697914
  - 16.32347798347473
  - 15.262910276651382
  - 16.224618911743164
  - 16.186519876122475
  - 16.7905742675066
  - 14.528394043445587
  - 14.96497768163681
  - 15.568664222955704
  - 14.480468541383743
  - 16.177353128790855
  - 19.680897057056427
  - 18.334733858704567
  - 25.03963991999626
  - 20.20589691400528
  - 20.760905891656876
  - 17.289881005883217
  - 15.76873642206192
  - 17.383815228939056
  - 16.41280287504196
  - 17.328990668058395
  - 16.88734942674637
  - 14.293949380517006
  - 14.824787199497223
  - 17.20396652817726
  - 14.427934855222702
  - 15.99417895078659
  - 15.43299226462841
  - 15.406269788742065
  - 15.489658534526825
  - 15.384073331952095
  - 17.13048940896988
  - 15.793897092342377
  - 14.508138477802277
  - 15.07223317027092
  - 15.90723292529583
  - 15.286952659487724
  - 15.111998081207275
  - 14.898266673088074
  - 14.310185462236404
  validation_losses:
  - 2.9131619930267334
  - 0.65524822473526
  - 3.7110540866851807
  - 0.5066600441932678
  - 0.5644605755805969
  - 0.45052269101142883
  - 0.38040781021118164
  - 0.5647732019424438
  - 0.6452592015266418
  - 0.6218956112861633
  - 0.4914425313472748
  - 0.4358999729156494
  - 0.6219120025634766
  - 0.39261680841445923
  - 0.40710899233818054
  - 0.6436794996261597
  - 0.5177621841430664
  - 0.7461931109428406
  - 0.4491330087184906
  - 0.47561967372894287
  - 0.45606181025505066
  - 6.504849910736084
  - 0.6125765442848206
  - 0.4402041435241699
  - 0.4971560537815094
  - 0.6399543881416321
  - 0.395438015460968
  - 0.7333468198776245
  - 0.40193086862564087
  - 0.4067498743534088
  - 0.3836350739002228
  - 0.4400331676006317
  - 0.9430731534957886
  - 0.3971518576145172
  - 0.3928660750389099
  - 0.40252143144607544
  - 0.38358646631240845
  - 0.4167301654815674
  - 0.44963544607162476
  - 0.41440102458000183
  - 2.389821767807007
  - 0.4308890104293823
  - 0.3816732168197632
  - 0.39490216970443726
  - 0.6378486752510071
  - 0.3978559374809265
  - 0.3891902565956116
  - 0.4414563477039337
  - 0.38926780223846436
  - 0.3955458700656891
  - 0.4872322976589203
  - 0.38594698905944824
  - 0.3839057385921478
  - 0.37951022386550903
  - 0.3838912546634674
  - 0.41019466519355774
  - 0.3782820999622345
  - 0.3814617097377777
  - 0.4149313271045685
  - 0.3788462281227112
  - 0.37241658568382263
  - 0.39923712611198425
  - 0.37980177998542786
  - 0.3910396993160248
  - 0.40250054001808167
  - 0.3770652711391449
  - 0.410964697599411
  - 0.38614213466644287
  - 0.3830387592315674
  - 0.3756996691226959
  - 0.3882167935371399
  - 0.6640564203262329
  - 5.382378578186035
  - 0.6366816163063049
  - 0.38596346974372864
  - 1.689098834991455
  - 0.4483528733253479
  - 0.38216978311538696
  - 0.40496954321861267
  - 0.3821624517440796
  - 0.3907873034477234
  - 0.3822110593318939
  - 0.3852492570877075
  - 0.3787479102611542
  - 0.39543429017066956
  - 0.4075882136821747
  - 0.4052271544933319
  - 0.41468092799186707
  - 0.4374929964542389
  - 0.3788943886756897
  - 0.3760516941547394
  - 0.3863043785095215
  - 0.5214937925338745
  - 0.4275877773761749
  - 0.4491107761859894
  - 1.4682104587554932
  - 0.39138132333755493
  - 0.39067164063453674
  - 0.381780207157135
  - 0.3799300789833069
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 52 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:06:15.736056'
