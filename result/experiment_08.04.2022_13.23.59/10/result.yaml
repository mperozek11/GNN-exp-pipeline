config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 14:23:24.674278'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/10/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/10/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/10/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/10/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/10/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/10/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/10/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/10/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/10/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/10/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 14.89446297287941
  - 15.71244415640831
  - 15.53314146399498
  - 15.527468085289001
  - 15.187813699245453
  - 15.451790660619736
  - 15.21317371726036
  - 14.853633135557175
  - 15.137196496129036
  - 14.94432282447815
  - 15.257966056466103
  - 14.930810660123825
  - 15.073766499757767
  - 15.101515218615532
  - 15.485209435224533
  - 14.837520271539688
  - 14.77553516626358
  - 15.239446297287941
  - 15.016671657562256
  - 15.19393390417099
  - 15.228188753128052
  - 14.981923460960388
  - 15.041842922568321
  - 15.140481799840927
  - 15.2425577044487
  - 15.117437243461609
  - 15.321064308285713
  - 15.276548236608505
  - 14.769126176834106
  - 15.099587321281433
  - 15.113703221082687
  - 15.138878017663956
  - 15.001870423555374
  - 14.779962360858917
  - 14.965800404548645
  - 14.890915855765343
  - 15.039097398519516
  - 15.00229588150978
  - 15.07518845796585
  - 14.854380011558533
  - 14.842006266117096
  - 15.464272171258926
  - 15.04898652434349
  - 14.426435127854347
  - 14.72411933541298
  - 15.154407814145088
  - 15.08934137225151
  - 14.845859795808792
  - 15.381844282150269
  - 14.920298755168915
  - 14.987795174121857
  - 14.81794399023056
  - 14.76913070678711
  - 15.132797628641129
  - 15.306385532021523
  - 14.959055066108704
  - 14.906712889671326
  - 14.949019640684128
  - 14.992855966091156
  - 14.932884365320206
  - 14.546078100800514
  - 14.992928624153137
  - 15.243645340204239
  - 15.194366127252579
  - 14.857864901423454
  - 14.642753660678864
  - 14.875363871455193
  - 15.18699549138546
  - 15.338406890630722
  - 14.857094377279282
  - 14.947295010089874
  - 15.246201932430267
  - 15.06911951303482
  - 14.894687533378601
  - 14.870193719863892
  - 15.400554865598679
  - 15.029693484306335
  - 14.59428933262825
  - 14.918135523796082
  - 14.847004666924477
  - 14.549912631511688
  - 14.597581088542938
  - 14.84780690073967
  - 14.891969621181488
  - 14.78624239563942
  - 15.13258059322834
  - 15.223969399929047
  - 15.022573083639145
  - 14.52840980887413
  - 14.986700534820557
  - 14.802109763026237
  - 15.109034389257431
  - 15.313045531511307
  - 14.765129804611206
  - 14.869936302304268
  - 14.615316390991211
  - 14.680188298225403
  - 14.607452496886253
  - 14.80624544620514
  - 14.79492273926735
  validation_losses:
  - 0.4076330363750458
  - 0.39118847250938416
  - 0.3907364308834076
  - 0.3940001428127289
  - 0.4001923203468323
  - 0.45238009095191956
  - 0.406668484210968
  - 0.38290777802467346
  - 0.3891657888889313
  - 0.3841094672679901
  - 0.38626858592033386
  - 0.43768438696861267
  - 0.3870048224925995
  - 0.40734007954597473
  - 0.5184781551361084
  - 0.4738745093345642
  - 0.42387181520462036
  - 0.40200212597846985
  - 0.3879837393760681
  - 0.3851836919784546
  - 0.3984859883785248
  - 0.3890567123889923
  - 0.4277738034725189
  - 0.39568495750427246
  - 0.4046301245689392
  - 0.3873792588710785
  - 0.3884260654449463
  - 0.41797107458114624
  - 0.42472153902053833
  - 0.3946898281574249
  - 0.4619619846343994
  - 0.5054042935371399
  - 1.192212462425232
  - 0.41211286187171936
  - 0.4131127893924713
  - 0.43298617005348206
  - 0.5595653057098389
  - 0.39752811193466187
  - 0.3955734074115753
  - 0.3856760859489441
  - 0.3833402693271637
  - 0.4328835606575012
  - 0.47677263617515564
  - 0.4989467263221741
  - 0.41354668140411377
  - 0.4003143012523651
  - 0.39212265610694885
  - 0.41327524185180664
  - 0.42859944701194763
  - 0.5626566410064697
  - 0.5235927700996399
  - 0.5013836026191711
  - 0.5140745043754578
  - 0.6786870956420898
  - 0.7226006984710693
  - 0.4691169559955597
  - 0.4063206911087036
  - 0.39544788002967834
  - 0.4862110912799835
  - 0.4050002694129944
  - 1.1851601600646973
  - 0.9443917870521545
  - 0.5231514573097229
  - 0.47822606563568115
  - 0.4905640482902527
  - 0.5111293196678162
  - 0.41478782892227173
  - 0.4042176902294159
  - 0.41539958119392395
  - 0.45086023211479187
  - 0.39983880519866943
  - 0.3959844708442688
  - 0.4060164988040924
  - 0.4271113872528076
  - 0.4307153820991516
  - 0.42363226413726807
  - 0.4378907382488251
  - 0.47692549228668213
  - 0.522821843624115
  - 0.5291970372200012
  - 0.8396548628807068
  - 0.41275113821029663
  - 0.4089430868625641
  - 0.8110463619232178
  - 0.5032438635826111
  - 0.52447509765625
  - 0.7165831327438354
  - 0.7346513867378235
  - 0.5296803116798401
  - 0.6358453035354614
  - 0.5844411849975586
  - 0.5205952525138855
  - 0.6157009601593018
  - 0.8831129670143127
  - 0.7356567978858948
  - 1.2626172304153442
  - 0.7206377983093262
  - 1.058242678642273
  - 1.7601737976074219
  - 0.6990212798118591
loss_records_fold2:
  train_losses:
  - 14.659569770097733
  - 14.84091728925705
  - 15.083753824234009
  - 14.88157805800438
  - 14.593813806772232
  - 14.980395644903183
  - 15.113320231437683
  - 14.520290285348892
  - 14.926902800798416
  - 14.693406969308853
  - 14.754783898591995
  - 14.925224900245667
  - 14.42265285551548
  - 14.71243703365326
  - 14.47972759604454
  - 14.978636175394058
  - 14.888778418302536
  - 14.38999655842781
  - 14.807211220264435
  - 14.732360646128654
  - 14.72023293375969
  - 14.787949949502945
  - 14.88162276148796
  - 14.697546780109406
  - 14.90452279150486
  - 14.658714175224304
  - 14.969216033816338
  - 14.913297951221466
  - 14.902924418449402
  - 14.971322238445282
  - 14.796786203980446
  - 14.391657054424286
  - 14.560472935438156
  - 14.55647474527359
  - 14.72458615899086
  - 14.665174394845963
  - 14.657220840454102
  - 14.670851111412048
  - 14.770067363977432
  - 14.818032875657082
  - 14.60060054063797
  - 14.553188517689705
  - 14.85920563340187
  - 14.72373652458191
  - 14.935067176818848
  - 14.818323194980621
  - 14.609233677387238
  - 14.407377392053604
  - 14.850521638989449
  - 14.70756122469902
  - 14.688717722892761
  - 14.562831193208694
  - 14.565518543124199
  - 14.510937809944153
  - 14.726622074842453
  - 14.709732040762901
  - 14.45213395357132
  - 14.849813520908356
  - 14.869675129652023
  - 15.140685766935349
  - 14.809260815382004
  - 14.757199913263321
  - 14.654790073633194
  - 14.847870469093323
  - 14.754920154809952
  - 14.736529111862183
  - 14.927700638771057
  - 14.570921808481216
  - 14.686324641108513
  - 14.452613145112991
  - 14.453007608652115
  - 14.989968359470367
  - 14.322066649794579
  - 14.627808600664139
  - 14.888086825609207
  - 14.241599261760712
  - 14.567557781934738
  - 14.522047728300095
  - 14.993036180734634
  - 14.735187664628029
  - 14.628761753439903
  - 14.673765376210213
  - 14.718975320458412
  - 14.842495113611221
  - 14.70864987373352
  - 14.81833353638649
  - 14.590727090835571
  - 14.63156595826149
  - 14.891740515828133
  - 14.490438520908356
  - 14.68847906589508
  - 14.203005522489548
  - 14.147342935204506
  - 14.873421058058739
  - 14.597840398550034
  - 14.81649523973465
  - 14.641994893550873
  - 14.460079371929169
  - 14.855942487716675
  - 14.154402017593384
  validation_losses:
  - 1.6234662532806396
  - 0.42215803265571594
  - 0.37929630279541016
  - 0.4672587811946869
  - 0.4426116943359375
  - 0.7424893975257874
  - 0.9393914937973022
  - 0.5363329648971558
  - 0.6769223809242249
  - 0.627918004989624
  - 0.5876081585884094
  - 0.4026114344596863
  - 0.49542516469955444
  - 0.6681134700775146
  - 0.43046990036964417
  - 0.514450192451477
  - 0.5963917970657349
  - 1.397009253501892
  - 0.7693456411361694
  - 0.5791568160057068
  - 1.339398980140686
  - 3.3423824310302734
  - 1.7197082042694092
  - 3.024892807006836
  - 0.841681957244873
  - 0.41607415676116943
  - 0.41230133175849915
  - 0.3944278657436371
  - 0.48267027735710144
  - 0.6126527190208435
  - 0.520733118057251
  - 0.4325980544090271
  - 1.1653751134872437
  - 1.364279866218567
  - 0.5048455595970154
  - 0.46501868963241577
  - 0.4783896803855896
  - 0.6303638815879822
  - 0.5755300521850586
  - 0.5522308349609375
  - 0.7264044880867004
  - 0.6759686470031738
  - 0.8388886451721191
  - 0.8178402781486511
  - 0.4481702148914337
  - 0.7351077795028687
  - 0.5841882228851318
  - 1.3650747537612915
  - 0.5477904081344604
  - 1.3445680141448975
  - 0.9153521060943604
  - 1.3416380882263184
  - 2.7853946685791016
  - 2.472639799118042
  - 1.3900266885757446
  - 2.0960586071014404
  - 0.561827540397644
  - 0.40847983956336975
  - 0.5492663979530334
  - 0.4382305443286896
  - 0.47127360105514526
  - 0.6243700981140137
  - 0.5388908982276917
  - 0.5925349593162537
  - 0.4066447913646698
  - 0.8520868420600891
  - 0.4803425669670105
  - 0.45752277970314026
  - 0.6069220304489136
  - 1.1677240133285522
  - 0.6911211013793945
  - 0.4170142710208893
  - 0.40785667300224304
  - 0.5113874077796936
  - 0.7415662407875061
  - 0.439352422952652
  - 1.0061001777648926
  - 0.44641631841659546
  - 0.7527165412902832
  - 0.48429763317108154
  - 0.5754870176315308
  - 0.5902191996574402
  - 0.5098274946212769
  - 1.3627574443817139
  - 0.699214518070221
  - 1.9343836307525635
  - 0.540432870388031
  - 0.4360162913799286
  - 0.4249056279659271
  - 0.6519718766212463
  - 0.5950413346290588
  - 0.5828099846839905
  - 0.4901468753814697
  - 0.6625996828079224
  - 0.46922117471694946
  - 0.6114692687988281
  - 0.7249295711517334
  - 2.313997983932495
  - 0.4115281105041504
  - 0.5189598798751831
loss_records_fold4:
  train_losses:
  - 14.862552672624588
  - 14.883844822645187
  - 14.573676139116287
  - 14.878696650266647
  - 14.987891405820847
  - 15.156611293554306
  - 14.896124839782715
  - 14.902523279190063
  - 14.974021524190903
  - 14.758465126156807
  - 14.40099686384201
  - 14.708754807710648
  - 14.635107725858688
  - 14.926700800657272
  - 14.650537937879562
  - 14.84111475944519
  - 14.608581483364105
  - 14.339886158704758
  - 14.701994687318802
  - 14.849625512957573
  - 14.426265329122543
  - 14.50730213522911
  - 14.589164942502975
  - 14.35389356315136
  - 14.549736574292183
  - 14.519906625151634
  - 14.626552164554596
  - 14.142328292131424
  - 14.480562970042229
  - 14.833658218383789
  - 14.361262425780296
  - 15.00405216217041
  - 14.429046839475632
  - 14.375132128596306
  - 14.564471364021301
  - 14.635804116725922
  - 14.500126004219055
  - 14.916662275791168
  - 14.849945589900017
  - 14.205801874399185
  - 14.56829285621643
  - 14.113548263907433
  - 14.480971962213516
  - 15.358660370111465
  - 15.25951436161995
  - 14.93272152543068
  - 14.872681736946106
  - 14.460434049367905
  - 14.425859868526459
  - 14.532852053642273
  - 14.78202973306179
  - 14.360381871461868
  - 13.835580125451088
  - 14.307487055659294
  - 14.684891045093536
  - 14.91250166296959
  - 14.950532406568527
  - 14.262082666158676
  - 14.134144470095634
  - 14.638361632823944
  - 14.854790359735489
  - 15.292349338531494
  - 14.782688185572624
  - 14.823223114013672
  - 14.040055245161057
  - 14.480085045099258
  - 14.077227890491486
  - 14.190250933170319
  - 14.748085349798203
  - 14.713188737630844
  - 13.843348890542984
  - 14.436891615390778
  - 14.188845813274384
  - 14.404356002807617
  - 14.829180598258972
  - 14.776039510965347
  - 15.070068538188934
  - 15.096196129918098
  - 14.302188247442245
  - 14.611177891492844
  - 14.966350674629211
  - 14.802757665514946
  - 15.047920882701874
  - 14.172253981232643
  - 14.424839317798615
  - 14.320181608200073
  - 14.466304540634155
  - 14.284941762685776
  - 14.452093064785004
  - 14.748987570405006
  - 14.412349313497543
  - 14.430520445108414
  - 14.355927050113678
  - 14.645911425352097
  - 14.338531002402306
  - 14.240408539772034
  - 14.383556365966797
  - 14.245617374777794
  - 14.373883858323097
  - 14.100458890199661
  validation_losses:
  - 0.3782773017883301
  - 0.38933253288269043
  - 0.4040801525115967
  - 0.39979586005210876
  - 0.47352585196495056
  - 0.4556337296962738
  - 0.4446640908718109
  - 0.42662346363067627
  - 0.4484374523162842
  - 0.42258119583129883
  - 0.4527961313724518
  - 0.44218435883522034
  - 0.45618316531181335
  - 0.4070027768611908
  - 0.5178536176681519
  - 0.3973792493343353
  - 0.49104586243629456
  - 0.5008974671363831
  - 0.42370983958244324
  - 0.4004015624523163
  - 0.4719301164150238
  - 0.4654662609100342
  - 0.38076290488243103
  - 0.43803662061691284
  - 0.5013805627822876
  - 0.4702248275279999
  - 0.5467326045036316
  - 0.6460276246070862
  - 0.37564465403556824
  - 0.4185037910938263
  - 0.4043181538581848
  - 0.4122760593891144
  - 0.5716121196746826
  - 0.9094993472099304
  - 0.9786849021911621
  - 0.37119966745376587
  - 0.8573126196861267
  - 0.39325380325317383
  - 0.4128466248512268
  - 0.49198636412620544
  - 0.43166548013687134
  - 1.2908929586410522
  - 1.3795175552368164
  - 1.4237685203552246
  - 0.4517799913883209
  - 0.558814287185669
  - 0.5493632555007935
  - 0.4937613606452942
  - 0.45373010635375977
  - 0.7991122007369995
  - 0.562391459941864
  - 0.5064173936843872
  - 0.6411815881729126
  - 0.3889602720737457
  - 0.5610864758491516
  - 0.6530590057373047
  - 0.39363160729408264
  - 0.4386242926120758
  - 0.713179886341095
  - 0.5301643013954163
  - 0.5059451460838318
  - 0.4159630239009857
  - 0.6354928016662598
  - 0.44129106402397156
  - 0.4126804769039154
  - 0.47755321860313416
  - 0.5102755427360535
  - 0.4908055067062378
  - 0.6385977268218994
  - 0.56276535987854
  - 0.6589888334274292
  - 0.6149513125419617
  - 0.5432765483856201
  - 0.5608204007148743
  - 0.5368661880493164
  - 0.7048530578613281
  - 0.5210845470428467
  - 0.49847790598869324
  - 0.5262148976325989
  - 0.5396573543548584
  - 0.5082284212112427
  - 0.4822387099266052
  - 0.4994255304336548
  - 0.4780471622943878
  - 0.5307767987251282
  - 0.5583978295326233
  - 0.5883890390396118
  - 0.5495442748069763
  - 0.5171042084693909
  - 0.46558982133865356
  - 0.4539085924625397
  - 0.5855444669723511
  - 0.4952571392059326
  - 0.5004797577857971
  - 0.5748168230056763
  - 0.5964094996452332
  - 0.4007348418235779
  - 0.49831703305244446
  - 0.6582546234130859
  - 0.642188310623169
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8593481989708405, 0.855917667238422,
    0.7646048109965635]'
  fold_eval_f1: '[0.0, 0.023255813953488372, 0.06818181818181818, 0.023255813953488372,
    0.21714285714285714]'
  mean_eval_accuracy: 0.8386842555097758
  mean_f1_accuracy: 0.0663672606463304
  total_train_time: '0:08:34.025486'
