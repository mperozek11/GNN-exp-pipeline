config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.177552'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_7fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_7fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_7fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_7fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_7fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_7fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_7fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_7fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_7fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_7fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.1237823247909546
  - 0.8844310522079468
  - 0.8787824332714081
  - 0.8728160440921784
  - 0.8805994510650635
  - 0.819689828157425
  - 0.841176974773407
  - 0.8851828157901764
  - 0.8399880945682526
  - 0.8125215470790863
  - 0.7995414316654206
  - 0.8054206907749176
  - 0.8664380192756653
  - 0.8383143603801728
  - 0.8534363687038422
  - 0.8372734248638154
  - 0.7979668319225312
  - 0.807321584224701
  - 0.819817864894867
  - 0.7970840811729432
  validation_losses:
  - 0.4044560492038727
  - 0.43168318271636963
  - 0.4007323086261749
  - 0.4183039963245392
  - 0.4098968803882599
  - 0.4111495018005371
  - 0.39125970005989075
  - 0.39448922872543335
  - 0.3915826380252838
  - 0.38711127638816833
  - 0.3997569680213928
  - 0.38857901096343994
  - 0.3843172490596771
  - 0.4148705005645752
  - 0.39737141132354736
  - 0.39639776945114136
  - 0.3870987892150879
  - 0.3951379358768463
  - 0.387734055519104
  - 0.3976283371448517
loss_records_fold1:
  train_losses:
  - 0.793209844827652
  - 0.846123868227005
  - 0.8091156482696533
  - 0.806224912405014
  - 0.8837873160839081
  - 0.8217861115932465
  - 0.8158770322799683
  - 0.7774686038494111
  - 0.8215955138206482
  - 0.7972590148448945
  - 0.8105326056480409
  - 0.7900395870208741
  - 0.8130535364151001
  - 0.8110644578933717
  - 0.8117135584354401
  - 0.7982658982276917
  - 0.8130925714969636
  - 0.7771538138389588
  - 0.7796282887458802
  - 0.8224531531333924
  - 0.8295992851257324
  - 0.7812513291835785
  - 0.8308172762393952
  - 0.8439447641372682
  - 0.8325588345527649
  validation_losses:
  - 0.3890027701854706
  - 0.39952296018600464
  - 0.3945491313934326
  - 0.3941907584667206
  - 0.39701345562934875
  - 0.4009203612804413
  - 0.3983204662799835
  - 0.38540875911712646
  - 0.39860963821411133
  - 0.38756775856018066
  - 0.3850426971912384
  - 0.3953123390674591
  - 0.39616888761520386
  - 0.42499595880508423
  - 0.40244582295417786
  - 0.3883340656757355
  - 0.3915484845638275
  - 0.3863196074962616
  - 0.4044322967529297
  - 0.38466888666152954
  - 0.3928372263908386
  - 0.3849702477455139
  - 0.39108148217201233
  - 0.3974190652370453
  - 0.3916430175304413
loss_records_fold2:
  train_losses:
  - 0.788452309370041
  - 0.7951529800891877
  - 0.7981761932373047
  - 0.7700825393199922
  - 0.8594349324703217
  - 0.8278958797454834
  - 0.8231320679187775
  - 0.8080774784088135
  - 0.7687027394771576
  - 0.8350350201129914
  - 0.8080347299575806
  - 0.7656745791435242
  - 0.7945474684238434
  - 0.822239089012146
  - 0.8075199782848359
  - 0.8188187062740326
  - 0.9009383738040925
  - 0.8185044467449188
  - 0.7831429183483124
  - 0.7879660904407502
  - 0.776163187623024
  - 0.8013285338878632
  - 0.8050393760204315
  - 0.7746699512004853
  - 0.7791096806526184
  - 0.8566665172576905
  - 0.8231142103672028
  - 0.7949463367462158
  - 0.8519386529922486
  - 0.7992343842983246
  - 0.7864320456981659
  - 0.8200913310050965
  - 0.7976586341857911
  - 0.7748713672161103
  validation_losses:
  - 0.39563682675361633
  - 0.3943444788455963
  - 0.4520529806613922
  - 0.573615312576294
  - 0.4759535789489746
  - 0.39608949422836304
  - 0.3993808627128601
  - 0.44358551502227783
  - 0.47959595918655396
  - 0.4445194602012634
  - 0.47520574927330017
  - 0.4474037289619446
  - 0.3864818513393402
  - 0.38196343183517456
  - 0.4342760741710663
  - 0.42085081338882446
  - 0.45645672082901
  - 0.46197709441185
  - 0.5122568607330322
  - 0.40181964635849
  - 0.41215813159942627
  - 0.3866233825683594
  - 0.40470144152641296
  - 0.3853316009044647
  - 0.4013376235961914
  - 0.38384780287742615
  - 0.3889005780220032
  - 0.39971020817756653
  - 0.40176618099212646
  - 0.39738398790359497
  - 0.3872717618942261
  - 0.39283305406570435
  - 0.3879435956478119
  - 0.39440813660621643
loss_records_fold3:
  train_losses:
  - 0.8006817042827606
  - 0.8112508296966553
  - 0.8008020341396332
  - 0.7662771463394166
  - 0.79630885720253
  - 0.8207887768745423
  - 0.8084613263607026
  - 0.8869585633277893
  - 0.8329033732414246
  - 0.8401412010192871
  - 0.8906843781471253
  - 0.8704648017883301
  - 0.8267583847045898
  - 0.810022395849228
  - 0.8238944351673126
  - 0.8403637170791627
  - 0.828119307756424
  - 0.8446241319179535
  - 0.8135354161262512
  - 0.8869252264499665
  - 0.8718214213848114
  - 0.8375176906585694
  - 0.8240959644317627
  - 0.8525988042354584
  - 0.8657993614673615
  - 0.7970817506313325
  - 0.8379793405532837
  - 0.88314288854599
  - 0.8581755697727204
  - 0.8290087759494782
  - 0.8321535885334015
  - 0.7987430632114411
  - 0.791469818353653
  validation_losses:
  - 0.3908107578754425
  - 0.3808451294898987
  - 0.37954607605934143
  - 0.37671592831611633
  - 0.3804693818092346
  - 0.3843604028224945
  - 0.37617889046669006
  - 0.3843748867511749
  - 0.37528952956199646
  - 0.3795502781867981
  - 0.39059868454933167
  - 0.41983360052108765
  - 0.38636866211891174
  - 0.3920656740665436
  - 0.3905573785305023
  - 0.3768611550331116
  - 0.3818407356739044
  - 0.3922608494758606
  - 0.3989318013191223
  - 0.3697799742221832
  - 0.36966708302497864
  - 0.3839466869831085
  - 0.382941871881485
  - 0.3883175849914551
  - 0.3785456120967865
  - 0.375692754983902
  - 0.3934864401817322
  - 0.3933302164077759
  - 0.3736872375011444
  - 0.3789992034435272
  - 0.3661119043827057
  - 0.37334463000297546
  - 0.37003418803215027
loss_records_fold4:
  train_losses:
  - 0.8295443117618562
  - 0.7897496283054353
  - 0.8323527336120606
  - 0.809996646642685
  - 0.8110638439655304
  - 0.8131875753402711
  - 0.8135275602340699
  - 0.8592516541481019
  - 0.8000464797019959
  - 0.8254813313484193
  - 0.8397094786167145
  - 0.8187091231346131
  - 0.7973322749137879
  - 0.7966874837875366
  - 0.7952941656112671
  - 0.7843369245529175
  - 0.8332502603530885
  - 0.8164104640483857
  - 0.802237367630005
  - 0.8241395652294159
  - 0.8721282899379731
  - 0.8207413315773011
  - 0.8005929827690125
  - 0.7778390109539033
  - 0.8170332729816437
  - 0.7940361201763153
  - 0.8158311188220978
  - 0.7890793859958649
  - 0.8142832100391388
  - 0.838324671983719
  - 0.8322711944580079
  - 0.8070229530334473
  - 0.8165616214275361
  - 0.7919449150562287
  - 0.7948847711086273
  - 0.8145659506320954
  - 0.8404389679431916
  - 0.7944576501846314
  - 0.8405445456504822
  - 0.8157306790351868
  - 0.8263684630393983
  - 0.8138123452663422
  validation_losses:
  - 0.37820297479629517
  - 0.38239148259162903
  - 0.38836294412612915
  - 0.4009336233139038
  - 0.3738549053668976
  - 0.38012754917144775
  - 0.38729381561279297
  - 0.3808651864528656
  - 0.38581421971321106
  - 0.38972142338752747
  - 0.40043604373931885
  - 0.3760451078414917
  - 0.3803921639919281
  - 0.38112592697143555
  - 0.37625259160995483
  - 0.42321905493736267
  - 0.3807714581489563
  - 0.3851583003997803
  - 0.3765633702278137
  - 0.3847057521343231
  - 0.39663493633270264
  - 0.44878578186035156
  - 0.4115173816680908
  - 0.389159619808197
  - 0.38695797324180603
  - 0.4196753203868866
  - 0.41771721839904785
  - 0.3864215016365051
  - 0.5080928802490234
  - 0.3744968771934509
  - 0.38631418347358704
  - 0.39485251903533936
  - 0.38442090153694153
  - 0.38781142234802246
  - 0.37618324160575867
  - 0.4519350230693817
  - 0.39804327487945557
  - 0.3717467784881592
  - 0.3738285005092621
  - 0.3812105059623718
  - 0.3772979974746704
  - 0.3718327581882477
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 33 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 42 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:12:17.166488'
