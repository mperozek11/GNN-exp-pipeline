config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.223650'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_3fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_3fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_3fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_3fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_3fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_3fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_3fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_3fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_3fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_3fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.0317793607711792
  - 0.8139366745948792
  - 0.8181508958339692
  - 0.8018090903759003
  - 0.8060559570789337
  - 0.7678307890892029
  - 0.7759121775627137
  - 0.8108601689338685
  - 0.7590428054332734
  - 0.7486196875572205
  - 0.7520217597484589
  - 0.7658911526203156
  - 0.8182569146156311
  - 0.7801546514034272
  - 0.8234519064426422
  - 0.7818526029586792
  - 0.7701704025268555
  - 0.7540493309497833
  validation_losses:
  - 0.41074204444885254
  - 0.41616424918174744
  - 0.4181835949420929
  - 0.39464473724365234
  - 0.39045628905296326
  - 0.3923527002334595
  - 0.39745137095451355
  - 0.4097239673137665
  - 0.3843904137611389
  - 0.3845152258872986
  - 0.38299331068992615
  - 0.4037763476371765
  - 0.3847467005252838
  - 0.39106428623199463
  - 0.3891151547431946
  - 0.39152321219444275
  - 0.39389654994010925
  - 0.37967953085899353
loss_records_fold1:
  train_losses:
  - 0.7441011488437653
  - 0.7273865550756455
  - 0.7294726520776749
  - 0.7950973510742188
  - 0.7303051054477692
  - 0.7460850358009339
  - 0.8373925805091859
  - 0.7523037910461426
  - 0.7521880507469177
  - 0.7310063481330872
  - 0.7567294239997864
  - 0.7488944411277771
  - 0.7500087499618531
  - 0.7484429895877839
  - 0.7574397444725037
  - 0.7497173964977265
  - 0.7313294887542725
  - 0.7363024473190308
  - 0.7451580762863159
  - 0.7325844407081604
  - 0.722465804219246
  - 0.7371371924877167
  - 0.74739248752594
  - 0.7110785990953445
  - 0.7676259994506837
  - 0.7788827359676361
  - 0.7387417972087861
  - 0.7227341920137406
  - 0.7669542372226715
  - 0.7301570415496826
  - 0.7136241018772126
  - 0.7843766391277314
  - 0.7472086906433106
  - 0.7416505217552185
  - 0.7101766586303712
  - 0.7784927964210511
  - 0.7628474473953247
  - 0.7417099356651307
  - 0.7234221339225769
  - 0.7215113312005997
  - 0.704865112900734
  - 0.7421439111232758
  - 0.7606871187686921
  - 0.7906667292118073
  - 0.7135681986808777
  - 0.7338818669319154
  - 0.7399259746074677
  - 0.7213266968727112
  - 0.712211298942566
  - 0.7803290665149689
  - 0.7395567774772644
  - 0.7689719557762147
  - 0.7408155143260956
  - 0.7242701053619385
  - 0.7365408420562745
  - 0.7509939908981323
  - 0.7477244913578034
  - 0.7455902099609375
  - 0.7258209228515625
  - 0.7828673779964448
  - 0.7439689993858338
  validation_losses:
  - 0.3943396210670471
  - 0.39079976081848145
  - 0.3931165337562561
  - 0.4321267008781433
  - 0.39091506600379944
  - 0.39106103777885437
  - 0.4954546093940735
  - 0.42943012714385986
  - 0.38581353425979614
  - 0.38664567470550537
  - 0.38636326789855957
  - 0.390047550201416
  - 0.42678403854370117
  - 0.40474647283554077
  - 0.3950654864311218
  - 0.3890566825866699
  - 0.38734135031700134
  - 0.40178534388542175
  - 0.3899722099304199
  - 0.3876907527446747
  - 0.3877592980861664
  - 0.4476657807826996
  - 0.4026316702365875
  - 0.4495643377304077
  - 0.4240453839302063
  - 0.45541974902153015
  - 0.5317754745483398
  - 0.39220359921455383
  - 0.3884226381778717
  - 0.39046651124954224
  - 0.40708789229393005
  - 0.5016390681266785
  - 0.4555356502532959
  - 0.5271682143211365
  - 0.5465311408042908
  - 0.3929915726184845
  - 0.39003658294677734
  - 0.6029680967330933
  - 0.5090132355690002
  - 0.44127848744392395
  - 0.39369162917137146
  - 0.5444918870925903
  - 0.42162612080574036
  - 0.6914514899253845
  - 0.47263282537460327
  - 0.3921290934085846
  - 0.4398922622203827
  - 0.432538777589798
  - 0.4844464063644409
  - 0.4298100173473358
  - 0.43890467286109924
  - 0.47365638613700867
  - 0.548154354095459
  - 0.7032443881034851
  - 0.7475920915603638
  - 0.5344452857971191
  - 0.39588937163352966
  - 0.40026286244392395
  - 0.3886145353317261
  - 0.3896563649177551
  - 0.3861103951931
loss_records_fold2:
  train_losses:
  - 0.8065916895866394
  - 0.7150834202766418
  - 0.7612195611000061
  - 0.7517590761184693
  - 0.7344893336296082
  - 0.7381668627262116
  - 0.7061773926019669
  - 0.7763631939888
  - 0.7933638393878937
  - 0.7576874613761903
  - 0.7688739001750946
  - 0.735266137123108
  - 0.7103810966014863
  - 0.723889571428299
  - 0.7350859761238099
  - 0.7423809111118317
  - 0.7254897415637971
  - 0.7329522192478181
  - 0.7166244626045227
  - 0.7186052978038788
  - 0.7286685287952424
  - 0.7669832706451416
  - 0.737423461675644
  - 0.7401832818984986
  - 0.7175726294517517
  - 0.7281199753284455
  - 0.8043272256851197
  - 0.7330038785934448
  - 0.7680253088474274
  - 0.7279117524623872
  - 0.7358271658420563
  - 0.7432334959506989
  - 0.7782083630561829
  - 0.7247378051280976
  - 0.7175070822238923
  - 0.7243480265140534
  - 0.6905999928712845
  - 0.7428727805614472
  - 0.7590514183044434
  - 0.7523378551006318
  - 0.7287413895130158
  - 0.7066571593284607
  - 0.747130024433136
  - 0.7189013659954071
  - 0.7198579788208008
  - 0.7551388680934906
  - 0.7427974283695221
  - 0.6957280516624451
  - 0.7612503170967102
  - 0.7464746475219727
  - 0.7215534031391144
  - 0.7146349132061005
  validation_losses:
  - 0.38436228036880493
  - 0.3859653174877167
  - 0.38366150856018066
  - 0.37740692496299744
  - 0.38092318177223206
  - 0.3896785080432892
  - 0.3897482752799988
  - 0.3944869041442871
  - 0.42770183086395264
  - 0.4343738257884979
  - 0.4630371630191803
  - 0.4494641125202179
  - 0.5535139441490173
  - 0.4606100916862488
  - 0.38685062527656555
  - 0.3747915029525757
  - 0.44727280735969543
  - 0.4223426580429077
  - 0.4645186960697174
  - 0.4207007586956024
  - 0.48846253752708435
  - 0.4048468768596649
  - 0.3814939856529236
  - 0.3894640803337097
  - 0.389676034450531
  - 0.4944410026073456
  - 0.571440577507019
  - 0.455709844827652
  - 0.428337037563324
  - 0.4024806022644043
  - 0.3833269774913788
  - 0.4055824875831604
  - 0.4241716265678406
  - 0.43761491775512695
  - 0.42787304520606995
  - 0.3850988745689392
  - 0.3948069214820862
  - 0.39937689900398254
  - 0.46608859300613403
  - 0.44349852204322815
  - 0.4144136309623718
  - 0.39658400416374207
  - 0.39933329820632935
  - 0.38490766286849976
  - 0.39707523584365845
  - 0.427001029253006
  - 0.4178752899169922
  - 0.4035801291465759
  - 0.394420862197876
  - 0.39341360330581665
  - 0.38153156638145447
  - 0.37659692764282227
loss_records_fold3:
  train_losses:
  - 0.8520139575004578
  - 0.7660325109958649
  - 0.7486383676528932
  - 0.7242913246154785
  - 0.7384999811649323
  - 0.7498904347419739
  - 0.7440064787864685
  - 0.7339799225330353
  - 0.7523451983928681
  - 0.7496138334274293
  - 0.707867991924286
  - 0.7159891486167909
  - 0.7320793449878693
  - 0.7591890692710876
  - 0.7636873066425324
  - 0.7370466649532319
  - 0.707638841867447
  - 0.7312681734561921
  - 0.7852437138557434
  - 0.7626797139644623
  - 0.7302430987358094
  - 0.7314513206481934
  - 0.7572382628917694
  - 0.7199014365673065
  - 0.7513430655002594
  - 0.739430958032608
  - 0.7164476990699769
  - 0.7482662200927734
  - 0.7168690443038941
  - 0.726274162530899
  - 0.7358529388904572
  - 0.7495472431182861
  - 0.7557374000549317
  - 0.7333289206027985
  - 0.7837902247905731
  - 0.738299086689949
  - 0.7554723381996156
  - 0.7570932626724244
  - 0.8024462819099427
  - 0.7976520121097566
  - 0.7255642950534821
  - 0.7183222323656082
  - 0.7679160356521607
  validation_losses:
  - 0.41248688101768494
  - 0.43955546617507935
  - 0.3838028013706207
  - 0.3756342828273773
  - 0.4350878894329071
  - 0.3882234990596771
  - 0.3606862723827362
  - 0.3693252205848694
  - 0.3743305206298828
  - 0.4100460708141327
  - 0.3846363127231598
  - 0.49152040481567383
  - 0.6992478966712952
  - 0.6598368287086487
  - 0.5966598391532898
  - 0.4452667236328125
  - 0.40829309821128845
  - 0.3719729483127594
  - 0.38327744603157043
  - 0.4480416178703308
  - 0.4432995021343231
  - 0.3836574852466583
  - 0.4124843180179596
  - 0.4558872580528259
  - 0.6422103047370911
  - 0.36488959193229675
  - 0.433301717042923
  - 0.46709275245666504
  - 0.4452670216560364
  - 0.45160290598869324
  - 0.3992539942264557
  - 0.48337212204933167
  - 0.6779085397720337
  - 0.38956403732299805
  - 0.4183250069618225
  - 0.37379175424575806
  - 0.3898627460002899
  - 0.3792306184768677
  - 0.37149128317832947
  - 0.3647020161151886
  - 0.3664968013763428
  - 0.37154850363731384
  - 0.374788373708725
loss_records_fold4:
  train_losses:
  - 0.7265517950057984
  - 0.721787041425705
  - 0.7498194038867951
  - 0.756464558839798
  - 0.7548674106597901
  - 0.7358345091342926
  - 0.7267706036567688
  - 0.7308101534843445
  - 0.7194274544715882
  - 0.7380836665630341
  - 0.7385813891887665
  - 0.7378949820995331
  - 0.8008499622344971
  - 0.7438204824924469
  - 0.7373222827911378
  - 0.727236306667328
  - 0.7123568296432495
  - 0.7313993096351624
  - 0.7388220250606538
  - 0.7636856436729431
  - 0.7457574725151063
  - 0.7653703451156617
  - 0.7601953506469727
  - 0.7272226452827454
  - 0.7254326701164246
  - 0.7322084546089173
  - 0.7518812000751496
  - 0.7320918917655945
  - 0.7156022429466248
  - 0.7715915083885193
  - 0.7512716293334961
  - 0.7068961024284364
  - 0.7494459033012391
  - 0.7309802114963532
  - 0.7703789055347443
  - 0.7528746485710145
  - 0.7343840003013611
  - 0.7167411267757416
  - 0.7260452151298523
  - 0.7180701911449433
  - 0.7232471764087678
  - 0.8331590056419373
  - 0.7477116107940674
  - 0.7483872890472413
  - 0.8826302587985992
  - 0.7975718915462494
  - 0.7379765927791596
  - 0.7263361513614655
  - 0.7676997840404511
  - 0.7635124206542969
  - 0.7400329411029816
  validation_losses:
  - 0.35836589336395264
  - 0.366531103849411
  - 0.3699130713939667
  - 0.3708864748477936
  - 0.3677738606929779
  - 0.3872811794281006
  - 0.370735764503479
  - 0.3762827515602112
  - 0.373576283454895
  - 0.3921150267124176
  - 0.39255863428115845
  - 0.3621438145637512
  - 0.3621150255203247
  - 0.3724287748336792
  - 0.36340221762657166
  - 0.3626093864440918
  - 0.3586251139640808
  - 0.3865770101547241
  - 0.40544581413269043
  - 0.37076735496520996
  - 0.3666316270828247
  - 0.45734816789627075
  - 0.38714003562927246
  - 0.3672172427177429
  - 0.37154191732406616
  - 0.3648780584335327
  - 0.42086294293403625
  - 0.47695809602737427
  - 0.43526196479797363
  - 0.4093278646469116
  - 0.35434913635253906
  - 0.42885324358940125
  - 0.44536662101745605
  - 0.3622284531593323
  - 0.4702557623386383
  - 0.39593973755836487
  - 0.394941121339798
  - 0.3561397194862366
  - 0.3842897117137909
  - 0.3726915121078491
  - 0.3762172758579254
  - 0.3755296468734741
  - 0.37284648418426514
  - 0.37898096442222595
  - 0.39236095547676086
  - 0.3683529794216156
  - 0.3703259229660034
  - 0.36644265055656433
  - 0.3748447299003601
  - 0.3691067397594452
  - 0.36865195631980896
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 61 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 52 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 43 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8542024013722127, 0.8507718696397941, 0.8524871355060034,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.0, 0.06451612903225806, 0.022727272727272724, 0.023529411764705882]'
  mean_eval_accuracy: 0.8544965311547689
  mean_f1_accuracy: 0.022154562704847332
  total_train_time: '0:17:53.710597'
