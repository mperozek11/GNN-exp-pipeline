config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:49:51.711068'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_71fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_71fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_71fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_71fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_71fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_71fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_71fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_71fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_71fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_71fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.0088808596134187
  - 0.9290337502956391
  - 0.831470149755478
  - 0.8376262784004211
  - 0.8768232822418214
  - 0.84836984872818
  - 0.8471565663814545
  - 0.7966262370347977
  - 0.8525599241256714
  - 0.8199600756168366
  - 0.8943922996520997
  validation_losses:
  - 0.4019868075847626
  - 0.4454960227012634
  - 0.42709586024284363
  - 0.40201738476753235
  - 0.3953636586666107
  - 0.39755719900131226
  - 0.39763617515563965
  - 0.39691439270973206
  - 0.39960795640945435
  - 0.40342700481414795
  - 0.39515992999076843
loss_records_fold1:
  train_losses:
  - 0.8185599088668823
  - 0.839071160554886
  - 0.8452441573143006
  - 0.8604169487953186
  - 0.8398507714271546
  - 0.8268710911273957
  - 0.8320771992206574
  - 0.8167093098163605
  - 0.8851164519786835
  - 0.819931709766388
  - 0.833726865053177
  - 0.7998574197292329
  - 0.7839987635612489
  - 0.8208286345005036
  - 0.783871203660965
  - 0.8136703968048096
  - 0.8716058373451233
  - 0.8089540183544159
  - 0.8308611750602722
  - 0.8021563708782197
  - 0.7779361248016358
  - 0.8213172197341919
  - 0.8505794703960419
  validation_losses:
  - 0.39560890197753906
  - 0.3975485861301422
  - 0.4022039473056793
  - 0.40372416377067566
  - 0.3953266143798828
  - 0.3929409980773926
  - 0.3978680968284607
  - 0.3924625515937805
  - 0.4133046567440033
  - 0.4300285577774048
  - 0.40597182512283325
  - 0.39194256067276
  - 0.4030381739139557
  - 0.3963526785373688
  - 0.39983251690864563
  - 0.39608854055404663
  - 0.41481664776802063
  - 0.398245245218277
  - 0.40178486704826355
  - 0.3989512622356415
  - 0.40732672810554504
  - 0.400610089302063
  - 0.39314913749694824
loss_records_fold2:
  train_losses:
  - 0.8259237408638
  - 1.0485463678836824
  - 0.8637330234050751
  - 0.8716194689273835
  - 0.8975289642810822
  - 0.8338486731052399
  - 0.8299532532691956
  - 0.8467130899429322
  - 0.803330147266388
  - 0.8035569429397583
  - 0.7862182170152665
  - 0.825922179222107
  - 0.8212977766990662
  - 0.8951761841773987
  - 0.8158632099628449
  - 0.8228293716907502
  - 0.8132958352565766
  - 1.0180937349796295
  - 1.0036609649658204
  - 0.8545179903507233
  - 0.944368052482605
  - 0.8884919285774231
  - 0.868173110485077
  - 0.8608247637748718
  validation_losses:
  - 0.38648897409439087
  - 0.4141308069229126
  - 0.42410698533058167
  - 0.3956427574157715
  - 0.41694125533103943
  - 0.39177262783050537
  - 0.4003813564777374
  - 0.40576037764549255
  - 0.4292953610420227
  - 0.41882309317588806
  - 0.4018040895462036
  - 0.7274107933044434
  - 0.8779205679893494
  - 0.8198755979537964
  - 0.42762377858161926
  - 0.39087265729904175
  - 0.40189328789711
  - 0.4472220838069916
  - 0.43324169516563416
  - 0.40896645188331604
  - 0.40096041560173035
  - 0.39578568935394287
  - 0.3982570171356201
  - 0.39816704392433167
loss_records_fold3:
  train_losses:
  - 0.8595082700252533
  - 0.8161940574645996
  - 0.8155876398086548
  - 0.8058460116386414
  - 0.8132914543151856
  - 0.8028182625770569
  - 0.8283501029014588
  - 0.7997909009456635
  - 0.8209994018077851
  - 0.9139541268348694
  - 0.8010481297969818
  - 0.8507590532302857
  - 0.8353404164314271
  - 0.8406068205833436
  - 0.8685347080230713
  - 0.7849851667881013
  - 0.8713128447532654
  - 0.8108620405197144
  - 0.8232965588569642
  - 0.8472499608993531
  - 0.856603753566742
  - 0.8253117501735687
  - 0.8607174098491669
  - 0.8025180101394653
  - 0.86669921875
  - 0.8125909864902496
  - 0.9094984948635102
  - 0.8585189700126649
  - 1.0181806206703186
  - 0.9102118253707886
  - 0.8189190685749055
  - 0.7694903671741486
  - 0.8456851422786713
  - 0.8158181369304658
  - 0.7902161568403244
  - 0.7961257010698319
  - 0.7915566205978394
  - 0.8336100876331329
  - 1.1696607768535614
  - 0.8370874166488648
  - 0.8338537931442261
  - 0.7808485954999924
  - 0.7997555375099182
  - 0.8511232018470765
  - 0.8885505080223084
  - 0.7788180977106095
  - 0.8247284412384034
  - 0.7963070631027223
  - 0.8042113900184632
  - 0.799735027551651
  - 0.7754229009151459
  - 0.8381985783576966
  - 0.9272606909275055
  - 0.8598547637462617
  - 0.8201541483402253
  - 0.7859199166297913
  - 0.8079422473907472
  - 0.7988503932952882
  - 0.8379753112792969
  - 0.7965546607971192
  - 0.8046106338500977
  - 0.7932305634021759
  - 0.8034552216529847
  - 0.7967431068420411
  - 0.804721873998642
  - 0.7879287600517273
  - 0.8128467559814454
  - 0.7929748833179474
  - 0.7983723104000092
  - 0.8172620654106141
  - 0.7682527989149094
  - 0.7734888017177582
  - 0.8325978696346283
  - 0.7809243381023407
  - 0.7968376517295838
  - 0.766611248254776
  - 0.8138782560825348
  - 0.7637488961219788
  - 0.8130236089229584
  - 0.7888375639915467
  - 0.7781948745250702
  - 0.841982650756836
  - 0.7740484565496445
  - 0.7832188487052918
  - 0.771264123916626
  - 0.8065318763256073
  - 0.8273993074893952
  - 0.8321946799755097
  - 0.8116541504859924
  - 0.7987436294555664
  - 0.8422027111053467
  - 0.798487001657486
  - 0.7832539081573486
  - 0.8047734916210175
  - 0.7949281871318817
  - 0.9180319964885713
  - 0.8224465787410736
  - 0.8111211836338044
  - 0.8132875978946686
  - 0.7877680838108063
  validation_losses:
  - 0.3915553092956543
  - 0.3956606388092041
  - 0.3987404406070709
  - 0.3979206383228302
  - 0.3865705728530884
  - 0.37742799520492554
  - 0.38566824793815613
  - 0.3814980983734131
  - 0.38571980595588684
  - 0.42360666394233704
  - 0.432699590921402
  - 0.39339807629585266
  - 0.40514200925827026
  - 0.4037235379219055
  - 0.40021517872810364
  - 0.38179856538772583
  - 0.3933325409889221
  - 0.39941126108169556
  - 0.43240582942962646
  - 0.4727080762386322
  - 0.49258869886398315
  - 0.3937235474586487
  - 0.42390871047973633
  - 0.44896823167800903
  - 0.7210893034934998
  - 0.4643443822860718
  - 0.45073941349983215
  - 0.4315071403980255
  - 0.4761776030063629
  - 0.4145432710647583
  - 0.40853944420814514
  - 0.3723517954349518
  - 0.372372567653656
  - 0.3836491107940674
  - 0.4427548348903656
  - 0.4866327941417694
  - 0.5862805843353271
  - 0.5625067949295044
  - 0.8374376893043518
  - 0.4417359232902527
  - 0.3862842619419098
  - 0.3896558880805969
  - 0.5897232890129089
  - 0.4537152647972107
  - 0.5035383701324463
  - 0.38889142870903015
  - 0.5459421873092651
  - 0.49341249465942383
  - 0.745526134967804
  - 0.4400120973587036
  - 1.272430658340454
  - 0.5667669177055359
  - 0.4499339163303375
  - 0.3749011158943176
  - 0.39755240082740784
  - 0.4269595146179199
  - 0.4648188352584839
  - 0.5459485054016113
  - 0.5818566679954529
  - 0.4562892019748688
  - 0.5282641053199768
  - 0.5235797762870789
  - 0.5997970104217529
  - 0.6763767600059509
  - 0.9030424952507019
  - 0.8081458806991577
  - 0.7849173545837402
  - 0.9165354371070862
  - 0.5300180315971375
  - 0.6335374712944031
  - 0.39738088846206665
  - 0.546841561794281
  - 1.0640780925750732
  - 1.1423598527908325
  - 0.508922815322876
  - 0.5892122387886047
  - 0.9062307476997375
  - 0.6375012397766113
  - 0.5980058312416077
  - 0.39561551809310913
  - 0.38016781210899353
  - 0.5194175839424133
  - 0.775040864944458
  - 0.8432961702346802
  - 1.199291467666626
  - 0.9247636795043945
  - 0.39548033475875854
  - 0.38600435853004456
  - 0.3900046944618225
  - 0.37920498847961426
  - 0.3961229920387268
  - 0.37367406487464905
  - 0.3822193145751953
  - 0.40495026111602783
  - 0.7420781850814819
  - 0.6086405515670776
  - 0.3896258771419525
  - 0.37871748208999634
  - 0.3825214207172394
  - 0.374538779258728
loss_records_fold4:
  train_losses:
  - 0.8344034969806672
  - 0.7777668714523316
  - 0.8233126401901245
  - 0.7867157816886903
  - 0.8028520882129669
  - 0.8073826730251312
  - 0.7887937307357789
  - 0.7747015953063965
  - 0.7932009100914001
  - 0.8111849248409272
  - 0.8124118566513062
  - 0.7971138656139374
  - 0.8454375505447388
  - 0.7842583656311035
  - 0.8186625421047211
  - 0.8278216660022736
  - 0.7893965244293213
  - 0.8340263664722443
  - 0.7973743379116058
  - 0.7861997902393342
  - 0.7982863008975983
  - 0.8162548363208771
  - 0.8457513928413392
  - 0.8679970502853394
  - 0.8357815802097321
  - 0.794704669713974
  - 0.7776149481534959
  - 0.7998304665088654
  validation_losses:
  - 0.3893338739871979
  - 0.39835861325263977
  - 0.37564948201179504
  - 0.40812572836875916
  - 0.3926929235458374
  - 0.40872716903686523
  - 0.3723825216293335
  - 0.38203486800193787
  - 0.36949822306632996
  - 0.4056655168533325
  - 0.3793903887271881
  - 0.37497270107269287
  - 0.40245136618614197
  - 0.38527682423591614
  - 0.405407190322876
  - 0.4122215807437897
  - 0.40916678309440613
  - 0.40105539560317993
  - 0.38753852248191833
  - 0.3735712468624115
  - 0.3871443271636963
  - 0.40391361713409424
  - 0.4136599898338318
  - 0.40384384989738464
  - 0.3842558264732361
  - 0.383770614862442
  - 0.3761225640773773
  - 0.3783578872680664
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:15:29.886578'
