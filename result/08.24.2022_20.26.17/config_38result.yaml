config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:15:05.021931'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_38fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_38fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_38fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_38fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_38fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_38fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_38fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_38fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_38fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_38fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.9529337823390962
  - 1.5905963450670244
  - 1.7088850378990175
  - 1.6583355009555818
  - 1.6427903890609743
  - 1.601455491781235
  - 1.6236929953098298
  - 1.584492874145508
  - 1.52012454867363
  - 1.6051519095897675
  - 1.587580806016922
  - 1.624835342168808
  - 1.538161724805832
  - 1.5661470353603364
  - 1.5254926621913911
  - 1.538206785917282
  - 1.6262433111667634
  - 1.559623193740845
  - 1.542953896522522
  - 1.5641911745071413
  - 1.593415153026581
  - 1.5663028717041017
  - 1.564740937948227
  - 1.6500336825847626
  - 1.5877132415771484
  - 1.6145398288965227
  - 1.5858951449394227
  - 1.5742398381233216
  - 1.5657295346260072
  - 1.515626633167267
  - 1.5068768203258516
  - 1.5875558197498323
  - 1.5386248290538789
  - 1.5945171356201173
  - 1.5082875967025757
  - 1.5380070924758913
  - 1.5336895108222963
  validation_losses:
  - 0.42121827602386475
  - 0.4070928394794464
  - 0.40975308418273926
  - 0.4519004225730896
  - 0.39842453598976135
  - 0.3891131579875946
  - 0.3949446380138397
  - 0.39209112524986267
  - 0.3965751826763153
  - 0.43152299523353577
  - 0.40570729970932007
  - 0.39062002301216125
  - 0.4046342372894287
  - 0.39327773451805115
  - 0.3926731050014496
  - 0.4117177128791809
  - 0.398897260427475
  - 0.39317867159843445
  - 0.3867374062538147
  - 0.39959269762039185
  - 0.39342036843299866
  - 0.39006417989730835
  - 0.39926692843437195
  - 0.3976283073425293
  - 0.38547560572624207
  - 0.39955639839172363
  - 0.4006320834159851
  - 0.4034515619277954
  - 0.3905301094055176
  - 0.3923858404159546
  - 0.4107244610786438
  - 0.39451172947883606
  - 0.3998914659023285
  - 0.39363446831703186
  - 0.4012974798679352
  - 0.3911489248275757
  - 0.39812609553337097
loss_records_fold1:
  train_losses:
  - 1.5000448465347291
  - 1.5566556274890901
  - 1.538439530134201
  - 1.4989517331123352
  - 1.5514653325080872
  - 1.6017379105091096
  - 1.5636574923992157
  - 1.6672378182411194
  - 1.5551588416099549
  - 1.5355532765388489
  - 1.5465386331081392
  - 1.504347735643387
  validation_losses:
  - 0.3885289430618286
  - 0.40385618805885315
  - 0.3963880240917206
  - 0.3882260024547577
  - 0.39361676573753357
  - 0.4142197072505951
  - 0.391876220703125
  - 0.3877919614315033
  - 0.3930022418498993
  - 0.3857477903366089
  - 0.3932173252105713
  - 0.4008190631866455
loss_records_fold2:
  train_losses:
  - 1.5396092474460603
  - 1.5387028515338899
  - 1.5151047229766847
  - 1.5632468283176424
  - 1.5412747502326967
  - 1.507073587179184
  - 1.5796804249286653
  - 1.5328715562820436
  - 1.5351100862026215
  - 1.4872793197631837
  - 1.491005325317383
  - 1.5376801133155824
  - 1.532098400592804
  - 1.5430370271205902
  - 1.5483867585659028
  - 1.5087762653827668
  - 1.5379475057125092
  - 1.523523145914078
  - 1.574711334705353
  - 1.5199356973171234
  validation_losses:
  - 0.41068658232688904
  - 0.3947191536426544
  - 0.3970479965209961
  - 0.39030367136001587
  - 0.38725268840789795
  - 0.38988909125328064
  - 0.4141193628311157
  - 0.3862645626068115
  - 0.40117666125297546
  - 0.3911723792552948
  - 0.3991273045539856
  - 0.3970317244529724
  - 0.3895392417907715
  - 0.40168431401252747
  - 0.38979005813598633
  - 0.38986897468566895
  - 0.3885534405708313
  - 0.39628663659095764
  - 0.39254656434059143
  - 0.3950152099132538
loss_records_fold3:
  train_losses:
  - 1.5788081705570223
  - 1.548356753587723
  - 1.5428480565547944
  - 1.544997662305832
  - 1.5776677906513215
  - 1.515384101867676
  - 1.6152996242046358
  - 1.5584024131298067
  - 1.590001803636551
  - 1.549262058734894
  - 1.5591928362846375
  - 1.5647782802581789
  validation_losses:
  - 0.3765941560268402
  - 0.378853440284729
  - 0.3850950300693512
  - 0.3737375736236572
  - 0.3800881803035736
  - 0.396892786026001
  - 0.3956184387207031
  - 0.3806942105293274
  - 0.3827948570251465
  - 0.37941214442253113
  - 0.37914252281188965
  - 0.3788279891014099
loss_records_fold4:
  train_losses:
  - 1.5209541976451875
  - 1.525409746170044
  - 1.541104006767273
  - 1.554004502296448
  - 1.473338395357132
  - 1.4980391800403596
  - 1.5570043623447418
  - 1.5181433856487274
  - 1.5357904195785523
  - 1.506836885213852
  - 1.508717340230942
  - 1.5422754228115083
  - 1.5645438432693481
  - 1.4946060717105867
  - 1.5050638139247896
  - 1.5264792740345001
  - 1.5277854204177856
  - 1.5181263983249664
  - 1.5377686202526093
  - 1.5137014448642732
  - 1.6087359309196474
  - 1.5212635815143587
  - 1.528149712085724
  - 1.5353769659996033
  - 1.5426939010620118
  - 1.5210764944553377
  - 1.5431676268577577
  - 1.536158937215805
  - 1.5452367484569551
  - 1.5508550107479095
  - 1.542718756198883
  - 1.520377153158188
  - 1.4564353913068773
  - 1.5806818783283234
  - 1.528304272890091
  validation_losses:
  - 0.3861556053161621
  - 0.38383081555366516
  - 0.3842373788356781
  - 0.38381582498550415
  - 0.37623536586761475
  - 0.39145904779434204
  - 0.381425678730011
  - 0.38187462091445923
  - 0.3895000219345093
  - 0.3733174502849579
  - 0.3871651291847229
  - 0.3820643126964569
  - 0.39963629841804504
  - 0.3944873809814453
  - 0.37746402621269226
  - 0.4325767457485199
  - 0.3841989040374756
  - 0.39250731468200684
  - 0.4091801643371582
  - 0.37616869807243347
  - 0.38891226053237915
  - 0.3786412477493286
  - 0.3867747485637665
  - 0.3803565204143524
  - 0.3768889904022217
  - 0.39150404930114746
  - 0.37752410769462585
  - 0.3775036633014679
  - 0.4030503034591675
  - 0.38610541820526123
  - 0.3820561468601227
  - 0.3909216821193695
  - 0.39409133791923523
  - 0.3893052041530609
  - 0.3792678713798523
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 37 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 35 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:10:06.447350'
