config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:16:36.389929'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_123fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_123fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_123fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_123fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_123fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_123fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_123fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_123fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_123fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_123fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 13.2187162399292
  - 3.5407985091209415
  - 3.7947386622428896
  - 9.675692200660706
  - 7.833310544490814
  - 3.187475872039795
  - 4.72861317396164
  - 2.300357681512833
  - 3.1812543272972107
  - 1.3023546934127808
  - 1.3982910156250001
  - 1.0627042531967164
  - 0.8576312899589539
  validation_losses:
  - 6.241117477416992
  - 13.686120986938477
  - 3.3882408142089844
  - 4.083116054534912
  - 0.9888666272163391
  - 1.4115673303604126
  - 2.187521457672119
  - 2.023193836212158
  - 0.6120107173919678
  - 0.537407636642456
  - 0.45174485445022583
  - 0.408123254776001
  - 0.39360511302948
loss_records_fold1:
  train_losses:
  - 0.8984629809856415
  - 0.800189757347107
  - 0.9798721849918366
  - 3.2048985958099365
  - 3.907027852535248
  - 2.2535874724388125
  - 1.9133702874183656
  - 7.291667538881303
  - 3.090837585926056
  - 1.4458653569221498
  - 7.038169413805008
  - 1.2659548997879029
  - 1.0864912629127503
  - 1.0778559863567352
  - 1.153246682882309
  - 1.9674261868000031
  - 3.9325020492076876
  - 1.4007309436798097
  - 0.928381633758545
  - 0.9701076567173005
  - 6.175585782527924
  - 1.549116063117981
  - 1.1640320420265198
  - 1.3439650952816011
  - 1.5048094749450684
  - 1.1962309300899505
  - 1.5160995483398438
  - 1.5555689156055452
  - 0.9299827605485916
  - 0.9620423078536988
  - 0.9660670161247253
  - 3.9060370087623597
  - 1.7670662462711335
  - 1.794454163312912
  - 2.027678978443146
  - 1.5207616686820984
  - 2.228937619924545
  - 0.9556726932525635
  - 1.614767247438431
  - 1.2127024710178376
  - 0.887935358285904
  - 0.9067134022712708
  - 17.8657065987587
  - 2.286062443256378
  - 0.9543498396873474
  - 3.312499898672104
  - 1.0479086875915529
  - 1.6274044811725616
  - 1.0637925803661348
  - 0.835686755180359
  - 0.9078337550163269
  - 1.0632530510425569
  - 1.031783777475357
  validation_losses:
  - 0.48420971632003784
  - 0.6078671216964722
  - 0.90137779712677
  - 2.0335662364959717
  - 1.148015022277832
  - 0.8311815857887268
  - 0.6296359300613403
  - 0.494606614112854
  - 0.7585445046424866
  - 0.663754940032959
  - 0.6587635278701782
  - 0.7639548182487488
  - 0.47213274240493774
  - 0.6529402732849121
  - 0.5756422877311707
  - 0.49962857365608215
  - 0.41804760694503784
  - 0.4505659341812134
  - 0.3949185013771057
  - 0.4101378321647644
  - 0.3907586336135864
  - 0.5056111812591553
  - 0.44241946935653687
  - 0.4364011287689209
  - 0.41701850295066833
  - 0.45254114270210266
  - 0.56903475522995
  - 0.3988614082336426
  - 0.39918532967567444
  - 0.41545772552490234
  - 0.39734965562820435
  - 0.4225592017173767
  - 0.4303304851055145
  - 0.6082345843315125
  - 0.565197229385376
  - 0.7601304054260254
  - 0.48848289251327515
  - 0.4239242374897003
  - 0.44011813402175903
  - 0.46102839708328247
  - 0.5025854706764221
  - 0.5146868228912354
  - 0.5177196860313416
  - 0.4643351137638092
  - 0.423211932182312
  - 0.459623783826828
  - 0.7939074635505676
  - 0.4337097108364105
  - 0.40119993686676025
  - 0.40138959884643555
  - 0.40140771865844727
  - 0.4005376994609833
  - 0.3986488878726959
loss_records_fold2:
  train_losses:
  - 0.8819288313388824
  - 1.3124975264072418
  - 1.0652709007263184
  - 3.7040614545345307
  - 1.705977886915207
  - 2.7185505449771883
  - 1.2908733665943146
  - 0.9741730749607087
  - 0.9944836974143982
  - 1.495670121908188
  - 1.0482973277568817
  - 0.9779670476913452
  - 1.1560533463954925
  - 0.9255965113639832
  validation_losses:
  - 0.3851078450679779
  - 0.5717023015022278
  - 0.40818706154823303
  - 0.5460413098335266
  - 1.2829077243804932
  - 0.42318275570869446
  - 0.3923313319683075
  - 0.42272475361824036
  - 0.3985590636730194
  - 0.39792147278785706
  - 0.39075347781181335
  - 0.39778372645378113
  - 0.3793247640132904
  - 0.3855425715446472
loss_records_fold3:
  train_losses:
  - 6.497450125217438
  - 1.0097886621952057
  - 0.87173792719841
  - 0.7813555657863618
  - 0.8075773298740387
  - 0.8893666863441467
  - 0.9370548188686372
  - 2.1490720570087434
  - 3.1310651659965516
  - 2.3197270750999452
  - 0.9499899089336395
  - 7.0586229979991915
  - 8.27966429591179
  - 0.9421758711338044
  - 0.84305759370327
  validation_losses:
  - 0.48479199409484863
  - 0.4886140823364258
  - 0.4367850720882416
  - 0.40577688813209534
  - 0.40424004197120667
  - 0.41273990273475647
  - 0.4240506589412689
  - 0.4640919864177704
  - 1.1153463125228882
  - 0.45031750202178955
  - 0.42248040437698364
  - 0.39867398142814636
  - 0.3920653462409973
  - 0.38703298568725586
  - 0.38582804799079895
loss_records_fold4:
  train_losses:
  - 1.0687968730926514
  - 0.8319546163082123
  - 2.751820081472397
  - 0.9921135306358337
  - 0.7964707851409912
  - 2.47107430100441
  - 0.7954442024230958
  - 1.773394697904587
  - 1.1396484076976776
  - 0.9968960344791413
  - 1.1347358644008636
  - 1.3303540766239168
  - 2.9820200502872467
  - 2.325823873281479
  - 0.8863014936447144
  - 0.9245242118835449
  - 1.1712199866771698
  - 1.6261632084846498
  - 0.8188989520072938
  - 3.347889405488968
  - 0.9863845348358155
  - 0.8137955367565155
  validation_losses:
  - 0.47109681367874146
  - 0.43529608845710754
  - 0.4298464357852936
  - 0.4276246726512909
  - 0.43428078293800354
  - 0.4560369849205017
  - 1.533522129058838
  - 0.49471473693847656
  - 0.5511525869369507
  - 0.5649238228797913
  - 0.5870051980018616
  - 0.5245950818061829
  - 0.5041863918304443
  - 0.7924091815948486
  - 0.4565446674823761
  - 0.5166885852813721
  - 0.48110732436180115
  - 0.47200024127960205
  - 0.4647461175918579
  - 0.47256675362586975
  - 0.46096503734588623
  - 0.46132510900497437
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 53 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8505154639175257]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.023809523809523808, 0.0]'
  mean_eval_accuracy: 0.8568955456136937
  mean_f1_accuracy: 0.0047619047619047615
  total_train_time: '0:10:01.515257'
