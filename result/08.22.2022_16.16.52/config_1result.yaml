config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.212639'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_1fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_1fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_1fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_1fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_1fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_1fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_1fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_1fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_1fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_1fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.274290317296982
  - 2.9992984741926194
  - 2.915415567159653
  - 2.9060729801654817
  - 2.955838525295258
  - 2.841929477453232
  - 2.8965730309486393
  - 2.950323843955994
  - 2.9020146369934086
  - 2.8702533543109894
  - 2.9896624207496645
  - 2.8273358285427097
  - 2.839712929725647
  - 2.824032774567604
  - 2.87915555536747
  - 2.8702248275280002
  - 2.8505671232938767
  - 2.863658121228218
  - 2.983395379781723
  validation_losses:
  - 0.4385802149772644
  - 0.40701979398727417
  - 0.39321640133857727
  - 0.3927622437477112
  - 0.3981926739215851
  - 0.39225438237190247
  - 0.39317429065704346
  - 0.39963430166244507
  - 0.3951418399810791
  - 0.4209304749965668
  - 0.38462942838668823
  - 0.3905157148838043
  - 0.41480550169944763
  - 0.38595667481422424
  - 0.39181163907051086
  - 0.3847830593585968
  - 0.3826811611652374
  - 0.3814406991004944
  - 0.38375940918922424
loss_records_fold1:
  train_losses:
  - 2.8563126236200334
  - 2.829025959968567
  - 2.864623320102692
  - 2.840211716294289
  - 2.8011657118797304
  - 2.860832539200783
  - 2.794429436326027
  - 2.8111159682273867
  - 2.769974485039711
  - 2.81835578083992
  - 2.8617217540740967
  - 2.877247378230095
  - 2.814758762717247
  - 2.7930010586977008
  - 2.910543179512024
  - 2.824930116534233
  - 2.7953548669815067
  - 2.8213439345359803
  - 2.778881570696831
  - 2.7532737076282503
  - 2.7806238532066345
  - 2.802703720331192
  - 2.7612625509500504
  - 2.780261516571045
  - 2.7564880013465882
  - 2.768135365843773
  - 2.788187411427498
  - 2.7748829424381256
  - 2.7629006624221804
  - 2.764361807703972
  - 2.7613028287887573
  - 2.7990893155336383
  - 2.775553095340729
  - 2.7557433962821962
  - 2.749901306629181
  - 2.7957625627517704
  - 2.7735045790672306
  - 2.7625556319952014
  - 2.7274379223585132
  - 2.887975645065308
  - 2.7912296980619433
  - 2.7480575025081637
  - 2.8427940011024475
  - 2.7467390269041063
  - 2.736973962187767
  - 2.7967402160167696
  - 2.778108161687851
  - 2.733612221479416
  - 2.7687030255794527
  - 2.7610959589481356
  - 2.7548654019832615
  - 2.731932294368744
  - 2.699817490577698
  - 2.7007636308670047
  - 2.75703789293766
  - 2.7393346041440965
  - 2.7308960616588593
  - 2.7343055069446565
  - 2.755105248093605
  - 2.703861340880394
  - 2.7362678557634355
  - 2.719429984688759
  - 2.743086650967598
  - 2.7418711155653
  - 2.738816106319428
  - 2.750062531232834
  - 2.7244833350181583
  - 2.7637081384658817
  - 2.721802219748497
  - 2.7423429042100906
  - 2.704005825519562
  - 2.75581071972847
  - 2.7305289059877396
  - 2.7111534386873246
  - 2.6867844402790073
  - 2.734901747107506
  - 2.720047587156296
  - 2.686751160025597
  - 2.7778310954570773
  - 2.6890739381313327
  - 2.704718753695488
  - 2.716265708208084
  - 2.709441086649895
  - 2.709680745005608
  - 2.911735048890114
  - 2.7737748473882675
  - 2.722142913937569
  - 2.7283008694648743
  - 2.686566033959389
  - 2.722966393828392
  - 2.6880180627107624
  - 2.680003708600998
  - 2.7392375767230988
  - 2.690524843335152
  - 2.781165510416031
  - 2.775688070058823
  - 2.7335628122091293
  - 2.7494368612766267
  - 2.7369085580110553
  - 2.6865717709064487
  validation_losses:
  - 0.40591058135032654
  - 0.3869709074497223
  - 0.3917691707611084
  - 0.3842677175998688
  - 0.38703346252441406
  - 0.4410160779953003
  - 0.6972703337669373
  - 0.38241687417030334
  - 0.44258588552474976
  - 0.4117496907711029
  - 0.5201497673988342
  - 0.4545489251613617
  - 0.3844473659992218
  - 0.3985651433467865
  - 0.39336591958999634
  - 0.3961067497730255
  - 0.3914077579975128
  - 0.40762487053871155
  - 0.48870766162872314
  - 0.47065675258636475
  - 0.4445611834526062
  - 0.399112343788147
  - 0.5441771745681763
  - 0.45446306467056274
  - 0.48255300521850586
  - 0.5431416034698486
  - 0.42743945121765137
  - 1.0158836841583252
  - 0.5456048250198364
  - 0.40743526816368103
  - 0.46447882056236267
  - 0.4980633854866028
  - 0.5703034400939941
  - 0.4879762828350067
  - 0.39992159605026245
  - 0.45780450105667114
  - 0.613119900226593
  - 0.4787006378173828
  - 0.762687087059021
  - 0.558134913444519
  - 0.6730145812034607
  - 0.6199065446853638
  - 0.4644499123096466
  - 0.4815903306007385
  - 0.5259557366371155
  - 0.6307703852653503
  - 0.5373360514640808
  - 0.4713430106639862
  - 0.47284606099128723
  - 0.47782811522483826
  - 0.5499914288520813
  - 0.5974763035774231
  - 0.6183591485023499
  - 0.7947118878364563
  - 0.6193285584449768
  - 0.5781537890434265
  - 0.5287571549415588
  - 0.5304726958274841
  - 0.6264166831970215
  - 0.567097008228302
  - 0.4772639572620392
  - 0.5968144536018372
  - 0.5015941262245178
  - 0.5387440919876099
  - 0.49484679102897644
  - 0.4706442952156067
  - 0.6369185447692871
  - 0.5011823773384094
  - 0.5970970392227173
  - 0.68092280626297
  - 0.626772940158844
  - 0.5650315284729004
  - 0.6534208655357361
  - 0.5978401303291321
  - 0.7882187366485596
  - 0.7359102964401245
  - 0.6776626706123352
  - 0.598507285118103
  - 0.7563517689704895
  - 0.6343156695365906
  - 0.7780837416648865
  - 0.659443736076355
  - 0.43593665957450867
  - 0.7496578097343445
  - 0.5256339311599731
  - 0.4517746865749359
  - 0.5500126481056213
  - 0.48552170395851135
  - 0.6242292523384094
  - 0.7264124751091003
  - 0.7287962436676025
  - 0.7178139686584473
  - 1.0263864994049072
  - 0.5160545706748962
  - 0.4405827522277832
  - 0.5035580396652222
  - 0.5595208406448364
  - 0.6247379779815674
  - 0.5878971219062805
  - 0.6494969725608826
loss_records_fold2:
  train_losses:
  - 2.720595443248749
  - 2.6725026547908786
  - 2.7060175716876986
  - 2.7388833671808244
  - 2.7233140736818315
  - 2.729925948381424
  - 2.728108206391335
  - 2.7311345398426057
  - 2.7599350005388263
  - 2.746713855862618
  - 2.7740448266267776
  - 2.820537316799164
  - 2.723188662528992
  - 2.782480239868164
  - 2.755730003118515
  - 2.7436894834041596
  - 2.7398682266473773
  - 2.700514167547226
  - 2.771501487493515
  - 2.7384793162345886
  - 2.718270191550255
  - 2.704685866832733
  - 2.735985937714577
  - 2.7398986458778385
  - 2.717134580016136
  - 2.7189084887504578
  - 2.6934073448181155
  - 2.7142993211746216
  - 2.787118113040924
  - 2.8987321317195893
  - 2.853369551897049
  - 2.7405162781476977
  - 2.9545126497745517
  - 2.8061456739902497
  - 2.713760903477669
  - 2.811108493804932
  - 2.7890424191951753
  - 2.7762049317359927
  - 2.756279334425926
  - 2.780382934212685
  - 2.756916707754135
  - 2.816848999261856
  validation_losses:
  - 0.4696398377418518
  - 0.551307201385498
  - 0.41650906205177307
  - 0.555974543094635
  - 0.520401120185852
  - 1.136617660522461
  - 1.120110273361206
  - 0.42413198947906494
  - 0.5623443126678467
  - 0.448291152715683
  - 0.4038906693458557
  - 0.3942868113517761
  - 0.3815157115459442
  - 0.46050235629081726
  - 0.38835179805755615
  - 0.413510262966156
  - 0.4115639925003052
  - 0.41562795639038086
  - 0.5302658081054688
  - 0.7958462834358215
  - 0.44334399700164795
  - 0.4217071533203125
  - 0.5337182283401489
  - 0.5706954598426819
  - 0.45847222208976746
  - 0.41200509667396545
  - 0.5319724678993225
  - 1.3365485668182373
  - 0.4914431571960449
  - 1.6760687828063965
  - 0.43071407079696655
  - 0.38211309909820557
  - 0.3949406147003174
  - 0.38474154472351074
  - 0.38062185049057007
  - 0.4330974519252777
  - 0.3818505108356476
  - 0.38229942321777344
  - 0.3813157379627228
  - 0.3893343210220337
  - 0.39290669560432434
  - 0.38996490836143494
loss_records_fold3:
  train_losses:
  - 2.7721491754055023
  - 2.777597013115883
  - 2.7805166453123094
  - 2.8539990842342378
  - 2.7942885130643846
  - 2.7359961718320847
  - 2.7410286962985992
  - 2.7632919162511826
  - 2.7892984777688983
  - 2.7513692140579225
  - 2.7391104817390444
  - 2.75668580532074
  - 2.8300825864076615
  - 2.765453588962555
  - 2.783701825141907
  - 2.8627646148204806
  - 2.7123774200677873
  - 2.7669386208057407
  - 2.761088633537293
  - 2.720009306073189
  - 2.7501221895217896
  - 2.774308314919472
  - 2.7132375180721287
  - 2.7457559645175937
  - 2.72361575961113
  - 2.7678418397903446
  - 2.7379428982734684
  - 2.691039735078812
  - 2.753108236193657
  - 2.7728741466999054
  - 2.739947831630707
  - 2.8443966060876846
  - 2.753664326667786
  - 2.788556990027428
  - 2.750496339797974
  - 2.69867143034935
  - 2.708173328638077
  - 2.7421025812625888
  - 2.751870730519295
  - 2.7393743336200718
  - 2.698997762799263
  - 2.738690432906151
  - 2.729988729953766
  - 2.779031401872635
  - 2.738117787241936
  - 2.7349065721035006
  - 2.697402447462082
  - 2.7181708812713623
  - 2.757999724149704
  - 2.7164099991321566
  - 2.6778521984815598
  - 2.692625045776367
  - 2.714818125963211
  - 2.6968335419893266
  - 2.7062747061252597
  - 2.7080764770507812
  - 2.69483078122139
  - 2.7225030153989795
  - 2.682412171363831
  - 2.6789121329784393
  - 2.6730365186929705
  - 2.6514486491680147
  - 2.66702701151371
  - 2.6737046003341676
  - 2.731914660334587
  - 2.6597795307636263
  - 2.671568685770035
  - 2.6882618814706802
  - 2.743011450767517
  - 2.7005240708589557
  - 2.9174495279788974
  - 2.8175756245851518
  - 2.802221491932869
  - 2.748804593086243
  - 2.794713550806046
  - 2.7558735430240633
  validation_losses:
  - 0.3773820400238037
  - 0.90155428647995
  - 0.36183157563209534
  - 0.38286879658699036
  - 0.6138005256652832
  - 0.3839106559753418
  - 0.4710676074028015
  - 0.4008382558822632
  - 0.42139163613319397
  - 0.43905550241470337
  - 0.5245376825332642
  - 0.5001137852668762
  - 0.3652515411376953
  - 0.6441797018051147
  - 1.603562593460083
  - 0.4176834523677826
  - 1.3267217874526978
  - 0.9780075550079346
  - 0.41062286496162415
  - 2.538381576538086
  - 0.5521296858787537
  - 0.39744848012924194
  - 0.6604011654853821
  - 0.4658966064453125
  - 0.5229682922363281
  - 0.6093503832817078
  - 0.5104624629020691
  - 0.6236244440078735
  - 0.48856592178344727
  - 0.4550962448120117
  - 4.410385608673096
  - 0.554755449295044
  - 0.5997946858406067
  - 0.47760728001594543
  - 0.40485939383506775
  - 0.4278484880924225
  - 0.39873555302619934
  - 0.45833656191825867
  - 0.5943694114685059
  - 0.45111986994743347
  - 2.818535804748535
  - 0.986486554145813
  - 0.5849192142486572
  - 0.45856237411499023
  - 0.5415517687797546
  - 0.7031099200248718
  - 0.7153121829032898
  - 1.066823124885559
  - 0.4217076301574707
  - 0.4005168080329895
  - 0.5048438906669617
  - 0.5447642207145691
  - 0.44920453429222107
  - 0.5076673030853271
  - 0.4281213879585266
  - 0.6356101036071777
  - 0.5907260775566101
  - 0.6336329579353333
  - 0.7738001346588135
  - 0.566818356513977
  - 1.0765342712402344
  - 0.5429064631462097
  - 0.4576326012611389
  - 0.5606671571731567
  - 0.778459370136261
  - 0.818454384803772
  - 1.0367913246154785
  - 0.6985769867897034
  - 0.5636605024337769
  - 0.8455654382705688
  - 0.37212663888931274
  - 0.3699802756309509
  - 0.3694802224636078
  - 0.3667382299900055
  - 0.3689827024936676
  - 0.3675762116909027
loss_records_fold4:
  train_losses:
  - 2.8075651079416275
  - 2.8561004489660267
  - 2.785444787144661
  - 2.775634428858757
  - 2.770125859975815
  - 2.7941270917654037
  - 2.751251977682114
  - 2.792120310664177
  - 2.7641741037368774
  - 2.7402560859918594
  - 2.7530990928411487
  - 2.7823408126831057
  - 2.7621805667877197
  - 2.73598148226738
  - 2.7638013809919357
  validation_losses:
  - 0.3669573664665222
  - 0.3812776803970337
  - 0.3677864670753479
  - 0.36574751138687134
  - 0.36964982748031616
  - 0.3668311536312103
  - 0.36498185992240906
  - 0.3652614951133728
  - 0.377045214176178
  - 0.36435040831565857
  - 0.3659577965736389
  - 0.3660356402397156
  - 0.36378777027130127
  - 0.36871784925460815
  - 0.3657516837120056
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 42 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 76 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8439108061749572, 0.8576329331046312, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.14953271028037382, 0.023529411764705882, 0.0, 0.0]'
  mean_eval_accuracy: 0.8555262801129363
  mean_f1_accuracy: 0.03461242440901594
  total_train_time: '0:22:19.636278'
