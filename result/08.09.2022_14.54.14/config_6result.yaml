config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cpu
  dropout: 0.0
  epochs: 30
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.05
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 5
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-09 14:59:03.990561'
fold_0_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_6fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_6fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_6fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_6fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_6fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_6fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_6fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_6fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_6fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.09.2022_14.54.14/config_6fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 7.966740769147873
  - 6.07752503156662
  - 6.280647552013398
  - 6.15491084754467
  - 6.051089304685593
  - 6.025552728772164
  - 6.08145514279604
  - 6.277388644218445
  - 6.084186759591103
  - 6.092819245159626
  - 7.402443006634712
  - 6.613956534862519
  - 6.075796961784363
  - 6.282919549942017
  - 6.066923406720162
  - 6.262172415852547
  - 6.023683130741119
  - 6.206994934380055
  - 6.012175658345223
  - 6.015009161829949
  - 6.106579256057739
  - 6.068660488724709
  - 6.007554313540459
  - 6.163149148225784
  - 6.06518923342228
  - 6.139634469151497
  - 6.3523290961980825
  - 6.065020781755448
  - 6.158096522092819
  - 6.2352726206183435
  validation_losses:
  - 0.4127149283885956
  - 0.40785062313079834
  - 0.4106217920780182
  - 0.40895214676856995
  - 0.40589964389801025
  - 0.43266481161117554
  - 187.9781036376953
  - 0.4297834634780884
  - 2.166982650756836
  - 0.40790191292762756
  - 59.318817138671875
  - 18889.955078125
  - 85373665280.0
  - 752149266432.0
  - 6225957224448.0
  - 2180478140416.0
  - 9167132688384.0
  - 42843913584640.0
  - 9.595282497785037e+16
  - 86386187173888.0
  - 9.106701621080883e+16
  - 1.273070500293116e+18
  - 5.595373098468639e+17
  - 0.40979671478271484
  - 1.001215618106314e+19
  - 1.3320554783203918e+19
  - 1.213677356061819e+18
  - 3.03533335006633e+19
  - 7.782546711049667e+18
  - 2.6206529794552627e+19
loss_records_fold1:
  train_losses:
  - 6.06737765967846
  - 6.1063461929559715
  - 6.129984277486802
  - 6.206228774785996
  - 6.062629498541355
  - 6.0398214250803
  - 6.187583154439927
  - 6.117721208930016
  - 6.056530679762364
  - 6.1004758983850484
  - 6.100869569182397
  - 6.194086170196534
  - 6.125245398283005
  - 6.223648014664651
  - 6.144947218894959
  - 6.091929078102112
  - 6.006916274130345
  - 6.046905100345612
  - 6.062108138203621
  - 6.06318461894989
  - 5.999193367362023
  - 6.0907650411129
  - 6.146559801697731
  - 6.165412636101246
  - 6.0876819610595705
  - 6.084323078393936
  - 6.512804770097137
  - 6.118949010968208
  - 6.127439850568772
  - 6.011192560195923
  validation_losses:
  - 0.410942405462265
  - 5.862408395130143e+17
  - 6.419755477814477e+16
  - 5.284826828161155e+18
  - 8.746015765120942e+18
  - 1.9878028112486728e+18
  - 8.029140780880757e+18
  - 3.534608963343483e+17
  - 1.0525686385350803e+19
  - 1.4916692723502154e+19
  - 1.0219623030108193e+19
  - 1.5964803007313347e+18
  - 2.088266700507054e+17
  - 2.0486280968629387e+19
  - 0.42544227838516235
  - 2.33848421090748e+19
  - 7.591196640883507e+17
  - 0.4301999807357788
  - 9.831558097666048e+17
  - 3.82776748804984e+18
  - 6.497692607202722e+18
  - 9.274174971616166e+18
  - 1.828046486152151e+17
  - 1.5765713062482412e+18
  - 1.9249218786951168e+18
  - 0.41668736934661865
  - 2.014990957536713e+19
  - 0.41111427545547485
  - 9.940650541862355e+18
  - 3.148215975770325e+18
loss_records_fold2:
  train_losses:
  - 6.226680870354176
  - 6.019010132551194
  - 6.289182776212693
  - 6.058691366016865
  - 6.093109160661697
  - 6.21212267279625
  - 6.208442768454552
  - 6.064232650399209
  - 6.196272075176239
  - 6.168111297488213
  - 6.027209642529488
  - 6.122772040963174
  - 6.121283054351807
  - 6.101819708943367
  - 6.074032601714134
  - 6.147198715806008
  - 5.999339774250984
  - 6.054837250709534
  - 6.154989078640938
  - 6.097056716680527
  - 6.086696515977383
  - 6.056546121835709
  validation_losses:
  - 1.448593705308835e+19
  - 0.4067823886871338
  - 6.198273050970948e+18
  - 1.0075783819450909e+19
  - 1.9451498689892188e+18
  - 2.2238079656289894e+17
  - 0.4321371614933014
  - 9.888453426356945e+18
  - 6.681425398250602e+18
  - 1.419579874938454e+18
  - 0.41215649247169495
  - 2.592904054749266e+18
  - 0.40757372975349426
  - 1.221094194210354e+19
  - 4.0268245849708954e+17
  - 0.40624597668647766
  - 0.4364781081676483
  - 2.1789776192613646e+19
  - 1.7745076434793136e+19
  - 0.42761263251304626
  - 0.4105827212333679
  - 0.45479950308799744
loss_records_fold3:
  train_losses:
  - 6.243002378940583
  - 6.116987204551697
  - 6.081203390657902
  - 6.088868659734726
  - 6.129581782221795
  - 6.300512123107911
  - 6.095807951688767
  - 6.061465409398079
  - 6.16967882886529
  - 6.093818178772927
  - 6.040998306870461
  - 6.283463177084923
  - 6.013228464126588
  - 6.188286611437798
  - 6.1132201790809635
  - 6.070080637931824
  - 6.058846026659012
  - 6.097462740540505
  - 6.140528675913811
  - 6.13247130215168
  - 6.055354841053486
  - 6.082870599627495
  - 6.089833298325539
  - 6.075077041983604
  - 6.429974991083146
  - 8.319055771827697
  - 8.719866007566452
  validation_losses:
  - 2.8770909768898314e+19
  - 1.269321852837167e+18
  - 5.01592050630656e+17
  - 0.40627455711364746
  - 1.5282359504574874e+19
  - 4.6682274418130944e+17
  - 9.116840355239559e+18
  - 1.659774512217981e+18
  - 5.488150786112225e+17
  - 3.9149914706034033e+18
  - 2.4298978481589453e+17
  - 0.4238302409648895
  - 1.8497202861331448e+19
  - 0.43409842252731323
  - 6.326861265692839e+19
  - 5.202427667557017e+19
  - 6.80148327191072e+18
  - 5.737389772395617e+19
  - 2.7858207365704712e+19
  - 3.3652356567154033e+19
  - 5.482831280137568e+18
  - 0.4319019615650177
  - 2.843004357210171e+19
  - 1.778888647560187e+19
  - 9.200067887904063e+18
  - 7.03795862197876
  - 0.45203882455825806
loss_records_fold4:
  train_losses:
  - 7.052212217450142
  - 6.539042195677758
  - 6.663545110821724
  - 6.30001295208931
  - 6.2693228125572205
  - 6.665828451514244
  - 5.847778853774071
  - 6.240009739995003
  - 6.003348258137703
  - 6.240664026141167
  - 6.111773531883955
  - 6.344414472579956
  - 6.178307089209557
  - 6.000797235965729
  validation_losses:
  - 0.4353886544704437
  - 0.4266800582408905
  - 1.276958703994751
  - 0.42987528443336487
  - 0.3931170701980591
  - 0.39205771684646606
  - 0.3965519070625305
  - 0.4226868152618408
  - 0.425558865070343
  - 1.484544038772583
  - 0.48975038528442383
  - 0.4106001555919647
  - 0.4240707457065582
  - 0.39589419960975647
training fold messages:
  fold 0 training message: completed 30 epochs without stopping early
  fold 1 training message: completed 30 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.855917667238422,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8575840097139455
  mean_f1_accuracy: 0.0
  total_train_time: '0:04:24.969921'
