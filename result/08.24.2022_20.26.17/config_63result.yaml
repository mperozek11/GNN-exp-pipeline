config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:51:02.589745'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_63fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_63fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_63fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_63fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_63fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_63fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_63fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_63fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_63fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_63fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 24.905800437927248
  - 14.26305332183838
  - 17.20375747680664
  - 4.723486649990082
  - 4.737586450576782
  - 2.535014581680298
  - 3.239835107326508
  - 2.351754093170166
  - 1.9700145900249482
  - 1.363016986846924
  - 1.3666083455085756
  validation_losses:
  - 5.035702228546143
  - 2.1174349784851074
  - 2.073589324951172
  - 1.415101170539856
  - 4.1670002937316895
  - 0.7624214291572571
  - 0.6632987260818481
  - 0.5441129207611084
  - 0.49641653895378113
  - 0.48888644576072693
  - 0.44405829906463623
loss_records_fold1:
  train_losses:
  - 1.171728152036667
  - 1.0260638058185578
  - 0.9766136825084687
  - 0.9242189109325409
  - 3.742217350006104
  - 2.6891224145889283
  - 2.417189204692841
  - 2.8961332082748417
  - 1.5918729424476625
  - 1.2885585635900498
  - 3.5265440106391908
  - 1.2531170308589936
  - 1.9391128897666932
  - 2.066853952407837
  - 0.9870559513568878
  - 1.137211400270462
  - 1.054322785139084
  - 1.0394377768039704
  - 1.1404217660427094
  - 0.9608650803565979
  - 1.5424026131629944
  - 0.93373042345047
  - 0.9391179323196411
  - 0.9254539549350739
  - 0.9391644328832627
  - 0.8985143542289734
  - 0.8842046409845352
  - 1.0970901608467103
  - 0.9046728193759919
  - 0.8745286405086518
  - 1.218106633424759
  - 1.0829539358615876
  - 0.8978146344423295
  - 7.837109827995301
  - 3.7097843289375305
  - 1.2756577134132385
  - 1.978822922706604
  - 2.355324721336365
  - 16.057586348056795
  - 2.0723665356636047
  - 1.9448717296123506
  - 1.3178814470767977
  - 0.8862364947795869
  - 1.0264380991458892
  - 0.9630880296230316
  - 0.9635852545499802
  - 0.9721048593521119
  - 0.8861245334148408
  - 0.8843920588493348
  - 0.9177974760532379
  - 0.8836328387260437
  - 0.8696378231048585
  - 1.3129265308380127
  - 0.9287347972393036
  - 0.8817296206951142
  - 0.8490434527397156
  - 0.9230928778648377
  - 0.8282911479473114
  - 0.8065238296985626
  - 0.8859458565711975
  - 0.8849533379077912
  - 0.8489212691783905
  - 1.902349466085434
  - 1.0951243042945862
  - 0.9622905731201172
  - 0.9778351902961732
  - 1.1895001530647278
  - 1.0175296664237976
  - 1.1380167186260224
  - 2.238117927312851
  - 1.066766721010208
  - 1.2992195963859559
  - 0.9477904558181763
  - 0.9432184994220734
  - 0.938635003566742
  - 1.034789752960205
  - 0.8645859062671661
  - 0.8631592571735383
  - 0.9085404336452485
  - 1.098764306306839
  - 1.2977707445621491
  - 0.9209496200084687
  - 1.0395965218544008
  - 1.277047747373581
  - 0.8729913711547852
  - 1.1629484832286836
  - 0.9258607268333435
  - 0.9343726456165314
  - 1.9897715091705324
  - 5.32430739402771
  - 1.5545773625373842
  - 1.1945118963718415
  - 1.5080383062362672
  - 1.1232685029506684
  - 0.9387277960777283
  - 0.9110884785652161
  - 1.3989040672779085
  - 0.8733946681022644
  - 0.9774886250495911
  - 0.9032733738422394
  validation_losses:
  - 0.6287604570388794
  - 0.44908663630485535
  - 0.4322075843811035
  - 0.43812280893325806
  - 1.1111611127853394
  - 0.47944551706314087
  - 0.6237057447433472
  - 0.7479609251022339
  - 0.6054481267929077
  - 0.516016960144043
  - 0.6207056641578674
  - 0.6154189705848694
  - 0.49745652079582214
  - 0.4701519310474396
  - 0.41740289330482483
  - 0.4801771938800812
  - 0.4705662429332733
  - 0.43189945816993713
  - 0.41433581709861755
  - 0.4496699869632721
  - 0.4954133629798889
  - 0.5529364943504333
  - 0.4232518672943115
  - 0.4193078279495239
  - 0.425484299659729
  - 0.42262959480285645
  - 0.526823878288269
  - 0.42291590571403503
  - 0.4521222710609436
  - 0.46852755546569824
  - 0.41132664680480957
  - 0.5552153587341309
  - 0.4303567409515381
  - 1.2562735080718994
  - 0.5446620583534241
  - 0.5364716053009033
  - 0.4948062598705292
  - 0.6415074467658997
  - 0.7280668616294861
  - 0.5931065082550049
  - 0.5446310639381409
  - 0.39832186698913574
  - 0.4436821937561035
  - 0.4333723187446594
  - 0.42434531450271606
  - 0.4113495945930481
  - 0.4539492428302765
  - 0.4313685894012451
  - 0.424064040184021
  - 0.4116837680339813
  - 0.4076180160045624
  - 0.4160592257976532
  - 0.43978196382522583
  - 0.4099465012550354
  - 0.4275960326194763
  - 0.44115501642227173
  - 0.4390689730644226
  - 0.40553370118141174
  - 0.4151952266693115
  - 0.39980587363243103
  - 0.41084790229797363
  - 0.44555434584617615
  - 0.4259967803955078
  - 0.41784393787384033
  - 0.4617880880832672
  - 0.5548436045646667
  - 0.46369653940200806
  - 0.5393385887145996
  - 0.4463067650794983
  - 0.47638559341430664
  - 0.4240182936191559
  - 0.4411991834640503
  - 0.42163577675819397
  - 0.42369544506073
  - 0.41432225704193115
  - 0.4408244490623474
  - 0.40896204113960266
  - 0.41489240527153015
  - 0.410839319229126
  - 0.41119056940078735
  - 0.4122712016105652
  - 0.45813846588134766
  - 0.4610416293144226
  - 0.41586676239967346
  - 0.42889338731765747
  - 0.4311082363128662
  - 0.41800928115844727
  - 0.4318453073501587
  - 0.5675745606422424
  - 0.5258080363273621
  - 0.42469990253448486
  - 0.4548983871936798
  - 0.4889954626560211
  - 0.43651169538497925
  - 0.42629551887512207
  - 0.3951573073863983
  - 0.3990960121154785
  - 0.43003520369529724
  - 0.40862128138542175
  - 0.43727272748947144
loss_records_fold2:
  train_losses:
  - 1.5011498630046844
  - 1.0147711157798767
  - 1.0013107299804689
  - 0.9214136183261872
  - 0.9274699449539185
  - 0.900886607170105
  - 1.0678457498550415
  - 0.9273101449012757
  - 0.8898290634155274
  - 1.4791881620883942
  - 1.002164489030838
  - 0.8823953211307526
  - 0.8908260226249696
  - 0.8905830264091492
  - 0.8454333543777466
  - 0.9146508157253266
  - 0.8184269607067108
  - 1.1467924058437349
  - 1.3862878441810609
  - 1.0605101704597473
  - 0.8139962613582612
  - 8.024806541204454
  - 1.9620358467102053
  - 1.0502375483512878
  - 1.375666004419327
  - 3.2021717309951785
  - 1.4209864616394043
  - 1.3413202285766603
  - 1.0682023465633392
  - 2.669190859794617
  - 1.7293134272098543
  - 1.5125744998455048
  - 1.0635618269443512
  - 1.1516828060150146
  - 2.3961642861366275
  - 6.099761891365052
  - 1.1952271640300751
  - 1.8151762902736666
  - 1.1808137059211732
  - 0.9014231562614441
  - 0.8881331205368043
  - 1.2834383487701417
  - 1.220739769935608
  - 1.8678691387176514
  - 6.93041285276413
  - 2.406183785200119
  - 2.115175211429596
  - 0.9704471468925476
  - 1.5312528252601625
  - 0.9658065378665924
  - 0.9489214450120926
  - 0.9022525191307068
  - 10.874636816978455
  - 1.1407923579216004
  - 1.1117531418800355
  - 1.170045393705368
  - 1.6808290779590607
  - 0.9479272961616516
  - 0.8303158938884736
  - 0.8445247739553452
  - 0.870553994178772
  - 0.8315513491630555
  - 1.0689334630966187
  - 0.8572015166282654
  - 0.8457283794879914
  - 0.8766100525856019
  - 1.3897061169147493
  - 1.3608177423477175
  - 0.8657901525497437
  - 0.9004984021186829
  - 0.8544622033834458
  - 1.005210852622986
  - 0.9081138730049134
  - 1.0920093417167664
  - 1.2710337281227113
  - 0.9284139275550842
  - 0.8288747787475587
  - 0.996643102169037
  - 0.8423294544219971
  - 1.0281309008598327
  - 0.874796223640442
  - 0.8896212577819824
  - 3.534509646892548
  - 0.9949757516384126
  - 0.9200396180152893
  - 0.9033335208892823
  - 0.8478246808052063
  - 0.8660869598388672
  - 0.8750027894973755
  - 1.0825769424438476
  - 0.914627230167389
  - 1.1358303785324098
  - 0.9054819464683533
  - 0.8457042634487153
  - 0.8846058249473572
  - 0.8194084882736207
  - 0.8241975069046021
  - 0.8352445423603059
  - 0.8269406139850617
  - 0.8149574041366577
  validation_losses:
  - 0.4184761047363281
  - 0.40246450901031494
  - 0.389356404542923
  - 0.38302361965179443
  - 0.37960946559906006
  - 0.38693782687187195
  - 0.37566864490509033
  - 0.38159260153770447
  - 0.3983442485332489
  - 0.40334221720695496
  - 0.42142459750175476
  - 0.41565456986427307
  - 0.42709067463874817
  - 0.4074065089225769
  - 0.40314528346061707
  - 0.3865371644496918
  - 0.3824036121368408
  - 0.43964460492134094
  - 1.3365941047668457
  - 0.40920010209083557
  - 0.39889222383499146
  - 0.44678887724876404
  - 0.4266025722026825
  - 0.42726391553878784
  - 0.4289036989212036
  - 0.5032097101211548
  - 0.9112207889556885
  - 0.5024733543395996
  - 0.4446718394756317
  - 0.7296011447906494
  - 0.47408509254455566
  - 0.4457380771636963
  - 0.4234427511692047
  - 0.3898317217826843
  - 0.443131685256958
  - 0.5039648413658142
  - 0.4263034164905548
  - 0.3967600464820862
  - 0.38059940934181213
  - 0.3960484564304352
  - 0.4141104817390442
  - 0.3977745771408081
  - 0.4421735405921936
  - 0.5743408203125
  - 0.49580252170562744
  - 3.3326032161712646
  - 0.4335823655128479
  - 0.4160456955432892
  - 0.4082190990447998
  - 0.4288818836212158
  - 0.39867323637008667
  - 0.39886078238487244
  - 0.4038296341896057
  - 0.4045454263687134
  - 0.38614368438720703
  - 0.409026175737381
  - 0.4031974971294403
  - 0.41422781348228455
  - 0.39426058530807495
  - 0.3901100754737854
  - 0.4190026819705963
  - 0.39285391569137573
  - 0.4034648537635803
  - 0.4054965674877167
  - 0.3971339464187622
  - 0.41165289282798767
  - 0.39730435609817505
  - 0.4295845031738281
  - 0.4240235984325409
  - 0.4478954076766968
  - 0.4185168147087097
  - 0.4069848954677582
  - 0.4131721258163452
  - 0.45215463638305664
  - 0.40176188945770264
  - 0.402408629655838
  - 0.4050823450088501
  - 0.39678850769996643
  - 0.4105042815208435
  - 0.4428420960903168
  - 0.4019007980823517
  - 0.39480140805244446
  - 0.4173850119113922
  - 0.3940626084804535
  - 0.3987526297569275
  - 0.41421589255332947
  - 0.41076692938804626
  - 0.41405045986175537
  - 0.41193023324012756
  - 0.3998299539089203
  - 0.4164454936981201
  - 0.40540602803230286
  - 0.4007801413536072
  - 0.39831045269966125
  - 0.40102097392082214
  - 0.39105892181396484
  - 0.4035049080848694
  - 0.4096382260322571
  - 0.3971152603626251
  - 0.4047011137008667
loss_records_fold3:
  train_losses:
  - 0.862917971611023
  - 2.8933977246284486
  - 3.2011272788047793
  - 3.0402035474777223
  - 4.710002279281617
  - 2.1756301999092105
  - 1.766381096839905
  - 1.4776337802410127
  - 0.9922955930233002
  - 1.1205674171447755
  - 0.8940131604671478
  - 0.9328511893749237
  - 0.9221992433071137
  - 0.9419528126716614
  - 0.8382436275482178
  - 0.8308054149150849
  - 0.840719211101532
  - 0.9105976760387421
  - 1.294498610496521
  - 0.8858698725700379
  - 0.8372989177703858
  - 0.8519152581691742
  - 1.6161381661891938
  - 0.8341022849082947
  - 1.0541673064231873
  - 0.8827385187149048
  - 0.8898885011672975
  - 2.5178092539310457
  - 0.8047304570674897
  - 0.8907898426055909
  - 1.076750385761261
  - 0.8543133437633514
  - 0.883264034986496
  - 0.8732573568820954
  - 0.8088832318782807
  - 0.9783646762371063
  - 0.8968587458133698
  - 0.8397372603416443
  - 0.8035897195339203
  - 0.850805538892746
  - 0.8366718828678131
  - 0.7966406911611558
  - 0.8520744681358338
  - 0.8163248419761658
  - 0.8388105988502503
  - 0.8413466393947602
  - 0.8838293075561524
  - 0.8528660595417024
  - 0.8233247399330139
  - 0.837974089384079
  - 0.8748945832252503
  - 0.8514718174934388
  - 0.857742327451706
  - 0.8508135616779328
  - 0.8878165125846863
  - 0.8154384672641755
  validation_losses:
  - 0.40724295377731323
  - 0.4292810559272766
  - 9.079498291015625
  - 4.271917819976807
  - 4.343804359436035
  - 1.221358060836792
  - 0.7746502757072449
  - 0.5227759480476379
  - 0.43801552057266235
  - 0.4883883595466614
  - 0.42525988817214966
  - 0.40997833013534546
  - 0.4005524218082428
  - 0.43571385741233826
  - 0.44098666310310364
  - 0.40891173481941223
  - 0.4078231453895569
  - 0.45550256967544556
  - 0.40118643641471863
  - 0.4016452431678772
  - 0.46408647298812866
  - 0.41079023480415344
  - 0.4151018261909485
  - 0.4206145703792572
  - 0.41295379400253296
  - 0.408179372549057
  - 0.4306591749191284
  - 0.40396177768707275
  - 0.41135743260383606
  - 0.40892064571380615
  - 0.411090612411499
  - 0.4276672601699829
  - 0.4471479654312134
  - 0.4069821238517761
  - 0.41980236768722534
  - 0.39337489008903503
  - 0.40910544991493225
  - 0.4142528176307678
  - 0.3973155915737152
  - 0.3975997269153595
  - 0.4155440330505371
  - 0.4009791612625122
  - 0.39960741996765137
  - 0.40462571382522583
  - 0.3962853252887726
  - 0.4019584059715271
  - 0.41607901453971863
  - 0.42072057723999023
  - 0.40203574299812317
  - 0.4246410131454468
  - 0.4056852161884308
  - 0.40950852632522583
  - 0.4114479422569275
  - 0.4126250147819519
  - 0.4070989191532135
  - 0.41064316034317017
loss_records_fold4:
  train_losses:
  - 0.9245708525180817
  - 0.8232014536857606
  - 0.8731044054031373
  - 0.8211228489875794
  - 0.8301117360591889
  - 0.8050497651100159
  - 0.917397552728653
  - 1.0555832862854004
  - 0.9253429234027863
  - 1.0490366995334626
  - 0.9298754870891571
  - 1.1041658639907836
  - 0.9150468528270722
  - 0.901071572303772
  - 0.842997545003891
  - 0.864425015449524
  - 0.985264801979065
  - 0.8325439870357514
  - 0.8618346989154816
  validation_losses:
  - 0.4050978124141693
  - 0.40705329179763794
  - 0.422623872756958
  - 0.4119996428489685
  - 0.41514864563941956
  - 0.4081432521343231
  - 0.41581135988235474
  - 0.4524838626384735
  - 0.4117410480976105
  - 0.4244155287742615
  - 0.42906293272972107
  - 0.41604241728782654
  - 0.4529872536659241
  - 0.4287700653076172
  - 0.42466893792152405
  - 0.4119113087654114
  - 0.41830170154571533
  - 0.4046860635280609
  - 0.40301382541656494
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 56 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.855917667238422, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.0
  total_train_time: '0:24:01.362327'
