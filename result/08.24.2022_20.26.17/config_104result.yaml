config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:49:35.366449'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_104fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_104fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_104fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_104fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_104fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_104fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_104fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_104fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_104fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_104fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 66.61935180053115
  - 40.72717058062554
  - 42.47236040681601
  - 42.557958733290434
  - 22.757307639718057
  - 11.103625065088274
  - 17.851683795452118
  - 19.47910596728325
  - 28.709044682979584
  - 13.307004296779633
  - 14.743945607542992
  - 21.082869252562524
  - 29.032624164223673
  - 14.89834681749344
  - 12.059675547480584
  - 7.280695030093193
  - 8.574660414457322
  - 10.130966120958329
  - 6.594372510910034
  - 6.951028519868851
  - 7.040089184045792
  - 7.624552470445633
  - 7.1381663441658025
  - 8.10557780265808
  - 7.214938935637474
  - 8.591642874479295
  - 7.118159592151642
  - 6.963360625505448
  - 6.9379485785961155
  - 7.47126584649086
  - 6.396324090659618
  - 6.5226634532213215
  - 6.095567479729652
  - 6.797034707665444
  - 6.4814930945634845
  - 6.152291610836983
  - 5.932825732231141
  - 6.42369827926159
  - 5.954105997085572
  - 7.542394709587097
  - 6.8821495294570925
  - 6.406420364975929
  - 6.018189266324043
  - 5.944022974371911
  - 6.186459708213807
  - 5.813490946590901
  - 5.7269059956073765
  - 5.79096292257309
  - 6.115324437618256
  - 5.8846767306327825
  - 6.102331730723382
  - 6.1422121137380605
  - 5.812802731990814
  - 6.519530667364598
  - 5.922657704353333
  - 5.762319657206536
  - 5.787044423818589
  - 5.9955876767635345
  - 6.1149937734007835
  - 5.868596932291985
  - 5.756887239217758
  - 5.788392451405525
  - 5.74084801375866
  - 5.682825413346291
  - 5.704652664065361
  - 5.714225758612156
  - 5.7550892561674125
  - 5.798114743828774
  - 5.777338069677353
  - 5.658464133739471
  - 5.65211760699749
  - 5.877747219800949
  - 5.915028935670853
  - 5.955170188844204
  - 5.664309886097908
  - 6.023642678558827
  - 5.59844046831131
  - 5.8653316475451
  - 5.86125494837761
  - 5.757057255506516
  - 5.88456654548645
  - 5.910959941148758
  - 6.023293250799179
  - 5.725785496830941
  - 5.906181839108467
  - 5.769945941865444
  - 5.650814062356949
  - 5.726393458247185
  - 13.932380822300912
  - 7.423661717772484
  - 6.33870410323143
  - 6.661128917336464
  - 6.265873965620995
  - 6.169088807702065
  - 6.659142971038818
  - 6.03989172577858
  - 6.1055391311645515
  - 5.9873995095491415
  - 6.166307558119297
  - 6.117284750938416
  validation_losses:
  - 5.541080951690674
  - 1.3020232915878296
  - 0.9688180685043335
  - 1.2397741079330444
  - 0.4210815727710724
  - 0.9678683280944824
  - 2.431382656097412
  - 0.4954952299594879
  - 0.7520760893821716
  - 0.46830931305885315
  - 0.7995253205299377
  - 0.4557623863220215
  - 0.39648646116256714
  - 0.42434191703796387
  - 0.4243980944156647
  - 0.4169245958328247
  - 0.9900040626525879
  - 0.4047878384590149
  - 0.473713755607605
  - 0.3783884048461914
  - 0.39405712485313416
  - 0.44117656350135803
  - 0.386772096157074
  - 1.377257227897644
  - 0.38243818283081055
  - 0.4058534801006317
  - 0.4442434012889862
  - 0.4350368082523346
  - 0.4886036515235901
  - 0.39691901206970215
  - 0.38343751430511475
  - 0.3897961378097534
  - 0.5516726970672607
  - 0.5575668215751648
  - 0.38387390971183777
  - 0.37817272543907166
  - 2.62937331199646
  - 0.3850889205932617
  - 0.4233068525791168
  - 0.4226243495941162
  - 0.38362622261047363
  - 0.4067549705505371
  - 0.4022650420665741
  - 0.3952523469924927
  - 0.3840259909629822
  - 0.3882284462451935
  - 0.40034252405166626
  - 0.394784152507782
  - 0.38862940669059753
  - 0.40907225012779236
  - 0.3926176428794861
  - 0.7220879793167114
  - 0.3909021317958832
  - 0.38581523299217224
  - 0.9294949173927307
  - 1.4835894107818604
  - 0.47110605239868164
  - 1.0905768871307373
  - 1.6664540767669678
  - 7.14469575881958
  - 3.5330302715301514
  - 0.4057285487651825
  - 0.6577960252761841
  - 0.8484570384025574
  - 0.3903360068798065
  - 0.5130788087844849
  - 1.8903855085372925
  - 0.3887583017349243
  - 0.4047785699367523
  - 0.563200831413269
  - 0.5099233388900757
  - 0.551681637763977
  - 1.1206482648849487
  - 0.3849441409111023
  - 0.45651230216026306
  - 0.39904215931892395
  - 0.38765230774879456
  - 0.39830681681632996
  - 0.7616695165634155
  - 0.377673476934433
  - 0.3815497159957886
  - 0.8048554062843323
  - 0.3951612114906311
  - 0.4097713232040405
  - 0.6541007161140442
  - 0.38143855333328247
  - 17.856443405151367
  - 141.05230712890625
  - 0.3875330090522766
  - 0.4053216576576233
  - 0.3894668519496918
  - 0.4142298698425293
  - 0.3908621370792389
  - 0.4068501591682434
  - 0.3974537253379822
  - 0.3913639187812805
  - 0.42354199290275574
  - 0.6195411086082458
  - 0.46870163083076477
  - 0.44474324584007263
loss_records_fold1:
  train_losses:
  - 6.0120396465063095
  - 5.78996611237526
  - 5.886747916787863
  - 6.328243696689606
  - 6.070208832621574
  - 6.098060163855553
  - 6.002267521619797
  - 5.84769444167614
  - 5.97019296437502
  - 5.8936323642730715
  - 6.007180091738701
  - 6.075468695163727
  - 5.8788353443145756
  - 5.842519900202752
  - 5.794685709476472
  - 5.817163372039795
  - 5.890790334343911
  - 5.9391305476427085
  - 5.965631809830666
  - 5.823844987154008
  - 5.866542086005211
  - 5.985628536343575
  - 5.755915701389313
  - 5.805132627487183
  - 5.831245589256287
  - 5.8448656588792804
  - 5.898746263980866
  - 6.169229373335838
  - 5.973477786779404
  - 5.8553914636373525
  - 5.849452102184296
  - 5.898488152027131
  - 5.956170174479485
  - 6.274187286198139
  - 6.1405064702034
  - 6.348862209916115
  - 6.6167346611619
  - 5.859526732563973
  - 6.115707761049271
  validation_losses:
  - 0.40458011627197266
  - 0.4277782440185547
  - 0.4246663451194763
  - 0.40437233448028564
  - 0.44659551978111267
  - 0.4053211212158203
  - 0.4145398437976837
  - 0.4045678973197937
  - 0.418423593044281
  - 0.41452598571777344
  - 0.41959384083747864
  - 0.5420845150947571
  - 0.41712260246276855
  - 0.44997185468673706
  - 0.40902256965637207
  - 0.4038916826248169
  - 0.43470489978790283
  - 0.4038274884223938
  - 0.4467945396900177
  - 0.40669065713882446
  - 0.4096159040927887
  - 0.42622047662734985
  - 0.42353594303131104
  - 0.40404999256134033
  - 0.40523895621299744
  - 0.43243005871772766
  - 0.40514877438545227
  - 0.4451615512371063
  - 0.404930055141449
  - 0.4047349691390991
  - 0.40536731481552124
  - 0.42954474687576294
  - 0.5306934714317322
  - 0.4769384562969208
  - 0.41943708062171936
  - 0.4044915735721588
  - 0.4058862626552582
  - 0.412925660610199
  - 0.4180889427661896
loss_records_fold2:
  train_losses:
  - 5.9193576842546465
  - 6.132704797387124
  - 5.935388445854187
  - 5.896076104044915
  - 6.028290078043938
  - 5.955426451563835
  - 5.996353676915169
  - 6.032758450508118
  - 6.232397866249085
  - 5.931403967738152
  - 5.926521971821785
  - 5.9669723391532905
  - 5.938269498944283
  - 7.276216903328896
  - 5.988576030731202
  - 6.484390829503536
  - 5.918132889270783
  - 5.948512837290764
  - 5.823310428857804
  - 5.884596452116966
  - 6.682606002688408
  - 5.914800110459328
  - 6.495072281360627
  - 6.608789011836052
  - 7.647430714964867
  - 6.299953615665436
  - 6.30744141638279
  - 6.151842972636223
  - 6.011310130357742
  - 5.958593679964543
  - 5.9558728486299515
  - 5.856311574578285
  - 6.176173630356789
  - 5.957813379168511
  - 5.921661937236786
  - 6.037610709667206
  - 5.9560509651899345
  - 5.888688585162163
  - 6.110961758345366
  - 5.861830201745033
  - 6.046031564474106
  - 5.883758270740509
  - 6.128425788879395
  - 5.973512387275696
  - 5.940754845738411
  - 5.887389707565308
  - 5.900406214594842
  - 6.0683920055627825
  - 5.91745055615902
  - 6.154471457004547
  - 6.245120500028134
  - 6.379555204510689
  - 5.944597974419594
  - 6.023597374558449
  - 5.932650142908097
  - 6.0847878664731985
  - 6.212648093700409
  - 6.128256446123124
  - 6.034919692575932
  - 5.869561344385147
  - 5.991045090556145
  - 5.843833109736443
  - 6.031437781453133
  - 5.886089542508126
  - 5.980730792880059
  - 5.931511574983597
  - 5.945387959480286
  - 5.927801512181759
  - 5.922182509303093
  - 5.977758219838143
  - 6.15142026245594
  - 6.146786279976368
  - 6.275864356756211
  - 5.8571843802928925
  validation_losses:
  - 0.3939363956451416
  - 2.0009005069732666
  - 4.874042987823486
  - 0.6707264184951782
  - 2.922952890396118
  - 2.5214180946350098
  - 3.920851707458496
  - 1.1666102409362793
  - 2.245863199234009
  - 6.177448272705078
  - 1.5518993139266968
  - 3.106238603591919
  - 0.45505160093307495
  - 0.38443225622177124
  - 0.3827979862689972
  - 3.749626874923706
  - 1.8133538961410522
  - 1.4870178699493408
  - 1.2498823404312134
  - 0.3942839205265045
  - 9.250018119812012
  - 2.6381094455718994
  - 33288.4609375
  - 0.40803226828575134
  - 0.40921905636787415
  - 0.3877013325691223
  - 0.3992815613746643
  - 1.3059086799621582
  - 0.49057266116142273
  - 3.635789632797241
  - 1.1092784404754639
  - 2.1326968669891357
  - 0.8608632683753967
  - 1.8226838111877441
  - 0.763541579246521
  - 1.429146647453308
  - 2.5874271392822266
  - 0.45746132731437683
  - 0.9842124581336975
  - 2.081404447555542
  - 0.8905860781669617
  - 4.47735595703125
  - 1.1620006561279297
  - 0.7967024445533752
  - 15.714402198791504
  - 12.067879676818848
  - 5.5874810218811035
  - 0.5104541182518005
  - 7.471304893493652
  - 11.032505989074707
  - 1.3218601942062378
  - 2333.00048828125
  - 683.81201171875
  - 0.38815584778785706
  - 0.38433530926704407
  - 0.39049530029296875
  - 0.3836768865585327
  - 0.40755629539489746
  - 0.383906751871109
  - 0.38824155926704407
  - 0.4102921485900879
  - 0.38539791107177734
  - 0.38920241594314575
  - 0.38438844680786133
  - 0.42858603596687317
  - 0.38573533296585083
  - 0.3892948627471924
  - 0.40933990478515625
  - 0.38573163747787476
  - 0.39268621802330017
  - 0.3884758949279785
  - 0.3854788839817047
  - 0.3833613693714142
  - 0.38503339886665344
loss_records_fold3:
  train_losses:
  - 5.900727951526642
  - 5.7856750160455706
  - 5.90244851410389
  - 5.929307314753533
  - 5.949380394816399
  - 5.973526212573052
  - 6.074230661988259
  - 5.990550771355629
  - 5.960900244116783
  - 5.968725842237473
  - 5.872293251752854
  - 6.142199531197548
  - 5.990780808031559
  - 5.840219941735268
  - 5.944304186105729
  - 5.817295381426812
  - 5.883917842805386
  - 5.875316235423089
  - 5.907796257734299
  - 5.8229584962129595
  - 5.865714776515961
  - 6.267927706241608
  - 5.937839907407761
  - 5.935405153036118
  - 6.060902509093285
  - 5.880952316522599
  validation_losses:
  - 0.39885348081588745
  - 0.4302080571651459
  - 0.3995693325996399
  - 0.4220576882362366
  - 0.399308443069458
  - 0.398479700088501
  - 0.524086058139801
  - 0.5271373391151428
  - 0.40798214077949524
  - 66667868.0
  - 1454897488723968.0
  - 0.43729186058044434
  - 0.4219273328781128
  - 0.4019644260406494
  - 0.40301477909088135
  - 0.420740008354187
  - 0.4016087055206299
  - 0.39771193265914917
  - 0.3984103500843048
  - 0.4089699387550354
  - 0.39850571751594543
  - 0.3982284963130951
  - 0.40819478034973145
  - 0.41747593879699707
  - 0.4034968614578247
  - 0.3997732102870941
loss_records_fold4:
  train_losses:
  - 5.956785482168198
  - 6.0447056367993355
  - 5.955133274197578
  - 6.053738391399384
  - 5.8773176908493046
  - 6.037446027994156
  - 6.015719184279442
  - 6.083070307970047
  - 5.874471560120583
  - 6.141526824235917
  - 5.974806216359139
  - 6.046143707633019
  - 5.9939048290252686
  - 5.985126429796219
  - 5.94437983930111
  - 5.9107449710369115
  - 5.885686677694321
  - 6.289251574873925
  - 5.865261426568032
  - 5.995552636682987
  - 5.945054215192795
  - 6.179665842652321
  - 5.982951724529267
  - 5.868324935436249
  - 5.9263782680034645
  - 5.910180205106736
  - 5.899296474456787
  - 5.883432397246361
  - 6.080641832947731
  - 6.117249834537507
  - 6.245357050001622
  - 6.027476584911347
  - 6.026962888240814
  - 5.984554144740105
  - 6.082291516661645
  - 6.170972886681557
  - 6.182861125469208
  - 5.952116513252259
  - 5.9178423374891285
  - 5.905639445781708
  - 6.012311652302742
  - 5.916132345795631
  - 5.980395948886872
  - 6.002964267134667
  validation_losses:
  - 0.39224836230278015
  - 0.443709135055542
  - 2885740288.0
  - 0.39154645800590515
  - 0.40006688237190247
  - 0.43261730670928955
  - 0.39951291680336
  - 0.39156094193458557
  - 0.40857309103012085
  - 0.4179588854312897
  - 0.5110141038894653
  - 0.4025924801826477
  - 5.118975669593702e+16
  - 0.40654072165489197
  - 0.39105963706970215
  - 1.7668182913738342e+17
  - 0.4036528468132019
  - 0.3916950225830078
  - 0.39599692821502686
  - 0.39277884364128113
  - 0.39153286814689636
  - 1035503616.0
  - 798009084346368.0
  - 0.4033029079437256
  - 0.3924257159233093
  - 0.392013281583786
  - 0.4100896120071411
  - 0.45975229144096375
  - 0.40456295013427734
  - 0.41032999753952026
  - 0.42820146679878235
  - 0.3935624659061432
  - 0.3918178379535675
  - 0.39234238862991333
  - 0.4513069987297058
  - 0.3926641345024109
  - 0.40009915828704834
  - 2934995161186304.0
  - 0.39768582582473755
  - 0.39117631316185
  - 0.39391401410102844
  - 0.3910619616508484
  - 0.40051618218421936
  - 0.3910601735115051
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 74 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 44 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:31:50.867907'
