config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 22:52:36.303416'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/87/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/87/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/87/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/87/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/87/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/87/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/87/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/87/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/87/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/87/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 244.55585038661957
  - 26.695546627044678
  - 105.9464413523674
  - 11.54878842830658
  - 9.128031551837921
  - 7.768888473510742
  - 6.778301119804382
  - 5.380706191062927
  - 4.9430734515190125
  - 5.864393830299377
  - 11.564331114292145
  - 6.075881481170654
  - 4.637202829122543
  - 5.13396093249321
  - 4.332134336233139
  - 4.666447788476944
  - 5.255910515785217
  - 6.37078931927681
  - 4.864296555519104
  - 4.452504426240921
  - 4.818865478038788
  - 5.511860728263855
  - 23.93837535381317
  - 128.11576110124588
  - 7.523634076118469
  - 4.636178880929947
  - 3.8088318407535553
  - 4.354936927556992
  - 4.454646438360214
  - 4.0814719796180725
  - 4.058711886405945
  - 4.1478942930698395
  - 5.013533592224121
  - 5.369287669658661
  - 4.654456853866577
  - 5.970916509628296
  - 5.634605824947357
  - 6.160397112369537
  - 5.283313825726509
  - 5.223193883895874
  - 4.355568140745163
  - 4.693099290132523
  - 89.15039044618607
  - 6.66755485534668
  - 4.671312123537064
  - 5.524012625217438
  - 4.351724237203598
  - 4.221361607313156
  - 5.091095685958862
  - 4.216934353113174
  - 4.5382503271102905
  - 4.6682465970516205
  - 4.297216534614563
  - 5.400060623884201
  - 4.950240612030029
  - 8.933668851852417
  - 6.369588643312454
  - 5.235248684883118
  - 3.87903168797493
  - 3.9467047452926636
  - 3.906260848045349
  - 3.923701524734497
  - 4.013301610946655
  - 5.332759737968445
  - 4.483809411525726
  - 4.523983359336853
  - 5.298108369112015
  - 5.37528058886528
  - 4.247189402580261
  - 4.034769386053085
  - 4.4570096135139465
  - 4.006228923797607
  - 6.850754946470261
  - 8.22661617398262
  - 4.166002035140991
  - 3.967950254678726
  - 3.930026561021805
  - 3.9264678359031677
  - 3.9385966062545776
  - 3.844766527414322
  - 4.0356185138225555
  - 3.955274820327759
  - 4.2149830758571625
  - 5.588613003492355
  - 9.07825192809105
  - 9.321856498718262
  - 8.658723831176758
  - 6.330578476190567
  - 4.173975914716721
  - 4.135315418243408
  - 3.977986752986908
  - 3.9138348400592804
  - 9.810492932796478
  - 8.009855538606644
  - 9.125544548034668
  - 5.526877701282501
  - 6.695552080869675
  - 6.328190624713898
  - 4.503112509846687
  - 4.931010395288467
  validation_losses:
  - 52.87194061279297
  - 32.99329376220703
  - 14.415308952331543
  - 9.911932945251465
  - 5.7388787269592285
  - 9.370283126831055
  - 3.4853756427764893
  - 0.5186672210693359
  - 0.42287951707839966
  - 0.43206775188446045
  - 0.38525086641311646
  - 0.4024196267127991
  - 0.4652445316314697
  - 0.5085567831993103
  - 0.4948500394821167
  - 0.5613839626312256
  - 0.6188125610351562
  - 0.4346010088920593
  - 0.43912869691848755
  - 0.42121797800064087
  - 0.6891676783561707
  - 0.4531469941139221
  - 1.0471655130386353
  - 0.7942091822624207
  - 0.7011546492576599
  - 0.3907893896102905
  - 0.40408506989479065
  - 0.40063682198524475
  - 0.4138869345188141
  - 0.4117276668548584
  - 0.4033720791339874
  - 0.6105894446372986
  - 0.38959836959838867
  - 0.46273788809776306
  - 0.4554939270019531
  - 0.4482165575027466
  - 0.391249418258667
  - 0.7337021231651306
  - 2.8230350017547607
  - 0.4323868155479431
  - 0.4297953248023987
  - 0.46033209562301636
  - 4.688085556030273
  - 46.3112907409668
  - 11.477325439453125
  - 0.38764962553977966
  - 0.3949815332889557
  - 0.4926990270614624
  - 0.39409083127975464
  - 0.4584468901157379
  - 0.468973845243454
  - 0.3892677128314972
  - 0.6870034337043762
  - 0.6975074410438538
  - 1.0857620239257812
  - 0.7058098316192627
  - 0.5893555879592896
  - 0.39068150520324707
  - 0.41059738397598267
  - 0.3968594968318939
  - 0.3916104733943939
  - 0.424064964056015
  - 0.42044779658317566
  - 0.41079023480415344
  - 0.5243971943855286
  - 0.5583156943321228
  - 0.43179962038993835
  - 0.476800799369812
  - 0.42728787660598755
  - 0.41541779041290283
  - 0.38973286747932434
  - 0.4033931791782379
  - 0.8789349794387817
  - 0.41501009464263916
  - 0.4093623459339142
  - 0.41084232926368713
  - 1.4509207010269165
  - 1.6118667125701904
  - 3.6201868057250977
  - 0.392022967338562
  - 0.3907124996185303
  - 0.3917665183544159
  - 0.5801341533660889
  - 1.0246899127960205
  - 0.7980952262878418
  - 0.5888456702232361
  - 0.44978466629981995
  - 0.4087906777858734
  - 0.4129575490951538
  - 0.45155245065689087
  - 0.40554705262184143
  - 0.4318349063396454
  - 11.629528045654297
  - 0.9079887270927429
  - 0.7035136222839355
  - 0.8493106365203857
  - 0.6262628436088562
  - 0.38892650604248047
  - 0.44739198684692383
  - 0.3956282436847687
loss_records_fold1:
  train_losses:
  - 11.9799445271492
  - 7.386960417032242
  - 5.932786375284195
  - 5.54357248544693
  - 4.458363026380539
  - 4.1592018604278564
  - 4.988361716270447
  - 4.144024044275284
  - 3.884062275290489
  - 4.481276780366898
  - 3.8767439872026443
  - 14.593619674444199
  - 34.50833302736282
  - 61.11313039064407
  - 5.515212297439575
  - 6.022369831800461
  - 12.495193392038345
  - 7.150908589363098
  - 6.41999477148056
  - 8.688049405813217
  - 5.371938854455948
  - 5.271512091159821
  - 5.027915984392166
  - 162.34621381759644
  - 31.467924937605858
  - 4.5629677176475525
  - 3.8151498436927795
  - 4.347759693861008
  - 4.718590706586838
  - 4.225423365831375
  - 4.205977916717529
  - 4.335880011320114
  - 4.071602523326874
  - 4.12677189707756
  - 3.8423474431037903
  - 4.167286813259125
  - 5.600582629442215
  - 7.4003556072711945
  - 7.905292093753815
  - 6.963002055883408
  - 4.097203895449638
  - 5.321288570761681
  - 5.186715126037598
  - 4.181816875934601
  - 3.997705176472664
  - 3.8443017303943634
  - 4.2452011704444885
  - 4.231842577457428
  - 4.415702015161514
  - 4.031167954206467
  - 4.221719175577164
  - 4.530948519706726
  - 3.9875194132328033
  - 5.552222013473511
  - 16.9619177877903
  - 54.03445562720299
  - 87.73592948913574
  - 82.49015784263611
  - 30.369595289230347
  - 87.95416349172592
  - 11.905459523200989
  - 9.733609080314636
  - 8.48245856165886
  - 7.747326582670212
  - 8.58680573105812
  - 14.4036246240139
  - 4.387345403432846
  - 5.418427795171738
  - 6.064428389072418
  - 7.8995329439640045
  - 8.246810555458069
  - 7.099114269018173
  - 10.815832674503326
  - 204.7057044506073
  - 32.38992038369179
  - 83.69485878944397
  - 51.56119513511658
  - 10.531932562589645
  - 13.22014856338501
  - 8.74171131849289
  - 6.391470909118652
  - 4.903724372386932
  - 30.94828200340271
  - 32.07428973913193
  - 9.594181060791016
  - 5.1178030371665955
  - 38.3632073700428
  - 4.908861696720123
  - 4.1146500408649445
  - 3.780043914914131
  - 4.187488853931427
  - 4.482447266578674
  - 184.36967927217484
  - 4.601800203323364
  - 5.373336762189865
  - 12.977683395147324
  - 7.386023253202438
  - 4.317100241780281
  - 7.892588645219803
  - 47.88045385479927
  validation_losses:
  - 0.7068188786506653
  - 0.8218111395835876
  - 0.43090203404426575
  - 0.4086840748786926
  - 0.4231507182121277
  - 0.4184894561767578
  - 0.42551717162132263
  - 0.4223613142967224
  - 0.46232739090919495
  - 0.46699538826942444
  - 0.5559645295143127
  - 7.187629222869873
  - 0.4366888999938965
  - 0.45848092436790466
  - 0.4074161648750305
  - 55.826473236083984
  - 0.8019458651542664
  - 1.1622788906097412
  - 0.8844751119613647
  - 2.2860448360443115
  - 0.5570355653762817
  - 0.42013463377952576
  - 0.45639264583587646
  - 18.082460403442383
  - 109.4516372680664
  - 168.3667449951172
  - 215.1284637451172
  - 3401.55810546875
  - 35424.03515625
  - 252877.1875
  - 1486337.75
  - 10141582.0
  - 29999446.0
  - 0.4129730463027954
  - 0.42440125346183777
  - 0.5445099472999573
  - 1.233561396598816
  - 1.8023717403411865
  - 6.028092861175537
  - 9.2527056700777e+21
  - 3.59902930435898e+23
  - 3.991419148865299e+24
  - 1.8792599772504523e+25
  - 4.71936196942864e+25
  - 8.385695242758385e+25
  - 1.2004441911569783e+26
  - 16915794.0
  - 3779961856.0
  - 150499426304.0
  - 4037326405632.0
  - 90737265868800.0
  - 1910703040495616.0
  - 469192114176.0
  - 412966.0625
  - 110.69708251953125
  - 3108.61376953125
  - 62.30579376220703
  - 5.459321022033691
  - 2.272989511489868
  - 76.95858001708984
  - 11.483505249023438
  - 0.6993764638900757
  - 16.432680130004883
  - 1.9045994281768799
  - 0.8838474154472351
  - 0.4396783709526062
  - 14.414652824401855
  - 1.2142068147659302
  - 0.49949371814727783
  - 48.8051643371582
  - 3.920686960220337
  - 0.43685007095336914
  - 0.4382263720035553
  - 0.4043290317058563
  - 156.99899291992188
  - 511.9088439941406
  - 86.46086120605469
  - 28.562238693237305
  - 0.9396158456802368
  - 27.20679473876953
  - 2.8971633911132812
  - 0.41310280561447144
  - 0.40543535351753235
  - 0.4096736013889313
  - 0.4394755959510803
  - 0.42682284116744995
  - 0.4603940546512604
  - 0.4112814664840698
  - 0.4144369959831238
  - 0.4360809922218323
  - 0.5137484073638916
  - 0.4064141809940338
  - 0.4053875803947449
  - 0.4055747389793396
  - 0.42527875304222107
  - 0.4051388204097748
  - 0.40322786569595337
  - 0.43262070417404175
  - 0.42607685923576355
  - 0.43861687183380127
loss_records_fold3:
  train_losses:
  - 34.38809987902641
  - 16.44440385699272
  - 7.12517973780632
  - 12.668973803520203
  - 5.146714687347412
  - 32.999753683805466
  - 17.064331203699112
  - 9.489296898245811
  - 4.4678893983364105
  - 4.7252249121665955
  - 5.04231858253479
  - 7.0711807906627655
  - 4.362941950559616
  - 4.5876025557518005
  - 4.009536862373352
  - 4.202868014574051
  - 5.615053743124008
  - 9.28685250878334
  - 4.5869777500629425
  - 5.401590198278427
  - 4.6596634685993195
  - 21.4357787668705
  - 5.260731130838394
  - 4.513282388448715
  - 18.073286309838295
  - 9.028221786022186
  - 726.2449060976505
  - 6.46383199095726
  - 4.553301751613617
  - 4.068406939506531
  - 4.332223504781723
  - 7.036436378955841
  - 4.000211983919144
  - 3.946515738964081
  - 22.42248809337616
  - 13.942867159843445
  - 4.551171183586121
  - 3.798333317041397
  - 4.320249170064926
  - 4.264088869094849
  - 3.8084668070077896
  - 4.067953199148178
  - 3.99756520986557
  - 3.957418590784073
  - 4.6776509284973145
  - 8.560027360916138
  - 4.590360164642334
  - 4.173229515552521
  - 4.249879151582718
  - 4.159459322690964
  - 5.302467614412308
  - 4.955696910619736
  - 5.966629087924957
  - 7.045556128025055
  - 7.169338524341583
  - 5.413285493850708
  - 18.644231766462326
  - 11.079771429300308
  - 47.02373602986336
  - 11.027420997619629
  - 8.900445282459259
  - 4.597167611122131
  - 4.288632586598396
  - 4.555422976613045
  - 54.60217899084091
  - 8.911811709403992
  - 4.848818898200989
  - 4.324606314301491
  - 6.344341069459915
  - 5.665667966008186
  - 4.2732768058776855
  - 12.940294235944748
  - 4.143304765224457
  - 5.541187912225723
  - 6.724373489618301
  - 10.679933190345764
  - 11.930343329906464
  - 5.524024218320847
  - 6.513038009405136
  - 11.13926649093628
  - 5.649203389883041
  - 30.186901837587357
  - 15.591776549816132
  - 4.002568930387497
  - 4.622526258230209
  - 4.874331206083298
  - 164.27729314565659
  - 17.397549718618393
  - 6.056575119495392
  - 6.348208159208298
  - 7.69582986831665
  - 71.53438803553581
  - 17.56994616985321
  - 13.07722944021225
  - 71.55049231648445
  - 59.71772938966751
  - 10.169426321983337
  - 27.426350980997086
  - 9.334915339946747
  - 9.189557120203972
  validation_losses:
  - 0.47988107800483704
  - 0.529960572719574
  - 0.5065726637840271
  - 0.3948894143104553
  - 0.41618043184280396
  - 0.46931126713752747
  - 0.5484235882759094
  - 0.4765606224536896
  - 0.43096596002578735
  - 0.5292850732803345
  - 0.4477560222148895
  - 0.46094372868537903
  - 0.4480537474155426
  - 0.44278618693351746
  - 0.4016057550907135
  - 0.442943811416626
  - 0.5046236515045166
  - 5.05874490737915
  - 0.4556403160095215
  - 0.41605034470558167
  - 0.39884692430496216
  - 23.988252639770508
  - 80.01292419433594
  - 64.95757293701172
  - 18.580068588256836
  - 0.432247519493103
  - 28.717390060424805
  - 68.45048522949219
  - 26.803003311157227
  - 0.44726234674453735
  - 0.4040272533893585
  - 0.3930838406085968
  - 0.3934303820133209
  - 0.42774492502212524
  - 0.49501532316207886
  - 0.6390359997749329
  - 0.39773064851760864
  - 0.5281575322151184
  - 0.4777465760707855
  - 0.3968738913536072
  - 0.4376757740974426
  - 0.4045611023902893
  - 0.40137001872062683
  - 0.39896294474601746
  - 0.4316898286342621
  - 0.5827915668487549
  - 0.48209086060523987
  - 0.3967995047569275
  - 0.3974383771419525
  - 0.5552275776863098
  - 0.6710497140884399
  - 0.5917506217956543
  - 0.8437920212745667
  - 0.4466964304447174
  - 0.43088987469673157
  - 0.5518200397491455
  - 0.4038650095462799
  - 0.41405990719795227
  - 0.39262011647224426
  - 0.39277154207229614
  - 0.5760967135429382
  - 0.4683602452278137
  - 0.4610041081905365
  - 0.39303484559059143
  - 0.3947503864765167
  - 0.4321514666080475
  - 0.42562541365623474
  - 0.3993971347808838
  - 0.4228518307209015
  - 0.4282708764076233
  - 0.43121784925460815
  - 0.4142817556858063
  - 0.4073799252510071
  - 0.40049123764038086
  - 0.9217637181282043
  - 1.4807145595550537
  - 0.5787542462348938
  - 0.39132386445999146
  - 0.8197879195213318
  - 40.88325500488281
  - 12.187105178833008
  - 1.4906022548675537
  - 13.338662147521973
  - 0.45996415615081787
  - 0.39762452244758606
  - 0.4646083116531372
  - 0.6374439001083374
  - 0.49190956354141235
  - 0.41880300641059875
  - 0.3996817171573639
  - 0.8176169991493225
  - 0.42843884229660034
  - 1.054160714149475
  - 0.5527516007423401
  - 0.5283244848251343
  - 0.8461532592773438
  - 113.23458099365234
  - 0.7566836476325989
  - 0.4618380069732666
  - 0.5243067145347595
loss_records_fold4:
  train_losses:
  - 5.837248772382736
  - 4.99910244345665
  - 4.881660342216492
  - 6.427532583475113
  - 5.476545333862305
  - 4.331268489360809
  - 3.998788207769394
  - 4.509134024381638
  - 4.902094542980194
  - 4.505614280700684
  - 4.390883684158325
  - 5.0619954615831375
  - 149.25923764705658
  - 6.573265939950943
  - 14.344054698944092
  - 4.444877028465271
  - 4.364540010690689
  - 6.716143876314163
  - 5.257650122046471
  - 5.504107266664505
  - 3.8911424428224564
  - 5.0404330641031265
  - 10.785684078931808
  - 8.188773453235626
  - 15.094588816165924
  - 5.416157633066177
  - 11.679455816745758
  - 143.42614850401878
  - 4.547873079776764
  - 4.653385639190674
  - 5.064345896244049
  - 41.13114619255066
  - 17.920717239379883
  - 4.860358119010925
  - 5.568970322608948
  - 12.415829509496689
  - 4.456470340490341
  - 5.015874773263931
  - 4.326102703809738
  - 4.597420483827591
  - 3.9409965574741364
  - 5.114530593156815
  - 5.632314831018448
  - 4.099285364151001
  - 11.061729848384857
  - 6.048019826412201
  - 4.0279170870780945
  - 4.61322757601738
  - 4.494034767150879
  - 6.286695122718811
  - 6.709626615047455
  - 6.3052666783332825
  - 5.964252829551697
  - 5.254816353321075
  - 4.303540214896202
  - 4.871922701597214
  - 4.050741940736771
  - 4.483039408922195
  - 4.175877571105957
  - 4.02837809920311
  - 7.632668346166611
  - 4.039874792098999
  - 3.837935894727707
  - 5.424317270517349
  - 6.065074294805527
  - 4.762850791215897
  - 5.24862477183342
  - 4.854183793067932
  - 9.420155823230743
  - 4.838977754116058
  - 5.301379203796387
  - 5.320162117481232
  - 6.0850458443164825
  - 6.830581605434418
  - 4.942766845226288
  - 4.091484755277634
  - 5.148327350616455
  - 7.262185990810394
  - 17.55963909626007
  - 8.075397819280624
  - 10.278272926807404
  - 6.476725935935974
  - 5.586431413888931
  - 5.972066283226013
  - 5.544853895902634
  - 5.582240581512451
  - 4.250921234488487
  - 4.615670680999756
  - 5.304287314414978
  - 4.804847240447998
  - 17.00999653339386
  - 7.936816304922104
  - 5.116898640990257
  - 3.8968783020973206
  - 14.626399844884872
  - 5.099487453699112
  - 4.472925513982773
  - 4.088592916727066
  - 8.89625659584999
  - 4.247227311134338
  validation_losses:
  - 0.7915212512016296
  - 0.8400164842605591
  - 1.0327991247177124
  - 0.9973419308662415
  - 1.1017478704452515
  - 1.008678913116455
  - 0.9578068852424622
  - 1.0300920009613037
  - 0.9569154977798462
  - 1.0152767896652222
  - 1.0312880277633667
  - 1.1917169094085693
  - 1.2422711849212646
  - 0.8323463797569275
  - 0.7967715859413147
  - 276.8407287597656
  - 57.195701599121094
  - 4.753087997436523
  - 1.5527950525283813
  - 1.7106878757476807
  - 1.723327398300171
  - 1.8704544305801392
  - 1.8675141334533691
  - 1.7038309574127197
  - 1.444262146949768
  - 1.3287770748138428
  - 0.9912835359573364
  - 1.2485260963439941
  - 1.0560109615325928
  - 0.9229520559310913
  - 1.0207979679107666
  - 0.8760700821876526
  - 0.826215922832489
  - 1.1836087703704834
  - 0.9372533559799194
  - 0.9808573126792908
  - 0.6749598979949951
  - 0.8292648792266846
  - 0.8879880309104919
  - 0.7264150381088257
  - 0.8419846892356873
  - 0.8477767705917358
  - 0.7472826838493347
  - 0.810076117515564
  - 0.9144983291625977
  - 0.4866997003555298
  - 0.5857040286064148
  - 0.6669667363166809
  - 0.8046795129776001
  - 1.2303978204727173
  - 0.7449734210968018
  - 0.6098155975341797
  - 0.7429812550544739
  - 0.606635570526123
  - 0.6043891906738281
  - 0.573900043964386
  - 0.5298323631286621
  - 0.5657123327255249
  - 0.5099001526832581
  - 0.534610390663147
  - 2.762908458709717
  - 0.43417003750801086
  - 0.43390172719955444
  - 0.6291763782501221
  - 0.46415770053863525
  - 0.5622391104698181
  - 0.7196608781814575
  - 0.543677031993866
  - 0.47211286425590515
  - 0.5524989366531372
  - 0.7599536776542664
  - 0.696867823600769
  - 0.7566457986831665
  - 0.7349093556404114
  - 0.5442211627960205
  - 0.5783449411392212
  - 0.8965488076210022
  - 0.8464040160179138
  - 0.8450646996498108
  - 1.6284992694854736
  - 1.0582212209701538
  - 0.5594828724861145
  - 0.6347812414169312
  - 0.796937108039856
  - 0.7048165798187256
  - 0.5308648347854614
  - 0.5685205459594727
  - 0.7886355519294739
  - 0.600935697555542
  - 0.5120542049407959
  - 0.9132657051086426
  - 0.7721913456916809
  - 0.5669436454772949
  - 0.5774993300437927
  - 0.5858088135719299
  - 0.5017791390419006
  - 0.48179322481155396
  - 0.4135904014110565
  - 0.4410744905471802
  - 2.6411774158477783
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8470790378006873]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.856208260390326
  mean_f1_accuracy: 0.0
  total_train_time: '0:04:02.810791'
