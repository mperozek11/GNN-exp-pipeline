config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:51:47.178977'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_20fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_20fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_20fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_20fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_20fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_20fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_20fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_20fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_20fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_20fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.909565755724907
  - 6.323417776823044
  - 6.3287253290414816
  - 6.481761568784714
  - 6.279763573408127
  - 6.153232976794243
  - 6.2929260522127155
  - 6.210566741228104
  - 6.289125120639802
  - 6.0828599199652675
  - 6.252540621161462
  - 6.176948747038842
  - 6.05467629134655
  - 5.884440493583679
  - 6.098745080828667
  - 5.964457604289056
  - 5.970683765411377
  - 6.074690410494805
  - 5.913489452004433
  - 5.963595947623253
  - 6.140368965268135
  - 6.112121504545212
  - 6.02751624584198
  - 6.076734030246735
  - 6.042759680747986
  validation_losses:
  - 0.5537927150726318
  - 0.39353060722351074
  - 0.3960924744606018
  - 0.41587555408477783
  - 0.434444397687912
  - 0.38482651114463806
  - 0.40251481533050537
  - 0.4156503677368164
  - 0.3939524292945862
  - 0.4432534873485565
  - 0.38265761733055115
  - 0.39604097604751587
  - 0.41032230854034424
  - 0.3940470218658447
  - 0.40829673409461975
  - 0.3985461890697479
  - 0.39352157711982727
  - 0.3915032744407654
  - 0.41465306282043457
  - 0.4046512246131897
  - 0.4108588695526123
  - 0.4087948501110077
  - 0.3967870771884918
  - 0.39774611592292786
  - 0.3948821723461151
loss_records_fold1:
  train_losses:
  - 6.014488786458969
  - 6.123807528614998
  - 6.035691836476326
  - 5.986166289448739
  - 6.044287109375
  - 6.172668635845184
  - 6.124098551273346
  - 6.068527114391327
  - 6.120655703544617
  - 5.917612671852112
  - 6.020872461795808
  - 6.0268117696046835
  validation_losses:
  - 0.3881867825984955
  - 0.3918449282646179
  - 0.38964369893074036
  - 0.3901960849761963
  - 0.3860127627849579
  - 0.39835411310195923
  - 0.3940751850605011
  - 0.38457244634628296
  - 0.3911663591861725
  - 0.3970220386981964
  - 0.40062007308006287
  - 0.39768803119659424
loss_records_fold2:
  train_losses:
  - 6.0114120811223986
  - 6.195893317461014
  - 5.945771065354347
  - 5.891798305511475
  - 6.130813997983933
  - 6.0248824775218965
  - 5.9961650878191
  - 6.160248202085495
  - 6.168250659108162
  - 5.964142316579819
  - 5.953293007612229
  - 5.963544166088105
  - 5.944897189736366
  - 5.982973757386208
  - 5.89639841914177
  - 6.0200764596462255
  - 6.090168535709381
  validation_losses:
  - 1.1903895139694214
  - 0.3844825029373169
  - 0.39276495575904846
  - 0.4190814793109894
  - 0.3945550322532654
  - 0.3954603374004364
  - 0.39679431915283203
  - 0.39784979820251465
  - 0.4004209041595459
  - 0.40002819895744324
  - 0.4119530916213989
  - 0.39506447315216064
  - 0.39523404836654663
  - 0.402799129486084
  - 0.39356228709220886
  - 0.40123942494392395
  - 0.39800041913986206
loss_records_fold3:
  train_losses:
  - 6.024029928445817
  - 6.106749269366265
  - 6.029059931635857
  - 6.150334027409554
  - 6.161980924010277
  - 6.054095983505249
  - 6.373813360929489
  - 6.054165810346603
  - 6.088584709167481
  - 6.093597909808159
  - 6.004858136177063
  - 6.10682812333107
  - 5.948366892337799
  - 6.079526764154434
  - 6.015260124206543
  - 6.063863703608513
  - 5.929985347390176
  - 5.91347645521164
  - 6.022746786475182
  - 5.948548817634583
  - 5.818942758440972
  validation_losses:
  - 0.3878416419029236
  - 0.38069936633110046
  - 0.38621026277542114
  - 0.3860344886779785
  - 0.39753052592277527
  - 0.39174437522888184
  - 0.39572814106941223
  - 0.3811955451965332
  - 0.37566736340522766
  - 0.38698244094848633
  - 0.3864035904407501
  - 0.38232678174972534
  - 0.38961711525917053
  - 0.39549723267555237
  - 0.43682482838630676
  - 0.39024487137794495
  - 0.3843350410461426
  - 0.3911086916923523
  - 0.3866724967956543
  - 0.38729843497276306
  - 0.37551170587539673
loss_records_fold4:
  train_losses:
  - 6.035270872712136
  - 6.085658311843872
  - 6.030705511569977
  - 5.973942443728447
  - 6.060545095801354
  - 5.939619779586792
  - 6.001491221785546
  - 5.831775718927384
  - 6.0983438998460775
  - 5.942642146348954
  - 5.985103788971902
  - 6.016714996099473
  validation_losses:
  - 0.3752659559249878
  - 0.3761661946773529
  - 0.39078330993652344
  - 0.37982144951820374
  - 0.39799928665161133
  - 0.4097229838371277
  - 0.3829249143600464
  - 0.3864248991012573
  - 0.38622403144836426
  - 0.38435348868370056
  - 0.38661807775497437
  - 0.3908970355987549
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:08:45.612594'
