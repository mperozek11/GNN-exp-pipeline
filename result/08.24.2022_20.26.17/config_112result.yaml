config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:58:56.568960'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_112fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_112fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_112fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_112fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_112fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_112fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_112fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_112fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_112fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_112fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.371512252092362
  - 6.148218253254891
  - 6.020228338241577
  - 6.439116132259369
  - 6.244912365078926
  - 5.870672515034676
  - 6.026674753427506
  - 5.879447653889656
  - 5.990463587641717
  - 6.3862650245428085
  - 5.7408493041992195
  - 5.649474003911019
  - 5.744771033525467
  - 5.603577148914337
  - 5.681683522462845
  - 5.6194759726524355
  validation_losses:
  - 0.4006906747817993
  - 0.44463077187538147
  - 0.39006951451301575
  - 0.411814421415329
  - 0.38879621028900146
  - 0.40213802456855774
  - 0.40238404273986816
  - 0.3929656445980072
  - 0.3903983533382416
  - 0.40198805928230286
  - 0.4058874547481537
  - 0.3937602639198303
  - 0.38799071311950684
  - 0.38331151008605957
  - 0.38572049140930176
  - 0.38605836033821106
loss_records_fold1:
  train_losses:
  - 5.59800888299942
  - 5.664453086256981
  - 5.588543653488159
  - 5.835538956522942
  - 5.649131724238396
  - 5.624573311209679
  - 5.691647994518281
  - 5.639164546132088
  - 5.782903647422791
  - 5.585938414931298
  - 5.62751692533493
  - 5.555167922377587
  - 5.586567142605782
  - 5.574175903201104
  - 5.854896992444992
  validation_losses:
  - 0.3860951066017151
  - 0.39022138714790344
  - 0.39933860301971436
  - 0.3977194130420685
  - 0.3878936469554901
  - 0.3923768699169159
  - 0.3924127519130707
  - 0.38757190108299255
  - 0.40336838364601135
  - 0.39620715379714966
  - 0.3891606032848358
  - 0.3904772996902466
  - 0.3980127274990082
  - 0.39158570766448975
  - 0.3911755383014679
loss_records_fold2:
  train_losses:
  - 5.65812526345253
  - 5.619349366426468
  - 5.563474974036217
  - 5.59656889140606
  - 5.629733093082905
  - 5.59984005689621
  - 5.5800844758749015
  - 5.624682568013668
  - 5.558866119384766
  - 5.612890413403512
  - 5.695279356837273
  - 5.525155541300774
  - 5.59251746237278
  - 5.640820690989495
  - 5.6977564811706545
  - 5.702908876538277
  - 5.533582505583763
  - 5.584875902533532
  - 5.593005487322808
  - 5.543001022934914
  - 5.536700972914696
  - 5.525384411215782
  - 5.684660859405994
  - 5.578392025828362
  - 5.553863671422005
  - 5.583028909564018
  - 5.512295287847519
  - 5.496435883641244
  - 5.499690806865693
  - 5.525066828727723
  - 5.557992923259736
  - 5.56567012667656
  - 5.52730268239975
  - 5.565980064868928
  - 5.461000472307205
  - 5.446250918507577
  - 5.488937634229661
  - 5.474098834395409
  - 5.462433567643166
  - 5.520489886403084
  - 5.498486840724945
  - 5.490763047337532
  - 5.454454755783082
  - 5.448255327343941
  - 5.454171761870384
  - 5.500919914245606
  - 5.468563449382782
  - 5.48780432343483
  - 5.5172063440084465
  - 5.457939463853837
  - 5.46579661667347
  - 5.445232704281807
  - 5.428413271903992
  - 5.569235318899155
  - 5.515920063853264
  - 5.536439296603203
  - 5.448079442977906
  - 5.612787663936615
  - 5.484916779398919
  - 5.484146076440812
  - 5.408207482099534
  - 5.359402778744698
  - 5.434638157486916
  - 5.394218522310258
  - 5.558047291636467
  - 5.45736295580864
  - 5.487254577875138
  - 5.4514492601156235
  - 5.573901718854905
  - 5.4894042938947685
  - 5.518567231297493
  - 5.375061950087548
  - 5.5015159219503404
  - 5.480243280529976
  - 5.416777685284615
  - 5.43505232334137
  - 5.490033152699471
  - 5.421363952755929
  - 5.372693985700607
  - 5.417355224490166
  - 5.493681010603905
  - 5.452597945928574
  - 5.412056624889374
  - 5.49555556178093
  - 5.611933133006096
  - 5.562409541010857
  - 5.588967800140381
  - 5.5639428198337555
  - 5.645686256885529
  - 5.555296859145165
  - 5.528828674554825
  - 5.612905934453011
  - 5.592333817481995
  - 5.538448089361191
  - 5.594789803028107
  - 5.597475692629814
  - 5.555735385417939
  - 5.546083447337151
  - 5.543343469500542
  - 5.6149019509553915
  validation_losses:
  - 0.3920383155345917
  - 0.39259064197540283
  - 0.38948580622673035
  - 0.3860885500907898
  - 0.3871118724346161
  - 0.38371503353118896
  - 0.3807590901851654
  - 0.38180863857269287
  - 0.39254724979400635
  - 0.3859492838382721
  - 0.3885801136493683
  - 0.4085807800292969
  - 0.44624996185302734
  - 0.39245036244392395
  - 0.4185938835144043
  - 0.40325212478637695
  - 0.3945331275463104
  - 0.3858684301376343
  - 0.3989083468914032
  - 0.42703044414520264
  - 1.8451546430587769
  - 0.4345462918281555
  - 0.43862688541412354
  - 0.37960511445999146
  - 0.38686037063598633
  - 0.385301798582077
  - 1.9666155576705933
  - 2.6619246006011963
  - 0.9869258403778076
  - 5.214235782623291
  - 0.4778440296649933
  - 0.4425458312034607
  - 0.43268465995788574
  - 0.40594351291656494
  - 0.46536579728126526
  - 0.4142919182777405
  - 0.43894046545028687
  - 0.44823670387268066
  - 0.49839216470718384
  - 0.4049525856971741
  - 0.6229973435401917
  - 0.4732833504676819
  - 0.44703584909439087
  - 0.43320968747138977
  - 0.4949399530887604
  - 0.4447854459285736
  - 0.4265816807746887
  - 0.45206862688064575
  - 0.5068097710609436
  - 0.5093817114830017
  - 0.43077415227890015
  - 0.45572271943092346
  - 0.46724173426628113
  - 0.413571834564209
  - 0.4556955099105835
  - 0.4269363284111023
  - 0.754498302936554
  - 0.3967131972312927
  - 0.6172338724136353
  - 0.585036039352417
  - 0.8440890908241272
  - 2.0273852348327637
  - 2.8790953159332275
  - 0.4993093013763428
  - 0.4384218454360962
  - 0.45407384634017944
  - 0.4641457796096802
  - 0.4840873181819916
  - 0.46939951181411743
  - 0.49702608585357666
  - 0.38640734553337097
  - 0.4257114827632904
  - 0.5265134572982788
  - 0.4704209864139557
  - 0.4593738317489624
  - 0.4813258647918701
  - 0.44596630334854126
  - 0.669066309928894
  - 1.0315550565719604
  - 0.5178400874137878
  - 1.8167632818222046
  - 0.5275617837905884
  - 0.5343272089958191
  - 0.4118465185165405
  - 0.6404176950454712
  - 0.38907116651535034
  - 0.42359647154808044
  - 0.3912951946258545
  - 0.39885780215263367
  - 0.38961905241012573
  - 0.4175902009010315
  - 0.3932265341281891
  - 0.3912827968597412
  - 0.3967733383178711
  - 0.39091986417770386
  - 0.38951975107192993
  - 0.4034356474876404
  - 0.39755135774612427
  - 0.3973485231399536
  - 0.38944748044013977
loss_records_fold3:
  train_losses:
  - 5.608163464069367
  - 5.632720255851746
  - 5.595005548000336
  - 5.544019542634487
  - 5.516366797685624
  - 5.925708711147308
  - 5.592849853634835
  - 5.643231064081192
  - 5.624784281849862
  - 5.632862260937691
  - 5.6223887860775
  validation_losses:
  - 0.37124159932136536
  - 0.3779330551624298
  - 0.376462459564209
  - 0.456474632024765
  - 0.4032403528690338
  - 0.3748222291469574
  - 0.38330429792404175
  - 0.38374415040016174
  - 0.3772883415222168
  - 0.37183162569999695
  - 0.3691360652446747
loss_records_fold4:
  train_losses:
  - 5.5693856388330465
  - 5.593519005179406
  - 5.569235697388649
  - 5.577525413036347
  - 5.642639538645745
  - 5.567544931173325
  - 5.544473877549172
  - 5.584406685829163
  - 5.517967507243156
  - 5.615382766723633
  - 5.550422579050064
  - 5.569126319885254
  - 5.645275524258614
  - 5.566915142536164
  validation_losses:
  - 0.37733733654022217
  - 0.37778863310813904
  - 0.37549519538879395
  - 0.3801973760128021
  - 0.3769858479499817
  - 0.3792974650859833
  - 0.3788400888442993
  - 0.4129946231842041
  - 0.402399480342865
  - 0.38517293334007263
  - 0.37625619769096375
  - 0.3828989267349243
  - 0.37959417700767517
  - 0.38086047768592834
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:17:24.312356'
