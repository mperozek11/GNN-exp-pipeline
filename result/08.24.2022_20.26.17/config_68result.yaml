config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:00:57.526260'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_68fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_68fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_68fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_68fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_68fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_68fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_68fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_68fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_68fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_68fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 7.133633196353912
  - 6.838985341787339
  - 6.606202897429466
  - 6.227192226052285
  - 6.791704460978508
  - 6.453163334727288
  - 6.424287244677544
  - 6.617319282889366
  - 6.631752687692643
  - 6.6643351405859
  - 6.2384916067123415
  - 6.350978310406209
  - 6.065553325414658
  - 6.116124394536019
  - 5.9891323238611225
  - 5.916404536366463
  - 6.190312430262566
  - 6.0784325242042545
  - 6.0385559678077705
  - 5.940714371204376
  - 5.92389609515667
  validation_losses:
  - 0.46439799666404724
  - 0.4278179705142975
  - 0.4115212857723236
  - 0.4039549231529236
  - 0.40597790479660034
  - 0.42628005146980286
  - 0.40151000022888184
  - 0.3904474973678589
  - 0.40193551778793335
  - 0.43609532713890076
  - 0.4174876809120178
  - 0.4241408109664917
  - 0.3929361402988434
  - 0.38495129346847534
  - 0.39877745509147644
  - 0.38589516282081604
  - 0.3913443684577942
  - 0.3967278003692627
  - 0.39290302991867065
  - 0.3901625871658325
  - 0.39679375290870667
loss_records_fold1:
  train_losses:
  - 5.969296926259995
  - 6.051566955447197
  - 5.983808225393296
  - 5.926511201262475
  - 6.079791235923768
  - 6.032412740588189
  - 6.197614750266076
  - 6.123497578501702
  - 6.01587829887867
  - 6.035697868466378
  - 6.011450564861298
  - 5.900074827671052
  - 5.871632809937001
  - 6.017103129625321
  - 5.914334809780121
  - 6.063316932320595
  - 5.997439834475518
  - 5.9861586719751365
  - 5.9566900908947
  - 5.998659083247185
  - 5.967381614446641
  - 5.890250039100647
  - 6.022256046533585
  - 5.928642144799233
  - 5.9912541002035145
  - 6.002532452344894
  - 6.014560955762864
  - 6.045889422297478
  - 5.891161531209946
  - 5.883055159449578
  - 5.969419491291046
  - 6.007049959897995
  - 6.132999289035798
  - 5.960635104775429
  - 5.95441456437111
  - 5.92227638065815
  validation_losses:
  - 0.40277746319770813
  - 0.41525009274482727
  - 0.44851773977279663
  - 0.3897704482078552
  - 0.40381935238838196
  - 0.4456782042980194
  - 0.3928554654121399
  - 0.40737539529800415
  - 0.3879536986351013
  - 0.4011650085449219
  - 0.38296398520469666
  - 0.3934227526187897
  - 0.39398282766342163
  - 0.3889422118663788
  - 0.3960447311401367
  - 0.39384669065475464
  - 0.4057910740375519
  - 0.400558203458786
  - 0.3891541063785553
  - 0.393076092004776
  - 0.41841810941696167
  - 0.3940548598766327
  - 0.38644471764564514
  - 0.40824228525161743
  - 0.4553462564945221
  - 0.41618844866752625
  - 0.3947044909000397
  - 0.39074966311454773
  - 0.411949098110199
  - 0.5225188732147217
  - 0.39209142327308655
  - 0.39222607016563416
  - 0.3993304371833801
  - 0.4051200747489929
  - 0.3908218741416931
  - 0.3934600055217743
loss_records_fold2:
  train_losses:
  - 6.04292596578598
  - 5.915441280603409
  - 6.030623912811279
  - 5.870135194063187
  - 5.845527341961861
  - 5.9532590508461
  - 6.009749618172646
  - 5.852163064479829
  - 5.824800592660904
  - 5.938147026300431
  - 5.889849391579628
  - 5.960404774546624
  - 5.838870832324028
  - 5.903241449594498
  - 5.715898045897484
  - 5.913732600212097
  - 5.98387317955494
  - 5.843342220783234
  - 6.02598070204258
  - 5.986579594016075
  - 6.145746931433678
  - 6.041877585649491
  - 5.90906497836113
  - 5.9026117593050005
  - 5.919628801941872
  - 5.86394964158535
  - 6.037187260389328
  - 5.822402995824814
  - 6.029368218779564
  - 5.923497790098191
  - 5.9812231242656715
  - 6.038166701793671
  - 5.924659276008606
  - 5.794455337524415
  - 5.9325788885355
  - 5.909062549471855
  - 5.960243141651154
  - 5.776201212406159
  - 5.806198319792748
  - 5.973342832922936
  - 5.757994484901428
  - 5.915867587924004
  - 5.95764097571373
  - 5.865666848421097
  - 5.835639220476151
  - 5.877432876825333
  - 5.8820415318012245
  - 5.9727495372295385
  - 5.988464945554734
  - 5.881295061111451
  - 5.800164273381234
  - 5.981378713250161
  - 5.841785728931427
  - 5.732721003890038
  - 5.832507702708245
  - 6.023300844430924
  - 5.838481685519219
  - 5.950459736585618
  - 5.974529737234116
  - 5.991516390442849
  - 6.062833213806153
  - 5.794767981767655
  - 5.94114959537983
  - 5.987631165981293
  - 5.909582820534706
  - 5.891452726721764
  - 5.707440695166588
  - 6.014982151985169
  - 6.013512459397316
  - 5.922877803444862
  - 5.816336557269096
  - 5.873604619503022
  - 5.929034638404847
  - 5.844331455230713
  - 5.827130302786827
  - 5.810508245229721
  - 5.869207632541657
  - 5.847072511911392
  - 5.989525336027146
  - 5.829948616027832
  - 5.966373574733734
  - 5.85968297123909
  - 5.843396446108819
  - 5.8478755205869675
  - 5.8401209086179735
  - 5.915879055857658
  - 5.913927286863327
  - 5.952714052796364
  - 6.101793682575226
  - 5.772133371233941
  - 5.925660461187363
  - 5.817502301931381
  - 5.947360879182816
  - 5.792649975419045
  - 5.7755583971738815
  - 5.901533928513527
  - 6.0986843585968025
  - 5.956585156917573
  - 5.89293439090252
  - 5.923301896452904
  validation_losses:
  - 0.39615726470947266
  - 0.397438645362854
  - 0.38807302713394165
  - 0.39613232016563416
  - 0.3849879503250122
  - 0.38724377751350403
  - 0.3856273889541626
  - 0.404737263917923
  - 0.411277711391449
  - 0.4008839428424835
  - 0.4354313611984253
  - 0.4364415109157562
  - 0.3849325180053711
  - 0.4569583535194397
  - 0.4087536931037903
  - 0.3911246955394745
  - 0.7509193420410156
  - 0.46670275926589966
  - 0.5453925132751465
  - 0.43272116780281067
  - 0.4055807590484619
  - 0.38942164182662964
  - 0.4398331642150879
  - 0.482721209526062
  - 0.5476728677749634
  - 0.6996828317642212
  - 0.44219496846199036
  - 0.5141040086746216
  - 0.4839496910572052
  - 0.4500608444213867
  - 0.41088101267814636
  - 0.443718284368515
  - 0.49089908599853516
  - 0.4493686854839325
  - 0.49931448698043823
  - 0.38337910175323486
  - 0.5065744519233704
  - 0.38905420899391174
  - 0.5210896730422974
  - 0.6314253807067871
  - 0.438137412071228
  - 0.3880627453327179
  - 0.8669970035552979
  - 0.5820863842964172
  - 0.3850421905517578
  - 0.3858726918697357
  - 0.7932754158973694
  - 0.48858654499053955
  - 1.472085952758789
  - 0.7455559372901917
  - 0.5034462213516235
  - 0.6012265086174011
  - 0.6054593920707703
  - 0.8048574328422546
  - 0.7126458287239075
  - 0.48452654480934143
  - 1.202476143836975
  - 0.48223909735679626
  - 0.6085894107818604
  - 0.45187175273895264
  - 0.5282037258148193
  - 0.5417072176933289
  - 0.397430956363678
  - 0.9283936619758606
  - 0.962897777557373
  - 0.7108899354934692
  - 0.7284562587738037
  - 0.5252524018287659
  - 0.5232822299003601
  - 0.8496201634407043
  - 0.3786051273345947
  - 0.8250013589859009
  - 0.5802971720695496
  - 0.9711509943008423
  - 1.9015262126922607
  - 1.2050913572311401
  - 0.8367313742637634
  - 1.0044490098953247
  - 1.109190583229065
  - 0.7878839373588562
  - 1.0204601287841797
  - 0.7535091638565063
  - 0.942923367023468
  - 1.284246802330017
  - 2.363154411315918
  - 0.6442241668701172
  - 1.3571596145629883
  - 1.5444705486297607
  - 1.485153079032898
  - 3.1220264434814453
  - 1.89281165599823
  - 2.4432125091552734
  - 2.019098997116089
  - 1.3518582582473755
  - 2.002279043197632
  - 0.8818035125732422
  - 0.3891480267047882
  - 0.39062318205833435
  - 0.39264988899230957
  - 0.38846853375434875
loss_records_fold3:
  train_losses:
  - 5.962337794899941
  - 5.880727258324623
  - 6.025534376502037
  - 5.963634070754051
  - 6.149769881367684
  - 5.7124160647392275
  - 5.998505708575249
  - 5.972143453359604
  - 6.126746612787247
  - 5.9324746370315555
  - 5.801909101009369
  - 5.933322581648827
  - 6.079278030991555
  - 5.974415576457978
  - 5.7775054514408115
  - 5.925038060545922
  - 5.921413779258728
  - 5.94882898926735
  - 5.843093025684357
  - 5.995423397421837
  - 5.901850056648255
  - 5.892316862940788
  - 6.00748682320118
  - 5.9327219307422645
  - 5.811766517162323
  - 5.952733951807023
  - 5.893436974287034
  - 5.825909569859505
  - 5.964632833003998
  - 5.856819295883179
  - 6.001023057103158
  - 5.796338886022568
  - 5.975291320681572
  - 6.081668558716775
  - 5.947852382063866
  - 5.990211486816406
  - 6.0041113823652275
  - 5.820874094963074
  - 5.941225820779801
  - 5.887835150957108
  - 5.955699008703232
  - 5.732969582080841
  - 5.850004404783249
  - 5.932537299394608
  - 5.687893173098565
  - 5.840887105464936
  - 5.726343566179276
  - 5.8700921654701235
  - 5.907698068022729
  - 5.8252172589302065
  - 5.8553589463233955
  - 5.991778108477593
  - 5.800456488132477
  - 5.928930893540382
  - 5.753987342119217
  - 5.82910093665123
  - 5.879834616184235
  - 5.739070174098015
  - 5.754933425784111
  - 5.845938733220101
  - 5.917470276355743
  - 5.939884352684022
  - 5.756705233454705
  - 6.114836969971657
  - 5.879626378417015
  - 5.869578957557678
  - 5.872745564579964
  - 5.942695835232735
  - 5.97597149014473
  - 5.886160749197007
  - 5.88761080801487
  - 5.858320587873459
  - 6.018214592337609
  - 6.0339813232421875
  - 6.023307156562805
  - 6.011459466814995
  - 5.914948770403862
  - 5.780821558833122
  - 5.844201377034188
  - 5.951163679361343
  - 6.008223897218705
  - 5.891326814889908
  - 5.834241753816605
  - 5.812442967295647
  - 5.84513875246048
  - 5.86232756972313
  - 5.890125703811646
  - 5.957092201709748
  - 5.88900055885315
  - 6.135094696283341
  - 5.9959970891475685
  - 5.968210145831108
  - 6.000295862555504
  - 5.719110852479935
  - 5.827564403414726
  - 5.890027397871018
  - 5.856298640370369
  - 5.812857073545456
  - 5.976660254597665
  - 5.955958300828934
  validation_losses:
  - 0.37164878845214844
  - 0.3664865493774414
  - 0.38168472051620483
  - 0.3875792324542999
  - 0.36060771346092224
  - 0.3679065704345703
  - 0.38977354764938354
  - 0.38099926710128784
  - 0.38860443234443665
  - 0.7280904054641724
  - 0.6480808258056641
  - 1.6681246757507324
  - 0.5564113259315491
  - 0.82405024766922
  - 0.39115259051322937
  - 2.2166967391967773
  - 1.6112945079803467
  - 1.6690869331359863
  - 1.150026798248291
  - 0.7030336856842041
  - 1.2787572145462036
  - 1.6194121837615967
  - 1.4242134094238281
  - 1.4596002101898193
  - 1.479018211364746
  - 1.3367962837219238
  - 1.158917784690857
  - 2.9833784103393555
  - 0.949871301651001
  - 1.5096760988235474
  - 1.639045000076294
  - 1.621867060661316
  - 1.7564458847045898
  - 0.4035123288631439
  - 0.5641118288040161
  - 1.2940877676010132
  - 1.3271065950393677
  - 1.2990326881408691
  - 1.9630380868911743
  - 1.5264027118682861
  - 2.4603261947631836
  - 2.8318140506744385
  - 1.1795918941497803
  - 2.517317295074463
  - 3.5115911960601807
  - 5.909958839416504
  - 2.5698013305664062
  - 1.1377321481704712
  - 1.1504815816879272
  - 0.8077571988105774
  - 0.5422310829162598
  - 0.6796310544013977
  - 0.7749564051628113
  - 0.6855061650276184
  - 2.279144048690796
  - 1.4104629755020142
  - 0.3763246238231659
  - 1.125493049621582
  - 0.4475417137145996
  - 1.6745579242706299
  - 1.2275036573410034
  - 0.4102124273777008
  - 1.3867969512939453
  - 0.6360327005386353
  - 1.7697038650512695
  - 1.3214977979660034
  - 0.5161016583442688
  - 0.7563780546188354
  - 1.073765516281128
  - 0.5589070320129395
  - 0.38288018107414246
  - 1.0735112428665161
  - 0.3997959494590759
  - 0.4011547565460205
  - 1.3172869682312012
  - 2.748387098312378
  - 4.639671802520752
  - 2.0822982788085938
  - 0.5153167843818665
  - 0.3869948983192444
  - 0.5412983894348145
  - 1.1747581958770752
  - 0.6883417367935181
  - 0.39124053716659546
  - 2.301787853240967
  - 1.6503390073776245
  - 2.6860406398773193
  - 0.3817117214202881
  - 0.38082388043403625
  - 1.271518588066101
  - 0.8872838020324707
  - 0.8786706924438477
  - 1.5968018770217896
  - 0.49393534660339355
  - 1.0471553802490234
  - 1.245745062828064
  - 0.5570154786109924
  - 2.38232684135437
  - 1.1519393920898438
  - 1.0788846015930176
loss_records_fold4:
  train_losses:
  - 5.8794977396726615
  - 5.789270535111427
  - 5.89660459458828
  - 5.853439339995385
  - 5.986634433269501
  - 6.093362048268318
  - 5.950956010818482
  - 5.850845292210579
  - 6.058195388317109
  - 5.959301936626435
  - 5.977219441533089
  - 6.024088108539582
  - 5.9713945388793945
  - 5.8504019558429725
  - 5.867549341917038
  - 6.045883131027222
  - 5.9279942423105245
  - 5.812753805518151
  - 5.931651479005814
  - 5.979011929035187
  - 5.927351582050324
  - 5.85709510743618
  - 5.947118049860001
  - 5.885794305801392
  - 6.010566854476929
  - 5.843446177244187
  - 5.815979102253914
  - 5.731483381986618
  - 5.883515557646752
  - 5.865370348095894
  - 6.020417964458466
  - 5.931159013509751
  - 5.884236577153207
  - 5.781836614012718
  - 6.000837871432305
  - 5.86932744383812
  - 5.9231420993804935
  - 5.811274307966233
  - 6.194981622695924
  - 5.892479169368745
  - 5.753335863351822
  - 5.910923737287522
  - 5.965290287137032
  - 5.840384230017662
  - 5.894644501805306
  - 5.852740973234177
  - 5.808792439103127
  - 5.924469485878944
  - 5.796111729741097
  - 5.981687512993813
  - 5.80927233695984
  - 5.878434380888939
  - 5.8524638503789905
  - 5.797159606218338
  - 5.850432386994362
  - 5.763419330120087
  - 5.920467638969422
  - 5.944164550304413
  - 5.86972768008709
  - 5.80349367260933
  - 5.898242816329002
  - 5.758939516544342
  - 5.806656464934349
  - 5.823252195119858
  - 5.826375943422318
  - 5.8889489650726325
  - 5.808929318189621
  - 5.8546476632356645
  - 5.904421639442444
  - 5.705155926942826
  - 5.819039165973663
  - 5.846476703882217
  - 5.970033037662507
  - 5.942305487394333
  - 5.909434223175049
  - 5.838982054591179
  - 5.977444514632225
  - 5.876399204134941
  - 5.8620456725358965
  - 5.895557641983032
  - 5.853351211547852
  - 5.873529821634293
  - 5.993823093175888
  - 5.842647886276246
  - 5.899686500430107
  - 5.849218001961709
  - 5.884897363185883
  - 5.8150948047637945
  - 5.921188604831696
  - 5.886681878566742
  - 5.81869767010212
  - 5.930355191230774
  - 5.898726657032967
  - 5.864460742473603
  - 5.821015876531601
  - 5.977825653553009
  - 5.8463942259550095
  - 5.895537480711937
  - 5.890174925327301
  - 5.7940654993057255
  validation_losses:
  - 0.38169845938682556
  - 0.3683266043663025
  - 0.3728705942630768
  - 0.42084553837776184
  - 0.3759715259075165
  - 0.39052528142929077
  - 0.3811657130718231
  - 0.37093403935432434
  - 0.37264224886894226
  - 0.38434988260269165
  - 0.5186123847961426
  - 0.41781577467918396
  - 0.36714377999305725
  - 0.3822905719280243
  - 0.3757341802120209
  - 0.37159067392349243
  - 0.3705895245075226
  - 0.39862024784088135
  - 0.3863195776939392
  - 0.4178704619407654
  - 0.39615190029144287
  - 0.36414286494255066
  - 0.40629684925079346
  - 0.37940219044685364
  - 0.3946152627468109
  - 0.40929508209228516
  - 0.4250366985797882
  - 0.38013866543769836
  - 0.3828585147857666
  - 0.42890357971191406
  - 0.38586726784706116
  - 0.3740476965904236
  - 0.3739449977874756
  - 0.42201197147369385
  - 0.384508341550827
  - 0.37112924456596375
  - 0.44202888011932373
  - 0.3969697654247284
  - 0.40506359934806824
  - 0.38087892532348633
  - 0.393322616815567
  - 0.4521598815917969
  - 0.43173569440841675
  - 0.39023441076278687
  - 0.5576128959655762
  - 0.3747178912162781
  - 0.5733012557029724
  - 0.5119680762290955
  - 0.3758111894130707
  - 0.7659156918525696
  - 0.4480518400669098
  - 0.4209137558937073
  - 0.36571845412254333
  - 0.3732456564903259
  - 0.49505123496055603
  - 0.396116703748703
  - 0.3819250166416168
  - 0.6010981202125549
  - 0.4306740462779999
  - 0.732538640499115
  - 0.5252283215522766
  - 0.4692116975784302
  - 1.0241540670394897
  - 1.1838372945785522
  - 1.770088791847229
  - 0.3871648907661438
  - 0.880663275718689
  - 0.8253422975540161
  - 0.36995598673820496
  - 1.2846180200576782
  - 0.7505013942718506
  - 1.196673035621643
  - 0.36914315819740295
  - 0.8806063532829285
  - 0.8319178819656372
  - 0.6737975478172302
  - 0.6304997205734253
  - 0.9054116606712341
  - 1.0984691381454468
  - 0.42867377400398254
  - 1.1228902339935303
  - 1.9370465278625488
  - 0.3717060685157776
  - 0.3749481737613678
  - 0.4276669919490814
  - 0.36842235922813416
  - 0.37252897024154663
  - 0.4235720634460449
  - 0.3683854937553406
  - 0.36260882019996643
  - 0.7608920335769653
  - 0.3609168827533722
  - 0.3705240488052368
  - 0.3829905688762665
  - 0.3685910999774933
  - 0.3732106685638428
  - 0.8863097429275513
  - 1.0430103540420532
  - 0.8706440329551697
  - 0.3931630551815033
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 36 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8490566037735849, 0.8576329331046312, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.022222222222222223, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8565554396326618
  mean_f1_accuracy: 0.0044444444444444444
  total_train_time: '0:37:09.059110'
