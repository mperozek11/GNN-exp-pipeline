config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 22:39:27.590038'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/84/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/84/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/84/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/84/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/84/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/84/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/84/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/84/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/84/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/84/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 144.38079750537872
  - 63.446927577257156
  - 31.42467001080513
  - 18.476556420326233
  - 16.4193776845932
  - 14.339584678411484
  - 16.086565792560577
  - 13.170750558376312
  - 9.004364490509033
  - 15.867064654827118
  - 10.523800104856491
  - 10.017610996961594
  - 12.033998548984528
  - 8.48962914943695
  - 9.967149168252945
  - 8.127344936132431
  - 29.200786590576172
  - 14.319951325654984
  - 15.60000392794609
  - 10.963154435157776
  - 8.752830535173416
  - 19.46707996726036
  - 22.98230767250061
  - 31.76248475909233
  - 16.77338832616806
  - 15.630572527647018
  - 18.927163064479828
  - 26.048221588134766
  - 22.11484134197235
  - 12.893667578697205
  - 12.054992601275444
  - 17.81362158060074
  - 11.936285823583603
  - 13.334409236907959
  - 9.151653319597244
  - 8.120899140834808
  - 10.933532029390335
  - 10.896905660629272
  - 17.314802095294
  - 10.729872018098831
  - 18.214097172021866
  - 8.819891825318336
  - 13.491047233343124
  - 8.120984613895416
  - 7.806204319000244
  - 9.267822444438934
  - 8.561450570821762
  - 7.6068006455898285
  - 7.9781942665576935
  - 7.338636070489883
  - 7.5418002009391785
  - 7.54027459025383
  - 7.454716503620148
  - 7.594643831253052
  - 7.699661701917648
  - 9.259138196706772
  - 11.879442572593689
  - 8.355637341737747
  - 8.974377363920212
  - 7.390452951192856
  - 7.3786933571100235
  - 8.541857570409775
  - 8.189978137612343
  - 8.751576751470566
  - 9.16866797208786
  - 8.646751195192337
  - 8.729348629713058
  - 7.52345472574234
  - 9.75612023472786
  - 8.313099101185799
  - 10.241592824459076
  - 15.17249645292759
  - 12.2783724963665
  - 9.030508905649185
  - 7.545608282089233
  - 9.46788503229618
  - 8.405320167541504
  - 7.301959574222565
  - 7.381116300821304
  - 7.46585077047348
  - 10.39347106218338
  - 9.518631249666214
  - 8.545826882123947
  - 7.951911211013794
  - 14.43622824549675
  - 7.909046530723572
  - 7.33662623167038
  - 7.399753361940384
  - 9.949058592319489
  - 8.33445492386818
  - 8.270855486392975
  - 7.22284522652626
  - 11.63221961259842
  - 8.836081117391586
  - 8.157201588153839
  - 13.706365764141083
  - 7.7014125138521194
  - 7.3224121034145355
  - 7.15153306722641
  - 7.323179721832275
  validation_losses:
  - 1.4783990383148193
  - 1.512384057044983
  - 1.1721181869506836
  - 0.8229188323020935
  - 2.049905776977539
  - 0.4889313876628876
  - 0.6928302645683289
  - 0.4716758131980896
  - 0.4745553731918335
  - 0.6556804776191711
  - 0.6611440777778625
  - 0.5051447749137878
  - 0.3948811888694763
  - 0.5409592390060425
  - 0.3832756280899048
  - 0.38878509402275085
  - 0.6131378412246704
  - 0.4660368263721466
  - 0.45514219999313354
  - 0.6208469867706299
  - 0.4968635141849518
  - 0.816002607345581
  - 0.4445572793483734
  - 0.40024444460868835
  - 0.4277326464653015
  - 0.38517335057258606
  - 0.5784776210784912
  - 1.3022245168685913
  - 0.6992437839508057
  - 0.6309249401092529
  - 0.39941295981407166
  - 0.40722572803497314
  - 0.42461761832237244
  - 0.39730361104011536
  - 0.5090182423591614
  - 0.39546912908554077
  - 0.4749986231327057
  - 0.3900947570800781
  - 0.49679645895957947
  - 0.4603556990623474
  - 0.40969523787498474
  - 0.4046684205532074
  - 0.5331746339797974
  - 0.39862260222435
  - 0.3945903182029724
  - 0.38576215505599976
  - 0.3836632966995239
  - 0.3883154094219208
  - 0.40567415952682495
  - 0.3769623041152954
  - 0.4211047291755676
  - 0.3765682876110077
  - 0.40309396386146545
  - 0.3977283239364624
  - 0.37942320108413696
  - 0.5416133999824524
  - 0.4406976103782654
  - 0.39656636118888855
  - 0.3896464407444
  - 0.3941609263420105
  - 0.3866516649723053
  - 0.41975128650665283
  - 0.4013354182243347
  - 0.3898718059062958
  - 0.3900080621242523
  - 0.4385685324668884
  - 0.3809325098991394
  - 0.5150321125984192
  - 0.37703990936279297
  - 0.37844493985176086
  - 0.3760395050048828
  - 0.3860102891921997
  - 0.3980514109134674
  - 0.41988345980644226
  - 0.3993651866912842
  - 0.43733543157577515
  - 0.37950727343559265
  - 0.38816800713539124
  - 0.3793138861656189
  - 0.39695867896080017
  - 0.40650925040245056
  - 0.3769303858280182
  - 0.5101319551467896
  - 0.39852795004844666
  - 0.46298056840896606
  - 0.37625348567962646
  - 0.3937481939792633
  - 0.39611926674842834
  - 0.4883483648300171
  - 0.44794127345085144
  - 0.411925345659256
  - 0.3951369524002075
  - 0.410398006439209
  - 0.3867589235305786
  - 0.372337281703949
  - 0.41983866691589355
  - 0.37675172090530396
  - 0.3751256763935089
  - 0.37301328778266907
  - 0.41257530450820923
loss_records_fold2:
  train_losses:
  - 8.30687552690506
  - 8.100649654865265
  - 7.446290522813797
  - 7.498109310865402
  - 7.324608325958252
  - 7.279523566365242
  - 7.833685204386711
  - 7.474784851074219
  - 7.492124170064926
  - 7.729258567094803
  - 7.521747678518295
  - 7.769512295722961
  - 7.352920889854431
  - 7.726660519838333
  - 7.2528955936431885
  - 7.12520906329155
  - 7.606105476617813
  - 7.166024833917618
  - 7.7841086983680725
  - 7.531373590230942
  - 7.304398864507675
  - 7.204652845859528
  - 7.2354812026023865
  - 7.082294046878815
  - 7.159136325120926
  - 7.616974964737892
  - 7.125094473361969
  - 7.291726037859917
  - 7.335331052541733
  - 7.322238996624947
  - 7.170491695404053
  - 7.404429912567139
  - 7.437494218349457
  - 7.253617137670517
  - 7.214757561683655
  - 7.433442175388336
  - 7.026563584804535
  - 7.677209764719009
  - 7.5743149518966675
  - 7.734324097633362
  - 7.753511875867844
  - 7.3201102912425995
  - 7.1469359546899796
  - 7.679613769054413
  - 7.130299746990204
  - 7.374125450849533
  - 7.752119958400726
  - 7.334249436855316
  - 7.824063628911972
  - 7.724713534116745
  - 7.37787738442421
  - 7.264257729053497
  - 7.008279055356979
  - 7.202319771051407
  - 7.292218267917633
  - 7.191467463970184
  - 7.187994867563248
  - 7.166447728872299
  - 7.401836693286896
  - 7.032315820455551
  - 7.2663781344890594
  - 7.175632655620575
  - 7.044842883944511
  - 7.170626550912857
  - 7.337463200092316
  - 7.1829449236392975
  - 7.388420760631561
  - 7.423862099647522
  - 7.523274585604668
  - 7.357591032981873
  - 7.4610461592674255
  - 7.073936820030212
  - 7.3876316249370575
  - 7.495698392391205
  - 7.3174697160720825
  - 7.611430585384369
  - 7.360800877213478
  - 7.310079038143158
  - 7.146843403577805
  - 7.402528494596481
  - 8.016187459230423
  - 7.131586968898773
  - 7.0532553642988205
  - 7.510430991649628
  - 7.12348011136055
  - 7.643994867801666
  - 7.269285187125206
  - 7.280846416950226
  - 7.283869504928589
  - 7.589603245258331
  - 7.47235007584095
  - 7.722095102071762
  - 7.516682326793671
  - 7.4919891357421875
  - 7.468720644712448
  - 7.491320639848709
  - 7.274662464857101
  - 7.152966648340225
  - 7.240110099315643
  - 7.413198232650757
  validation_losses:
  - 0.42287954688072205
  - 0.370532363653183
  - 0.42740398645401
  - 0.41752898693084717
  - 0.37350186705589294
  - 0.37293651700019836
  - 0.40085676312446594
  - 0.3953261375427246
  - 0.3735671639442444
  - 0.42257583141326904
  - 0.4803740680217743
  - 0.3886626064777374
  - 0.37136387825012207
  - 0.3800147771835327
  - 0.38642311096191406
  - 0.4058443307876587
  - 0.4126811623573303
  - 0.38435059785842896
  - 0.3740983307361603
  - 0.37207522988319397
  - 0.3860895335674286
  - 0.3773953914642334
  - 0.3990480303764343
  - 0.38605254888534546
  - 0.40905049443244934
  - 0.3982875347137451
  - 0.37161269783973694
  - 0.3912753760814667
  - 0.40938475728034973
  - 0.3868480920791626
  - 0.37798309326171875
  - 0.4585532546043396
  - 0.4907430410385132
  - 0.38363367319107056
  - 0.4135896861553192
  - 0.40016430616378784
  - 0.38190075755119324
  - 0.37867918610572815
  - 0.3753897249698639
  - 0.4382794201374054
  - 0.3764342963695526
  - 0.3968185484409332
  - 0.3687286972999573
  - 0.39615628123283386
  - 0.3695148229598999
  - 0.40961185097694397
  - 0.4328552782535553
  - 0.40133196115493774
  - 0.6596313714981079
  - 0.4234422445297241
  - 0.37139633297920227
  - 0.3812713921070099
  - 0.3702082931995392
  - 0.37529289722442627
  - 0.4174593389034271
  - 0.5147199034690857
  - 0.3963982164859772
  - 0.3684842586517334
  - 0.47116002440452576
  - 0.40661361813545227
  - 0.37337198853492737
  - 0.42557045817375183
  - 0.3672260642051697
  - 0.46292394399642944
  - 0.37064090371131897
  - 0.3682527542114258
  - 0.3713501989841461
  - 0.6391888856887817
  - 0.37229683995246887
  - 0.36978641152381897
  - 0.42587387561798096
  - 0.36997923254966736
  - 0.512923538684845
  - 0.4057824909687042
  - 0.4178726375102997
  - 0.4022330045700073
  - 0.36979642510414124
  - 0.3907874524593353
  - 0.37551406025886536
  - 0.41066017746925354
  - 0.38033708930015564
  - 0.3796519637107849
  - 0.36854782700538635
  - 0.3858906626701355
  - 0.3809214234352112
  - 0.37311965227127075
  - 0.38081294298171997
  - 0.37901046872138977
  - 0.4036766290664673
  - 0.3902471661567688
  - 0.3978421986103058
  - 0.3734748065471649
  - 0.3756888210773468
  - 0.3799994885921478
  - 0.4268093705177307
  - 0.3690299987792969
  - 0.3957747220993042
  - 0.3718341886997223
  - 0.4291604161262512
  - 0.3714631199836731
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.855917667238422, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.04597701149425288, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.009195402298850576
  total_train_time: '0:04:50.224698'
