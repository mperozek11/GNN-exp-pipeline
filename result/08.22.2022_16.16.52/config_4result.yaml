config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.229762'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_4fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_4fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_4fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_4fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_4fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_4fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_4fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_4fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_4fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_4fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.9619968950748445
  - 6.362041983008385
  - 6.273692092299462
  - 6.394288948178292
  - 6.232238823175431
  - 6.166391372680664
  - 6.429492527246476
  - 6.193889555335045
  - 6.2199120908975605
  - 6.036981534957886
  - 6.14931954741478
  - 6.228391814231873
  - 6.073168569803238
  - 5.827528190612793
  - 6.064526015520096
  - 5.884769877791405
  - 5.954167696833611
  - 6.056351542472839
  - 5.974280467629433
  - 5.986160892248154
  - 6.024404054880143
  - 6.228337496519089
  - 6.080625450611115
  - 6.089895415306092
  - 6.038790300488472
  validation_losses:
  - 0.5398936867713928
  - 0.3992208242416382
  - 0.39773648977279663
  - 0.40671446919441223
  - 0.4106970429420471
  - 0.38701099157333374
  - 0.4030798375606537
  - 0.4060842990875244
  - 0.3942440450191498
  - 0.4317626953125
  - 0.38892635703086853
  - 0.3995445668697357
  - 0.40876349806785583
  - 0.39223939180374146
  - 0.4114264249801636
  - 0.3958289623260498
  - 0.3981598913669586
  - 0.388353556394577
  - 0.4010198712348938
  - 0.4024989604949951
  - 0.4099195897579193
  - 0.40774470567703247
  - 0.3927535116672516
  - 0.3944379687309265
  - 0.3930225968360901
loss_records_fold1:
  train_losses:
  - 5.9895044505596164
  - 6.08706967830658
  - 6.056455320119858
  - 5.981914076209069
  - 5.959757632017136
  - 6.016922852396966
  - 6.117072099447251
  - 6.056587100028992
  - 6.023417747020722
  - 5.892520615458489
  - 6.0088097810745245
  validation_losses:
  - 0.3961425721645355
  - 0.3869427740573883
  - 0.39100128412246704
  - 0.39271092414855957
  - 0.39661210775375366
  - 0.38758161664009094
  - 0.39435499906539917
  - 0.3937506675720215
  - 0.4003237783908844
  - 0.3954724371433258
  - 0.3998136818408966
loss_records_fold2:
  train_losses:
  - 6.187436285614968
  - 6.031036695837975
  - 6.120147949457169
  - 5.936112767457963
  - 5.918262708187104
  - 6.125124934315682
  - 5.98975702226162
  - 5.98432774245739
  - 6.121432796120644
  - 6.126790335774422
  - 5.947375717759133
  - 5.942883098125458
  - 5.960385677218437
  - 5.903475812077523
  - 5.97319764494896
  - 5.899776586890221
  validation_losses:
  - 0.40421605110168457
  - 0.39857617020606995
  - 0.3881555497646332
  - 0.39406701922416687
  - 0.40885472297668457
  - 0.3945121169090271
  - 0.3939949572086334
  - 0.3938981294631958
  - 0.39682310819625854
  - 0.40817689895629883
  - 0.39575260877609253
  - 0.40016165375709534
  - 0.38957250118255615
  - 0.39364415407180786
  - 0.3999837636947632
  - 0.39493992924690247
loss_records_fold3:
  train_losses:
  - 6.047874838113785
  - 5.922153425216675
  - 5.98272089958191
  - 6.240620328485966
  - 6.0205714315176015
  - 6.107261344790459
  - 6.176899349689484
  - 6.053532302379608
  - 6.090172085165978
  - 5.9512270331382755
  - 6.037948548793793
  validation_losses:
  - 0.38832393288612366
  - 0.38441506028175354
  - 0.39827489852905273
  - 0.38335904479026794
  - 0.3870200216770172
  - 0.38949382305145264
  - 0.3947277069091797
  - 0.3910866975784302
  - 0.3851432204246521
  - 0.37793534994125366
  - 0.3784908950328827
loss_records_fold4:
  train_losses:
  - 5.926006627082825
  - 6.113619017601014
  - 5.9833612769842155
  - 5.975521403551102
  - 5.954224371910096
  - 6.019832962751389
  - 6.033990260958672
  - 5.991768375039101
  - 6.0084271222352985
  - 6.047312960028648
  - 5.980416014790535
  validation_losses:
  - 0.37635862827301025
  - 0.3776709735393524
  - 0.3760249614715576
  - 0.38292360305786133
  - 0.38313722610473633
  - 0.3839888572692871
  - 0.3730453550815582
  - 0.3800241947174072
  - 0.38855651021003723
  - 0.3808302879333496
  - 0.37174174189567566
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:07:20.541012'
