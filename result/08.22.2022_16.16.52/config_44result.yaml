config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:06:23.633238'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_44fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_44fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_44fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_44fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_44fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_44fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_44fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_44fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_44fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_44fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 78.16522970795631
  - 34.95174302607775
  - 27.264242213964465
  - 39.001547197997574
  - 22.360060170292854
  - 27.11303800344467
  - 27.174674248695375
  - 15.04803758263588
  - 20.840479588508607
  - 21.15787320137024
  - 21.05337085723877
  - 23.70990372002125
  - 17.26308915615082
  - 17.85137085914612
  - 14.386123210191727
  - 13.192299550771715
  - 22.28181746006012
  - 8.488544145226479
  - 10.459942001104356
  - 13.063553470373154
  - 9.952407842874528
  - 10.774140286445618
  - 15.157670575380326
  - 8.95987648665905
  - 7.521207219362259
  - 8.6692384660244
  - 7.487062576413155
  - 8.096788895130159
  - 9.316461411118508
  - 8.378150635957718
  - 6.932179671525955
  - 6.435555559396744
  - 7.852659460902214
  - 8.034924340248109
  - 6.802367028594017
  - 8.676037147641182
  - 6.948158180713654
  - 7.436337471008301
  - 7.03678534924984
  - 7.077835023403168
  - 6.497388544678689
  - 6.453933051228524
  - 6.418160647153854
  - 6.26306098997593
  - 6.6671594947576525
  - 6.38610731959343
  - 6.741041845083237
  - 6.679879629611969
  - 8.089322102069856
  - 9.30768980383873
  - 7.4084276258945465
  - 8.599096021056175
  - 8.423206144571305
  - 7.47180419266224
  - 6.797068479657174
  - 7.602857583761216
  - 7.1141926646232605
  - 6.8523716807365425
  - 6.712701198458672
  - 7.03501841723919
  - 7.247641250491142
  - 6.6801314651966095
  - 6.863756835460663
  - 6.586847335100174
  - 6.753436514735222
  - 6.988721434772015
  - 6.518609914183617
  - 6.635446983575822
  - 6.333723425865173
  - 6.484663145244122
  - 6.529587063193322
  - 6.685219836235047
  - 6.692599695920944
  - 6.664694574475289
  - 6.718436774611473
  - 6.796158519387245
  - 8.782282945513726
  - 7.376490420103074
  - 6.600864040851594
  - 6.366633471846581
  - 7.736639288067818
  - 6.78550573438406
  - 6.494836238026619
  - 6.496014979481697
  - 6.904040360450745
  - 6.591841363906861
  - 6.476024162769318
  - 6.54946862757206
  - 6.312130865454674
  - 6.358593285083771
  - 6.5629955559968955
  - 6.7904283285141
  - 6.4965307980775835
  - 6.955334424972534
  - 7.4540925353765495
  - 6.9633690267801285
  - 6.287960627675057
  - 6.355541172623635
  - 6.154848873615265
  - 6.221676075458527
  validation_losses:
  - 2.9477415084838867
  - 1.7244927883148193
  - 0.7798722982406616
  - 0.4057246744632721
  - 0.528160572052002
  - 0.9354987144470215
  - 0.4804459512233734
  - 0.5627173781394958
  - 0.5936403274536133
  - 0.4373808801174164
  - 0.3970518112182617
  - 0.5030336380004883
  - 0.6294872760772705
  - 0.4150128960609436
  - 0.4863571524620056
  - 0.49025532603263855
  - 0.6399158835411072
  - 0.43098244071006775
  - 0.46238192915916443
  - 0.4077456295490265
  - 0.43434959650039673
  - 0.42119666934013367
  - 0.41780754923820496
  - 0.44087129831314087
  - 0.4489968717098236
  - 0.4168313145637512
  - 0.3992198705673218
  - 0.43089723587036133
  - 0.629045844078064
  - 0.4083816707134247
  - 0.40900224447250366
  - 0.43718573451042175
  - 0.5852238535881042
  - 0.42932459712028503
  - 0.415738970041275
  - 0.4185843765735626
  - 0.4001706838607788
  - 0.42685508728027344
  - 0.39375099539756775
  - 0.3958657383918762
  - 0.403259813785553
  - 0.4202684164047241
  - 0.4011646807193756
  - 1.391652226448059
  - 0.4143887758255005
  - 1.467126488685608
  - 0.4095574915409088
  - 0.4497019648551941
  - 0.3965928256511688
  - 0.408468633890152
  - 0.4014819264411926
  - 0.4014770984649658
  - 0.3933809697628021
  - 0.40409302711486816
  - 0.429227352142334
  - 0.410856693983078
  - 0.41839343309402466
  - 0.4137062728404999
  - 0.4351453483104706
  - 0.43413272500038147
  - 0.4325331449508667
  - 0.4058082699775696
  - 0.4156900644302368
  - 0.4032022953033447
  - 0.8141621947288513
  - 0.4140276610851288
  - 0.4028068482875824
  - 0.406459242105484
  - 0.41890212893486023
  - 0.4194413721561432
  - 0.39323100447654724
  - 0.4599839746952057
  - 0.4175419211387634
  - 0.4344979226589203
  - 0.41066527366638184
  - 0.6606646776199341
  - 23.360639572143555
  - 0.41580522060394287
  - 24.235450744628906
  - 1027.3765869140625
  - 0.4311448633670807
  - 303.79486083984375
  - 4887.22900390625
  - 0.4438648521900177
  - 95117.21875
  - 96837.3671875
  - 135920.53125
  - 93990.1640625
  - 0.4039234519004822
  - 0.4046684205532074
  - 73.89970397949219
  - 0.5052663683891296
  - 0.4280252158641815
  - 11869.5830078125
  - 55460.82421875
  - 0.4109797775745392
  - 0.3967885971069336
  - 718389.5625
  - 2202.990478515625
  - 140665.59375
loss_records_fold1:
  train_losses:
  - 6.514425140619278
  - 6.296219247579575
  - 6.245043659210205
  - 6.538678675889969
  - 6.370728412270546
  - 6.293759751319886
  - 6.396672460436822
  - 6.154749235510827
  - 6.335566511750222
  - 6.361435818672181
  - 6.196028932929039
  - 6.385308924317361
  - 6.084063702821732
  - 6.481593275070191
  - 6.318167304992676
  - 6.406699678301812
  - 6.285607820749283
  - 6.189554786682129
  - 6.660681256651879
  - 6.498415237665177
  - 6.474669888615608
  - 6.1985173463821415
  - 6.514954373240471
  - 6.383655938506127
  - 6.404988372325898
  - 6.520957201719284
  - 6.371793186664582
  - 6.558607259392739
  - 6.427016639709473
  - 6.5520702958107
  - 6.461573779582977
  - 6.314152762293816
  - 6.444358381628991
  - 6.494627657532693
  - 6.418414697051048
  - 6.236487469077111
  - 6.6835043877363205
  - 6.528873237967492
  - 6.680052596330643
  - 6.508117765188217
  - 6.300569131970406
  - 6.275375071167947
  - 6.521335965394974
  - 6.266305366158486
  - 6.403447261452675
  - 7.070012037456036
  - 6.481159374117851
  - 6.444384202361107
  - 6.546782121062279
  - 6.587773677706719
  - 6.646272048354149
  - 6.364546674489976
  - 6.5827741682529455
  - 6.367074948549271
  - 6.255309173464775
  - 6.30929474234581
  - 6.430160716176033
  - 6.54019373357296
  - 6.700538617372513
  - 6.541785803437233
  - 6.517934346199036
  - 6.331893932819367
  - 6.45855230987072
  - 6.35888797044754
  - 6.299669319391251
  - 6.475332534313202
  - 6.374722009897233
  - 6.587814962863923
  - 6.504783290624619
  - 6.695215725898743
  - 6.351072007417679
  - 6.754706013202668
  - 6.237119030952454
  - 6.571369016170502
  - 6.370876377820969
  - 6.557693862915039
  - 6.263206714391709
  - 6.3923902183771135
  - 6.338240623474121
  - 6.360092639923096
  - 6.281279453635216
  - 6.224808424711227
  - 6.176475143432618
  - 6.386370781064034
  - 6.496681869029999
  - 6.507295915484429
  - 6.29786486029625
  - 6.373039165139199
  - 6.384687602519989
  - 6.387385651469231
  - 6.162611818313599
  - 6.752678275108337
  - 6.755045410990715
  - 6.506765246391296
  - 6.6511063307523735
  - 6.405587592720986
  - 6.379777017235757
  - 6.516259449720383
  - 6.569987332820893
  - 6.216306880116463
  validation_losses:
  - 0.44251304864883423
  - 0.3998415172100067
  - 0.4818127453327179
  - 0.4036036729812622
  - 0.4895724654197693
  - 0.4300236105918884
  - 0.41991621255874634
  - 0.4693700075149536
  - 0.4996640086174011
  - 0.42438963055610657
  - 0.41807469725608826
  - 0.4084826409816742
  - 0.41749829053878784
  - 0.40983784198760986
  - 21723726.0
  - 0.43985357880592346
  - 0.42723575234413147
  - 0.43800902366638184
  - 0.45915451645851135
  - 0.43067437410354614
  - 0.41849657893180847
  - 0.40854179859161377
  - 0.44754067063331604
  - 0.4554124176502228
  - 0.4671342074871063
  - 0.43454450368881226
  - 0.42157554626464844
  - 0.43630459904670715
  - 0.4224650263786316
  - 0.5677065849304199
  - 0.4148597717285156
  - 0.4290739595890045
  - 0.46664538979530334
  - 0.4206143319606781
  - 0.47831034660339355
  - 0.4924193024635315
  - 0.5108195543289185
  - 0.45514535903930664
  - 0.43037089705467224
  - 0.4457700252532959
  - 0.40106427669525146
  - 0.4527115821838379
  - 0.4576375186443329
  - 0.48291295766830444
  - 0.49922415614128113
  - 0.47956565022468567
  - 0.43114203214645386
  - 0.42816805839538574
  - 15159560192.0
  - 2892295045120.0
  - 0.4259626269340515
  - 0.4361823499202728
  - 0.4052869379520416
  - 0.4703487455844879
  - 0.4171351194381714
  - 0.43550997972488403
  - 0.45035308599472046
  - 0.4268001616001129
  - 0.4582383632659912
  - 0.41950294375419617
  - 0.4471738338470459
  - 0.4143110513687134
  - 0.4579775631427765
  - 0.4367557466030121
  - 0.43548935651779175
  - 0.4135344922542572
  - 0.45295605063438416
  - 0.4717355966567993
  - 0.46672946214675903
  - 0.43239808082580566
  - 0.40593940019607544
  - 0.42674386501312256
  - 0.42695537209510803
  - 0.42025983333587646
  - 0.4099428355693817
  - 0.48289594054222107
  - 0.42677074670791626
  - 0.5161183476448059
  - 0.41896307468414307
  - 0.42139172554016113
  - 0.4569733738899231
  - 0.4292944073677063
  - 0.4700744152069092
  - 0.4100455939769745
  - 0.43734705448150635
  - 0.4177555441856384
  - 0.4195193350315094
  - 0.4274568557739258
  - 0.43958452343940735
  - 0.42302948236465454
  - 0.42331013083457947
  - 79492956160.0
  - 5523845939200.0
  - 0.4216839373111725
  - 0.46546828746795654
  - 0.4236232042312622
  - 0.4185506999492645
  - 0.4248897135257721
  - 0.5161024332046509
  - 0.4314180612564087
loss_records_fold2:
  train_losses:
  - 6.5904998838901525
  - 6.395895466208458
  - 6.3646731048822405
  - 6.434388452768326
  - 6.3509703278541565
  - 6.630524078011513
  - 6.414994716644287
  - 6.372032219171524
  - 6.3854303687810905
  - 6.38171820640564
  - 6.946966940164566
  - 6.320543655753136
  - 6.425538486242295
  - 6.403826430439949
  - 6.662892258167267
  - 6.601691266894341
  - 6.341815373301507
  - 6.581318843364716
  - 6.534951525926591
  - 6.658971986174584
  - 6.532028689980507
  - 6.317737117409706
  - 6.379003919661045
  - 6.370195302367211
  - 6.627975827455521
  - 6.559466123580933
  - 6.410990208387375
  - 6.300318536162377
  - 6.298561108112335
  - 6.444487687945366
  - 6.2984093010425575
  - 6.347353252768517
  - 6.80027451813221
  - 6.216868901252747
  - 6.282781949639321
  - 6.482351797819138
  - 6.472619065642357
  - 6.623523342609406
  - 6.5691914558410645
  - 6.268435859680176
  - 6.307655870914459
  - 6.5374464184045795
  - 6.641117417812348
  - 6.293380117416382
  - 6.524505519866944
  - 6.4768787026405334
  - 6.59479051232338
  - 6.430527606606484
  - 6.4612873792648315
  - 6.373771354556084
  - 6.441439613699913
  - 6.385271281003952
  - 6.378256604075432
  - 6.215972110629082
  - 6.484300979971886
  - 6.601095518469811
  - 6.561103034019471
  - 6.490048560500146
  - 6.25877859890461
  - 6.581896913051605
  - 6.286021882295609
  - 6.478195893764497
  - 6.4215100646018985
  - 6.822805747389793
  - 6.433302885293961
  - 6.473734459280968
  - 6.512183824181557
  - 6.426699620485306
  - 6.426217418909073
  - 6.447454425692559
  - 6.38973993062973
  - 6.5285545825958256
  - 6.441749525070191
  - 6.374385806918145
  - 6.509300425648689
  - 6.417090478539468
  - 6.488432270288468
  - 6.527433487772942
  - 6.356608211994171
  - 6.910963875055313
  - 6.504190132021904
  - 6.521522986888886
  - 6.414485877752305
  - 6.4160036444664
  - 6.507735002040864
  - 6.754205478727818
  - 6.307862359285355
  - 6.44956216365099
  - 6.3423903465271
  - 6.453085878491402
  - 6.825770980119706
  - 6.941699340939522
  - 6.355925270915032
  - 6.317912247776985
  - 6.222772347927094
  - 6.52044595181942
  - 6.510762888193131
  - 6.758125141263008
  - 6.392560356855393
  - 6.590530073642731
  validation_losses:
  - 0.4608066976070404
  - 0.401577353477478
  - 0.3991584777832031
  - 0.4092269241809845
  - 0.49928581714630127
  - 0.40984606742858887
  - 0.4083883762359619
  - 0.46441221237182617
  - 0.4102745056152344
  - 0.5091575980186462
  - 0.39626216888427734
  - 0.42928650975227356
  - 0.4100523591041565
  - 0.3983835279941559
  - 0.4514731764793396
  - 0.40388768911361694
  - 0.4155414402484894
  - 0.4173325002193451
  - 0.40333348512649536
  - 0.47742944955825806
  - 0.44774481654167175
  - 0.3980407416820526
  - 0.4519999027252197
  - 0.3988458514213562
  - 0.40351101756095886
  - 0.4588247239589691
  - 0.392518550157547
  - 0.4346878230571747
  - 0.4002642333507538
  - 0.4394783079624176
  - 0.42268049716949463
  - 0.43621304631233215
  - 0.4126701354980469
  - 0.4144521653652191
  - 0.3962867259979248
  - 0.42072317004203796
  - 0.41646328568458557
  - 0.41255369782447815
  - 0.4029395580291748
  - 0.42110368609428406
  - 0.4023236036300659
  - 0.4218786358833313
  - 0.4781540334224701
  - 0.4099707007408142
  - 0.4032615125179291
  - 0.404693067073822
  - 0.4755324423313141
  - 0.39887621998786926
  - 0.40367811918258667
  - 0.4141875207424164
  - 0.44714978337287903
  - 0.40002337098121643
  - 0.397522896528244
  - 0.4080303907394409
  - 0.400684654712677
  - 0.4404138922691345
  - 0.4171426296234131
  - 0.41724807024002075
  - 0.396849125623703
  - 0.40960437059402466
  - 0.4196457862854004
  - 0.46769949793815613
  - 0.39769768714904785
  - 0.4090712070465088
  - 0.4023524522781372
  - 0.3916017711162567
  - 0.407248854637146
  - 0.4178565442562103
  - 0.4080278277397156
  - 0.4159310758113861
  - 0.42477160692214966
  - 0.41202253103256226
  - 0.39630964398384094
  - 0.43642839789390564
  - 0.396992027759552
  - 0.395603209733963
  - 0.41491594910621643
  - 0.4009072482585907
  - 0.40594202280044556
  - 0.4772771894931793
  - 0.41084617376327515
  - 0.4166315495967865
  - 0.41650861501693726
  - 0.42220789194107056
  - 0.5530611276626587
  - 0.41225212812423706
  - 0.43928635120391846
  - 0.43723005056381226
  - 0.399471640586853
  - 0.47130653262138367
  - 0.4321426749229431
  - 0.41900089383125305
  - 0.39362746477127075
  - 0.41181814670562744
  - 0.4189201593399048
  - 0.4166153073310852
  - 0.47825485467910767
  - 0.4211335778236389
  - 0.4008367657661438
  - 0.4081175625324249
loss_records_fold3:
  train_losses:
  - 6.610785523056984
  - 6.50461412370205
  - 6.283256527781487
  - 6.452467125654221
  - 6.496253886818886
  - 6.658006829023361
  - 6.454410922527313
  - 6.510360300540924
  - 6.482653033733368
  - 6.666021347045898
  - 6.523896476626397
  - 6.553006929159165
  - 6.588299006223679
  - 6.473278629779816
  - 6.323284283280373
  - 6.520551103353501
  - 6.405317103862763
  - 6.690103617310524
  - 6.700064250826836
  - 6.307308280467987
  - 6.356244266033173
  - 6.479030016064645
  - 6.291286915540695
  - 6.694114169478417
  - 6.35878176689148
  - 6.466289508342744
  - 6.396725702285767
  - 6.4264184683561325
  - 6.485240155458451
  - 6.298063397407532
  - 6.637231332063675
  - 6.268625023961068
  - 6.363475117087365
  - 6.387898474931717
  - 6.381932786107064
  - 6.318213111162186
  - 6.367150977253914
  - 6.395802375674248
  - 6.420976719260216
  - 6.46585896909237
  - 6.32529485821724
  - 6.689542365074158
  - 6.524830564856529
  - 6.480266550183297
  - 6.2920445770025255
  - 6.240020304918289
  - 6.338640969991684
  - 6.201377457380295
  - 6.535815718770028
  - 6.735712975263596
  - 6.4812140464782715
  - 6.53313102722168
  - 6.359698417782784
  - 6.362404257059097
  - 6.5926989942789085
  - 6.386235255002976
  - 6.178118255734444
  - 6.472881847620011
  - 6.548899385333062
  - 6.438033974170685
  - 6.956623819470406
  - 6.58197238445282
  - 6.498079761862755
  - 6.55021613240242
  - 6.356028133630753
  - 6.398925548791886
  - 6.234147021174431
  - 6.616576230525971
  - 6.56613671630621
  - 6.367828986048699
  - 6.3061234027147295
  - 6.351393696665764
  - 6.506118029356003
  - 6.560717910528183
  - 6.2929878532886505
  - 6.34766430258751
  - 6.459641900658608
  - 6.405927476286888
  - 6.3606500893831255
  validation_losses:
  - 0.41621696949005127
  - 0.42963913083076477
  - 0.411486953496933
  - 0.4150846600532532
  - 0.44395875930786133
  - 0.4902041256427765
  - 0.4213314950466156
  - 0.42078661918640137
  - 0.4544469118118286
  - 0.40778887271881104
  - 0.4180854558944702
  - 0.40734317898750305
  - 0.4569045305252075
  - 0.40665462613105774
  - 0.432270884513855
  - 0.3979054391384125
  - 0.4737909138202667
  - 0.40926092863082886
  - 0.46586111187934875
  - 0.4144684672355652
  - 0.4062778651714325
  - 0.44413530826568604
  - 0.4187667667865753
  - 0.41044408082962036
  - 0.4092041850090027
  - 0.43920812010765076
  - 0.4519793689250946
  - 0.4028891324996948
  - 0.4026505649089813
  - 0.40741831064224243
  - 0.43718284368515015
  - 0.4228767156600952
  - 0.41530609130859375
  - 0.4190150201320648
  - 0.48331862688064575
  - 0.41540125012397766
  - 0.4250621795654297
  - 0.49899372458457947
  - 0.4204455018043518
  - 0.43238121271133423
  - 0.42009028792381287
  - 0.5430130958557129
  - 0.41461795568466187
  - 0.40170687437057495
  - 0.4182303249835968
  - 0.4277358651161194
  - 0.4914340376853943
  - 0.4023285210132599
  - 0.45478302240371704
  - 0.4502716362476349
  - 0.42445266246795654
  - 0.4102938175201416
  - 0.4231564402580261
  - 0.40721845626831055
  - 0.4207630455493927
  - 0.4066051244735718
  - 0.4021483361721039
  - 0.4708428978919983
  - 0.46846458315849304
  - 0.39804738759994507
  - 0.43091413378715515
  - 0.43566763401031494
  - 0.41437116265296936
  - 0.43714773654937744
  - 0.40889331698417664
  - 0.4356323778629303
  - 0.44097012281417847
  - 0.42489880323410034
  - 0.424179345369339
  - 0.4220600426197052
  - 0.4042843282222748
  - 0.4723728597164154
  - 0.48784589767456055
  - 0.41685011982917786
  - 0.41813886165618896
  - 0.4255095422267914
  - 0.43455734848976135
  - 0.4371054768562317
  - 0.4348430037498474
loss_records_fold4:
  train_losses:
  - 6.392810088396073
  - 6.587356522679329
  - 6.41192955672741
  - 6.348279035091401
  - 6.488766729831696
  - 6.983118858933449
  - 6.577213016152382
  - 6.8862363487482074
  - 6.503268894553185
  - 6.559177961945534
  - 6.199441096186638
  - 6.390222582221032
  - 6.5316258609294895
  - 6.381073540449143
  - 6.396453288197518
  - 6.901200896501542
  - 6.529680165648461
  - 6.576695686578751
  - 6.50326406955719
  - 6.709396713972092
  - 6.4835738182067875
  - 6.629626166820526
  validation_losses:
  - 0.4155897796154022
  - 0.40583956241607666
  - 0.40679094195365906
  - 0.4322315752506256
  - 0.3963701128959656
  - 0.4254762828350067
  - 0.41675233840942383
  - 0.4164763391017914
  - 0.42289936542510986
  - 0.40332916378974915
  - 0.42551755905151367
  - 0.4085538685321808
  - 0.4373270869255066
  - 0.4124032258987427
  - 0.40570738911628723
  - 0.4644864499568939
  - 0.43493616580963135
  - 0.4329179525375366
  - 0.4304805397987366
  - 0.4078907072544098
  - 0.4109322428703308
  - 0.4041532576084137
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 79 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:42:46.026635'
