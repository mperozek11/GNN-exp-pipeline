config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 04:34:05.917186'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/124/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/124/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/124/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/124/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/124/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/124/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/124/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/124/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/124/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/124/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 204.03898692131042
  - 53.62997141480446
  - 42.560930788517
  - 21.99080002307892
  - 16.125551223754883
  - 17.265619307756424
  - 21.69950705766678
  - 16.33635452389717
  - 10.897548854351044
  - 13.220634043216705
  - 10.838912785053253
  - 10.927161365747452
  - 11.093805849552155
  - 10.299584925174713
  - 9.95546381175518
  - 9.033458709716797
  - 27.84255139529705
  - 16.85708224773407
  - 20.38709244132042
  - 14.255837887525558
  - 15.618501007556915
  - 29.720830529928207
  - 54.09690651297569
  - 34.443713426589966
  - 29.551243007183075
  - 28.178757309913635
  - 27.149574398994446
  - 17.783939361572266
  - 15.698670029640198
  - 9.764886319637299
  - 12.788870811462402
  - 53.03067925572395
  - 42.0092910528183
  - 40.18668693304062
  - 17.011671602725983
  - 11.973334312438965
  - 13.538115710020065
  - 14.384922325611115
  - 10.791653886437416
  - 11.411210656166077
  - 24.199028879404068
  - 11.109136670827866
  - 17.98833990097046
  - 14.65267127752304
  - 14.147747874259949
  - 12.17193654179573
  - 9.535873919725418
  - 8.49286949634552
  - 10.472620218992233
  - 10.461347475647926
  - 9.575366407632828
  - 10.073412656784058
  - 9.970391392707825
  - 9.313378274440765
  - 10.859875530004501
  - 12.251244485378265
  - 9.615055233240128
  - 9.62001422047615
  - 9.88115793466568
  - 8.63479208946228
  - 9.095823660492897
  - 9.313837796449661
  - 12.402755841612816
  - 15.53740519285202
  - 11.582606375217438
  - 11.119261533021927
  - 9.014805287122726
  - 9.389673620462418
  - 10.62950474023819
  - 9.785712078213692
  - 13.556609183549881
  - 20.519507706165314
  - 13.470051944255829
  - 15.021805703639984
  - 8.496156573295593
  - 17.94753435254097
  - 11.304875433444977
  - 9.105842769145966
  - 9.527122586965561
  - 8.789437353610992
  - 10.09757262468338
  - 10.380049049854279
  - 10.656952500343323
  - 8.644077688455582
  - 16.41544359922409
  - 9.321579456329346
  - 8.683057874441147
  - 8.563663899898529
  - 11.270430818200111
  - 8.197829335927963
  - 10.220882385969162
  - 9.22317761182785
  - 14.329269379377365
  - 17.780917197465897
  - 12.159754902124405
  - 18.947461932897568
  - 12.369592279195786
  - 9.473871141672134
  - 10.235886007547379
  - 8.861585021018982
  validation_losses:
  - 1.8759692907333374
  - 1.4835772514343262
  - 1.799607753753662
  - 0.6037589311599731
  - 0.5471915006637573
  - 0.5676601529121399
  - 0.5117135643959045
  - 0.6599785685539246
  - 0.5602667331695557
  - 0.4712887704372406
  - 0.4063910245895386
  - 0.5455154776573181
  - 0.4998622536659241
  - 0.45822039246559143
  - 0.40357697010040283
  - 0.44232237339019775
  - 0.688377857208252
  - 0.5458997488021851
  - 0.4679878056049347
  - 0.45888209342956543
  - 0.5648925304412842
  - 0.5094733834266663
  - 0.9663820862770081
  - 0.5156604647636414
  - 0.4936350882053375
  - 0.5231052041053772
  - 0.505560576915741
  - 0.6601261496543884
  - 0.4563423991203308
  - 0.4242003560066223
  - 0.4122571349143982
  - 0.47379666566848755
  - 1.912175178527832
  - 1.1025968790054321
  - 0.575704038143158
  - 0.8179447054862976
  - 0.3990500569343567
  - 0.6617312431335449
  - 0.6624940037727356
  - 0.8831144571304321
  - 0.4842627942562103
  - 0.4435455799102783
  - 0.47759267687797546
  - 0.46197059750556946
  - 0.4142877459526062
  - 0.4034092128276825
  - 0.39792340993881226
  - 0.41906043887138367
  - 0.40919965505599976
  - 0.4809887707233429
  - 0.5071516633033752
  - 0.4168243408203125
  - 0.43427935242652893
  - 0.42838776111602783
  - 0.4356392025947571
  - 0.4419291913509369
  - 0.4284704625606537
  - 0.39164212346076965
  - 0.4174902141094208
  - 0.39157381653785706
  - 0.3994658887386322
  - 0.41248464584350586
  - 0.4784165620803833
  - 0.4615654945373535
  - 0.4022645056247711
  - 0.4343532919883728
  - 0.3866955041885376
  - 0.690604567527771
  - 0.3995586037635803
  - 0.5264413952827454
  - 0.39785632491111755
  - 0.4825657904148102
  - 0.9274895191192627
  - 0.41184669733047485
  - 0.4294397234916687
  - 0.5006897449493408
  - 0.42753276228904724
  - 0.3995591104030609
  - 0.43389344215393066
  - 0.42689549922943115
  - 0.39666056632995605
  - 0.520630955696106
  - 0.4227280914783478
  - 0.4815499782562256
  - 0.4386955499649048
  - 0.3825814425945282
  - 0.3932037651538849
  - 0.490032434463501
  - 0.41838160157203674
  - 0.4163667857646942
  - 0.45388832688331604
  - 0.42504024505615234
  - 0.44934219121932983
  - 0.41305404901504517
  - 0.39520448446273804
  - 0.41097667813301086
  - 0.4603344798088074
  - 0.43071267008781433
  - 0.46079784631729126
  - 0.4111582934856415
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 92 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 9 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 72 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:04:47.274391'
