config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:46:17.775084'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_112fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_112fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_112fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_112fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_112fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_112fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_112fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_112fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_112fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_112fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.4608240038156515
  - 6.2001352429389955
  - 6.035816642642022
  - 6.102700319886208
  - 6.380613836646081
  - 5.891050931811333
  - 6.0407211750745775
  - 5.909110102057458
  - 5.989182609319688
  - 7.048397332429886
  - 5.804645824432374
  validation_losses:
  - 0.4093169569969177
  - 0.43046146631240845
  - 0.39966481924057007
  - 0.4012393653392792
  - 0.3933248519897461
  - 0.3922443687915802
  - 0.3976075053215027
  - 0.3922692537307739
  - 0.40082353353500366
  - 0.3949087858200073
  - 0.4005573093891144
loss_records_fold1:
  train_losses:
  - 5.916108945012093
  - 5.8375803351402284
  - 5.924789118766785
  - 5.854639720916748
  - 6.036950755119324
  - 5.750244382023812
  - 5.834605583548546
  - 5.635676172375679
  - 5.825760987401009
  - 5.661642137169839
  - 5.639954391121865
  - 5.663022619485855
  - 5.642917847633362
  - 5.786086565256119
  - 5.61366870701313
  - 5.550503951311112
  - 5.5448327541351325
  - 5.538002753257752
  - 5.451747161149979
  - 5.756008253991604
  - 5.600713708996773
  - 5.629210507869721
  - 5.624280253052712
  - 5.598867297172546
  - 5.646634939312936
  - 5.547934785485268
  - 5.560022929310799
  - 5.53313050866127
  - 5.594368356466294
  - 5.549015626311302
  - 5.583569276332856
  - 5.504875740408898
  - 5.5494604974985124
  - 5.497219914197922
  - 5.457426279783249
  - 5.548064374923706
  - 5.488084208965302
  - 5.603278669714928
  - 5.538454410433769
  - 5.532839676737786
  - 5.534459140896797
  - 5.57499142587185
  - 5.545377212762833
  - 5.508865189552307
  - 5.4978218376636505
  - 5.523441550135613
  - 5.493570044636726
  - 5.416673308610917
  - 5.614113116264344
  - 5.511813935637474
  - 5.432782009243965
  - 5.54188075363636
  - 5.55252937078476
  - 5.559994798898697
  - 5.564846536517144
  - 5.565993481874466
  - 5.505569297075272
  - 5.495243695378304
  - 5.614121899008751
  - 5.544336134195328
  - 5.645534473657609
  - 5.527500867843628
  - 5.492322337627411
  - 5.572287008166313
  - 5.471439105272293
  - 5.5126911625266075
  - 5.458629789948464
  - 5.4803945332765585
  - 5.509498366713524
  - 5.453387942910195
  - 5.522157171368599
  - 5.499236544966698
  - 5.547934374213219
  - 5.43718838095665
  - 5.507486966252327
  - 5.5622260004282005
  - 5.4654586762189865
  - 5.435310247540475
  - 5.665687307715416
  - 5.496693974733353
  - 5.421996578574181
  - 5.477158311009408
  - 5.582885175943375
  - 5.516213965415955
  - 5.458841365575791
  - 5.4961283832788475
  - 5.4548919886350635
  - 5.526761409640312
  - 5.549839371442795
  - 5.490412431955338
  - 5.465939921140671
  - 5.5808297991752625
  - 5.44057012796402
  - 5.476022726297379
  - 5.47989416718483
  - 5.450316828489304
  - 5.474965208768845
  - 5.453123918175698
  - 5.482991492748261
  - 5.470564836263657
  validation_losses:
  - 0.3902524709701538
  - 0.3908071517944336
  - 0.40932410955429077
  - 2.5089263916015625
  - 0.5828608274459839
  - 0.45312851667404175
  - 0.41989198327064514
  - 0.7420298457145691
  - 0.4425472915172577
  - 0.6369162201881409
  - 0.5589154362678528
  - 0.5012792348861694
  - 0.5833067297935486
  - 0.44106265902519226
  - 2.1738555431365967
  - 1.2692698240280151
  - 0.40716153383255005
  - 2.6923670768737793
  - 0.5103359222412109
  - 0.491712749004364
  - 0.4123215675354004
  - 0.5230502486228943
  - 0.39513811469078064
  - 0.3890017569065094
  - 0.44966578483581543
  - 0.4202430546283722
  - 0.43542006611824036
  - 0.45945852994918823
  - 0.612209141254425
  - 0.5999208688735962
  - 0.6525091528892517
  - 0.6947837471961975
  - 0.606432318687439
  - 0.5484856963157654
  - 0.7528412342071533
  - 0.4397982060909271
  - 0.6972445845603943
  - 0.41221678256988525
  - 0.4022853672504425
  - 0.49130919575691223
  - 0.8676096200942993
  - 0.6287373900413513
  - 0.6865525841712952
  - 0.5780822038650513
  - 0.5429791212081909
  - 0.5741528868675232
  - 0.5987585783004761
  - 0.5637184977531433
  - 0.49398884177207947
  - 0.7343745231628418
  - 0.5449140667915344
  - 0.6981096863746643
  - 0.9452961683273315
  - 0.42528656125068665
  - 0.4178542196750641
  - 0.38622695207595825
  - 0.4141390323638916
  - 0.39009609818458557
  - 0.38189995288848877
  - 0.4536568522453308
  - 0.379411906003952
  - 0.3874998092651367
  - 0.40422478318214417
  - 0.4320184290409088
  - 0.4846678376197815
  - 0.5681509375572205
  - 0.4847617447376251
  - 0.5667529106140137
  - 0.49734044075012207
  - 0.5006600618362427
  - 0.6762722730636597
  - 0.5599384307861328
  - 0.4610573947429657
  - 0.4628959894180298
  - 0.48553746938705444
  - 0.557693600654602
  - 1.0289862155914307
  - 0.527488648891449
  - 0.4055037796497345
  - 0.58692866563797
  - 0.9112687110900879
  - 0.47387146949768066
  - 1.3601763248443604
  - 0.5239816904067993
  - 0.7470692992210388
  - 0.5203617811203003
  - 0.509380042552948
  - 0.5330910682678223
  - 0.88638836145401
  - 0.4050343632698059
  - 0.4340269863605499
  - 0.40310922265052795
  - 0.8925129175186157
  - 0.5135512948036194
  - 0.8525671362876892
  - 0.6552669405937195
  - 0.3918088972568512
  - 0.4932709038257599
  - 0.6459113359451294
  - 1.4429166316986084
loss_records_fold2:
  train_losses:
  - 5.470006689429283
  - 5.488220041990281
  - 5.524481871724129
  - 5.545527833700181
  - 5.418132185935974
  - 5.451217204332352
  - 5.518184158205987
  - 5.5886198580265045
  - 5.529000291228295
  - 5.577415242791176
  - 5.46753979921341
  - 5.512925726175308
  - 5.545696631073952
  - 5.494219103455544
  - 5.462253457307816
  - 5.514850315451622
  - 5.441572922468186
  - 5.447073236107826
  - 5.469725811481476
  - 5.5271515369415285
  - 5.448091343045235
  - 5.47326894402504
  - 5.418099865317345
  - 5.4896363675594335
  - 5.457657417654992
  - 5.425683015584946
  - 5.594920966029168
  - 5.517276847362519
  - 5.479731965065003
  - 5.519041150808334
  - 5.381149831414223
  - 5.435813611745835
  - 5.462518656253815
  - 5.406005269289017
  - 5.5361221760511405
  - 5.458797606825829
  - 5.497128140926361
  - 5.360339504480362
  - 5.583293235301972
  - 5.457440993189812
  - 5.418431025743485
  - 5.379293280839921
  - 5.477361392974854
  - 5.446397045254708
  - 5.3707205861806875
  - 5.389978447556496
  - 5.352971833944321
  - 5.420711947977543
  - 5.437852993607521
  - 5.366833421587945
  - 5.5464431792497635
  - 5.394579395651817
  - 5.405742590129376
  - 5.461302709579468
  - 5.41271048784256
  - 5.401038929820061
  - 5.456902697682381
  - 5.464007928967476
  - 5.388957473635674
  - 5.417133754491807
  - 5.415383169054985
  - 5.2739347368478775
  - 5.319869726896286
  - 5.325155901908875
  - 5.431118363142014
  - 5.394821646809579
  - 5.469290451705456
  - 5.375506895780564
  - 5.314060208201409
  - 5.504004976153374
  - 5.468227165937424
  - 5.488393107056618
  - 5.336249539256096
  - 5.452420195937157
  - 5.318928334116936
  - 5.374164068698883
  - 5.308032044768334
  - 5.30775386095047
  - 5.392604540288449
  - 5.230981609225274
  - 5.316937363147736
  - 5.32907230257988
  - 5.39625074416399
  - 5.332195112109185
  - 5.367545974254608
  - 5.283358162641526
  - 5.333111995458603
  - 5.477118760347366
  - 5.3979436457157135
  - 5.379803749918938
  - 5.430478268861771
  - 5.407534226775169
  - 5.413278356194496
  - 5.413576683402062
  - 5.3342200756073
  - 5.413812318444252
  - 5.378896594047546
  - 5.450144177675248
  - 5.36240263581276
  - 5.348027178645134
  validation_losses:
  - 0.48813721537590027
  - 0.6620540618896484
  - 0.7118825912475586
  - 0.6360610127449036
  - 0.6076158881187439
  - 0.5300662517547607
  - 4.0204291343688965
  - 0.4549996852874756
  - 12.398191452026367
  - 1.0996862649917603
  - 4.115537643432617
  - 1.6237322092056274
  - 9.960657119750977
  - 0.8034303784370422
  - 1.4838078022003174
  - 0.9449281096458435
  - 1.8017737865447998
  - 3.2282602787017822
  - 3.7959413528442383
  - 2.717108964920044
  - 1.72309410572052
  - 0.3808363080024719
  - 1.1645535230636597
  - 4.052651405334473
  - 0.6997422575950623
  - 10.244510650634766
  - 2.4975342750549316
  - 0.7316263318061829
  - 0.6852666139602661
  - 2.1592934131622314
  - 0.5715622305870056
  - 0.4120156168937683
  - 1.160820484161377
  - 3.3657379150390625
  - 0.49810290336608887
  - 0.8713809251785278
  - 0.8939008116722107
  - 2.1718757152557373
  - 0.4694310426712036
  - 0.44692859053611755
  - 0.5697765946388245
  - 4.620441436767578
  - 1.4974217414855957
  - 1.9055900573730469
  - 11.416741371154785
  - 0.6943074464797974
  - 0.7749989628791809
  - 5.424326419830322
  - 1.5836784839630127
  - 2.775975227355957
  - 3.112720251083374
  - 6.738267421722412
  - 2.7989470958709717
  - 14.385774612426758
  - 33.73329162597656
  - 1.2075376510620117
  - 1.5641177892684937
  - 1.6955033540725708
  - 8.246142387390137
  - 4.368091106414795
  - 7.934621810913086
  - 9.236576080322266
  - 12.11257266998291
  - 5.000880718231201
  - 1.7416200637817383
  - 5.268723487854004
  - 1.9687565565109253
  - 9.028204917907715
  - 3.054124593734741
  - 0.40218353271484375
  - 0.3831981122493744
  - 1.3393069505691528
  - 2.312382698059082
  - 3.8340046405792236
  - 2.1016807556152344
  - 4.38917350769043
  - 1.1335818767547607
  - 1.185124397277832
  - 5.314291000366211
  - 1.1999661922454834
  - 0.998385488986969
  - 0.9356426000595093
  - 0.7720133662223816
  - 2.2713370323181152
  - 5.012606620788574
  - 5.6839423179626465
  - 1.0570573806762695
  - 1.3578909635543823
  - 0.601665198802948
  - 0.9891033172607422
  - 2.193587064743042
  - 0.6734922528266907
  - 0.6955170035362244
  - 0.45352351665496826
  - 1.81504487991333
  - 0.5995837450027466
  - 3.336792469024658
  - 0.6537526249885559
  - 2.1840901374816895
  - 4.487924098968506
loss_records_fold3:
  train_losses:
  - 5.49592092037201
  - 5.362256629765034
  - 5.381528967618943
  - 5.370659491419793
  - 5.332701128721237
  - 5.477671849727631
  - 5.39118499159813
  - 5.434045395255089
  - 5.351370239257813
  - 5.5135608404874805
  - 5.4538138002157215
  - 5.481505066156387
  - 5.435651162266732
  - 5.480005991458893
  - 5.551576101779938
  - 5.400264489650727
  - 5.459764331579208
  - 5.445834282040597
  - 5.354722830653191
  - 5.331788602471352
  - 5.405133983492852
  - 5.319432178139687
  - 5.355417552590371
  - 5.313553354144097
  - 5.341427490115166
  - 5.386444029211998
  - 5.332739308476448
  - 5.685185614228249
  - 5.6158747464418415
  - 5.3944000035524375
  - 5.330991733074189
  - 5.385921385884285
  - 5.392157492041588
  - 5.385983496904373
  - 5.373629227280617
  - 5.262258386611939
  - 5.436271181702614
  - 5.420755848288536
  - 5.354544007778168
  - 5.39509679377079
  - 5.302021625638009
  - 5.310118117928505
  - 5.323895215988159
  - 5.293827149271966
  - 5.361523172259331
  - 5.472304281592369
  - 5.311748987436295
  - 5.321147763729096
  - 5.495395538210869
  - 5.374631643295288
  - 5.35265970826149
  - 5.317411705851555
  - 5.340178388357163
  - 5.342986239492894
  - 5.259182631969452
  - 5.299036517739296
  - 5.33923546075821
  - 5.210681921243668
  - 5.281334805488587
  - 5.288063713908196
  - 5.620913082361222
  - 5.310177797079087
  - 5.310889303684235
  - 5.406058922410011
  - 5.405321222543717
  - 5.331835788488388
  - 5.305885204672814
  - 5.252926108241081
  - 5.2614354461431505
  - 5.311380970478059
  - 5.268489581346512
  - 5.243210652470589
  - 5.251933991909027
  - 5.316086608171464
  - 5.2090649813413625
  - 5.37604479789734
  - 5.251148614287377
  - 5.188603964447975
  - 5.249163463711739
  - 5.2619900643825535
  - 5.271176075935364
  - 5.231076776981354
  - 5.303506508469582
  - 5.29302319586277
  - 5.249548670649529
  - 5.2215402156114585
  - 5.289689531922341
  - 5.2119806706905365
  - 5.264024223387242
  - 5.237431094050407
  - 5.449286831915379
  - 5.277563861012459
  - 5.269972893595696
  - 5.272430440783501
  - 5.2201364815235145
  - 5.21446382701397
  - 5.262904277443886
  - 5.270759922266007
  - 5.129537725448609
  - 5.119954431056977
  validation_losses:
  - 2.9484801292419434
  - 6.301241397857666
  - 8.383371353149414
  - 6.325321197509766
  - 13.815010070800781
  - 3.5390734672546387
  - 6.6766886711120605
  - 0.39208853244781494
  - 1.09804368019104
  - 7.59035587310791
  - 1.0972766876220703
  - 5.479996204376221
  - 4.511474132537842
  - 4.531238079071045
  - 2.1663942337036133
  - 1.6160589456558228
  - 6.252342224121094
  - 3.7388386726379395
  - 6.717509746551514
  - 8.06240177154541
  - 12.693207740783691
  - 3.370859384536743
  - 5.193990230560303
  - 3.4906084537506104
  - 3.71282696723938
  - 1.9040088653564453
  - 3.7910449504852295
  - 1.8526978492736816
  - 8.952432632446289
  - 2.672534942626953
  - 3.3265178203582764
  - 18.200565338134766
  - 1.7466408014297485
  - 1.3408771753311157
  - 1.6312015056610107
  - 0.6225599646568298
  - 0.8846855759620667
  - 1.3591147661209106
  - 1.9783871173858643
  - 2.8543920516967773
  - 1.1405333280563354
  - 1.2847871780395508
  - 0.5306305289268494
  - 2.1933131217956543
  - 9.015358924865723
  - 4.4110612869262695
  - 4.185670375823975
  - 1.6479812860488892
  - 1.2317231893539429
  - 3.401148796081543
  - 2.565516710281372
  - 0.8430001735687256
  - 2.5282838344573975
  - 5.504414081573486
  - 9.211287498474121
  - 7.39882230758667
  - 7.604834079742432
  - 5.962043285369873
  - 5.462988376617432
  - 8.028791427612305
  - 3.0018088817596436
  - 7.563958168029785
  - 20.461997985839844
  - 4.400732517242432
  - 4.154021263122559
  - 2.8343234062194824
  - 4.163912773132324
  - 3.7489614486694336
  - 2.482952356338501
  - 2.3859875202178955
  - 1.1893084049224854
  - 1.1213420629501343
  - 2.0140326023101807
  - 1.2546228170394897
  - 1.0664957761764526
  - 0.8912190198898315
  - 0.8895158171653748
  - 2.9751741886138916
  - 0.9067342281341553
  - 0.7306737303733826
  - 5.0589919090271
  - 6.337207317352295
  - 20.18134880065918
  - 11.250781059265137
  - 3.6465187072753906
  - 23.531158447265625
  - 41.1971435546875
  - 12.030529975891113
  - 13.42721176147461
  - 4.653295516967773
  - 6.51218843460083
  - 5.999451637268066
  - 21.58172607421875
  - 9.770919799804688
  - 17.877899169921875
  - 3.414940357208252
  - 9.199048042297363
  - 4.745718955993652
  - 8.084333419799805
  - 2.8997843265533447
loss_records_fold4:
  train_losses:
  - 5.211280447244644
  - 5.202565601468087
  - 5.279898458719254
  - 5.260632237792016
  - 5.310296471416951
  - 5.311090400815011
  - 5.258862009644509
  - 5.161184930801392
  - 5.323665145039559
  - 5.208737745881081
  - 5.3071177273988726
  - 5.3305396974086765
  - 5.256641325354576
  - 5.299416238069535
  - 5.284937006235123
  - 5.262312728166581
  - 5.202762162685395
  - 5.30891399383545
  - 5.232364949584007
  - 5.332678592205048
  - 5.246901059150696
  - 5.1769134432077415
  - 5.147828993201256
  - 5.3229528814554214
  - 5.24002757370472
  - 5.259548264741898
  - 5.251314815878868
  - 5.189504384994507
  - 5.243282580375672
  - 5.246114104986191
  - 5.2823528289794925
  - 5.21131021976471
  - 5.212689056992531
  - 5.314583674073219
  - 5.209902539849281
  - 5.312973842024803
  - 5.2631406635046005
  - 5.2864710032939914
  - 5.217952375113964
  - 5.204299324750901
  - 5.2228850573301315
  - 5.144701558351517
  - 5.153935331106187
  - 5.259616646170617
  - 5.177471613883973
  - 5.175566405057907
  - 5.237525555491448
  - 5.247226279973984
  - 5.211752876639366
  - 5.191702562570573
  - 5.17480163872242
  - 5.1133234977722175
  - 5.635246166586876
  - 5.5620221376419074
  - 5.602583119273186
  - 5.561970269680024
  - 5.516664153337479
  validation_losses:
  - 0.8600688576698303
  - 0.9103651642799377
  - 0.9937735795974731
  - 0.8726456165313721
  - 0.7076102495193481
  - 0.8462719917297363
  - 1.0011389255523682
  - 1.0970815420150757
  - 0.5412465333938599
  - 0.7392933964729309
  - 0.8663554787635803
  - 1.2519525289535522
  - 1.3725237846374512
  - 0.9742825031280518
  - 0.8857610821723938
  - 0.8579069375991821
  - 0.7000225186347961
  - 0.6529686450958252
  - 0.8516753315925598
  - 0.6233320236206055
  - 0.7217497229576111
  - 0.8219987154006958
  - 0.8683757185935974
  - 1.0457401275634766
  - 0.9121328592300415
  - 1.4525765180587769
  - 2.506953477859497
  - 0.569538950920105
  - 0.6926112174987793
  - 0.7056854963302612
  - 0.8543321490287781
  - 0.6516745686531067
  - 0.5819995403289795
  - 0.7502318620681763
  - 0.7800614237785339
  - 0.7838660478591919
  - 0.82048499584198
  - 0.8292975425720215
  - 0.8144991397857666
  - 0.8788757920265198
  - 0.6733811497688293
  - 0.6331316828727722
  - 0.6186016201972961
  - 0.6449212431907654
  - 0.7464430332183838
  - 0.6706436276435852
  - 0.5978028774261475
  - 0.6280543804168701
  - 0.6867772340774536
  - 0.6460801959037781
  - 0.6655433177947998
  - 0.6036729216575623
  - 0.36694657802581787
  - 0.36543530225753784
  - 0.367861807346344
  - 0.37000828981399536
  - 0.36606836318969727
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 57 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8130360205831904, 0.8404802744425386, 0.7753001715265866,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.11382113821138211, 0.20512820512820512, 0.25142857142857145,
    0.0]'
  mean_eval_accuracy: 0.8291111857733137
  mean_f1_accuracy: 0.11407558295363174
  total_train_time: '0:39:01.789325'
