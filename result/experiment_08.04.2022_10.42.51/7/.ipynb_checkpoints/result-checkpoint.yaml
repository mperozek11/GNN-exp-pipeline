config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 11:00:40.684305'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/7/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/7/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/7/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/7/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/7/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/7/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/7/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/7/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/7/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/7/fold_4_state_dict.pt
loss_records:
  train_losses:
  - 15.267040431499481
  - 15.151765644550323
  - 15.228901207447052
  - 15.011434972286224
  - 15.011587679386139
  - 15.0490140914917
  - 15.411448270082474
  - 15.414881139993668
  - 15.486232981085777
  - 16.09275172650814
  - 15.23217985033989
  - 14.980634406208992
  - 15.318349570035934
  - 15.622490674257278
  - 15.201011389493942
  - 15.17171335220337
  - 15.689253956079483
  - 15.108988374471664
  - 15.039293587207794
  - 15.11387214064598
  - 15.125957787036896
  - 15.021598786115646
  - 15.090414866805077
  - 15.165203019976616
  - 17.82904089987278
  - 15.157721042633057
  - 15.131031274795532
  - 15.238477677106857
  - 15.216789722442627
  - 15.14770421385765
  - 15.115785092115402
  - 15.437448561191559
  - 15.772053599357605
  - 15.651531010866165
  - 15.415257260203362
  - 15.43879508972168
  - 15.581225216388702
  - 15.12217329442501
  - 15.373476013541222
  - 15.533849403262138
  - 15.90810789167881
  - 15.113924384117126
  - 15.196390196681023
  - 15.080870300531387
  - 15.202110767364502
  - 15.18798416852951
  - 15.064502537250519
  - 14.896079018712044
  - 15.03423723578453
  - 15.344094187021255
  - 15.008820116519928
  - 15.166453629732132
  - 15.281647264957428
  - 15.837474703788757
  - 15.286000698804855
  - 15.064861446619034
  - 15.1721131503582
  - 15.335694521665573
  - 15.257242858409882
  - 15.046616345643997
  - 15.189857959747314
  - 15.212787538766861
  - 15.94514325261116
  - 15.566932663321495
  - 15.628473043441772
  - 15.27846246957779
  - 15.053106725215912
  - 15.334268629550934
  - 15.034609496593475
  - 15.480944767594337
  - 15.532068252563477
  - 15.904710680246353
  - 15.569049090147018
  - 15.55382764339447
  - 15.243505030870438
  - 15.265956044197083
  - 15.12009434401989
  - 15.41860482096672
  - 15.277619689702988
  - 15.111594259738922
  - 15.075469970703125
  - 15.3341613560915
  - 15.187948554754257
  - 15.273770600557327
  - 15.097935676574707
  - 15.185803920030594
  - 15.28823247551918
  - 15.356828436255455
  - 15.21192279458046
  - 15.098689898848534
  - 15.784365832805634
  - 16.36844350397587
  - 15.320657700300217
  - 15.206255048513412
  - 15.386839538812637
  - 15.327190101146698
  - 15.274809300899506
  - 15.361441195011139
  - 15.064842537045479
  - 15.185157477855682
  validation_losses:
  - 3.0866180908230115e+19
  - 2.6942426329953337e+19
  - 1.866755679489255e+19
  - 6.54468463466224e+19
  - 3.10000288567258e+19
  - 6.9548275406878015e+19
  - 4.485239542405241e+19
  - 2.6258618058406887e+19
  - 8.56105262056158e+19
  - 2.850892253627836e+19
  - 1.9604837681013457e+19
  - 5.449260111509009e+19
  - 1.7044530737541703e+20
  - 9.582906181872412e+19
  - 1.257884490992483e+20
  - 7.138335591558965e+19
  - 6.570240363524312e+19
  - 7.507942142696332e+19
  - 2.6728503148629983e+19
  - 9.421961429017166e+19
  - 6.412607339691927e+19
  - 8.856334824489196e+19
  - 1.2910755804415525e+20
  - 1.1454756118635479e+20
  - 17.88167381286621
  - 114926.6484375
  - 60872052.0
  - 132599406592.0
  - 4127343849242624.0
  - 7.801116912686989e+18
  - 1.7038760500519135e+20
  - 6.561131041629583e+20
  - 7.402273973533105e+20
  - 3.7166976271665594e+20
  - 4.938473540576367e+20
  - 9.296979854326938e+20
  - 9.71094158413809e+20
  - 2.2111393459245102e+20
  - 7.151895648562001e+20
  - 4.906706978392244e+20
  - 2.3590051880650356e+20
  - 6.594356347880509e+20
  - 3.4339809939648886e+20
  - 6.803763878929052e+20
  - 1.0798493340153655e+21
  - 2.4252939524552786e+21
  - 1.2298492314683808e+20
  - 2.2387406062188967e+20
  - 9.044066143503112e+20
  - 8.753764815210252e+20
  - 9.333624377857456e+20
  - 1.466020106990015e+20
  - 4.821446800571703e+20
  - 7.376045431515764e+20
  - 7.622991872833324e+20
  - 1.1965651849082527e+21
  - 3.876773853796793e+20
  - 1.6743978794284483e+20
  - 1.99528093617534e+20
  - 8.428389593756918e+20
  - 7.416425127987233e+20
  - 8.978590841795563e+20
  - 8.405611231266608e+20
  - 2.883803495377404e+20
  - 1.4449057717854604e+21
  - 9.443113217985809e+20
  - 2.3001380743857224e+21
  - 6.381089091213821e+20
  - 1.624206844878148e+20
  - 4.538738963463054e+20
  - 2.994842734861851e+20
  - 5.432626435721223e+20
  - 3.755137961048492e+20
  - 3.317260357997397e+20
  - 1.0473092785952417e+20
  - 3.6383075498400835e+21
  - 6.73911118784094e+20
  - 1.9325521617374172e+21
  - 2.4081579317949943e+20
  - 9.210404484677716e+20
  - 4.730826283663428e+20
  - 7.098005856895862e+20
  - 2.7168167062252028e+20
  - 4.214726784584442e+20
  - 3.611018902691706e+20
  - 8.725346046030381e+19
  - 3.5624250627123787e+20
  - 3.691300140017957e+20
  - 1.2201416696950256e+21
  - 6.657727624137146e+20
  - 1.8629655309669158e+20
  - 1.6584649883717416e+20
  - 1.1643916802765504e+21
  - 1.1813212038570453e+21
  - 8.845249108453307e+20
  - 2.7613956575054738e+20
  - 1.763189688597406e+20
  - 2.1360589921116865e+21
  - 1.8523694054686432e+20
  - 3.100662900512501e+20
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.1423670668953688, 0.8576329331046312, 0.8593481989708405, 0.14065180102915953,
    0.140893470790378]'
  fold_eval_f1: '[0.2492492492492493, 0.0, 0.0, 0.24661654135338348, 0.24698795180722893]'
  mean_eval_accuracy: 0.4281786941580756
  mean_f1_accuracy: 0.14857074848197235
  total_train_time: '0:11:37.436138'
