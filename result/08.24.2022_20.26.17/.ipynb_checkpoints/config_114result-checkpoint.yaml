config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:03:48.163945'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_114fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_114fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_114fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_114fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_114fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_114fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_114fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_114fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_114fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_114fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.752699238061905
  - 1.5633702039718629
  - 1.5342747509479524
  - 1.5108968913555145
  - 1.635446047782898
  - 1.5421664118766785
  - 1.5301568150520326
  - 1.4523663699626923
  - 1.4616056382656097
  - 1.4350810766220095
  - 1.4625668108463288
  - 1.459195852279663
  - 1.4445268213748932
  - 1.5775250256061555
  - 1.4960686147212983
  - 1.4625540673732758
  - 1.4606062352657319
  - 1.4237917780876161
  - 1.4634866237640383
  - 1.4478244960308075
  - 1.5154902637004852
  - 1.7110105991363527
  - 1.6747602164745332
  - 1.4832066714763643
  - 1.6942502856254578
  - 1.4550842642784119
  - 1.480498480796814
  - 1.5763884246349336
  - 1.5314943909645082
  - 1.4806965172290802
  validation_losses:
  - 0.43442678451538086
  - 0.40815848112106323
  - 0.40915849804878235
  - 0.4008390009403229
  - 0.39046725630760193
  - 0.49478816986083984
  - 0.3971693217754364
  - 0.38612547516822815
  - 0.40326154232025146
  - 0.38611412048339844
  - 0.42734062671661377
  - 0.4067783057689667
  - 0.3818342089653015
  - 0.4199388325214386
  - 0.39077121019363403
  - 0.38351646065711975
  - 0.3967990279197693
  - 0.38431817293167114
  - 0.38595011830329895
  - 0.4093271493911743
  - 0.4017225205898285
  - 0.39324751496315
  - 0.39287880063056946
  - 0.4351833164691925
  - 0.4187361001968384
  - 0.38477110862731934
  - 0.3808339834213257
  - 0.38783419132232666
  - 0.3898584544658661
  - 0.387617290019989
loss_records_fold1:
  train_losses:
  - 1.4459593057632447
  - 1.4141800224781038
  - 1.439163100719452
  - 1.4662586152553558
  - 1.4476124763488771
  - 1.4499442279338837
  - 1.4313798129558564
  - 1.444681477546692
  - 1.444317102432251
  - 1.470836228132248
  - 1.4034382581710816
  - 1.414988911151886
  - 1.5010851860046388
  - 1.4476624786853791
  - 1.4400021672248842
  - 1.4044706910848619
  - 1.4354767382144928
  - 1.4921102404594422
  - 1.4929944574832916
  - 1.4706690311431885
  - 1.4421644985675812
  - 1.431521236896515
  - 1.4357249259948732
  - 1.4184209048748018
  - 1.4531600832939149
  - 1.4916042923927308
  - 1.4220104932785036
  - 1.4175362467765809
  validation_losses:
  - 0.3877882957458496
  - 0.3912191390991211
  - 0.38946887850761414
  - 0.4413434565067291
  - 0.43012097477912903
  - 0.392109751701355
  - 0.39046522974967957
  - 0.4241672456264496
  - 0.3968101739883423
  - 0.39170023798942566
  - 0.43338751792907715
  - 0.3964521288871765
  - 0.4624462127685547
  - 0.5234678983688354
  - 0.3991360664367676
  - 0.40898606181144714
  - 0.6733229756355286
  - 0.3894091248512268
  - 0.4075709879398346
  - 0.4099028408527374
  - 0.39073142409324646
  - 0.4080507755279541
  - 0.41089487075805664
  - 0.41083502769470215
  - 0.3904724717140198
  - 0.38892287015914917
  - 0.38827061653137207
  - 0.39051374793052673
loss_records_fold2:
  train_losses:
  - 1.4427734971046449
  - 1.4433802843093873
  - 1.4199901640415193
  - 1.4379871666431427
  - 1.4735217511653902
  - 1.4481168806552889
  - 1.5174118995666506
  - 1.4326214253902436
  - 1.3962399244308472
  - 1.4596200227737428
  - 1.4138266354799272
  - 1.4598432660102845
  - 1.4806494474411012
  - 1.4365004897117615
  - 1.4529323160648346
  - 1.4233686804771424
  - 1.3894677311182022
  - 1.4739506542682648
  validation_losses:
  - 0.3838672637939453
  - 0.4156273901462555
  - 0.4012559652328491
  - 0.38841837644577026
  - 0.4026145339012146
  - 0.42232653498649597
  - 0.7017682790756226
  - 0.3919532597064972
  - 0.38691243529319763
  - 0.7429749965667725
  - 0.6281661987304688
  - 1.2491213083267212
  - 0.43452659249305725
  - 0.3905433118343353
  - 0.3814341723918915
  - 0.3847646415233612
  - 0.38037046790122986
  - 0.3818299472332001
loss_records_fold3:
  train_losses:
  - 1.4395900070667267
  - 1.4182764530181886
  - 1.424194258451462
  - 1.4535006046295167
  - 1.5376396358013154
  - 1.4539536476135255
  - 1.423928713798523
  - 1.4127130091190339
  - 1.4109590709209443
  - 1.4619613826274873
  - 1.4322088420391084
  - 1.4270615875720978
  - 1.468720555305481
  - 1.447852635383606
  - 1.4614256262779237
  - 1.466822785139084
  - 1.4844975709915162
  - 1.4181309342384338
  - 1.4010403692722322
  - 1.4721687495708466
  - 1.4345168948173523
  - 1.403609073162079
  - 1.4882782697677612
  - 1.497287103533745
  - 1.4024149596691133
  - 1.4167725920677186
  - 1.4054862707853317
  - 1.4268556833267212
  - 1.395444393157959
  - 1.3849352061748506
  - 1.361094304919243
  - 1.3689593553543091
  - 1.3781445682048798
  - 1.4354592859745026
  - 1.3682288706302643
  - 1.439111912250519
  - 1.4365355491638185
  - 1.4161075294017793
  - 1.419840368628502
  - 1.444916296005249
  - 1.4031085491180422
  - 1.4298778653144837
  - 1.4356871604919434
  - 1.382913625240326
  - 1.380722838640213
  - 1.417383396625519
  - 1.3947254598140717
  - 1.378620982170105
  - 1.3637404650449754
  - 1.37267444729805
  - 1.39360368847847
  - 1.3966944724321366
  - 1.4457742214202882
  - 1.3782516062259675
  - 1.3528361082077027
  - 1.3494565337896347
  - 1.3983741760253907
  - 1.387103718519211
  - 1.39536714553833
  - 1.3854868471622468
  - 1.3557911932468416
  - 1.3681444585323335
  - 1.382537716627121
  - 1.407254159450531
  - 1.3813543915748596
  - 1.3570167362689973
  - 1.3659503579139711
  - 1.3547190129756927
  - 1.3574346005916595
  - 1.3922686517238618
  - 1.3555913686752321
  - 1.3899599492549897
  - 1.3767706394195558
  - 1.3516419410705567
  - 1.367141443490982
  - 1.3685105293989182
  - 1.4074898898601533
  - 1.3636681258678438
  - 1.3730132758617402
  - 1.375714385509491
  - 1.3983458578586578
  - 1.3814260721206666
  - 1.370428663492203
  - 1.3462556123733522
  - 1.3669729828834534
  - 1.3126329183578491
  - 1.3481686353683473
  - 1.3736324787139893
  - 1.4267476022243502
  - 1.3809770762920381
  - 1.3573130548000336
  - 1.3536129534244539
  - 1.3390642285346985
  - 1.2989093840122223
  - 1.34838644862175
  - 1.3536867499351501
  - 1.3184144318103792
  - 1.357739818096161
  - 1.4022582888603212
  - 1.353056037425995
  validation_losses:
  - 0.6074346899986267
  - 0.4852437376976013
  - 0.4017079770565033
  - 0.6251330971717834
  - 0.4557168483734131
  - 0.46856409311294556
  - 0.5308665037155151
  - 0.37211909890174866
  - 0.40316516160964966
  - 0.40515363216400146
  - 0.5790076851844788
  - 0.5423101186752319
  - 0.3944689631462097
  - 0.390762597322464
  - 0.42528173327445984
  - 0.4192430078983307
  - 0.41241690516471863
  - 0.4110890328884125
  - 0.39077702164649963
  - 0.4111831486225128
  - 0.4028915464878082
  - 0.38608479499816895
  - 0.36409690976142883
  - 0.3681912422180176
  - 0.3868240416049957
  - 0.45308199524879456
  - 0.39954277873039246
  - 0.37191179394721985
  - 0.4431226849555969
  - 0.7066966891288757
  - 0.45837685465812683
  - 0.447657972574234
  - 0.5270916223526001
  - 0.4766015112400055
  - 1.0018434524536133
  - 0.505571722984314
  - 0.5117213129997253
  - 0.5630565285682678
  - 0.4195553660392761
  - 0.8421328663825989
  - 0.399618536233902
  - 0.39428794384002686
  - 0.5291473865509033
  - 0.6514333486557007
  - 0.845336377620697
  - 0.4287300109863281
  - 0.6359701156616211
  - 0.4265730381011963
  - 0.6009907126426697
  - 1.1132915019989014
  - 0.4301001727581024
  - 0.48155879974365234
  - 0.4453047513961792
  - 0.39136388897895813
  - 0.7217957973480225
  - 0.5709073543548584
  - 0.670660674571991
  - 0.5179343223571777
  - 0.6712713837623596
  - 0.6051393151283264
  - 0.386453241109848
  - 0.5753850340843201
  - 0.45244017243385315
  - 1.1201143264770508
  - 0.38295090198516846
  - 0.48577043414115906
  - 0.49769484996795654
  - 0.6225090622901917
  - 0.5542044639587402
  - 0.8325334191322327
  - 0.4358256161212921
  - 0.5655013918876648
  - 0.9248817563056946
  - 0.9421992301940918
  - 0.9599961638450623
  - 0.5659140944480896
  - 0.4652818441390991
  - 2.500424385070801
  - 0.4796864986419678
  - 0.5140590071678162
  - 0.5249684453010559
  - 0.4901561737060547
  - 0.4612389802932739
  - 0.9103386402130127
  - 0.5831472873687744
  - 1.4850202798843384
  - 0.7113044261932373
  - 0.5764712691307068
  - 0.5359916090965271
  - 0.46241042017936707
  - 0.48728084564208984
  - 0.5929886102676392
  - 0.6427216529846191
  - 1.6002053022384644
  - 0.5646160840988159
  - 1.1867599487304688
  - 3.6275243759155273
  - 0.5388467907905579
  - 0.6234873533248901
  - 0.49405500292778015
loss_records_fold4:
  train_losses:
  - 1.3452719181776047
  - 1.4271269261837007
  - 1.416964465379715
  - 1.3834015488624574
  - 1.334816962480545
  - 1.379944244027138
  - 1.3371760904788972
  - 1.33996838927269
  - 1.365173301100731
  - 1.3654696106910706
  - 1.337447386980057
  - 1.34732962846756
  - 1.3861046612262726
  - 1.3718074440956116
  - 1.4262423753738405
  - 1.3715430200099945
  - 1.3413310825824738
  - 1.3613081455230713
  - 1.3569241166114807
  - 1.349954363703728
  - 1.3144813507795334
  - 1.3053555995225907
  - 1.3692089796066285
  - 1.4119590759277345
  - 1.355073145031929
  - 1.3211432576179505
  - 1.3250261813402178
  - 1.3112696945667268
  - 1.310569304227829
  - 1.3461514115333557
  - 1.3419020771980286
  - 1.34340860247612
  - 1.3189848184585573
  - 1.3265713155269623
  - 1.333209577202797
  - 1.3525651693344116
  - 1.3580294013023377
  - 1.3130993932485582
  - 1.3074001014232637
  - 1.3444181174039842
  - 1.338482728600502
  - 1.3843667894601823
  - 1.3354443550109865
  - 1.3321534723043442
  - 1.3735640108585359
  - 1.261007508635521
  - 1.340522676706314
  - 1.301439729332924
  - 1.2748608052730561
  - 1.3093019247055055
  - 1.3062853634357454
  - 1.336361062526703
  - 1.3298617899417877
  - 1.350478082895279
  - 1.2953823953866959
  - 1.3327431112527848
  - 1.3029031485319138
  - 1.2676169514656068
  - 1.262484258413315
  - 1.2750495612621309
  - 1.2862094342708588
  - 1.2688056588172913
  - 1.2896891862154007
  - 1.3215602815151215
  - 1.2760285139083862
  - 1.302943754196167
  - 1.280013418197632
  - 1.3014771699905396
  - 1.2444768249988556
  - 1.2434537053108217
  - 1.269716811180115
  - 1.27647562623024
  - 1.262960571050644
  - 1.2767840147018434
  - 1.2655255854129792
  - 1.3099497735500336
  - 1.252822607755661
  - 1.2280576705932618
  - 1.2239621222019197
  - 1.2635236382484436
  - 1.27549706697464
  - 1.2649731874465944
  - 1.2137488007545472
  - 1.2206896901130677
  - 1.2264455318450929
  - 1.2468821078538896
  - 1.2461978763341905
  - 1.226433089375496
  - 1.229144275188446
  - 1.2675817251205446
  - 1.2535737097263338
  - 1.237028992176056
  - 1.2292936116456987
  - 1.2700559020042421
  - 1.2100623577833176
  - 1.2271663129329682
  - 1.2046544522047045
  - 1.2450095802545549
  - 1.3429024696350098
  - 1.2486473679542542
  validation_losses:
  - 0.4455915689468384
  - 0.3758760094642639
  - 0.3629673719406128
  - 0.5037247538566589
  - 0.5735979080200195
  - 0.7420414686203003
  - 1.5135782957077026
  - 0.4022621512413025
  - 0.591794490814209
  - 0.3529445230960846
  - 0.3840392827987671
  - 0.41651272773742676
  - 0.4338526427745819
  - 0.37841421365737915
  - 1.811043620109558
  - 1.1914186477661133
  - 0.8701748847961426
  - 0.37956514954566956
  - 0.3704959750175476
  - 0.7104059457778931
  - 0.9778832793235779
  - 1.1406846046447754
  - 0.8977706432342529
  - 0.57219398021698
  - 0.5815631151199341
  - 0.8874499201774597
  - 0.3501119017601013
  - 0.542666494846344
  - 0.6129910349845886
  - 1.2434380054473877
  - 1.2985997200012207
  - 0.5990100502967834
  - 0.4115446209907532
  - 0.3497551679611206
  - 0.35716110467910767
  - 0.4790746867656708
  - 0.7397090792655945
  - 0.36446550488471985
  - 0.5631435513496399
  - 0.6577531099319458
  - 0.4835105538368225
  - 0.49696865677833557
  - 0.40828919410705566
  - 0.33691275119781494
  - 0.43020114302635193
  - 0.5060952305793762
  - 0.4597783088684082
  - 0.4133835732936859
  - 0.4769349992275238
  - 0.6901424527168274
  - 0.5533297657966614
  - 0.4715680181980133
  - 0.4869219660758972
  - 0.4218871295452118
  - 0.4135848581790924
  - 0.6808086633682251
  - 0.595334529876709
  - 0.44776585698127747
  - 0.4327102303504944
  - 0.39261630177497864
  - 0.5515389442443848
  - 0.47780606150627136
  - 0.9172444343566895
  - 0.6731755137443542
  - 0.7180622220039368
  - 0.6800687313079834
  - 0.4119284152984619
  - 0.9123154878616333
  - 0.46718522906303406
  - 0.7774395942687988
  - 0.7653698325157166
  - 0.467957079410553
  - 0.47061750292778015
  - 0.568474531173706
  - 0.4577180743217468
  - 0.974450409412384
  - 1.5254703760147095
  - 0.695618748664856
  - 0.624250054359436
  - 0.8297255039215088
  - 0.7597037553787231
  - 0.41364625096321106
  - 0.7173051834106445
  - 0.7695543169975281
  - 0.8283411264419556
  - 0.9091262221336365
  - 0.776768147945404
  - 0.879195511341095
  - 0.6190125942230225
  - 0.7163233757019043
  - 0.6323661208152771
  - 0.8032181859016418
  - 0.7124679088592529
  - 0.6867527961730957
  - 0.9047423601150513
  - 0.8484042882919312
  - 0.9292192459106445
  - 0.3912532925605774
  - 0.44625604152679443
  - 0.6959989070892334
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8490566037735849, 0.8576329331046312, 0.8078902229845626,
    0.8092783505154639]'
  fold_eval_f1: '[0.0, 0.043478260869565216, 0.023529411764705882, 0.24324324324324323,
    0.18978102189781024]'
  mean_eval_accuracy: 0.8362982086965747
  mean_f1_accuracy: 0.1000063875550649
  total_train_time: '0:24:25.827345'
