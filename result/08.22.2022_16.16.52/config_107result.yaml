config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:44:21.589265'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_107fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_107fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_107fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_107fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_107fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_107fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_107fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_107fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_107fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_107fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 15.818091297149659
  - 6.435477328300476
  - 3.730783921480179
  - 3.860384273529053
  - 2.4144336819648746
  - 5.79672293663025
  - 4.261396050453186
  - 2.5784443587064745
  - 2.0211025029420853
  - 1.5024847507476808
  - 1.3267831265926362
  - 0.9775380969047547
  - 0.9151697278022767
  validation_losses:
  - 3.3209400177001953
  - 3.7079203128814697
  - 7.165107727050781
  - 4.168105602264404
  - 3.7761948108673096
  - 1.9391510486602783
  - 3.540972948074341
  - 0.9417015314102173
  - 0.6365821361541748
  - 0.5109237432479858
  - 0.5187931060791016
  - 0.4348962903022766
  - 0.40448883175849915
loss_records_fold1:
  train_losses:
  - 0.8261473298072816
  - 0.9768276870250703
  - 2.020787739753723
  - 1.2353798627853394
  - 3.275605720281601
  - 2.2576912820339206
  - 2.1738116681575774
  - 5.334919917583466
  - 3.1458646774291994
  - 1.082048636674881
  - 4.377416038513184
  - 1.7175021648406983
  - 1.074008822441101
  - 2.2139848589897158
  - 1.6141436874866486
  - 2.131862884759903
  - 4.003133934736252
  - 1.488647645711899
  - 1.1126834273338317
  - 0.9682084918022156
  - 3.8275462567806247
  - 5.951632261276245
  - 8.787260472774506
  - 13.210026919841766
  - 4.172185921669007
  - 2.373546588420868
  - 2.7389101862907412
  - 3.0322799265384677
  - 2.732712590694428
  - 5.735205155611038
  - 2.121162194013596
  - 6.276462197303772
  - 1.2451580345630646
  - 1.3006344497203828
  - 1.0023438513278962
  - 0.9700095057487488
  - 1.0951334059238433
  validation_losses:
  - 0.4657772481441498
  - 0.664192259311676
  - 0.9530203342437744
  - 0.503930926322937
  - 0.9506730437278748
  - 1.174460530281067
  - 0.5585755705833435
  - 0.6899628043174744
  - 0.47093355655670166
  - 0.4606054723262787
  - 0.396572470664978
  - 0.4349760115146637
  - 0.5792962908744812
  - 0.6070111989974976
  - 1.0027704238891602
  - 1.048223614692688
  - 0.6150755286216736
  - 0.5196649432182312
  - 0.42747265100479126
  - 0.412395179271698
  - 0.41633427143096924
  - 5.416635990142822
  - 1.8123844861984253
  - 1.2758710384368896
  - 0.6459438800811768
  - 0.6471487879753113
  - 0.4255771338939667
  - 0.48699522018432617
  - 0.4886553883552551
  - 0.5900603532791138
  - 0.9084398746490479
  - 0.5351322889328003
  - 0.5387626886367798
  - 0.45212268829345703
  - 0.42226743698120117
  - 0.40697529911994934
  - 0.4139384627342224
loss_records_fold2:
  train_losses:
  - 0.884068900346756
  - 0.8173742949962617
  - 0.9460168361663819
  - 1.506314605474472
  - 0.9672483146190644
  - 0.8525216639041902
  - 0.8664818167686463
  - 0.9221461176872254
  - 1.3902470827102662
  - 0.8606068670749665
  - 0.8464581370353699
  - 0.9146414756774903
  - 1.1061170041561128
  - 1.0269045531749725
  - 0.9932193517684937
  - 1.0806493937969208
  - 0.9595284223556519
  - 1.5425567626953125
  - 0.9490046530961991
  - 2.7908312678337097
  - 3.400101274251938
  - 2.5067972898483277
  - 2.1222974598407744
  - 1.3003433585166932
  - 1.1909369975328445
  - 1.9549093425273896
  - 1.2938022136688234
  - 1.1394538104534149
  - 1.1628423810005188
  - 0.945951521396637
  - 0.9799740105867386
  - 0.9747931182384492
  - 0.9453743398189545
  - 1.1121457993984223
  - 0.9007242143154145
  - 0.9821089267730714
  - 0.8094393610954285
  - 0.8441644728183747
  - 0.7677773386240005
  - 0.8233295023441315
  - 0.8245945870876312
  - 0.8351130783557892
  - 0.9677958369255066
  - 0.8775052845478059
  - 0.8265315651893617
  - 0.7592020124197006
  - 1.2324257373809815
  - 0.8561128437519074
  - 0.863746166229248
  - 0.8050995290279389
  - 0.8154467523097992
  - 1.3776697754859926
  - 3.625946366786957
  - 1.7153940677642823
  - 1.0143870174884797
  - 1.3149941802024843
  - 1.058769315481186
  - 1.2737634420394899
  - 4.141694104671479
  - 1.2565889596939088
  - 0.9371380925178528
  - 1.7212853372097017
  - 0.9566144466400147
  - 1.1368901610374451
  - 0.8477483451366425
  - 1.0217139840126037
  - 0.8533888757228851
  - 0.8474015414714814
  - 0.8562339067459107
  - 0.8131414890289307
  validation_losses:
  - 0.384032279253006
  - 0.38818642497062683
  - 0.4650152921676636
  - 0.3842923641204834
  - 0.4352373778820038
  - 0.3838670551776886
  - 0.3798774480819702
  - 0.39035072922706604
  - 0.38101959228515625
  - 0.41234704852104187
  - 0.379569947719574
  - 0.39410778880119324
  - 0.39027470350265503
  - 0.381236732006073
  - 0.38350775837898254
  - 0.3996465802192688
  - 0.3960700035095215
  - 0.39535900950431824
  - 0.3864513635635376
  - 1.5183769464492798
  - 0.4450748562812805
  - 0.38795673847198486
  - 0.4898371398448944
  - 0.43704986572265625
  - 0.41517093777656555
  - 0.44830167293548584
  - 0.3951266407966614
  - 0.3935832679271698
  - 0.38542911410331726
  - 0.39328280091285706
  - 0.40457969903945923
  - 0.39972174167633057
  - 0.38487833738327026
  - 0.38221246004104614
  - 0.4260522127151489
  - 0.38994088768959045
  - 0.38027939200401306
  - 0.39350005984306335
  - 0.3996015787124634
  - 0.4070112109184265
  - 0.38059738278388977
  - 0.3824681341648102
  - 0.38087669014930725
  - 0.40374210476875305
  - 0.381820410490036
  - 0.3889426589012146
  - 0.4029007852077484
  - 0.486765593290329
  - 0.3839098811149597
  - 0.4005986154079437
  - 0.39139899611473083
  - 0.3873736560344696
  - 0.384966105222702
  - 0.4049685299396515
  - 0.45695430040359497
  - 0.40904030203819275
  - 0.39210420846939087
  - 0.4151492714881897
  - 0.38109251856803894
  - 0.3936889171600342
  - 0.43661490082740784
  - 0.44026419520378113
  - 0.3900035321712494
  - 0.4224433898925781
  - 0.3813704252243042
  - 0.3834741711616516
  - 0.3831135034561157
  - 0.38367360830307007
  - 0.3861127495765686
  - 0.3905886113643646
loss_records_fold3:
  train_losses:
  - 0.7832667708396912
  - 0.8542367994785309
  - 0.9110929608345032
  - 0.9014497101306915
  - 0.8931519448757173
  - 0.9527719467878342
  - 0.8256664156913758
  - 0.838907814025879
  - 0.8287135601043701
  - 0.7912976026535035
  - 0.7780361175537109
  validation_losses:
  - 0.3963128924369812
  - 0.3969043493270874
  - 0.40325191617012024
  - 0.39460745453834534
  - 0.392605721950531
  - 0.39681077003479004
  - 0.40097182989120483
  - 0.40057045221328735
  - 0.4008214473724365
  - 0.4026450216770172
  - 0.4017064571380615
loss_records_fold4:
  train_losses:
  - 0.7767470955848694
  - 0.8348640263080598
  - 0.7986383497714997
  - 0.7788588643074036
  - 0.7508720308542252
  - 1.6074219882488252
  - 0.928812712430954
  - 0.8177665770053864
  - 1.0438675999641418
  - 0.9589154481887818
  - 0.8379123568534852
  - 1.5641262471675874
  - 0.9591293513774872
  - 0.8421729266643525
  - 0.7913089692592621
  - 0.7897140920162201
  validation_losses:
  - 0.3944772481918335
  - 0.40967482328414917
  - 0.3913307785987854
  - 0.4057697355747223
  - 0.39948463439941406
  - 0.40940842032432556
  - 0.4561450481414795
  - 0.509166419506073
  - 0.5268561840057373
  - 0.5751510858535767
  - 0.5512802600860596
  - 0.5369436740875244
  - 0.4836089611053467
  - 0.4656948447227478
  - 0.44748228788375854
  - 0.4230661988258362
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 37 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 70 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.855917667238422, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.0
  total_train_time: '0:12:50.609085'
