config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 14:51:28.621955'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/16/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/16/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/16/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/16/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/16/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/16/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/16/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/16/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/16/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/16/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 296.4126476943493
  - 97.96393813192844
  - 95.8032423555851
  - 78.59097504615784
  - 81.87127086520195
  - 79.45488080382347
  - 124.7556668035686
  - 82.03007282316685
  - 92.49332794547081
  - 48.273955915123224
  - 55.68346454948187
  - 43.321234203875065
  - 35.72587475180626
  - 34.764000579714775
  - 38.54725496470928
  - 34.39691670238972
  - 32.89540205895901
  - 41.64791226387024
  - 35.88172173500061
  - 32.590144976973534
  - 39.754864268004894
  - 36.330444037914276
  - 36.81379772722721
  - 37.0113959312439
  - 30.16821400821209
  - 31.417602956295013
  - 32.6585114300251
  - 32.67474067211151
  - 33.14416082203388
  - 37.795495450496674
  - 28.35636292397976
  - 30.920595586299896
  - 30.647759690880775
  - 30.08784194290638
  - 29.175775080919266
  - 29.159255489706993
  - 29.058246612548828
  - 29.374255016446114
  - 28.84103985130787
  - 28.89810547977686
  - 28.72844634205103
  - 28.279788210988045
  - 28.764347210526466
  - 29.78956587612629
  - 28.715540155768394
  - 28.041464760899544
  - 28.36095568537712
  - 30.209128752350807
  - 28.42386668920517
  - 28.234730288386345
  - 29.13439942896366
  - 29.069733515381813
  - 28.02843986451626
  - 28.52685907483101
  - 29.405806347727776
  - 28.866165474057198
  - 28.3637615442276
  - 28.276932060718536
  - 28.890726000070572
  - 29.717652440071106
  - 30.261218145489693
  - 28.56958106160164
  - 27.77917055785656
  - 28.05988934636116
  - 28.82851678133011
  - 28.395770594477654
  - 28.607079833745956
  - 28.916610650718212
  - 29.23156328499317
  - 28.949448481202126
  - 29.515980944037437
  - 40.70277035981417
  - 46.08856908977032
  - 37.85638007521629
  - 31.0819009244442
  - 32.60330514609814
  - 30.685495257377625
  - 30.066044494509697
  - 30.178291589021683
  - 29.571933895349503
  - 29.877363614737988
  - 29.720861554145813
  - 29.201151348650455
  - 29.464355647563934
  - 29.89600719511509
  - 29.753058180212975
  - 29.913887098431587
  - 29.623884439468384
  - 29.31492504477501
  - 29.635055527091026
  - 30.171592846512794
  - 29.553857296705246
  - 29.88312017172575
  - 30.64554461836815
  - 29.67684578895569
  - 29.28636434674263
  - 30.124524176120758
  - 29.748339504003525
  - 30.623051017522812
  - 29.731281504034996
  validation_losses:
  - 5.345214366912842
  - 1.0879881381988525
  - 0.5319433808326721
  - 0.5179169178009033
  - 1.2731975317001343
  - 0.394947350025177
  - 1.1207513809204102
  - 0.49406179785728455
  - 0.41098955273628235
  - 0.46134257316589355
  - 0.3867918848991394
  - 0.39244019985198975
  - 0.3775996267795563
  - 0.3874055743217468
  - 0.3958668112754822
  - 0.4434981048107147
  - 0.4239499568939209
  - 0.42955097556114197
  - 0.38134652376174927
  - 0.4137341380119324
  - 0.3979049026966095
  - 0.4413960576057434
  - 0.9735226035118103
  - 0.38111189007759094
  - 0.4035210609436035
  - 0.4056563079357147
  - 0.37783166766166687
  - 0.5958813428878784
  - 0.3853536546230316
  - 0.3807215690612793
  - 0.3765468895435333
  - 0.39096254110336304
  - 0.4652363657951355
  - 0.44586676359176636
  - 0.3837372362613678
  - 0.41041335463523865
  - 0.5796377062797546
  - 0.4034874737262726
  - 0.46623679995536804
  - 0.37938740849494934
  - 0.3784785270690918
  - 0.37801221013069153
  - 0.47708141803741455
  - 0.4153751730918884
  - 0.39589184522628784
  - 0.38579973578453064
  - 0.4053843021392822
  - 0.3890465795993805
  - 0.38291656970977783
  - 0.3869199752807617
  - 0.417521208524704
  - 0.4753550589084625
  - 0.3800728917121887
  - 0.4077461361885071
  - 0.3859880268573761
  - 0.4097878634929657
  - 0.3963235914707184
  - 0.41442784667015076
  - 0.41623684763908386
  - 0.4192906618118286
  - 0.4207325279712677
  - 0.3735717833042145
  - 0.38987070322036743
  - 0.38326168060302734
  - 0.3784181475639343
  - 0.3823002278804779
  - 0.39170846343040466
  - 0.4378746449947357
  - 0.3786769211292267
  - 0.3825196921825409
  - 0.4220368266105652
  - 0.5793271064758301
  - 0.39195966720581055
  - 0.39256027340888977
  - 0.5558105707168579
  - 0.39541611075401306
  - 0.3930225372314453
  - 0.39390289783477783
  - 0.3942047655582428
  - 0.39053046703338623
  - 0.43017539381980896
  - 0.3909308612346649
  - 0.4338911771774292
  - 0.3922818899154663
  - 0.3921290934085846
  - 0.39244475960731506
  - 0.40304696559906006
  - 0.44126713275909424
  - 0.40657109022140503
  - 0.3911808431148529
  - 0.4108312726020813
  - 0.43614068627357483
  - 0.39638733863830566
  - 0.4508073627948761
  - 0.39056897163391113
  - 0.38951247930526733
  - 0.4377736747264862
  - 0.3926149904727936
  - 0.4246060848236084
  - 0.40106943249702454
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 43 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 75 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 55 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:11:25.331993'
