config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:02:00.368914'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_41fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_41fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_41fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_41fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_41fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_41fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_41fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_41fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_41fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_41fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 35.15310964584351
  - 10.718344283103944
  - 12.106749713420868
  - 14.552763146162034
  - 11.677673435211183
  - 10.531058943271638
  - 8.720752561092377
  - 5.706271705031395
  - 3.847147709131241
  - 4.012704336643219
  - 3.7580329418182377
  - 5.0944487154483795
  - 6.25715823173523
  - 4.08829088807106
  - 4.16652597784996
  - 7.3382164448499685
  - 10.736206069588661
  - 4.877889651060105
  - 4.227718484401703
  - 7.107757726311684
  - 7.197607034444809
  - 5.100980496406556
  - 5.047061735391617
  - 7.411746543645859
  - 4.186750188469887
  - 5.2131567865610124
  - 3.578392067551613
  - 8.473655185103416
  - 6.904792687296868
  - 3.8021511971950535
  - 3.298169764876366
  - 3.8099603414535523
  - 3.539955145120621
  - 4.630080085992813
  validation_losses:
  - 2.150285482406616
  - 0.9686344265937805
  - 3.7769720554351807
  - 1.2190574407577515
  - 1.1262680292129517
  - 0.5295670032501221
  - 0.47772499918937683
  - 0.4177769124507904
  - 0.4348454475402832
  - 0.42424964904785156
  - 0.8972112536430359
  - 1.6841453313827515
  - 0.39968377351760864
  - 0.39128220081329346
  - 0.3957425057888031
  - 0.6490249633789062
  - 0.410477876663208
  - 0.5255538821220398
  - 0.48663610219955444
  - 0.3980379104614258
  - 0.47223708033561707
  - 0.4030095040798187
  - 0.3976142704486847
  - 0.501159131526947
  - 0.4294258654117584
  - 0.38335588574409485
  - 0.39590904116630554
  - 0.4696039855480194
  - 0.39222007989883423
  - 0.38053274154663086
  - 0.3851656913757324
  - 0.38449278473854065
  - 0.3856985569000244
  - 0.3805730938911438
loss_records_fold1:
  train_losses:
  - 4.855760008096695
  - 3.1765182077884675
  - 3.2930489629507065
  - 5.260409635305405
  - 4.754444542527199
  - 3.1878494501113894
  - 4.849236080050469
  - 3.5232438266277315
  - 6.067049729824067
  - 4.413071095943451
  - 3.8254449486732485
  - 4.677203419804573
  - 5.230529856681824
  - 4.655206686258317
  - 4.879550987482071
  - 8.60550558269024
  - 5.651369273662567
  - 7.937620201706887
  - 4.519474339485169
  - 3.156140434741974
  - 3.58218469619751
  - 3.55221426486969
  - 3.3308141201734545
  - 3.426190823316574
  - 3.189985626935959
  - 3.393147882819176
  - 3.6231200337409977
  - 3.7058603286743166
  - 3.3155721306800845
  - 3.400969618558884
  - 3.123226594924927
  - 2.9117010205984117
  - 3.4779537975788117
  - 3.800890010595322
  - 3.991075760126114
  - 3.12863509953022
  - 3.8682575762271885
  - 3.438195210695267
  - 3.0336346209049228
  - 4.248514503240585
  - 4.3374996274709705
  - 3.347512522339821
  - 2.9308490812778474
  - 3.480972701311112
  - 3.332787269353867
  - 4.209295392036438
  - 3.1822788178920747
  - 3.3454387485980988
  - 3.5587984144687654
  - 3.5662765145301822
  - 2.814435064792633
  - 3.0745133936405185
  - 3.015856921672821
  validation_losses:
  - 0.40454933047294617
  - 0.39498913288116455
  - 0.3958861231803894
  - 0.40109431743621826
  - 0.40421509742736816
  - 0.3963913023471832
  - 0.43076181411743164
  - 0.5118393898010254
  - 0.5849531292915344
  - 0.39763322472572327
  - 1.5358819961547852
  - 0.5180379748344421
  - 2.193665027618408
  - 0.7254518270492554
  - 0.5487658977508545
  - 0.4905547499656677
  - 0.4291570782661438
  - 0.5116358399391174
  - 0.43847018480300903
  - 0.4034176766872406
  - 0.3996955454349518
  - 0.4236240088939667
  - 0.3979666829109192
  - 0.49134889245033264
  - 0.4221731722354889
  - 0.4189189374446869
  - 0.4634796380996704
  - 0.43911272287368774
  - 0.41740232706069946
  - 0.39679309725761414
  - 0.560531735420227
  - 0.6159244775772095
  - 0.40418514609336853
  - 0.42891424894332886
  - 0.40044257044792175
  - 0.4491039216518402
  - 0.5642960071563721
  - 0.3968978524208069
  - 0.41083669662475586
  - 0.41676458716392517
  - 0.40471315383911133
  - 0.39706557989120483
  - 0.42294031381607056
  - 0.4360179007053375
  - 0.4091019630432129
  - 0.38998451828956604
  - 0.47561410069465637
  - 0.4854888319969177
  - 0.48805999755859375
  - 0.4527868628501892
  - 0.4381546378135681
  - 0.4198404550552368
  - 0.4050277769565582
loss_records_fold2:
  train_losses:
  - 3.0297226041555407
  - 3.363971456885338
  - 3.2926878869533542
  - 3.754214066267014
  - 3.676090604066849
  - 3.1260876238346103
  - 5.096934255957604
  - 3.2447557002305984
  - 2.9957043111324313
  - 2.969387781620026
  - 3.553453412652016
  - 3.3230132251977924
  - 3.0137741148471835
  - 2.9899795204401016
  - 2.99506011903286
  - 3.120283001661301
  - 3.4972511626780034
  - 3.2448856055736544
  - 3.106308138370514
  - 3.0027664780616763
  - 3.182688629627228
  - 2.8590683549642564
  - 3.1045681923627857
  - 2.948793011903763
  - 3.006470772624016
  - 3.3298838973045353
  - 3.2733617544174196
  - 2.9525015711784364
  - 3.19207219183445
  - 2.969778516888619
  - 3.111151295900345
  - 3.5077538669109347
  - 3.261556845903397
  - 3.2978516578674317
  - 2.974255156517029
  - 2.9096182495355607
  - 2.9440784752368927
  - 2.9069808185100556
  - 2.9519823759794237
  - 3.067448976635933
  - 2.9950620949268343
  - 3.309199506044388
  - 2.968071645498276
  - 3.0288319408893587
  - 2.9210059285163883
  - 3.27732412815094
  - 2.895397326350212
  - 2.901823854446411
  - 3.0685236334800723
  - 3.4292765319347382
  - 2.968087148666382
  - 2.895081952214241
  validation_losses:
  - 0.41460347175598145
  - 0.5693955421447754
  - 0.466187059879303
  - 0.48526468873023987
  - 0.4053554832935333
  - 0.38947585225105286
  - 0.4460866451263428
  - 0.39549949765205383
  - 0.41498133540153503
  - 0.445380300283432
  - 0.49536406993865967
  - 0.3797963559627533
  - 0.3726525604724884
  - 0.3692857325077057
  - 5.078530311584473
  - 8.298051834106445
  - 0.4227466285228729
  - 0.48729372024536133
  - 0.40067848563194275
  - 0.9048421382904053
  - 2.9006004333496094
  - 2.327425718307495
  - 2.8659162521362305
  - 0.6816214323043823
  - 0.611955463886261
  - 5.671087265014648
  - 3.172886371612549
  - 0.41000744700431824
  - 0.5593298077583313
  - 1.5378150939941406
  - 1.1395938396453857
  - 1.5932902097702026
  - 0.6417588591575623
  - 0.9364698529243469
  - 0.658701479434967
  - 0.410457968711853
  - 0.4170559048652649
  - 0.6782451272010803
  - 6.5555925369262695
  - 1.139157772064209
  - 0.41187426447868347
  - 0.3835110366344452
  - 0.3960024118423462
  - 0.38019585609436035
  - 0.3757737874984741
  - 0.3880782127380371
  - 0.37573930621147156
  - 0.3857104778289795
  - 0.3786982297897339
  - 0.38552072644233704
  - 0.37912091612815857
  - 0.38031628727912903
loss_records_fold3:
  train_losses:
  - 3.2372671306133274
  - 3.08115416765213
  - 3.1769354194402695
  - 3.475693255662918
  - 2.9027786165475846
  - 3.1205889493227006
  - 3.0583814769983295
  - 2.849953180551529
  - 2.9414444804191593
  - 2.91574798822403
  - 2.890735423564911
  - 2.9262731820344925
  - 2.945983272790909
  - 2.9742859303951263
  - 3.015104964375496
  - 3.156658059358597
  - 2.9159849822521213
  - 2.888524639606476
  - 3.0112709015607835
  - 2.876841753721237
  - 2.935316240787506
  - 2.913154733181
  - 2.894826138019562
  - 2.808168685436249
  - 2.9859276831150057
  - 2.8858240574598315
  - 2.8862737685441973
  - 2.9219674289226534
  - 2.8996375501155853
  - 2.858160647749901
  - 2.9895825386047363
  - 2.884014791250229
  - 2.9199793934822083
  - 2.8719997942447666
  - 2.887374132871628
  - 3.0531054913997653
  - 3.1076558232307434
  - 3.0315393805503845
  - 2.892116814851761
  - 2.8216492235660553
  - 2.848627880215645
  - 2.903185534477234
  - 2.907988965511322
  - 2.864716222882271
  - 2.8876066118478776
  - 2.8788490831851963
  - 2.9413384944200516
  - 3.069824802875519
  - 2.945073473453522
  - 2.9101568818092347
  - 2.9359802424907686
  - 2.841730386018753
  - 2.84849568605423
  - 2.8643763005733494
  - 2.8426119029521946
  - 2.8745366096496583
  - 2.8567229866981507
  - 2.9091491609811784
  - 3.035504770278931
  - 2.9539782941341404
  - 2.9382020831108093
  - 2.8703400641679764
  - 2.89311403632164
  - 2.9315921008586887
  - 2.9218154072761537
  - 2.827409228682518
  - 3.1559253424406055
  - 3.0143427908420564
  - 2.9266376197338104
  - 2.8608889490365983
  - 2.9295275747776035
  - 2.8549520671367645
  validation_losses:
  - 0.38923585414886475
  - 0.4362132251262665
  - 0.4642992615699768
  - 0.3846408724784851
  - 0.3813994228839874
  - 0.40835294127464294
  - 0.3955930471420288
  - 0.4099029004573822
  - 0.4055159091949463
  - 0.38339993357658386
  - 0.3853904902935028
  - 0.4159224033355713
  - 0.3998979926109314
  - 0.3915420472621918
  - 0.3806379437446594
  - 0.4013337194919586
  - 0.3853534758090973
  - 0.395226389169693
  - 0.3886443078517914
  - 0.41898313164711
  - 0.4326627254486084
  - 0.4318094849586487
  - 0.40345925092697144
  - 0.42284372448921204
  - 0.4028191566467285
  - 0.38798511028289795
  - 0.399184912443161
  - 0.4019259512424469
  - 0.44424501061439514
  - 0.5256827473640442
  - 0.42990100383758545
  - 0.4066099226474762
  - 0.42671647667884827
  - 0.4169946014881134
  - 0.4321346879005432
  - 0.5126107931137085
  - 0.43936121463775635
  - 0.4367484152317047
  - 0.39545920491218567
  - 0.4109009802341461
  - 0.4520609378814697
  - 0.41450873017311096
  - 0.4553978741168976
  - 0.41690313816070557
  - 0.39625370502471924
  - 0.40992116928100586
  - 0.4293787181377411
  - 0.41238200664520264
  - 0.44416743516921997
  - 0.46011024713516235
  - 0.4009436070919037
  - 0.4257536828517914
  - 0.4640779495239258
  - 0.44076451659202576
  - 0.38511624932289124
  - 0.4156040847301483
  - 0.40737566351890564
  - 0.6496109366416931
  - 0.46429768204689026
  - 0.39719849824905396
  - 0.40191319584846497
  - 0.39760223031044006
  - 0.4108569622039795
  - 0.40670767426490784
  - 0.40082478523254395
  - 0.45705121755599976
  - 0.46472063660621643
  - 0.42665937542915344
  - 0.40723443031311035
  - 0.3962186276912689
  - 0.3985324800014496
  - 0.39967912435531616
loss_records_fold4:
  train_losses:
  - 3.1406711637973785
  - 3.0370666801929476
  - 2.8942650973796846
  - 3.0883239209651947
  - 2.9480346679687504
  - 3.9731019347906114
  - 4.815358266234398
  - 3.0023954629898073
  - 2.9364193737506867
  - 2.933423122763634
  - 2.9676288187503816
  - 2.9230485498905185
  - 2.9367948472499847
  - 2.984915679693222
  - 3.0617483854293823
  - 2.9569308280944826
  - 3.6932852894067767
  - 3.819199335575104
  - 5.171525216102601
  - 3.4615434944629673
  - 3.28412898182869
  - 3.034819355607033
  - 2.9270169675350193
  - 4.17918538749218
  - 3.277752482891083
  - 2.999931210279465
  - 3.368851125240326
  - 3.106675386428833
  - 3.0992428690195086
  - 3.246931529045105
  - 3.09620258808136
  - 2.954693168401718
  - 2.9700774371623995
  - 3.1897412806749346
  - 2.9625590801239015
  - 3.0390413403511047
  - 2.9390854358673097
  - 3.1015865683555606
  validation_losses:
  - 54.85877227783203
  - 151.5893096923828
  - 6.196230411529541
  - 11.129301071166992
  - 65.3551254272461
  - 0.4889887273311615
  - 0.616898775100708
  - 0.7687857151031494
  - 0.3930192291736603
  - 0.3944990336894989
  - 0.38983386754989624
  - 0.39089348912239075
  - 0.38968515396118164
  - 0.45811647176742554
  - 0.39213836193084717
  - 0.40495696663856506
  - 0.5546014308929443
  - 0.4276777505874634
  - 0.4329488277435303
  - 0.48356378078460693
  - 0.4349920451641083
  - 0.39614564180374146
  - 0.3959326446056366
  - 0.41498231887817383
  - 0.40831008553504944
  - 0.42524421215057373
  - 0.4107646048069
  - 0.4159766137599945
  - 0.4174700081348419
  - 0.4137146472930908
  - 0.3904290497303009
  - 0.4402105212211609
  - 0.4053468704223633
  - 0.40760961174964905
  - 0.3947586417198181
  - 0.39583978056907654
  - 0.39936813712120056
  - 0.4047648310661316
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 53 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 52 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 72 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 38 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:23:24.710812'
