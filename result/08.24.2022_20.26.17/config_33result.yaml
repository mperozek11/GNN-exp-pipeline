config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:04:43.525260'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_33fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_33fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_33fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_33fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_33fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_33fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_33fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_33fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_33fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_33fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.2363908618688586
  - 2.880312290787697
  - 3.0016341090202334
  - 2.9383737683296207
  - 2.854912453889847
  - 2.9320379137992862
  - 2.98872064948082
  - 2.857089251279831
  - 2.8453408598899843
  - 2.812838900089264
  - 2.8388907313346863
  - 2.866324055194855
  - 2.8747621864080433
  - 2.9147027611732486
  - 2.8055978536605837
  - 2.9190811932086946
  - 2.966205182671547
  - 2.8773816585540772
  - 2.8379469454288486
  - 2.815628492832184
  - 2.865307143330574
  - 2.8614102065563203
  - 2.8594860792160035
  - 2.8645244598388673
  - 2.851948541402817
  - 2.8340736716985706
  - 2.772962173819542
  validation_losses:
  - 0.404460072517395
  - 0.45877203345298767
  - 0.4148608148097992
  - 0.39405497908592224
  - 0.390074223279953
  - 0.38425174355506897
  - 0.38449400663375854
  - 0.3832748234272003
  - 0.3860286474227905
  - 0.3858571946620941
  - 0.3973224461078644
  - 0.38494738936424255
  - 0.3885837495326996
  - 0.4025387465953827
  - 0.38952454924583435
  - 0.405538946390152
  - 0.3824571967124939
  - 0.4444102942943573
  - 0.38711151480674744
  - 0.47333770990371704
  - 0.5402058362960815
  - 0.38678643107414246
  - 0.38742437958717346
  - 0.38981005549430847
  - 0.3916213810443878
  - 0.3853223919868469
  - 0.38396427035331726
loss_records_fold1:
  train_losses:
  - 2.843551141023636
  - 2.8011179715394974
  - 2.782084339857102
  - 2.8274507582187653
  - 2.8094772815704347
  - 2.8317833662033083
  - 2.834690606594086
  - 2.7838937103748322
  - 2.781921419501305
  - 2.7593393296003343
  - 2.8016131937503816
  - 2.7751673460006714
  - 2.819215789437294
  - 2.8072156190872195
  - 2.8040015369653704
  - 2.862382134795189
  - 2.788887465000153
  - 2.82216711640358
  - 2.7741162687540055
  - 2.7788936078548434
  - 2.7623459070920946
  - 2.759388464689255
  - 2.7577888876199723
  - 2.7808174550533296
  - 2.7780197322368623
  - 2.7206366240978244
  - 2.7518944948911668
  - 2.7200054764747623
  - 2.7259952962398533
  - 2.775107365846634
  - 2.8023182451725006
  - 2.77416450381279
  - 2.7361219555139544
  - 2.7388880252838135
  - 2.7508275121450425
  - 2.7488864064216614
  - 2.743111622333527
  - 2.798486575484276
  - 2.7620381951332096
  - 2.778245681524277
  - 2.7759266793727875
  - 2.7862266540527347
  - 2.73094462454319
  - 2.7549178987741474
  - 2.770652484893799
  - 2.687890625
  - 2.8080420911312105
  - 2.7610585510730745
  - 2.7584167420864105
  - 2.774603405594826
  - 2.8205098628997805
  - 2.7151373416185383
  - 2.707175859808922
  - 2.768262964487076
  - 2.7416473865509037
  - 2.740220546722412
  - 2.72881532907486
  - 2.7048431158065798
  - 2.7605063736438753
  - 2.7715294837951663
  - 2.7175762057304382
  - 2.726353180408478
  - 2.75988561809063
  - 2.782831510901451
  - 2.6870524644851685
  - 2.7482748836278916
  - 2.7258656829595567
  - 2.7211977899074555
  - 2.739207446575165
  - 2.7601609915494922
  - 2.7443688750267032
  - 2.7292433947324755
  - 2.7984021306037903
  - 2.756651371717453
  - 2.7739352941513062
  - 2.7480042397975923
  - 2.7241808027029037
  - 2.7798203915357593
  - 2.7038818985223774
  - 2.7164533495903016
  - 2.7265259623527527
  - 2.6805487602949145
  - 2.7375257790088656
  - 2.732907623052597
  - 2.7679964363574983
  - 2.757128444314003
  - 2.759188026189804
  - 2.7244367361068726
  - 2.6973885506391526
  - 2.708336037397385
  - 2.7274063110351565
  - 2.7578810155391693
  - 2.7200513929128647
  - 2.714776667952538
  - 2.7546834260225297
  - 2.661022052168846
  - 2.747561976313591
  - 2.725051033496857
  - 2.6689505726099014
  - 2.6838497668504715
  validation_losses:
  - 0.38384854793548584
  - 0.3881460726261139
  - 0.42835259437561035
  - 0.39152848720550537
  - 0.39677029848098755
  - 0.42840608954429626
  - 0.38590356707572937
  - 0.3998870551586151
  - 0.45505091547966003
  - 0.47687795758247375
  - 0.40268126130104065
  - 0.7201149463653564
  - 0.6223195195198059
  - 0.4429236054420471
  - 0.4150630831718445
  - 0.4925992786884308
  - 0.46146446466445923
  - 0.42841827869415283
  - 0.5022109150886536
  - 0.38223838806152344
  - 0.3927529752254486
  - 0.4155963063240051
  - 0.5138131976127625
  - 0.5608921647071838
  - 0.5576385855674744
  - 0.48308128118515015
  - 0.4971543848514557
  - 0.6261898279190063
  - 0.37807363271713257
  - 0.4369734227657318
  - 0.46032020449638367
  - 0.5010790824890137
  - 0.5465688109397888
  - 0.7484050989151001
  - 0.5989516973495483
  - 0.4996488690376282
  - 0.47276628017425537
  - 0.5878575444221497
  - 0.45298120379447937
  - 0.3934541344642639
  - 0.45114800333976746
  - 0.6373658180236816
  - 0.47606584429740906
  - 0.6131700873374939
  - 0.6017209887504578
  - 0.46320611238479614
  - 0.4796207547187805
  - 0.6243943572044373
  - 0.5361129641532898
  - 0.3990347385406494
  - 0.4211371839046478
  - 0.5395031571388245
  - 0.47230905294418335
  - 0.5652008652687073
  - 0.4796003997325897
  - 0.48654815554618835
  - 0.5201698541641235
  - 0.5028721690177917
  - 0.5055376887321472
  - 0.44136878848075867
  - 0.4893122613430023
  - 0.6879720091819763
  - 0.6725499033927917
  - 0.6049407124519348
  - 0.8280882835388184
  - 0.4040144085884094
  - 0.5420647263526917
  - 0.8756000399589539
  - 0.6518089175224304
  - 0.4406593441963196
  - 0.5364376306533813
  - 0.7102329730987549
  - 0.5084377527236938
  - 0.5758332014083862
  - 0.7171331644058228
  - 0.4329487979412079
  - 0.6714388728141785
  - 0.5660941004753113
  - 0.535573422908783
  - 0.527144730091095
  - 1.0159482955932617
  - 0.8321579694747925
  - 0.6202804446220398
  - 0.6988846063613892
  - 0.8239656090736389
  - 0.8691001534461975
  - 0.7905762195587158
  - 0.653286337852478
  - 0.5317460298538208
  - 0.7781186699867249
  - 0.6989668011665344
  - 0.386923611164093
  - 0.6394525766372681
  - 0.6466032862663269
  - 0.5593767762184143
  - 0.4752531349658966
  - 0.5672124028205872
  - 0.5396981239318848
  - 0.7037708163261414
  - 0.6699368357658386
loss_records_fold2:
  train_losses:
  - 2.6918522208929065
  - 2.7232887089252475
  - 2.7992001086473466
  - 2.7143209964036945
  - 2.679486906528473
  - 2.7800626486539843
  - 2.6609857589006425
  - 2.7109329849481583
  - 2.751243674755097
  - 2.7311200112104417
  - 2.7122194647789004
  - 2.683781006932259
  - 2.6913806408643723
  - 2.7675582408905033
  - 2.704345697164536
  - 2.725173479318619
  - 2.724631014466286
  - 2.727692034840584
  - 2.7164710730314257
  - 2.6643779426813126
  - 2.789568442106247
  - 2.6909088432788852
  - 2.710381203889847
  - 2.7102557301521304
  - 2.684326708316803
  - 2.758648264408112
  - 2.730483883619309
  - 2.6495699107646944
  - 2.741791763901711
  - 2.7495044440031053
  - 2.6289276480674744
  - 2.7974579095840455
  - 2.726884573698044
  - 2.687550276517868
  - 2.701303681731224
  - 2.789963573217392
  - 2.757151943445206
  - 2.719468885660172
  - 2.6996688425540927
  - 2.714410862326622
  - 2.815181213617325
  - 2.7485776722431186
  - 2.721632346510887
  - 2.742677292227745
  - 2.6927602887153625
  - 2.68517923951149
  - 2.7347813546657562
  - 2.692490017414093
  - 2.688253164291382
  - 2.7553051233291628
  - 2.7432943403720857
  - 2.7350857585668567
  - 2.7303786098957064
  - 2.708506143093109
  - 2.6570588797330856
  - 2.653228884935379
  - 2.7376586973667147
  - 2.664083883166313
  - 2.7186490029096606
  - 2.691565597057343
  - 2.6743080884218218
  - 2.7525670468807224
  - 2.6990159451961517
  - 2.6513118982315067
  - 2.717365264892578
  - 2.741728809475899
  - 2.7739525586366653
  - 2.816454362869263
  - 2.723704844713211
  - 2.741026610136032
  - 2.685655462741852
  - 2.676843631267548
  - 2.739286318421364
  - 2.687572264671326
  - 2.7141363203525546
  - 2.7127981126308445
  - 2.7256895154714584
  - 2.6647631525993347
  - 2.6812698245048523
  - 2.6926236718893053
  - 2.6740936636924744
  - 2.718233418464661
  - 2.641515937447548
  - 2.736371010541916
  - 2.725165390968323
  - 2.7461381256580353
  - 2.6829620957374574
  - 2.7173545539379123
  - 2.71040041744709
  - 2.656518715620041
  - 2.679243892431259
  - 2.690190440416336
  - 2.7437219798564914
  - 2.714906260371208
  - 2.6743113279342654
  - 2.734398192167282
  - 2.655613946914673
  - 2.699651423096657
  - 2.8090720981359483
  - 2.6738750636577606
  validation_losses:
  - 0.41983550786972046
  - 0.6134724617004395
  - 0.4530990421772003
  - 0.506532609462738
  - 0.4920651316642761
  - 0.5228561758995056
  - 0.5408114790916443
  - 2.5399162769317627
  - 0.39018407464027405
  - 0.8886544108390808
  - 0.5007867813110352
  - 0.5700592398643494
  - 0.534654974937439
  - 0.5116817951202393
  - 0.4745034873485565
  - 0.5686049461364746
  - 0.5498574376106262
  - 0.5710635781288147
  - 0.7627793550491333
  - 0.5983456373214722
  - 0.5641496181488037
  - 0.557046115398407
  - 0.4373275935649872
  - 0.6203906536102295
  - 0.9344950914382935
  - 0.4527147114276886
  - 0.46854859590530396
  - 0.6055916547775269
  - 0.7636739611625671
  - 0.5810379981994629
  - 0.6186770796775818
  - 0.5184001326560974
  - 0.4251522719860077
  - 0.48700767755508423
  - 0.5595434308052063
  - 0.4910639524459839
  - 0.4532301127910614
  - 0.5171979069709778
  - 0.9676576852798462
  - 0.7642713189125061
  - 0.43735453486442566
  - 0.40640008449554443
  - 0.5324209928512573
  - 0.45823878049850464
  - 0.5014540553092957
  - 0.5075199604034424
  - 0.43661412596702576
  - 0.5090795755386353
  - 0.48823633790016174
  - 0.49227240681648254
  - 0.41988682746887207
  - 0.4429802894592285
  - 0.38988399505615234
  - 0.4693739116191864
  - 0.5053090453147888
  - 0.49243271350860596
  - 0.40701860189437866
  - 0.7440442442893982
  - 0.6311439275741577
  - 0.4010540246963501
  - 0.610671877861023
  - 0.5055549740791321
  - 0.5051746964454651
  - 0.615492582321167
  - 0.5248357653617859
  - 0.49238091707229614
  - 0.534071147441864
  - 0.4200717806816101
  - 0.435768187046051
  - 0.4619331359863281
  - 0.4035091996192932
  - 0.43781259655952454
  - 0.5063045024871826
  - 0.5850287675857544
  - 0.3994148075580597
  - 0.46253731846809387
  - 0.4696003794670105
  - 0.3990747034549713
  - 0.5047288537025452
  - 0.4970473051071167
  - 0.4338575601577759
  - 0.5138417482376099
  - 0.6102132797241211
  - 0.40062353014945984
  - 0.5314796566963196
  - 0.5011478066444397
  - 0.42859721183776855
  - 0.4119226038455963
  - 0.5266343951225281
  - 1.7007359266281128
  - 2.023460626602173
  - 0.5037423968315125
  - 0.5007742047309875
  - 0.5190757513046265
  - 0.5180337429046631
  - 0.5424603223800659
  - 0.4851742684841156
  - 0.5133028626441956
  - 0.4677450656890869
  - 0.4776192605495453
loss_records_fold3:
  train_losses:
  - 2.7466943323612214
  - 2.7779203563928605
  - 2.7564472019672395
  - 2.751036933064461
  - 2.804864025115967
  - 2.760920822620392
  - 2.753730404376984
  - 2.701184391975403
  - 2.74283481836319
  - 2.724701145291329
  - 2.7175794631242756
  - 2.7888332664966584
  - 2.7105566173791886
  - 2.7195122212171556
  - 2.7533420205116275
  - 2.7355556547641755
  - 2.7857844769954685
  - 2.7467892169952393
  - 2.6740410417318348
  - 2.7901355594396593
  - 2.7274193823337556
  - 2.708049508929253
  - 2.7394645988941195
  - 2.735988223552704
  - 2.743470674753189
  - 2.6639859914779667
  - 2.767253750562668
  - 2.7476267993450167
  - 2.773155760765076
  - 2.7716564118862155
  - 2.736851155757904
  - 2.7401335418224337
  - 2.697744750976563
  - 2.7248380541801454
  - 2.729723656177521
  - 2.7380978733301164
  - 2.730847454071045
  - 2.7225802868604663
  - 2.7321160376071933
  - 2.6914132684469223
  - 2.7158400416374207
  - 2.727260446548462
  - 2.729632219672203
  - 2.705460870265961
  - 2.7014926224946976
  - 2.722298049926758
  - 2.699044117331505
  - 2.7248774081468583
  - 2.6712242990732196
  - 2.706192466616631
  - 2.691911682486534
  - 2.6724723458290103
  - 2.6929933190345765
  - 2.7209009379148483
  - 2.6875235438346863
  - 2.6863329261541367
  - 2.7006585747003555
  - 2.7915051490068437
  - 2.7373876482248307
  - 2.7091975927352907
  - 2.744835311174393
  - 2.730571848154068
  - 2.6957178711891174
  - 2.6907676935195926
  - 2.706094497442246
  - 2.722482740879059
  - 2.694193011522293
  - 2.7829640209674835
  - 2.6930462241172792
  - 2.7641879796981814
  - 2.798329120874405
  - 2.7987122535705566
  - 2.694874268770218
  - 2.6861037075519563
  - 2.722885596752167
  - 2.6738588333129885
  - 2.7138568043708804
  - 2.648562061786652
  - 2.7437087804079057
  - 2.690508779883385
  - 2.6998724669218066
  - 2.6819049358367923
  - 2.751307561993599
  - 2.752765634655953
  - 2.6557300716638568
  - 2.7568904221057893
  - 2.7152125984430313
  - 2.7224898576736454
  - 2.7298763722181323
  - 2.6997228622436524
  - 2.7087574511766435
  - 2.70173756480217
  - 2.7101020932197573
  - 2.633035105466843
  - 2.657474541664124
  - 2.7146926105022433
  - 2.7024366796016697
  - 2.6371440052986146
  - 2.714424639940262
  - 2.7351709038019183
  validation_losses:
  - 0.9727954864501953
  - 0.9026561975479126
  - 0.6692840456962585
  - 1.3011103868484497
  - 2.4607651233673096
  - 1.9596946239471436
  - 0.633320689201355
  - 0.6868561506271362
  - 2.1830053329467773
  - 5.202500820159912
  - 0.574275016784668
  - 0.6714893579483032
  - 0.9867538213729858
  - 1.7697645425796509
  - 3.131335496902466
  - 4.930634498596191
  - 3.743628978729248
  - 3.278301239013672
  - 10.853145599365234
  - 14.934582710266113
  - 18.084081649780273
  - 10.771160125732422
  - 32.42991638183594
  - 24.68770408630371
  - 19.16529083251953
  - 11.855939865112305
  - 33.603179931640625
  - 83.87507629394531
  - 20.11482048034668
  - 124.7392578125
  - 54.980960845947266
  - 126.19940185546875
  - 14.614792823791504
  - 63.14169692993164
  - 79.154541015625
  - 199.7522430419922
  - 272.9118347167969
  - 145.05032348632812
  - 33.872440338134766
  - 35.80668640136719
  - 65.7379150390625
  - 86.2730941772461
  - 62.6351318359375
  - 87.67572021484375
  - 67.04036712646484
  - 82.6671371459961
  - 152.9031524658203
  - 231.0467987060547
  - 167.98463439941406
  - 292.05908203125
  - 294.2200927734375
  - 397.64263916015625
  - 299.0766296386719
  - 143.7486572265625
  - 287.291748046875
  - 427.7038879394531
  - 574.0763549804688
  - 185.56072998046875
  - 104.40670776367188
  - 59.38581466674805
  - 167.41262817382812
  - 100.11965942382812
  - 58.08475875854492
  - 86.69904327392578
  - 105.72804260253906
  - 138.21527099609375
  - 252.74679565429688
  - 65.3509521484375
  - 228.8362579345703
  - 103.7613754272461
  - 115.04203796386719
  - 77.33057403564453
  - 80.24024963378906
  - 83.11600494384766
  - 92.36739349365234
  - 159.45079040527344
  - 97.1273193359375
  - 209.43145751953125
  - 133.91677856445312
  - 83.41239929199219
  - 155.0176544189453
  - 77.20631408691406
  - 49.79856872558594
  - 91.65831756591797
  - 155.9066925048828
  - 165.228759765625
  - 45.9808349609375
  - 29.872575759887695
  - 21.204011917114258
  - 30.500232696533203
  - 26.773944854736328
  - 11.245532035827637
  - 29.780061721801758
  - 12.035273551940918
  - 12.62032699584961
  - 81.08283233642578
  - 52.15715026855469
  - 81.963623046875
  - 86.16355895996094
  - 90.32634735107422
loss_records_fold4:
  train_losses:
  - 2.735984754562378
  - 2.7648938655853272
  - 2.6983570516109467
  - 2.743473166227341
  - 2.7191892206668857
  - 2.7210673272609713
  - 2.6976540565490725
  - 2.7253050416707993
  - 2.734994897246361
  - 2.697897204756737
  - 2.7152899622917177
  - 2.730899065732956
  - 2.7540888220071795
  - 2.7162098526954654
  - 2.720702657103539
  - 2.7147514104843142
  - 2.68714674115181
  - 2.6844680815935136
  - 2.770459032058716
  - 2.7242413431406023
  - 2.7403222173452377
  - 2.7429851025342944
  - 2.774766528606415
  - 2.779693287611008
  - 2.7991532355546953
  - 2.767696881294251
  - 2.7941872239112855
  validation_losses:
  - 8.826984405517578
  - 4.982253074645996
  - 6.058567047119141
  - 6.14067268371582
  - 6.841490745544434
  - 6.627610683441162
  - 6.122180938720703
  - 10.034283638000488
  - 8.49217700958252
  - 26.625728607177734
  - 10.018081665039062
  - 8.211894989013672
  - 9.102274894714355
  - 13.961974143981934
  - 8.253582000732422
  - 2.127333402633667
  - 1.8435282707214355
  - 2.111825942993164
  - 2.107370615005493
  - 1.2774080038070679
  - 3.328448534011841
  - 2.8931424617767334
  - 0.36975333094596863
  - 0.3638816475868225
  - 0.3675096035003662
  - 0.3627878427505493
  - 0.36737361550331116
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8439108061749572, 0.8439108061749572, 0.8473413379073756,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.11650485436893204, 0.09900990099009901, 0.18348623853211007,
    0.0]'
  mean_eval_accuracy: 0.8500368399026247
  mean_f1_accuracy: 0.07980019877822822
  total_train_time: '0:33:18.790874'
