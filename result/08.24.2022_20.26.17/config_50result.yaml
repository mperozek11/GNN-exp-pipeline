config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:34:30.798530'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_50fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_50fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_50fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_50fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_50fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_50fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_50fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_50fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_50fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_50fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.7217921555042268
  - 1.5248996853828432
  - 1.600224107503891
  - 1.5092795848846436
  - 1.4658122062683105
  - 1.4837193727493287
  - 1.5095094323158265
  - 1.4735713362693788
  - 1.4243324279785157
  - 1.4764675080776215
  - 1.486440247297287
  - 1.461337471008301
  - 1.5016509354114533
  - 1.5120532512664795
  - 1.4248606473207475
  - 1.4523823022842408
  - 1.5166731894016268
  - 1.454278814792633
  - 1.4455599546432496
  - 1.443655776977539
  - 1.4771317064762117
  - 1.4870998084545137
  - 1.5014268398284913
  - 1.5034744977951051
  - 1.5152226567268372
  - 1.4587010085582734
  validation_losses:
  - 0.4686349630355835
  - 0.40169593691825867
  - 0.42130163311958313
  - 0.39007988572120667
  - 0.410000205039978
  - 0.3926726281642914
  - 0.3962118625640869
  - 0.42715543508529663
  - 0.3796214163303375
  - 0.40043386816978455
  - 0.3906283676624298
  - 0.38351625204086304
  - 0.39976921677589417
  - 0.3946850001811981
  - 0.3818897306919098
  - 0.39845719933509827
  - 0.38773220777511597
  - 0.38472720980644226
  - 0.38302886486053467
  - 0.4068910479545593
  - 0.4140319526195526
  - 0.3845246732234955
  - 0.38395097851753235
  - 0.38903528451919556
  - 0.3851514160633087
  - 0.3890502154827118
loss_records_fold1:
  train_losses:
  - 1.4495392978191377
  - 1.4777556180953981
  - 1.4188591152429582
  - 1.4476674020290377
  - 1.4410419881343843
  - 1.4680663466453554
  - 1.4695122122764588
  - 1.4343035638332369
  - 1.4241946816444397
  - 1.4213664799928667
  - 1.4307967454195023
  - 1.4195801734924318
  - 1.4428741157054903
  - 1.4238603591918946
  - 1.4194188535213472
  - 1.4508416652679443
  - 1.4789783537387848
  - 1.4292734026908875
  - 1.5261617243289949
  validation_losses:
  - 0.3849416673183441
  - 0.4079332947731018
  - 0.4045441746711731
  - 0.4034433960914612
  - 0.387548565864563
  - 0.38784685730934143
  - 0.387258380651474
  - 0.3863355815410614
  - 0.40092116594314575
  - 0.3916671574115753
  - 0.4014603793621063
  - 0.38949814438819885
  - 0.4053458571434021
  - 0.39589205384254456
  - 0.38661816716194153
  - 0.3896726965904236
  - 0.39619752764701843
  - 0.3928191363811493
  - 0.38422703742980957
loss_records_fold2:
  train_losses:
  - 1.4272897869348526
  - 1.5175216555595399
  - 1.5021236389875412
  - 1.4427296161651613
  - 1.4523450374603273
  - 1.457370722293854
  - 1.45874742269516
  - 1.4486879587173462
  - 1.444062274694443
  - 1.423667848110199
  - 1.4756854176521301
  - 1.4689459085464478
  - 1.4533726513385774
  - 1.4277974367141724
  - 1.4292034298181535
  - 1.4188824117183687
  - 1.453442132472992
  validation_losses:
  - 0.4039080739021301
  - 0.4022328853607178
  - 0.4063892066478729
  - 0.3832930326461792
  - 0.4460126757621765
  - 0.3912910521030426
  - 0.40159446001052856
  - 0.38576701283454895
  - 0.3825818598270416
  - 0.3815271854400635
  - 0.39435476064682007
  - 0.390094131231308
  - 0.3893948197364807
  - 0.3844262957572937
  - 0.3803994059562683
  - 0.38252538442611694
  - 0.38433942198753357
loss_records_fold3:
  train_losses:
  - 1.4797668278217317
  - 1.4465438783168794
  - 1.4599648416042328
  - 1.4226464807987214
  - 1.4381965756416322
  - 1.4731602370738983
  - 1.4349550545215608
  - 1.4740501046180725
  - 1.465947425365448
  - 1.4498641312122347
  - 1.4607309937477113
  - 1.453393203020096
  - 1.475852119922638
  - 1.497551327943802
  - 1.442070257663727
  - 1.4774563789367676
  - 1.4736546456813813
  - 1.4588984429836274
  - 1.4361803591251374
  - 1.5136995196342469
  - 1.518473780155182
  - 1.4324011325836183
  - 1.4092947125434876
  - 1.442780077457428
  validation_losses:
  - 0.39848726987838745
  - 0.3725226819515228
  - 0.36846646666526794
  - 0.38548097014427185
  - 0.3735264241695404
  - 0.3897072970867157
  - 0.38126516342163086
  - 0.3688911497592926
  - 0.3695407807826996
  - 0.38161829113960266
  - 0.3654937148094177
  - 0.3799970746040344
  - 0.3689349293708801
  - 0.3764227628707886
  - 0.36635905504226685
  - 0.40754860639572144
  - 0.3710213005542755
  - 0.3847859501838684
  - 0.37270060181617737
  - 0.37543269991874695
  - 0.3667914569377899
  - 0.3719691038131714
  - 0.371291846036911
  - 0.36947932839393616
loss_records_fold4:
  train_losses:
  - 1.4373955309391022
  - 1.4765681326389313
  - 1.4708642542362214
  - 1.423675686120987
  - 1.4210556775331498
  - 1.4236955881118776
  - 1.446910208463669
  - 1.4527253568172456
  - 1.417710143327713
  - 1.432630318403244
  - 1.4572147309780121
  - 1.4132604837417604
  - 1.4174482822418213
  - 1.454188734292984
  - 1.4125018775463105
  - 1.4492934465408327
  - 1.4210888922214509
  validation_losses:
  - 0.39376088976860046
  - 0.4207613170146942
  - 0.4385514259338379
  - 0.43765732645988464
  - 0.38316643238067627
  - 0.3873544931411743
  - 0.37387776374816895
  - 0.37542903423309326
  - 0.3777316212654114
  - 0.37722015380859375
  - 0.3899146318435669
  - 0.37548625469207764
  - 0.37811005115509033
  - 0.3847973644733429
  - 0.3759252429008484
  - 0.37327098846435547
  - 0.37473657727241516
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8576329331046312,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:09:17.655314'
