config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.876747'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_0fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_0fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_0fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_0fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_0fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_0fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_0fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_0fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_0fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_0fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.23547335267067
  - 5.925691646337509
  - 5.8338029235601425
  - 5.828859519958496
  - 5.8620139300823215
  - 5.830983078479767
  - 5.736427971720696
  - 5.781645721197129
  - 5.836257803440095
  - 5.630459937453271
  - 5.739807364344597
  - 5.712389922142029
  - 5.786347448825836
  - 5.589308333396912
  - 5.613415476679802
  - 5.686361965537071
  - 5.606678152084351
  - 5.669497239589692
  - 5.630235725641251
  - 5.598403912782669
  - 5.650116527080536
  - 5.639680340886116
  - 5.6177065670490265
  - 5.619545146822929
  - 5.691571125388146
  - 5.641460210084915
  - 5.607339227199555
  - 5.618661034107209
  - 5.601973912119866
  - 5.634527695178986
  - 5.505905857682229
  - 5.604381087422372
  validation_losses:
  - 0.45571020245552063
  - 0.39473605155944824
  - 0.3880610764026642
  - 0.3961540758609772
  - 0.42859742045402527
  - 0.3894594609737396
  - 0.3879920244216919
  - 0.39891189336776733
  - 0.38640421628952026
  - 0.4546786844730377
  - 0.38920021057128906
  - 0.3922983407974243
  - 0.39838677644729614
  - 0.38646796345710754
  - 0.4200745224952698
  - 0.3938920795917511
  - 0.3878084421157837
  - 0.38762742280960083
  - 0.38932299613952637
  - 0.3921133577823639
  - 0.40292343497276306
  - 0.39858072996139526
  - 0.39949876070022583
  - 0.39093220233917236
  - 0.39320245385169983
  - 0.4049593508243561
  - 0.40557605028152466
  - 0.3990713655948639
  - 0.3896307051181793
  - 0.39861103892326355
  - 0.3896166682243347
  - 0.3879179060459137
loss_records_fold1:
  train_losses:
  - 5.618905651569367
  - 5.604944130778313
  - 5.637649038434029
  - 5.615922626852989
  - 5.6236967742443085
  - 5.594664111733437
  - 5.613581448793411
  - 5.639760518074036
  - 5.580508229136467
  - 5.603548833727837
  - 5.715813130140305
  - 5.618980106711388
  - 5.609417402744294
  - 5.605133721232415
  - 5.593976455926896
  - 5.676299676299095
  - 5.640655687451363
  - 5.615171551704407
  - 5.689399099349976
  - 5.618517914414406
  - 5.568299135565758
  validation_losses:
  - 0.39973005652427673
  - 0.38604193925857544
  - 0.3964439332485199
  - 0.3849749267101288
  - 0.38792067766189575
  - 0.3851584792137146
  - 0.38522616028785706
  - 0.3823569715023041
  - 0.40545228123664856
  - 0.38707032799720764
  - 0.39251816272735596
  - 0.38612428307533264
  - 0.3854432702064514
  - 0.38555610179901123
  - 0.4035678505897522
  - 0.39059746265411377
  - 0.391184538602829
  - 0.3864079713821411
  - 0.3905622661113739
  - 0.38543999195098877
  - 0.3877745568752289
loss_records_fold2:
  train_losses:
  - 5.564881199598313
  - 5.563571199774742
  - 5.5782103151083
  - 5.564787617325783
  - 5.592094370722771
  - 5.588411486148835
  - 5.564942896366119
  - 5.552900564670563
  - 5.547236451506615
  - 5.570683699846268
  - 5.568164503574372
  - 5.558583527803421
  - 5.529032161831856
  - 5.527695113420487
  - 5.56678213775158
  - 5.548219701647759
  - 5.51795788705349
  - 5.5645893007516865
  - 5.549417677521706
  - 5.54131418466568
  - 5.534145992994309
  - 5.509353199601174
  - 5.476355484127999
  - 5.4809686034917835
  - 5.5050951749086385
  - 5.47797122001648
  - 5.564611804485321
  - 5.569030615687371
  - 5.512501421570779
  - 5.483898568153382
  - 5.49216634631157
  - 5.547054725885392
  - 5.459036508202553
  - 5.524369686841965
  - 5.563230240345002
  - 5.537719592452049
  - 5.484345033764839
  validation_losses:
  - 0.38957512378692627
  - 0.389260470867157
  - 0.3832590579986572
  - 0.38654613494873047
  - 0.40111517906188965
  - 0.39481571316719055
  - 0.38528481125831604
  - 0.38132503628730774
  - 0.3842662274837494
  - 0.39457204937934875
  - 0.3856564462184906
  - 0.38320401310920715
  - 0.3987782895565033
  - 0.43230175971984863
  - 0.4011845588684082
  - 0.49379244446754456
  - 1.6730129718780518
  - 0.3872033655643463
  - 0.37963441014289856
  - 0.38359907269477844
  - 0.39072147011756897
  - 0.3950304388999939
  - 0.4253227114677429
  - 0.37818142771720886
  - 0.4374176561832428
  - 0.38605043292045593
  - 0.39205268025398254
  - 0.3868848979473114
  - 0.39205774664878845
  - 0.38111117482185364
  - 0.4954076111316681
  - 0.38814446330070496
  - 0.3933277726173401
  - 0.3777347505092621
  - 0.3869355022907257
  - 0.37506720423698425
  - 0.37725794315338135
loss_records_fold3:
  train_losses:
  - 5.538038948178292
  - 5.499356216192246
  - 5.5509233534336095
  - 5.5740942627191545
  - 5.571159863471985
  - 5.526574370265007
  - 5.567854741215706
  - 5.582324638962746
  - 5.484989532828331
  - 5.55260789692402
  - 5.550460469722748
  - 5.562809666991234
  - 5.4962782829999925
  - 5.590237769484521
  - 5.513118076324464
  - 5.529953065514565
  - 5.56257131099701
  - 5.4798064738512045
  - 5.502640104293824
  - 5.4572978734970095
  - 5.516848036646843
  - 5.51775036752224
  - 5.497106781601906
  - 5.498958009481431
  - 5.48996142745018
  - 5.510765343904495
  - 5.503048849105835
  - 5.484944227337838
  - 5.512117570638657
  - 5.387992018461228
  - 5.42410483956337
  - 5.472870802879334
  - 5.47090939283371
  - 5.5117888242006305
  - 5.508462221920491
  - 5.434101557731629
  - 5.433009523153306
  - 5.468758213520051
  - 5.5240399748086935
  - 5.539678472280503
  - 5.508106538653374
  - 5.538504499197007
  - 5.44062280356884
  - 5.398112544417382
  - 5.437302476167679
  - 5.498347038030625
  - 5.422086125612259
  - 5.386094692349435
  - 5.431517079472542
  - 5.471985045075417
  - 5.380878648161889
  - 5.459342309832573
  - 5.4570741862058645
  - 5.415116885304451
  - 5.4705707967281345
  - 5.462738019227982
  - 5.4100119352340705
  - 5.45270815640688
  - 5.446592018008232
  - 5.394519573450089
  - 5.405531206727028
  - 5.515262132883072
  - 5.433921504020692
  - 5.398016783595086
  - 5.416550090909005
  - 5.442277625203133
  - 5.37206327021122
  - 5.462187239527703
  - 5.475026732683182
  - 5.529235622286797
  - 5.488596889376641
  - 5.523254272341728
  - 5.525797459483147
  - 5.430672761797905
  - 5.519299083948136
  - 5.379443719983101
  - 5.376416140794754
  - 5.455919483304024
  - 5.397082568705082
  - 5.458926433324814
  - 5.46630064547062
  - 5.476399683952332
  - 5.40842105448246
  - 5.497465094923974
  - 5.405457299947739
  - 5.433355699479581
  - 5.4527570605278015
  - 5.386960339546204
  - 5.42317163348198
  - 5.435816940665245
  - 5.40479167252779
  - 5.435773369669914
  - 5.479936504364014
  - 5.355844014883042
  - 5.456126344203949
  - 5.341643649339677
  - 5.442011366784573
  - 5.4700905144214635
  - 5.375773015618325
  - 5.6005053460597995
  validation_losses:
  - 0.7473214864730835
  - 2.3334603309631348
  - 0.3735162317752838
  - 0.37316015362739563
  - 0.5569890737533569
  - 0.47648102045059204
  - 0.6089211702346802
  - 0.7587497234344482
  - 0.6150725483894348
  - 1.0349680185317993
  - 0.5245110988616943
  - 1.5898780822753906
  - 0.7818285226821899
  - 0.9503054618835449
  - 1.522330641746521
  - 0.39173075556755066
  - 0.38909944891929626
  - 0.44529104232788086
  - 1.4250538349151611
  - 1.5204540491104126
  - 0.44398754835128784
  - 2.247743844985962
  - 0.3765403628349304
  - 1.1548833847045898
  - 0.5076592564582825
  - 0.49523136019706726
  - 0.3840225338935852
  - 0.5564894676208496
  - 0.44312185049057007
  - 1.4348359107971191
  - 1.049573540687561
  - 0.3727022409439087
  - 0.7601597905158997
  - 0.8329833745956421
  - 0.6622769236564636
  - 0.5629215240478516
  - 0.4250306189060211
  - 0.3942257761955261
  - 0.8183165788650513
  - 0.41409239172935486
  - 1.6859817504882812
  - 0.500751256942749
  - 0.4165286421775818
  - 0.7503621578216553
  - 0.5187819004058838
  - 0.3775734007358551
  - 1.1000447273254395
  - 1.4060633182525635
  - 1.3192485570907593
  - 0.6187877058982849
  - 1.5582722425460815
  - 0.4723754823207855
  - 0.404763400554657
  - 0.4019964635372162
  - 0.40645015239715576
  - 3.508697748184204
  - 2.0401830673217773
  - 8.539932250976562
  - 4.723175048828125
  - 2.6472041606903076
  - 5.072055339813232
  - 1.4831628799438477
  - 3.5376367568969727
  - 6.305952548980713
  - 16.99728012084961
  - 8.300971984863281
  - 3.8056788444519043
  - 0.6589593887329102
  - 0.37383127212524414
  - 0.3843401074409485
  - 0.5091611742973328
  - 0.36567917466163635
  - 0.6730307936668396
  - 2.8503551483154297
  - 0.5889657735824585
  - 1.2292014360427856
  - 1.657791256904602
  - 1.3425557613372803
  - 1.0858956575393677
  - 0.7711332440376282
  - 2.9543230533599854
  - 0.9577789902687073
  - 0.6974116563796997
  - 2.684877872467041
  - 0.4342047870159149
  - 1.5674642324447632
  - 4.588771820068359
  - 1.2981773614883423
  - 2.6364047527313232
  - 2.468155860900879
  - 4.414915084838867
  - 8.642091751098633
  - 0.38015586137771606
  - 1.9358388185501099
  - 2.4714083671569824
  - 1.3232481479644775
  - 7.039172172546387
  - 0.7220359444618225
  - 1.6642364263534546
  - 0.3889625668525696
loss_records_fold4:
  train_losses:
  - 5.494907981157303
  - 5.528323608636857
  - 5.556738269329071
  - 5.4903366625309
  - 5.414641150832177
  - 5.401594406366349
  - 5.4951846450567245
  - 5.386222040653229
  - 5.458939427137375
  - 5.408920878171921
  - 5.367425754666328
  - 5.402561354637147
  - 5.495294168591499
  - 5.367922407388687
  - 5.51983312368393
  - 5.4050454050302505
  - 5.373114842176438
  - 5.404765620827675
  - 5.468723222613335
  - 5.416894376277924
  - 5.414099943637848
  - 5.459164333343506
  - 5.415616783499718
  - 5.372046509385109
  - 5.382465422153473
  - 5.389084163308144
  - 5.503728547692299
  - 5.442444881796837
  - 5.482163125276566
  - 5.376766583323479
  - 5.397802442312241
  - 5.417746046185494
  - 5.340742081403732
  - 5.3867853641510015
  - 5.441759258508682
  - 5.434168952703477
  - 5.398117515444756
  - 5.426894629001618
  - 5.402951782941819
  - 5.403885409235954
  - 5.458805251121522
  - 5.39825763553381
  - 5.406549967825413
  - 5.398457252979279
  - 5.377871170639992
  - 5.369189924001694
  - 5.34504118859768
  - 5.404659351706505
  - 5.389018785953522
  - 5.454364177584648
  - 5.407614016532898
  - 5.379841855168343
  - 5.447565796971322
  - 5.326016122102738
  - 5.45134206712246
  - 5.447166842222214
  - 5.433376693725586
  - 5.359048813581467
  - 5.449708005785943
  - 5.39702459871769
  - 5.415246278047562
  - 5.429313802719117
  - 5.320539194345475
  - 5.34016629755497
  - 5.318200194835663
  - 5.422073209285736
  - 5.391839970648289
  - 5.369391345977784
  - 5.471910580992699
  - 5.425442990660668
  - 5.3931657642126085
  - 5.326310023665428
  - 5.371493175625801
  - 5.399128708243371
  - 5.338580703735352
  - 5.492135238647461
  - 5.3796998634934425
  - 5.3286692857742315
  - 5.410799074172974
  - 5.386536368727684
  - 5.282654750347138
  - 5.33991242647171
  - 5.454445433616638
  - 5.388236618041993
  - 5.3911981880664825
  - 5.375564417243004
  - 5.373612603545189
  - 5.4603876113891605
  - 5.4072045445442205
  - 5.372609299421311
  - 5.336402815580368
  - 5.308132708072662
  - 5.393528074026108
  - 5.321940237283707
  - 5.342010337114335
  - 5.392351198196412
  - 5.413820241391659
  - 5.378958654403687
  - 5.302988058328629
  - 5.415760299563408
  validation_losses:
  - 0.3672051727771759
  - 0.3735176622867584
  - 0.3690735101699829
  - 0.3717637062072754
  - 0.37108221650123596
  - 0.3669038414955139
  - 0.41542911529541016
  - 0.37775781750679016
  - 0.3701545000076294
  - 0.36333104968070984
  - 0.456184059381485
  - 0.38884690403938293
  - 0.3669624924659729
  - 0.4553743898868561
  - 0.36383330821990967
  - 0.48549404740333557
  - 0.4539465606212616
  - 0.3682401478290558
  - 0.3719024360179901
  - 0.44093039631843567
  - 0.40175339579582214
  - 0.49448347091674805
  - 0.5797809362411499
  - 0.7123206853866577
  - 0.48061737418174744
  - 0.6015728712081909
  - 0.40256211161613464
  - 0.377273291349411
  - 0.3818332254886627
  - 0.3869229257106781
  - 0.3624694347381592
  - 0.3767286539077759
  - 0.48747017979621887
  - 0.4408600926399231
  - 0.5058668255805969
  - 0.5901880264282227
  - 0.41530343890190125
  - 0.6221150755882263
  - 0.4940551519393921
  - 0.6009848713874817
  - 0.43604546785354614
  - 0.44277599453926086
  - 0.4609638750553131
  - 0.6475305557250977
  - 0.39880236983299255
  - 0.37933409214019775
  - 1.0249849557876587
  - 0.3666057288646698
  - 0.5681664347648621
  - 0.5043958425521851
  - 0.48882240056991577
  - 0.4430767595767975
  - 0.4484196901321411
  - 0.36651718616485596
  - 0.4317251145839691
  - 0.5001663565635681
  - 0.5099228620529175
  - 0.49549588561058044
  - 0.4496488869190216
  - 0.4609546661376953
  - 0.6590293049812317
  - 0.7776059508323669
  - 0.4537780284881592
  - 0.46026983857154846
  - 0.4985618591308594
  - 0.6254987120628357
  - 0.4657805263996124
  - 0.6753329634666443
  - 0.7439989447593689
  - 1.4058467149734497
  - 0.8504594564437866
  - 0.4309603273868561
  - 0.48572736978530884
  - 1.2727371454238892
  - 1.0770460367202759
  - 0.8310130834579468
  - 0.5348862409591675
  - 0.7123249173164368
  - 0.5134347677230835
  - 0.4176430404186249
  - 0.5540602803230286
  - 0.429397851228714
  - 0.8757114410400391
  - 0.5141353607177734
  - 0.6488116383552551
  - 0.5552425384521484
  - 0.4223068654537201
  - 1.2681965827941895
  - 0.4057524502277374
  - 0.5158196091651917
  - 0.4773852229118347
  - 0.8071823120117188
  - 0.8901975750923157
  - 0.638248860836029
  - 1.0621155500411987
  - 1.3540679216384888
  - 0.8695665001869202
  - 0.870937705039978
  - 0.9776448011398315
  - 1.1667495965957642
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 32 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 37 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8505154639175257]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.08421052631578949]'
  mean_eval_accuracy: 0.8568955456136937
  mean_f1_accuracy: 0.016842105263157898
  total_train_time: '0:28:42.576669'
