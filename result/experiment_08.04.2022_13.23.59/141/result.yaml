config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 06:40:23.946115'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/141/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/141/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/141/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/141/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/141/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/141/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/141/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/141/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/141/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/141/fold_4_state_dict.pt
loss_records_fold4:
  train_losses:
  - 8.106485933065414
  - 8.514567792415619
  - 8.070220828056335
  - 8.35778459906578
  - 8.370630502700806
  - 8.536114037036896
  - 8.256410896778107
  - 8.329285621643066
  - 8.613953053951263
  - 8.26602390408516
  - 8.287009567022324
  - 8.249972999095917
  - 8.313581496477127
  - 8.153357207775116
  - 8.59603002667427
  - 8.64066857099533
  - 8.286616712808609
  - 8.355007350444794
  - 8.343350052833557
  - 8.21800372004509
  - 8.32651001214981
  - 8.531581610441208
  - 8.28448098897934
  - 8.277951717376709
  - 8.347022980451584
  - 8.659284830093384
  - 8.457617104053497
  - 8.220463335514069
  - 8.5170359313488
  - 8.924902439117432
  - 8.428266137838364
  - 8.185695722699165
  - 8.418402820825577
  - 8.295800685882568
  - 8.164183586835861
  - 8.237899154424667
  - 8.318524241447449
  - 8.54675617814064
  - 8.682057052850723
  - 8.521743685007095
  - 8.38118514418602
  - 8.041130051016808
  - 8.55965456366539
  - 8.337800741195679
  - 8.280412495136261
  - 8.317523419857025
  - 8.374556064605713
  - 8.667423009872437
  - 8.604601681232452
  - 8.427879363298416
  - 8.35085391998291
  - 8.254574686288834
  - 8.43041107058525
  - 8.440990656614304
  - 8.472756057977676
  - 8.415519118309021
  - 8.26167106628418
  - 8.357539057731628
  - 8.395556032657623
  - 8.041599601507187
  - 8.402964532375336
  - 8.452818900346756
  - 8.53405711054802
  - 8.493086874485016
  - 8.358503133058548
  - 8.43858540058136
  - 8.385304600000381
  - 8.440754503011703
  - 8.099912613630295
  - 8.36223354935646
  - 8.265055000782013
  - 8.271439641714096
  - 8.103416621685028
  - 8.33434522151947
  - 8.216434270143509
  - 8.05484639108181
  - 8.50432613492012
  - 8.518760293722153
  - 8.665811330080032
  - 8.55378606915474
  - 8.458560585975647
  - 8.193963512778282
  - 8.434966653585434
  - 8.541684120893478
  - 8.392828106880188
  - 8.335723370313644
  - 8.643945455551147
  - 8.281716912984848
  - 8.192331820726395
  - 8.505326241254807
  - 8.399196028709412
  - 8.3931325674057
  - 8.424857318401337
  - 8.374382019042969
  - 8.343157857656479
  - 8.179583609104156
  - 8.510222434997559
  - 8.317924231290817
  - 8.398524522781372
  - 8.28336986899376
  validation_losses:
  - 0.41576719284057617
  - 42801.1015625
  - 0.4181450307369232
  - 0.4266146421432495
  - 2574.806396484375
  - 28172.677734375
  - 0.41127192974090576
  - 0.42096102237701416
  - 0.4156036376953125
  - 0.40740966796875
  - 0.41156789660453796
  - 13541.9404296875
  - 0.4134857654571533
  - 17204.044921875
  - 0.4090043902397156
  - 72059.2109375
  - 0.41905102133750916
  - 0.4166102111339569
  - 28172.474609375
  - 0.4102693200111389
  - 0.43354514241218567
  - 0.42144301533699036
  - 31833.99609375
  - 72060.7890625
  - 13547.6904296875
  - 86691.4609375
  - 13547.0908203125
  - 64747.24609375
  - 72060.4140625
  - 0.4339013993740082
  - 0.4155561625957489
  - 68415.625
  - 0.4288923740386963
  - 90334.765625
  - 46455.5234375
  - 79383.6015625
  - 94014.8984375
  - 97660.8984375
  - 2576.219482421875
  - 31833.732421875
  - 156170.0625
  - 115933.1953125
  - 72059.796875
  - 115932.7890625
  - 90334.6953125
  - 0.41793540120124817
  - 90331.796875
  - 72060.03125
  - 97660.9765625
  - 94015.1484375
  - 148870.671875
  - 83035.3984375
  - 207362.890625
  - 145225.234375
  - 0.43472176790237427
  - 50118.36328125
  - 115933.4140625
  - 35495.875
  - 101336.3359375
  - 185418.25
  - 174460.6875
  - 130586.078125
  - 0.4232368469238281
  - 6232.6142578125
  - 156170.421875
  - 2576.087890625
  - 64746.4765625
  - 2574.958251953125
  - 46455.9453125
  - 0.4228641390800476
  - 112279.109375
  - 0.40897494554519653
  - 0.4051685631275177
  - 0.4201393723487854
  - 72060.1796875
  - 2571.73193359375
  - 0.411015123128891
  - 9890.21875
  - 17206.544921875
  - 39147.05859375
  - 0.42827320098876953
  - 0.42074987292289734
  - 0.42598018050193787
  - 0.42423054575920105
  - 119589.46875
  - 108643.5078125
  - 0.41280296444892883
  - 31833.71875
  - 0.42454466223716736
  - 0.42116209864616394
  - 0.40412279963493347
  - 0.42665404081344604
  - 94014.3828125
  - 72059.046875
  - 0.42489856481552124
  - 53769.9296875
  - 0.4263570010662079
  - 0.4077320396900177
  - 0.40896469354629517
  - 0.42170387506484985
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 87 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:03:43.189384'
