config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:49:43.790042'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_31fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_31fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_31fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_31fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_31fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_31fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_31fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_31fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_31fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_31fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 22.950736808776856
  - 10.205625784397126
  - 3.5969484090805057
  - 2.596316540241242
  - 2.424339771270752
  - 2.1265489161014557
  - 1.6615145623683931
  - 1.666110420227051
  - 1.4183243572711945
  - 1.074143213033676
  - 1.1258089661598205
  - 1.1012035667896272
  - 1.2997616648674013
  - 2.8043426275253296
  - 2.4209266006946564
  - 1.0613485634326936
  - 1.8199004888534547
  - 1.1750142276287079
  - 0.9993228495121003
  - 0.9920862317085266
  - 0.9516715824604035
  - 0.9894902646541596
  - 0.9109516382217407
  - 1.051599317789078
  - 3.838210493326187
  - 2.874997162818909
  - 7.786534184217453
  - 3.5965837597846986
  - 1.8337359189987184
  - 1.4658086776733399
  - 1.2861010551452638
  - 1.6375585436820985
  - 2.469875818490982
  - 1.4463296592235566
  - 1.370179706811905
  - 1.0991844952106475
  - 4.045626270771026
  - 3.0303442180156708
  - 2.2321797192096713
  - 9.198479324579239
  - 1.087164467573166
  - 1.6210427165031434
  - 3.755420994758606
  - 10.755144405364991
  - 1.8911063969135284
  - 1.9004164755344393
  - 0.9597100853919983
  - 0.8904253154993058
  - 1.1051215350627899
  - 0.9525699436664582
  - 1.1248211681842804
  - 1.460420233011246
  - 1.4422582536935806
  - 1.1864983022212983
  - 1.3724982261657717
  - 1.1436515152454376
  - 0.9745952367782593
  - 1.0022976458072663
  - 1.2348356306552888
  - 1.0454426527023315
  - 1.3206047832965853
  - 0.9198830664157868
  - 0.9990389943122864
  - 1.6632260084152222
  - 1.2795074343681336
  - 1.1093569457530976
  - 1.1370746612548828
  - 0.9547232687473297
  - 1.9938831806182862
  - 1.132785040140152
  - 0.9971656322479249
  - 0.8577926695346832
  - 1.035350489616394
  - 2.193500292301178
  - 1.0776048600673676
  - 1.248020225763321
  - 1.0941155135631562
  - 1.0215075552463533
  - 0.9429211795330048
  - 0.9913097262382508
  - 0.887342518568039
  - 0.8967048794031144
  - 0.9450017333030701
  - 0.9155843198299408
  - 0.966292542219162
  - 1.126975154876709
  - 1.7761400938034058
  - 1.2084889590740204
  - 1.01553293466568
  - 0.9729449987411499
  - 1.023089385032654
  - 1.0216953754425049
  - 1.0169995963573457
  - 0.9900562465190887
  - 0.9539318978786469
  - 1.1529641926288605
  - 0.8726104259490968
  - 0.8506593406200409
  - 1.0498357117176056
  - 0.9981976211071015
  validation_losses:
  - 6.828641891479492
  - 3.7785301208496094
  - 2.2341198921203613
  - 1.6261389255523682
  - 0.6171489953994751
  - 0.8160490989685059
  - 0.4898015558719635
  - 0.5860326290130615
  - 0.4286160171031952
  - 0.4136112630367279
  - 0.4163174033164978
  - 0.44893473386764526
  - 0.6157127022743225
  - 0.5433255434036255
  - 0.5247389078140259
  - 0.4082387685775757
  - 0.5704401135444641
  - 0.43691444396972656
  - 0.40943121910095215
  - 0.45771467685699463
  - 0.4301419258117676
  - 0.49106353521347046
  - 0.5229310989379883
  - 0.4525854289531708
  - 0.49577590823173523
  - 0.7799038290977478
  - 0.6556157469749451
  - 0.7800660729408264
  - 0.712916374206543
  - 0.7653515338897705
  - 0.5173123478889465
  - 0.5602234601974487
  - 0.5095028877258301
  - 0.5098371505737305
  - 0.49161437153816223
  - 0.4659713804721832
  - 0.44151771068573
  - 0.8892485499382019
  - 0.5103330612182617
  - 0.48334306478500366
  - 0.4829135239124298
  - 0.4262605905532837
  - 0.4375639855861664
  - 0.42409223318099976
  - 0.837631106376648
  - 0.4555896520614624
  - 0.4472091495990753
  - 0.4875599443912506
  - 0.48158514499664307
  - 0.4530780017375946
  - 0.4598434269428253
  - 0.4693313241004944
  - 0.5893008708953857
  - 0.680957019329071
  - 0.4657692313194275
  - 0.46678903698921204
  - 0.5977809429168701
  - 0.4262993335723877
  - 0.5366374254226685
  - 0.4362808167934418
  - 0.5151122212409973
  - 0.48833906650543213
  - 0.514692485332489
  - 0.641450822353363
  - 0.596015453338623
  - 0.5113378167152405
  - 0.427593469619751
  - 0.5201677680015564
  - 0.5744439363479614
  - 0.4952230453491211
  - 0.41605237126350403
  - 0.4210805594921112
  - 0.6237838864326477
  - 0.7033108472824097
  - 0.5014501810073853
  - 0.44993871450424194
  - 0.4465066194534302
  - 0.41370657086372375
  - 0.3929012715816498
  - 0.41822904348373413
  - 0.3926832377910614
  - 0.4392389953136444
  - 0.3886617124080658
  - 0.3928196430206299
  - 0.40930914878845215
  - 0.4215775430202484
  - 0.40691864490509033
  - 0.5068447589874268
  - 0.4315090775489807
  - 0.49094921350479126
  - 0.3855363428592682
  - 0.5472522377967834
  - 0.42640241980552673
  - 0.4241921603679657
  - 0.423628032207489
  - 0.39508071541786194
  - 0.4079148769378662
  - 0.40729382634162903
  - 0.3983282148838043
  - 0.4347832500934601
loss_records_fold1:
  train_losses:
  - 0.9068282902240754
  - 0.9548987329006196
  - 0.9525651037693024
  - 0.8727305114269257
  - 0.9369704902172089
  - 0.9749866008758545
  - 0.9597856044769287
  - 0.8999756157398224
  - 0.8543047249317169
  - 0.8438100099563599
  - 0.911718773841858
  - 0.983460283279419
  - 0.9179590463638306
  - 0.9439371645450593
  - 2.363529086112976
  - 1.2031582772731781
  - 1.1222053349018097
  - 1.5409294664859772
  - 1.1229141175746917
  - 0.9459533095359802
  - 0.8421452641487122
  - 0.8694600820541383
  - 0.9901875078678132
  - 0.9829727768898011
  - 0.9497566342353821
  - 0.8502970218658448
  - 0.8441183626651765
  - 0.8795479834079742
  - 0.8455612897872925
  - 0.8723401427268982
  - 0.8085693001747132
  - 2.0869752526283265
  - 1.6011588871479034
  - 0.9031317651271821
  - 0.9779789030551911
  - 0.918892127275467
  - 0.9521768987178802
  - 0.8742550015449524
  - 1.4792989790439606
  - 0.9455229043960571
  - 0.8865376353263855
  - 0.9086184144020081
  - 0.8423094689846039
  - 1.4803076505661013
  - 1.5098779916763307
  - 1.1971560776233674
  - 1.147471731901169
  - 2.737188768386841
  - 2.335197591781616
  - 1.0168931633234024
  - 1.3780777275562288
  - 2.311361163854599
  - 1.156638824939728
  - 1.001796942949295
  - 1.2797487497329714
  - 1.398020786046982
  - 5.08586710691452
  - 1.4112604379653932
  - 1.0911003291606904
  - 1.0280964195728302
  - 0.9375253438949586
  validation_losses:
  - 0.4205886423587799
  - 0.40568119287490845
  - 0.3974325954914093
  - 0.481222003698349
  - 0.4441240429878235
  - 0.42077937722206116
  - 0.4615480601787567
  - 0.4027823805809021
  - 0.3995107412338257
  - 0.42667075991630554
  - 0.3991067409515381
  - 0.3994305431842804
  - 0.5336357355117798
  - 0.43894290924072266
  - 0.43063199520111084
  - 0.5196854472160339
  - 0.42395952343940735
  - 0.4566498100757599
  - 0.45490318536758423
  - 0.45023050904273987
  - 0.4126530587673187
  - 0.4278816878795624
  - 0.48131588101387024
  - 0.4476584494113922
  - 0.40014269948005676
  - 0.4321501851081848
  - 0.41400110721588135
  - 0.41114339232444763
  - 0.4042714834213257
  - 0.40074506402015686
  - 0.41511157155036926
  - 0.4047829210758209
  - 0.40887123346328735
  - 0.41633841395378113
  - 0.4182034134864807
  - 0.41948527097702026
  - 0.4452166259288788
  - 0.4238910675048828
  - 0.4781416654586792
  - 0.4953773617744446
  - 0.4521598517894745
  - 0.4008713364601135
  - 0.4051644802093506
  - 0.43250590562820435
  - 0.44158935546875
  - 0.40392056107521057
  - 0.43215078115463257
  - 0.774631917476654
  - 0.47582611441612244
  - 0.4510310888290405
  - 0.41553011536598206
  - 0.4185159206390381
  - 0.4343579411506653
  - 0.4222557842731476
  - 0.43829309940338135
  - 0.41950350999832153
  - 0.4267110228538513
  - 0.42087820172309875
  - 0.41346752643585205
  - 0.4215281009674072
  - 0.3974821865558624
loss_records_fold2:
  train_losses:
  - 1.3665282666683198
  - 1.8140894770622253
  - 1.3084728538990023
  - 1.2356165289878847
  - 0.9814573228359222
  - 1.0021161437034607
  - 0.8715855360031128
  - 0.8676828861236573
  - 0.9314250469207764
  - 0.8753652036190034
  - 0.84194096326828
  - 3.8659371674060825
  - 0.8618892431259155
  - 0.8096821874380112
  - 0.8999953091144562
  - 0.945288360118866
  - 0.8505403816699982
  - 0.8743387162685394
  - 0.9219222962856293
  - 1.0094574928283693
  - 0.9118048906326295
  - 2.012341868877411
  - 1.1249905049800872
  - 1.33160959482193
  - 0.8763295084238053
  - 0.8592391431331635
  - 0.8742811858654023
  - 0.8471621215343476
  - 0.8677225351333618
  - 0.8621750771999359
  - 0.8253947138786316
  - 0.8317818641662598
  - 0.8464355170726776
  - 0.8706700325012208
  - 0.9877735912799835
  - 0.8487012803554536
  - 0.9148351788520813
  - 0.8757688105106354
  - 0.8711628258228302
  - 0.8513108253479005
  - 0.836702036857605
  - 0.898774254322052
  - 1.0780155539512635
  - 0.9856161534786225
  - 0.909015828371048
  - 0.8645740270614625
  - 0.8841603100299835
  - 0.8484201788902284
  - 0.8541901290416718
  - 0.8593531012535096
  - 0.8583805739879609
  - 1.0380964577198029
  - 0.8555130243301392
  validation_losses:
  - 0.4236351251602173
  - 0.45628052949905396
  - 0.4951503276824951
  - 0.46868422627449036
  - 0.3987923562526703
  - 0.38719019293785095
  - 0.3948349356651306
  - 0.39089715480804443
  - 0.3873864710330963
  - 0.41522809863090515
  - 0.3908770978450775
  - 0.38382822275161743
  - 0.3774355947971344
  - 0.3807593882083893
  - 0.40239712595939636
  - 0.3803764581680298
  - 0.398528128862381
  - 0.39142805337905884
  - 0.4438139498233795
  - 0.3935832381248474
  - 0.40554967522621155
  - 0.4028583765029907
  - 0.3895975351333618
  - 0.3783428966999054
  - 0.4107664227485657
  - 0.37753579020500183
  - 0.3857477605342865
  - 0.415219783782959
  - 0.39347732067108154
  - 0.38920700550079346
  - 0.374348521232605
  - 0.38902559876441956
  - 0.4327714145183563
  - 0.38935381174087524
  - 0.38991430401802063
  - 0.39691805839538574
  - 0.3795516788959503
  - 0.400246262550354
  - 0.3876429796218872
  - 0.4048874080181122
  - 0.42233121395111084
  - 0.41000989079475403
  - 0.6111498475074768
  - 0.4406752586364746
  - 0.3880331218242645
  - 0.3766474425792694
  - 0.4080030024051666
  - 0.37992405891418457
  - 0.38008353114128113
  - 0.3877345323562622
  - 0.37767577171325684
  - 0.38727185130119324
  - 0.3933676779270172
loss_records_fold3:
  train_losses:
  - 1.0020580351352693
  - 1.1965474486351013
  - 0.9340185821056366
  - 0.9993760943412782
  - 0.8721725821495057
  - 0.9543547689914704
  - 0.8636879086494447
  - 1.90448357462883
  - 1.4376709699630739
  - 0.988020384311676
  - 0.9310813665390015
  - 1.3211181044578553
  - 0.864870685338974
  - 0.9493866860866547
  - 0.8739169418811799
  - 0.9033740520477296
  - 1.0634791612625123
  - 0.9594960391521454
  - 0.8909874618053437
  - 0.9124437570571899
  - 0.818423366546631
  - 1.192045897245407
  - 0.9513758599758149
  - 0.8457790732383729
  - 0.9202587723731995
  - 0.8083382844924927
  - 0.8290766596794129
  - 0.953727400302887
  - 1.3782907903194428
  - 0.8334653437137605
  - 0.8361152648925781
  - 0.8412642836570741
  - 0.8353476524353027
  - 1.090768676996231
  - 1.1817683577537537
  - 1.0113206744194032
  - 0.8433388769626617
  - 0.8753484249114991
  - 0.8642905294895172
  - 0.9204272508621216
  - 0.8194059014320374
  - 0.8662160217761994
  - 0.8822950422763824
  - 0.8613707840442658
  - 0.9022206842899323
  - 0.8611109733581543
  - 0.8760469615459443
  - 0.8667477488517762
  - 0.7958456516265869
  validation_losses:
  - 0.40220746397972107
  - 0.4139079749584198
  - 0.40327587723731995
  - 0.3945353031158447
  - 0.3898000121116638
  - 0.3912607431411743
  - 0.40970897674560547
  - 0.561001181602478
  - 0.3906329870223999
  - 0.4171043634414673
  - 0.42857956886291504
  - 0.4500909447669983
  - 0.4637671113014221
  - 0.4320051074028015
  - 0.41720128059387207
  - 0.4449250102043152
  - 0.414741188287735
  - 0.42171165347099304
  - 0.41255640983581543
  - 0.3945615291595459
  - 0.4006570875644684
  - 0.45847463607788086
  - 0.4061000347137451
  - 0.4070228040218353
  - 0.4084486961364746
  - 0.4209606945514679
  - 0.4033089876174927
  - 0.40664809942245483
  - 0.4198146164417267
  - 0.4024253189563751
  - 0.40472862124443054
  - 0.4130271077156067
  - 0.4118987023830414
  - 0.41960033774375916
  - 0.49594351649284363
  - 0.43330180644989014
  - 0.4101962447166443
  - 0.41689202189445496
  - 0.4048512578010559
  - 0.4129646122455597
  - 0.424405038356781
  - 0.3981483578681946
  - 0.47415032982826233
  - 0.4077497720718384
  - 0.4022589325904846
  - 0.40525978803634644
  - 0.4072408378124237
  - 0.4079960584640503
  - 0.4047158658504486
loss_records_fold4:
  train_losses:
  - 0.868375438451767
  - 0.8505245923995972
  - 0.8524632036685944
  - 0.8668353319168092
  - 0.8340213894844055
  - 0.8681690275669098
  - 0.8243313580751419
  - 0.8732936143875123
  - 0.8839697718620301
  - 0.8452177345752716
  - 0.8292502522468568
  - 0.8729600191116333
  - 1.082153868675232
  - 1.2000775933265686
  - 0.8666324973106385
  - 0.8684159755706787
  - 0.8856447696685792
  - 0.8800026774406433
  - 0.9873803019523621
  - 0.8756182670593262
  - 0.823316103219986
  - 0.9291328132152558
  - 0.9665351510047913
  - 0.9811358749866486
  - 0.9186308324337006
  - 0.8930307626724243
  - 0.8892825603485108
  - 0.8352883100509644
  - 0.8548358500003815
  - 0.8420459985733033
  - 2.4039114296436312
  - 0.9920707762241364
  - 0.9826905727386475
  - 0.8460957705974579
  - 1.1241633832454683
  - 0.8732812404632568
  - 0.865405809879303
  - 0.9698243737220764
  - 0.8572917282581329
  - 1.5582069396972658
  - 1.073910439014435
  - 0.9872052013874054
  - 0.8308881342411042
  - 0.916081976890564
  - 0.8959678262472153
  - 0.8461775124073029
  - 0.9533207297325135
  - 0.8328625023365022
  - 0.8444648444652558
  - 0.8255142867565155
  - 0.8853811383247376
  - 0.8711516618728639
  - 0.8891617059707642
  - 0.8537223279476166
  - 0.8586378633975983
  - 0.8910614848136902
  - 0.8752024590969086
  - 0.9152176201343537
  - 0.85074183344841
  - 0.8554420471191406
  - 0.8487883746623993
  - 0.8154217064380647
  - 0.8646326005458832
  - 0.8356961250305176
  - 0.8589602887630463
  - 0.8727993845939637
  - 0.8051753133535385
  - 0.8844210267066956
  - 0.855749648809433
  - 0.8260602831840516
  - 0.8239847719669342
  - 0.8866801738739014
  - 0.8934520184993744
  - 0.8400058150291443
  - 0.8266294062137605
  - 0.8986224055290223
  - 0.8636298894882203
  - 0.8307489514350892
  - 0.8503296792507172
  - 0.8576820015907288
  - 0.8500585436820984
  - 0.8274561882019044
  - 0.8281744182109834
  - 0.8503242135047913
  - 0.8420985758304597
  validation_losses:
  - 0.40677741169929504
  - 0.4066864848136902
  - 0.4237973093986511
  - 0.41726261377334595
  - 0.41572749614715576
  - 0.4195046126842499
  - 0.41898313164711
  - 0.42545923590660095
  - 0.4072299301624298
  - 0.41861507296562195
  - 0.42442595958709717
  - 0.4202449917793274
  - 0.43292778730392456
  - 0.42964303493499756
  - 0.43325358629226685
  - 0.4577428996562958
  - 0.40180444717407227
  - 0.43221840262413025
  - 0.5043320059776306
  - 0.49339181184768677
  - 0.42697378993034363
  - 0.4460042417049408
  - 0.4544900059700012
  - 0.488730251789093
  - 0.47559380531311035
  - 0.4638916850090027
  - 0.4174031615257263
  - 0.46231991052627563
  - 0.4448508620262146
  - 0.44553524255752563
  - 0.518774688243866
  - 0.5284064412117004
  - 0.4051764905452728
  - 0.4235578179359436
  - 0.43422365188598633
  - 0.39755192399024963
  - 0.42733529210090637
  - 0.4367714822292328
  - 0.4384670853614807
  - 0.40875518321990967
  - 0.39409735798835754
  - 0.4483132064342499
  - 0.48467686772346497
  - 0.5058443546295166
  - 0.4794633686542511
  - 0.4295545518398285
  - 0.45805877447128296
  - 0.4324893355369568
  - 0.4222395420074463
  - 0.42080846428871155
  - 0.4072689712047577
  - 0.4094410240650177
  - 0.43796423077583313
  - 0.41342785954475403
  - 0.4349731206893921
  - 0.39367204904556274
  - 0.46084269881248474
  - 0.422960102558136
  - 0.41145408153533936
  - 0.42920732498168945
  - 0.40224209427833557
  - 0.432866632938385
  - 0.4182840585708618
  - 0.41450732946395874
  - 0.4348525404930115
  - 0.4182557463645935
  - 0.4248083531856537
  - 0.40450823307037354
  - 0.4154932200908661
  - 0.4354536831378937
  - 0.4117138087749481
  - 0.4108785390853882
  - 0.43055635690689087
  - 0.4459567964076996
  - 0.4381668269634247
  - 0.4239940941333771
  - 0.42310830950737
  - 0.40305212140083313
  - 0.4364578127861023
  - 0.42029422521591187
  - 0.4173642694950104
  - 0.41811734437942505
  - 0.42313510179519653
  - 0.4139591157436371
  - 0.4152218699455261
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 61 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 53 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 49 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 85 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8576329331046312, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:28:16.357876'
