config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 04:53:52.166895'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/128/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/128/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/128/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/128/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/128/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/128/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/128/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/128/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/128/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/128/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 27.482330858707428
  - 27.711093932390213
  - 27.498705461621284
  - 27.06562615931034
  - 27.356547445058823
  - 27.44269572198391
  - 27.10252532362938
  - 27.570720821619034
  - 28.085563585162163
  - 28.475871428847313
  - 27.683086082339287
  - 28.266132578253746
  - 27.714406356215477
  - 27.78661870956421
  - 27.731149300932884
  - 28.248129442334175
  - 27.53217914700508
  - 27.7377288043499
  - 27.315828755497932
  - 27.674299836158752
  - 27.333284080028534
  - 27.356392711400986
  - 27.39424131810665
  - 27.467332646250725
  - 27.44402876496315
  - 27.128645047545433
  - 27.394875429570675
  - 28.591761276125908
  - 27.38805864751339
  - 27.28426057100296
  - 27.511452242732048
  - 27.34000188857317
  - 27.351363137364388
  - 27.33550028502941
  - 27.21601927280426
  - 27.25692743062973
  - 27.10895550251007
  - 27.35591046512127
  - 27.100282207131386
  - 26.905115142464638
  - 27.409156046807766
  - 26.919116765260696
  - 26.865621209144592
  - 27.128971852362156
  - 26.953255280852318
  - 27.3239356726408
  - 27.263213366270065
  - 27.401438370347023
  - 27.39838482439518
  - 27.30387870967388
  - 27.547633036971092
  - 27.051664233207703
  - 27.345254972577095
  - 27.573016241192818
  - 27.419467449188232
  - 27.234733745455742
  - 27.09513971209526
  - 26.779798284173012
  - 27.027635514736176
  - 26.829615458846092
  - 27.347864754498005
  - 27.069241672754288
  - 26.944000124931335
  - 26.659866362810135
  - 27.25911045074463
  - 26.865717619657516
  - 27.282574772834778
  - 27.2635248452425
  - 27.187707483768463
  - 27.408032432198524
  - 27.1292532235384
  - 27.135234519839287
  - 27.102439522743225
  - 27.20338088274002
  - 27.2691782861948
  - 27.163750916719437
  - 27.266924127936363
  - 27.24378803372383
  - 27.14159817993641
  - 27.127660408616066
  - 27.095436856150627
  - 27.04577350616455
  - 26.924446433782578
  - 26.885718762874603
  - 27.404246166348457
  - 26.849911347031593
  - 27.295676320791245
  - 27.03160896897316
  - 27.038937479257584
  - 26.742281302809715
  - 26.956211403012276
  - 26.74507799744606
  - 27.149541586637497
  - 26.548086807131767
  - 26.940192714333534
  - 26.71374836564064
  - 27.020594343543053
  - 26.839865416288376
  - 26.514953941106796
  - 26.4575834274292
  validation_losses:
  - 0.46164458990097046
  - 0.40681999921798706
  - 0.4218469560146332
  - 0.5133545994758606
  - 0.5014734268188477
  - 0.48004350066185
  - 0.6546471118927002
  - 0.49668610095977783
  - 0.4263266921043396
  - 0.40941864252090454
  - 0.40457090735435486
  - 0.40686294436454773
  - 0.4554004967212677
  - 0.4011976718902588
  - 0.41835474967956543
  - 0.41319790482521057
  - 0.4104480743408203
  - 0.45571863651275635
  - 0.44250383973121643
  - 0.4916914403438568
  - 0.495970755815506
  - 0.5188814401626587
  - 0.4824078679084778
  - 0.7877466082572937
  - 0.5937963128089905
  - 0.5969375371932983
  - 0.5915641784667969
  - 0.8814714550971985
  - 0.5198120474815369
  - 0.5263283848762512
  - 0.540212094783783
  - 0.6013997197151184
  - 0.46839720010757446
  - 0.5319600105285645
  - 0.5551567077636719
  - 0.4565558135509491
  - 0.49990350008010864
  - 0.5248491764068604
  - 0.4528588354587555
  - 0.5517454743385315
  - 0.45333388447761536
  - 0.5091511607170105
  - 0.5416280627250671
  - 0.6431872248649597
  - 0.7348004579544067
  - 0.5861420035362244
  - 0.4465164244174957
  - 0.4656296372413635
  - 0.4251680076122284
  - 0.5030022859573364
  - 0.5013467669487
  - 0.47530433535575867
  - 0.4537796080112457
  - 0.6112460494041443
  - 0.5812740325927734
  - 0.5525810718536377
  - 0.4767916202545166
  - 0.6305029392242432
  - 0.47286513447761536
  - 0.6238170266151428
  - 0.5323278903961182
  - 0.5798782706260681
  - 0.6825136542320251
  - 0.7427201867103577
  - 0.5562866926193237
  - 0.542866587638855
  - 0.519814133644104
  - 0.4687524139881134
  - 0.5539489388465881
  - 0.5632085800170898
  - 0.5829952359199524
  - 0.5648230314254761
  - 0.6591904759407043
  - 0.48020097613334656
  - 0.5789266228675842
  - 0.45926931500434875
  - 0.6833925843238831
  - 0.5505602359771729
  - 0.5875385403633118
  - 0.48998120427131653
  - 0.6128526329994202
  - 0.5220764875411987
  - 0.7532473206520081
  - 0.48286736011505127
  - 0.5355887413024902
  - 0.5163813233375549
  - 0.5831921100616455
  - 0.5523682832717896
  - 0.5390859842300415
  - 0.5510821342468262
  - 0.6886682510375977
  - 0.6327671408653259
  - 0.6679165959358215
  - 0.7365280985832214
  - 0.6930316090583801
  - 0.6867762207984924
  - 0.7105198502540588
  - 0.7602835893630981
  - 0.6116828322410583
  - 0.6597667336463928
loss_records_fold2:
  train_losses:
  - 26.761492013931274
  - 26.92312218248844
  - 26.737204119563103
  - 27.058433637022972
  - 26.69605103135109
  - 26.517716497182846
  - 27.30680999159813
  - 27.05691246688366
  - 27.223124980926514
  - 26.942891508340836
  - 26.719680786132812
  - 27.087008133530617
  - 26.799315929412842
  - 27.1259708404541
  - 26.79069508612156
  - 26.619555667042732
  - 26.800667360424995
  - 28.09493611752987
  - 27.718184903264046
  - 27.986568219959736
  - 27.825017675757408
  - 27.708869144320488
  - 27.931379035115242
  - 27.65753084421158
  - 27.794972106814384
  - 27.52397757768631
  - 27.484224528074265
  - 27.83013467490673
  - 27.523615166544914
  - 27.681273251771927
  - 27.437981814146042
  - 27.48920403420925
  - 27.455441281199455
  - 27.55030332505703
  - 27.323129266500473
  - 27.50118201971054
  - 27.30137449502945
  - 27.4279373139143
  - 27.36167760193348
  - 27.62808220088482
  - 27.11483070254326
  - 27.15687097609043
  - 27.17783623933792
  - 27.17605021595955
  - 27.26752658188343
  - 26.942739352583885
  - 27.587263852357864
  - 27.140132188796997
  - 26.770970970392227
  - 27.06298117339611
  - 26.904072418808937
  - 26.9392771422863
  - 27.0421531945467
  - 26.976683512330055
  - 26.721985146403313
  - 27.433417558670044
  - 27.05110415816307
  - 26.836863294243813
  - 27.361497715115547
  - 27.018880769610405
  - 27.215591937303543
  - 26.767354786396027
  - 26.972444154322147
  - 27.396037504076958
  - 27.938840374350548
  - 28.024232983589172
  - 28.221942499279976
  - 27.547549471259117
  - 27.57501846551895
  - 27.65969116985798
  - 27.470861092209816
  - 27.482329174876213
  - 27.663589045405388
  - 27.141284614801407
  - 27.060277596116066
  - 27.318370908498764
  - 27.077304497361183
  - 26.76873606443405
  - 26.83866675198078
  - 26.75162997841835
  - 26.90296357870102
  - 27.313574507832527
  - 26.788379549980164
  - 27.13297103345394
  - 26.994410693645477
  - 27.23384089767933
  - 26.616498932242393
  - 26.962549969553947
  - 26.56895523518324
  - 26.937644138932228
  - 26.83388390392065
  - 26.747691690921783
  - 26.852860167622566
  - 26.859969571232796
  - 26.706768974661827
  - 26.82460018992424
  - 27.155705094337463
  - 26.755487576127052
  - 26.528823032975197
  - 26.56149296462536
  validation_losses:
  - 0.6485493779182434
  - 0.5356712341308594
  - 0.6166452169418335
  - 0.5110569000244141
  - 0.7329950332641602
  - 0.7866798043251038
  - 0.6283928751945496
  - 0.5497645735740662
  - 0.5210892558097839
  - 0.613071858882904
  - 0.6243237853050232
  - 0.5489353537559509
  - 0.6193655133247375
  - 0.5771703124046326
  - 0.49100741744041443
  - 0.4661952257156372
  - 0.6449547410011292
  - 0.3911293148994446
  - 0.42324161529541016
  - 0.38038170337677
  - 0.3725910186767578
  - 0.3773651421070099
  - 0.3790734112262726
  - 0.39353856444358826
  - 0.3851911425590515
  - 0.38368698954582214
  - 0.3970988988876343
  - 0.43907132744789124
  - 0.38234448432922363
  - 0.3846510052680969
  - 0.3842773139476776
  - 0.4123196303844452
  - 0.49693986773490906
  - 0.4158453643321991
  - 0.707667350769043
  - 0.3728998899459839
  - 0.4504046142101288
  - 0.40520086884498596
  - 0.41144198179244995
  - 0.42045068740844727
  - 0.4135870039463043
  - 0.44047629833221436
  - 0.4234861731529236
  - 0.4478875994682312
  - 0.5984492897987366
  - 0.4677382707595825
  - 0.5272060632705688
  - 0.47831860184669495
  - 0.5460522174835205
  - 0.46595558524131775
  - 0.42804285883903503
  - 0.46597009897232056
  - 0.4994604289531708
  - 0.47007426619529724
  - 0.5431100726127625
  - 0.531556248664856
  - 0.5691893696784973
  - 0.5982815623283386
  - 0.5684091448783875
  - 0.6101609468460083
  - 0.45314303040504456
  - 0.5524647831916809
  - 0.5285727381706238
  - 0.49863049387931824
  - 0.48451706767082214
  - 0.43763405084609985
  - 0.48042750358581543
  - 0.47630006074905396
  - 0.41828468441963196
  - 0.4861336350440979
  - 0.5614040493965149
  - 0.49515095353126526
  - 0.5437251925468445
  - 0.5673227906227112
  - 0.4851583242416382
  - 0.6715742349624634
  - 0.42761698365211487
  - 0.6219747066497803
  - 0.587874174118042
  - 0.7140403985977173
  - 0.7391708493232727
  - 0.6448163390159607
  - 0.5823072195053101
  - 0.6618594527244568
  - 0.7921482920646667
  - 0.4905063211917877
  - 0.7270631790161133
  - 0.6604416966438293
  - 0.648657500743866
  - 0.604751467704773
  - 0.6252200603485107
  - 0.5265177488327026
  - 0.4620879590511322
  - 0.793113112449646
  - 0.49233701825141907
  - 0.5274806022644043
  - 0.5532031059265137
  - 0.7449640035629272
  - 0.5874078273773193
  - 1.0160322189331055
loss_records_fold3:
  train_losses:
  - 27.226178750395775
  - 27.30109390616417
  - 27.885648623108864
  - 28.07149875164032
  - 27.592500790953636
  - 27.75882177054882
  - 27.52558720111847
  - 27.390467539429665
  - 27.681077525019646
  - 27.66137497127056
  - 27.415855675935745
  - 27.41299046576023
  - 27.29544261097908
  - 27.768936350941658
  - 27.57379373908043
  - 27.27146816253662
  - 27.813737154006958
  - 27.430376693606377
  - 27.407532140612602
  - 27.35705104470253
  - 27.329992800951004
  - 27.553419068455696
  - 27.840677112340927
  - 27.722172662615776
  - 27.68046921491623
  - 27.417928755283356
  - 27.59306898713112
  - 27.287596374750137
  - 27.565560966730118
  - 27.637638747692108
  - 27.111480459570885
  - 27.242291927337646
  - 27.17980767786503
  - 26.99097016453743
  - 26.905974626541138
  - 27.28387239575386
  - 26.758461460471153
  - 27.73673241585493
  - 27.736935555934906
  - 27.34129185974598
  - 27.237843737006187
  - 27.427416786551476
  - 27.596570014953613
  - 27.25417396426201
  - 27.15706081688404
  - 27.120480254292488
  - 27.597967728972435
  - 27.442158043384552
  - 26.7912325412035
  - 27.455149307847023
  - 27.114281937479973
  - 27.047482430934906
  - 26.78089927136898
  - 27.109633177518845
  - 27.306900933384895
  - 26.658342599868774
  - 27.096031546592712
  - 26.81703470647335
  - 27.117162749171257
  - 26.825316950678825
  - 27.761438697576523
  - 27.27303133904934
  - 27.323192462325096
  - 27.121418610215187
  - 26.94856159389019
  - 27.560258477926254
  - 27.003799363970757
  - 26.995757021009922
  - 27.543547615408897
  - 26.94951395690441
  - 26.673801228404045
  - 27.13376535475254
  - 26.90565139055252
  - 27.273891389369965
  - 27.13722924888134
  - 26.997496500611305
  - 26.998205289244652
  - 26.620235189795494
  - 27.066986680030823
  - 27.50117579102516
  - 26.564124792814255
  - 27.131809026002884
  - 26.79255099594593
  - 27.558156475424767
  - 26.896454364061356
  - 27.20731344819069
  - 26.912733644247055
  - 26.844329565763474
  - 27.23805420100689
  - 26.990727841854095
  - 26.754187047481537
  - 27.01624384522438
  - 26.67966964840889
  - 27.138900741934776
  - 27.158798441290855
  - 26.833137288689613
  - 26.884630501270294
  - 26.951077029109
  - 27.005554258823395
  - 26.851981163024902
  validation_losses:
  - 0.9190077185630798
  - 0.8769525289535522
  - 0.5630409717559814
  - 0.8323420882225037
  - 0.8758094906806946
  - 0.7399798035621643
  - 0.8704278469085693
  - 1.1188875436782837
  - 0.6441636681556702
  - 0.6229003071784973
  - 1.0456429719924927
  - 6.454068183898926
  - 0.9799126386642456
  - 0.6369740962982178
  - 0.5385254621505737
  - 0.5949258208274841
  - 0.6382014155387878
  - 0.5330193042755127
  - 0.6083501577377319
  - 0.5410383343696594
  - 0.6963070631027222
  - 0.6821138262748718
  - 0.6695874333381653
  - 0.5463139414787292
  - 0.5309739708900452
  - 0.7626879811286926
  - 0.5584726333618164
  - 0.6794415712356567
  - 0.7419312000274658
  - 0.5592057108879089
  - 0.5854274034500122
  - 0.7554817795753479
  - 0.9841940402984619
  - 0.5597490072250366
  - 0.8865645527839661
  - 0.9976577758789062
  - 0.9108685851097107
  - 0.6104742884635925
  - 0.730083703994751
  - 0.5889864563941956
  - 0.8577485084533691
  - 1.036087989807129
  - 0.9072752594947815
  - 0.9911739230155945
  - 1.295872449874878
  - 0.7692519426345825
  - 0.7646419405937195
  - 0.8111151456832886
  - 1.0995794534683228
  - 0.41418173909187317
  - 0.9353331327438354
  - 0.7692384719848633
  - 0.9597912430763245
  - 0.7957181930541992
  - 1.259536623954773
  - 1.010109305381775
  - 0.9395095705986023
  - 1.4416993856430054
  - 0.5040892958641052
  - 0.9839403629302979
  - 1.0774426460266113
  - 0.6535525321960449
  - 0.5315009951591492
  - 1.072265386581421
  - 0.5717757344245911
  - 0.5703469514846802
  - 0.4965278208255768
  - 0.7335482835769653
  - 0.811125636100769
  - 0.6672245860099792
  - 0.6076130867004395
  - 0.7462477684020996
  - 0.5628709197044373
  - 0.9522326588630676
  - 0.6279351711273193
  - 1.1314055919647217
  - 0.9850906729698181
  - 1.0763436555862427
  - 0.7519479990005493
  - 0.8580103516578674
  - 1.0125339031219482
  - 1.0948727130889893
  - 0.8660497069358826
  - 1.0154691934585571
  - 0.4822438061237335
  - 0.9716665148735046
  - 0.789979100227356
  - 0.7379091382026672
  - 1.010101556777954
  - 1.1125649213790894
  - 0.9058502316474915
  - 1.263998031616211
  - 0.7930883169174194
  - 0.8444149494171143
  - 1.072190284729004
  - 0.7813572287559509
  - 0.8731728196144104
  - 0.8139700293540955
  - 0.7254051566123962
  - 1.0304887294769287
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 56 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8370497427101201, 0.8507718696397941, 0.8284734133790738,
    0.8608247422680413]'
  fold_eval_f1: '[0.0, 0.14414414414414417, 0.2809917355371901, 0.20634920634920637,
    0.024096385542168676]'
  mean_eval_accuracy: 0.8469505402203321
  mean_f1_accuracy: 0.13111629431454186
  total_train_time: '0:16:50.469799'
