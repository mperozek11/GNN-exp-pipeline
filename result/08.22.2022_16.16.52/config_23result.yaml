config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:36:49.165029'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_23fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_23fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_23fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_23fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_23fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_23fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_23fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_23fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_23fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_23fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.1223216950893402
  - 0.8855142772197724
  - 0.8784152269363403
  - 0.862173342704773
  - 0.8665967166423798
  - 0.8263083040714264
  - 0.83008731007576
  - 0.8708089172840119
  - 0.857342278957367
  - 0.8044327855110169
  - 0.8036010563373566
  - 0.803931999206543
  - 0.8648669838905335
  - 0.8795114159584045
  - 0.8630720794200898
  - 0.8274778127670288
  - 0.7951775550842286
  - 0.8083709537982942
  - 0.8135285437107087
  - 0.7938127160072327
  - 0.846590805053711
  validation_losses:
  - 0.4054657816886902
  - 0.4222537577152252
  - 0.4003743529319763
  - 0.4145292639732361
  - 0.4114339053630829
  - 0.405269593000412
  - 0.38579046726226807
  - 0.38672152161598206
  - 0.390604704618454
  - 0.3931274116039276
  - 0.408209890127182
  - 0.39783212542533875
  - 0.39004674553871155
  - 0.39422157406806946
  - 0.41112151741981506
  - 0.39175888895988464
  - 0.3879467844963074
  - 0.39178264141082764
  - 0.387507826089859
  - 0.39629626274108887
  - 0.40047958493232727
loss_records_fold1:
  train_losses:
  - 0.846669727563858
  - 0.806462973356247
  - 0.8096033215522767
  - 0.8616456031799317
  - 0.8251757502555848
  - 0.809061586856842
  - 0.7738698661327362
  - 0.818638962507248
  - 0.8056068241596223
  - 0.8136196970939636
  - 0.7963352680206299
  - 0.8153035700321198
  - 0.82484050989151
  - 0.8090708613395692
  - 0.8013362705707551
  - 0.8115638256072999
  - 0.7786532044410706
  - 0.7823718667030335
  - 0.8217454493045807
  - 0.8295992672443391
  - 0.7759919583797456
  - 0.8268077433109284
  - 0.858320987224579
  - 0.7996598780155182
  - 0.7786928206682205
  - 0.8355938613414765
  - 0.7867145478725434
  - 0.775454917550087
  - 0.8152992069721222
  - 0.8026351988315583
  - 0.8246528863906861
  - 0.8085500121116639
  - 0.8491163074970246
  - 0.8661066293716431
  - 0.8379333317279816
  - 0.7953179061412812
  - 0.7687134295701981
  validation_losses:
  - 0.3965054750442505
  - 0.39854517579078674
  - 0.3973272442817688
  - 0.39873450994491577
  - 0.41290587186813354
  - 0.40321487188339233
  - 0.3931572735309601
  - 0.40335583686828613
  - 0.38758009672164917
  - 0.3914810121059418
  - 0.3950773775577545
  - 0.39379769563674927
  - 0.4192400276660919
  - 0.40532001852989197
  - 0.3975376486778259
  - 0.39815616607666016
  - 0.3890528678894043
  - 0.4122616648674011
  - 0.4772499203681946
  - 0.4042361080646515
  - 0.3911190927028656
  - 0.40150561928749084
  - 0.3964994251728058
  - 0.4012433588504791
  - 0.4239360988140106
  - 0.4829673171043396
  - 0.5639835000038147
  - 0.5230504870414734
  - 0.41697368025779724
  - 0.4652785062789917
  - 1.352044939994812
  - 0.6247900128364563
  - 0.40540269017219543
  - 0.4018949568271637
  - 0.40025779604911804
  - 0.40569207072257996
  - 0.40387117862701416
loss_records_fold2:
  train_losses:
  - 0.8522516667842865
  - 0.8167736411094666
  - 0.8284197568893433
  - 0.9480584263801575
  - 0.8213373541831971
  - 0.7925468683242798
  - 0.7935399949550629
  - 0.7855835795402527
  - 0.815461164712906
  - 0.8200514376163484
  - 0.7794705450534821
  - 0.7884513437747955
  - 0.7968029737472535
  validation_losses:
  - 0.3850747346878052
  - 0.3886570632457733
  - 0.38753753900527954
  - 0.3888372480869293
  - 0.40104207396507263
  - 0.38299980759620667
  - 0.40068942308425903
  - 0.4025822877883911
  - 0.3904194235801697
  - 0.3931451737880707
  - 0.3913134038448334
  - 0.3976764976978302
  - 0.3924906253814697
loss_records_fold3:
  train_losses:
  - 0.8053568840026856
  - 0.8202049493789674
  - 0.79986811876297
  - 0.794724053144455
  - 0.797756838798523
  - 0.8167691767215729
  - 0.8084251284599304
  - 0.7987033903598786
  - 0.7959528923034669
  - 0.8123146295547485
  - 0.8026417791843414
  - 0.7677759230136871
  - 0.7927316248416901
  validation_losses:
  - 0.3887384235858917
  - 0.39564967155456543
  - 0.38868045806884766
  - 0.3771478533744812
  - 0.3773390054702759
  - 0.38566792011260986
  - 0.4113122522830963
  - 0.39174574613571167
  - 0.3901576101779938
  - 0.37830910086631775
  - 0.3768169581890106
  - 0.3812595307826996
  - 0.38516655564308167
loss_records_fold4:
  train_losses:
  - 0.8114895343780518
  - 0.800231283903122
  - 0.7918493449687958
  - 0.784413143992424
  - 0.8411814987659455
  - 0.8185881793498994
  - 0.8352237641811371
  - 0.8178360044956208
  - 0.7875446200370789
  - 0.7859542012214661
  - 0.7881284594535828
  validation_losses:
  - 0.37808820605278015
  - 0.37579604983329773
  - 0.3810063600540161
  - 0.38189607858657837
  - 0.39077433943748474
  - 0.3877100944519043
  - 0.3869297504425049
  - 0.37932687997817993
  - 0.37828823924064636
  - 0.3798973262310028
  - 0.3767479956150055
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 37 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582701160604291
  mean_f1_accuracy: 0.0
  total_train_time: '0:07:40.743983'
