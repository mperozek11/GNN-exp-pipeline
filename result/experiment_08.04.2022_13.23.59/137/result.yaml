config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 06:06:30.664560'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/137/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/137/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/137/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/137/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/137/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/137/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/137/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/137/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/137/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/137/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 32.75541810691357
  - 32.417362704873085
  - 33.26032403111458
  - 32.85397827625275
  - 32.277083441615105
  - 32.87027920782566
  - 32.10502928495407
  - 33.005882650613785
  - 33.359325632452965
  - 32.10264164209366
  - 32.02239805459976
  - 33.163475930690765
  - 32.62302961945534
  - 32.160291239619255
  - 32.07658889889717
  - 32.72060629725456
  - 32.42956870794296
  - 33.1365906894207
  - 32.699568167328835
  - 33.5264987051487
  - 32.02571141719818
  - 32.978558510541916
  - 32.4547535777092
  - 32.065000772476196
  - 32.98622281849384
  - 32.740468204021454
  - 32.605544954538345
  - 32.505290538072586
  - 32.14699672162533
  - 31.95928366482258
  - 33.37172889709473
  - 32.58286543190479
  - 32.67434813082218
  - 32.450100257992744
  - 32.52919453382492
  - 33.2011843174696
  - 33.484191715717316
  - 33.317082181572914
  - 33.57588800787926
  - 32.71501524746418
  - 34.09918540716171
  - 31.91964055597782
  - 32.63912472128868
  - 32.928571164608
  - 33.108202293515205
  - 32.49442119896412
  - 32.50880879163742
  - 33.54990039765835
  - 33.23683041334152
  - 32.28666791319847
  - 32.794600903987885
  - 32.61211575567722
  - 32.571294248104095
  - 32.82715682685375
  - 32.212081998586655
  - 32.35391665995121
  - 33.84020924568176
  - 33.11306042969227
  - 32.401185899972916
  - 32.23916383087635
  - 33.02274677157402
  - 32.11812208592892
  - 33.26236966252327
  - 32.571430921554565
  - 32.88015827536583
  - 32.55537527799606
  - 32.9312387406826
  - 32.7068310379982
  - 33.07159312069416
  - 32.43742026388645
  - 32.30351386964321
  - 33.721311151981354
  - 32.11584536731243
  - 32.85049080848694
  - 33.40639618039131
  - 33.045868307352066
  - 32.612197279930115
  - 33.55115173757076
  - 31.68299800157547
  - 32.564123928546906
  - 32.295120641589165
  - 32.058705896139145
  - 32.33278614282608
  - 33.085562229156494
  - 33.46108555793762
  - 32.416467636823654
  - 33.57589837908745
  - 33.818924717605114
  - 32.39325077831745
  - 33.24837026000023
  - 32.36758668720722
  - 32.52093717455864
  - 33.48375082015991
  - 32.437502443790436
  - 32.60615722835064
  - 32.06183481216431
  - 32.60670129954815
  - 32.49036160111427
  - 32.37633150070906
  - 32.6764627546072
  validation_losses:
  - 498145132544.0
  - 542235492352.0
  - 2115852042240.0
  - 582713999360.0
  - 556683493376.0
  - 486764216320.0
  - 561152851968.0
  - 7180561416192.0
  - 411405385728.0
  - 2188862423040.0
  - 714971152384.0
  - 1506011250688.0
  - 2130197741568.0
  - 671669485568.0
  - 3686199721984.0
  - 866423930880.0
  - 3450293190656.0
  - 1144416632832.0
  - 477238198272.0
  - 1310683693056.0
  - 508982886400.0
  - 505456394240.0
  - 694985359360.0
  - 1667913875456.0
  - 1382349275136.0
  - 0.4237299859523773
  - 1215707873280.0
  - 786701615104.0
  - 772440064000.0
  - 1208293130240.0
  - 553550675968.0
  - 1921306853376.0
  - 798184439808.0
  - 350047600640.0
  - 1273872121856.0
  - 1941585920000.0
  - 2458923565056.0
  - 167969751040.0
  - 1322603642880.0
  - 592667082752.0
  - 2299016773632.0
  - 1521888657408.0
  - 2120915484672.0
  - 585637494784.0
  - 472102502400.0
  - 2094135508992.0
  - 578411167744.0
  - 1968540876800.0
  - 205544505344.0
  - 575176507392.0
  - 591894085632.0
  - 629759672320.0
  - 521522118656.0
  - 983855792128.0
  - 904486846464.0
  - 1155615424512.0
  - 710868795392.0
  - 1875864584192.0
  - 792801116160.0
  - 599452024832.0
  - 2399919931392.0
  - 0.46704939007759094
  - 1811980222464.0
  - 2220685000704.0
  - 539906375680.0
  - 894624268288.0
  - 153503989760.0
  - 654509015040.0
  - 3003516190720.0
  - 2065667981312.0
  - 649069002752.0
  - 1645389938688.0
  - 493858848768.0
  - 698011746304.0
  - 1034288300032.0
  - 131826573312.0
  - 974684553216.0
  - 890221232128.0
  - 774127878144.0
  - 912184901632.0
  - 1408230621184.0
  - 2670917582848.0
  - 1862694338560.0
  - 345008766976.0
  - 493858848768.0
  - 886814277632.0
  - 2437810749440.0
  - 758253420544.0
  - 1247847514112.0
  - 2138653196288.0
  - 1274727497728.0
  - 2143954010112.0
  - 539379957760.0
  - 2819865182208.0
  - 651862671360.0
  - 1092372660224.0
  - 2447145172992.0
  - 622152712192.0
  - 8772115234816.0
  - 2678335733760.0
loss_records_fold2:
  train_losses:
  - 32.54014268517494
  - 32.847527116537094
  - 32.86218237876892
  - 32.86402799189091
  - 33.180736884474754
  - 31.75265197455883
  - 32.21196800470352
  - 32.50661842525005
  - 32.79749707877636
  - 33.05323155224323
  - 33.99912701547146
  - 32.06202892959118
  - 33.393857434391975
  - 32.35403373837471
  - 32.928811714053154
  - 33.04794433712959
  - 32.776355147361755
  - 32.89925657212734
  - 32.63467079401016
  - 32.26638261973858
  - 32.154588639736176
  - 31.676988199353218
  - 33.97951930761337
  - 32.19570158421993
  - 32.52765196561813
  - 32.25656746327877
  - 32.29459473490715
  - 32.12902456521988
  - 32.674107208848
  - 33.159555450081825
  - 32.93501436710358
  - 32.34645891189575
  - 33.19143387675285
  - 32.644568383693695
  - 31.94368778169155
  - 32.77136594057083
  - 32.67484763264656
  - 32.7219555079937
  - 33.02345013618469
  - 32.919862776994705
  - 32.76699721813202
  - 33.289454728364944
  - 32.278894037008286
  - 33.191804975271225
  - 32.83691307902336
  - 32.71200630068779
  - 33.657591819763184
  - 33.6881363093853
  - 32.5999898314476
  - 33.07403400540352
  - 31.523012831807137
  - 32.094258069992065
  - 33.03983260691166
  - 32.661720022559166
  - 32.9198223054409
  - 31.376231402158737
  - 33.702932342886925
  - 32.47537590563297
  - 32.53389358520508
  - 33.25346755981445
  - 33.18669791519642
  - 32.13766211271286
  - 33.15975612401962
  - 33.08842544257641
  - 32.25849437713623
  - 33.8696548640728
  - 33.01481828093529
  - 33.343990713357925
  - 33.616560488939285
  - 32.96631821990013
  - 32.28067873418331
  - 33.67297039926052
  - 32.61867259442806
  - 33.142570197582245
  - 32.42899492383003
  - 32.95907808840275
  - 32.34719605743885
  - 31.375175297260284
  - 33.036324352025986
  - 32.812751203775406
  - 32.0805886387825
  - 32.567601695656776
  - 33.35068581998348
  - 32.814078733325005
  - 32.6410314142704
  - 33.05751574039459
  - 33.62392392754555
  - 33.31583322584629
  - 32.224806129932404
  - 32.61498583853245
  - 32.92113019526005
  - 32.11644521355629
  - 32.82099363207817
  - 32.62737254798412
  - 32.466173231601715
  - 32.78747637569904
  - 31.72634708881378
  - 32.21973882615566
  - 33.39182820916176
  - 32.997354194521904
  validation_losses:
  - 1010480316416.0
  - 846471430144.0
  - 482379628544.0
  - 675163078656.0
  - 12865997111296.0
  - 312141348864.0
  - 759484645376.0
  - 1077086126080.0
  - 2779006369792.0
  - 2114188476416.0
  - 625619369984.0
  - 721650515968.0
  - 1801253421056.0
  - 653795655680.0
  - 1090106949632.0
  - 5680211165184.0
  - 1771188649984.0
  - 810320396288.0
  - 2692975427584.0
  - 659540738048.0
  - 5509206245376.0
  - 7484743876608.0
  - 801355005952.0
  - 1994475962368.0
  - 371105005568.0
  - 894706712576.0
  - 667078098944.0
  - 1007470510080.0
  - 1196341854208.0
  - 561947279360.0
  - 2218633199616.0
  - 688522133504.0
  - 760424300544.0
  - 988505505792.0
  - 446965874688.0
  - 3511009411072.0
  - 2290276630528.0
  - 2683181465600.0
  - 1183972196352.0
  - 830735319040.0
  - 756515667968.0
  - 620410961920.0
  - 1789119561728.0
  - 1311055937536.0
  - 344198676480.0
  - 1816440995840.0
  - 938035511296.0
  - 2022811369472.0
  - 493326073856.0
  - 651490033664.0
  - 3885554466816.0
  - 612241047552.0
  - 2477724270592.0
  - 1588762247168.0
  - 6450207784960.0
  - 526203092992.0
  - 660221526016.0
  - 387169452032.0
  - 1370189332480.0
  - 135924621312.0
  - 2165320581120.0
  - 715924766720.0
  - 587703058432.0
  - 3295637143552.0
  - 1439200182272.0
  - 1092982669312.0
  - 2424685723648.0
  - 4646766641152.0
  - 1354371956736.0
  - 644463984640.0
  - 2372356538368.0
  - 654314700800.0
  - 527958048768.0
  - 2323334561792.0
  - 2044986654720.0
  - 603743256576.0
  - 6246401835008.0
  - 803560620032.0
  - 2150150569984.0
  - 853565374464.0
  - 2633662726144.0
  - 653592297472.0
  - 572667461632.0
  - 6171922530304.0
  - 3141841453056.0
  - 928119390208.0
  - 10296360960000.0
  - 594205671424.0
  - 661902458880.0
  - 561640767488.0
  - 1959243939840.0
  - 1706653908992.0
  - 2944265355264.0
  - 9836456574976.0
  - 7674951368704.0
  - 828518891520.0
  - 2125269827584.0
  - 156156198912.0
  - 2222232174592.0
  - 2462421614592.0
loss_records_fold3:
  train_losses:
  - 32.90651072561741
  - 33.54186135530472
  - 32.60333728790283
  - 33.045096054673195
  - 32.07033048570156
  - 33.169801875948906
  - 33.586326122283936
  - 32.58756525814533
  - 32.475101947784424
  - 32.75224484503269
  - 33.29196371138096
  - 33.118649274110794
  - 32.96588955819607
  - 34.18801671266556
  - 32.24378554522991
  - 33.102133855223656
  - 32.589230090379715
  - 32.61617286503315
  - 33.27138291299343
  - 33.16714483499527
  - 32.505278170108795
  - 32.49781736731529
  - 32.680385172367096
  - 33.03449746966362
  - 32.01309543848038
  - 31.88085000216961
  - 32.28616036474705
  - 32.436821177601814
  - 31.92404356598854
  - 32.55154348909855
  - 33.547761365771294
  - 32.269000232219696
  - 32.19652642309666
  - 32.88339006900787
  - 33.02637226879597
  - 32.9561657756567
  - 33.262711107730865
  - 32.50073444843292
  - 33.020737171173096
  - 32.89367562532425
  - 33.30538918077946
  - 32.51775163412094
  - 32.74790108203888
  - 33.07012630999088
  - 33.334909081459045
  - 31.853496551513672
  - 33.31334686279297
  - 32.902017027139664
  - 32.400899827480316
  - 32.707372188568115
  - 32.58055481314659
  - 33.02822062373161
  - 33.15157850086689
  - 32.528596580028534
  - 31.535503834486008
  - 32.592048332095146
  - 32.573396027088165
  - 32.75150388479233
  - 32.97840538620949
  - 32.2361159324646
  - 33.70630468428135
  - 32.23054227232933
  - 32.1828538030386
  - 32.46955743432045
  - 32.604014694690704
  - 32.861096017062664
  - 33.34949770569801
  - 33.01262620091438
  - 33.91656610369682
  - 32.066464334726334
  - 32.37590768933296
  - 32.45689915120602
  - 32.85432067513466
  - 33.314004838466644
  - 33.03159199655056
  - 32.422278732061386
  - 32.35150629281998
  - 33.37343746423721
  - 32.807349756360054
  - 33.28755924105644
  - 33.79024478793144
  - 32.83466458320618
  - 32.996234729886055
  - 32.89477322995663
  - 33.06274130940437
  - 32.99014949798584
  - 33.595207422971725
  - 32.033484131097794
  - 32.96780543029308
  - 33.01477527618408
  - 32.194809675216675
  - 33.10422579944134
  - 32.337420001626015
  - 32.55651417374611
  - 32.93018542230129
  - 33.026612773537636
  - 32.561236172914505
  - 33.60450127720833
  - 32.714057847857475
  - 32.4147264957428
  validation_losses:
  - 3571376717824.0
  - 1274304528384.0
  - 1891239329792.0
  - 704678068224.0
  - 1975933599744.0
  - 1740795150336.0
  - 435511558144.0
  - 1444229939200.0
  - 2314318381056.0
  - 1377914847232.0
  - 86367698944.0
  - 3559886946304.0
  - 1089577156608.0
  - 1465786826752.0
  - 1327423946752.0
  - 1821714677760.0
  - 875131240448.0
  - 179891306496.0
  - 918923509760.0
  - 1602711846912.0
  - 888966742016.0
  - 6031592128512.0
  - 3784334639104.0
  - 357572771840.0
  - 2212198875136.0
  - 725197783040.0
  - 1189041143808.0
  - 2491639660544.0
  - 2180697423872.0
  - 774316228608.0
  - 827224031232.0
  - 3383163879424.0
  - 719901884416.0
  - 1164622954496.0
  - 8361238069248.0
  - 11733373550592.0
  - 3757044137984.0
  - 2739831832576.0
  - 2727700070400.0
  - 2850031140864.0
  - 2564716232704.0
  - 896845479936.0
  - 1170297061376.0
  - 1474435219456.0
  - 7690214440960.0
  - 927134908416.0
  - 894790795264.0
  - 1315033448448.0
  - 852350664704.0
  - 1804805996544.0
  - 1429466251264.0
  - 742919438336.0
  - 998362578944.0
  - 2215479083008.0
  - 905070772224.0
  - 8826072858624.0
  - 517607686144.0
  - 2421235908608.0
  - 3196775825408.0
  - 560246226944.0
  - 1191268319232.0
  - 6267854127104.0
  - 2355242205184.0
  - 863983697920.0
  - 7175033323520.0
  - 1685703098368.0
  - 1089141211136.0
  - 1010106761216.0
  - 980816429056.0
  - 1853198303232.0
  - 1736390213632.0
  - 3705603883008.0
  - 666103185408.0
  - 1415403143168.0
  - 917908160512.0
  - 1041122656256.0
  - 2365654040576.0
  - 4340194476032.0
  - 503024123904.0
  - 996251467776.0
  - 2457233260544.0
  - 1246073061376.0
  - 889495683072.0
  - 3713205796864.0
  - 2635064672256.0
  - 7052012290048.0
  - 2150881296384.0
  - 5078639443968.0
  - 1458376540160.0
  - 1223936049152.0
  - 2568292401152.0
  - 3547054735360.0
  - 829969793024.0
  - 5621788180480.0
  - 973360005120.0
  - 803413622784.0
  - 2600943484928.0
  - 754938281984.0
  - 17649421516800.0
  - 906959257600.0
loss_records_fold4:
  train_losses:
  - 33.65050607919693
  - 32.608724132180214
  - 32.9793296456337
  - 32.59932658076286
  - 32.31589850783348
  - 33.02505370974541
  - 32.4786659181118
  - 32.3892025500536
  - 31.979828365147114
  - 32.21568639576435
  - 32.91388863325119
  - 31.92452184855938
  - 32.423244431614876
  - 34.39288383722305
  - 32.148123517632484
  - 32.86294595897198
  - 32.581115171313286
  - 33.689132899045944
  - 32.728743836283684
  - 33.10758762061596
  - 32.22169825434685
  - 33.12619933485985
  - 32.3075415045023
  - 32.42106471955776
  - 33.782067731022835
  - 32.52306014299393
  - 33.492466285824776
  - 33.177416548132896
  - 33.147930815815926
  - 32.09553824365139
  - 33.41087183356285
  - 33.1321632117033
  - 33.18780218064785
  - 31.90440285205841
  - 32.90729360282421
  - 31.554188922047615
  - 32.941329807043076
  - 32.57420364022255
  - 32.66454695165157
  - 32.54294529557228
  - 33.42765589058399
  - 32.69659170508385
  - 32.029696241021156
  - 33.931254148483276
  - 32.038276106119156
  - 33.47165313363075
  - 32.35272994637489
  - 33.12287589907646
  - 32.80787879228592
  - 31.88890876621008
  - 32.80001123249531
  - 33.28845426440239
  - 33.525257885456085
  - 32.29641780257225
  - 31.84759183228016
  - 32.755970150232315
  - 32.81604936718941
  - 33.26618388295174
  - 32.74833919107914
  - 33.95996206998825
  - 33.08467102050781
  - 32.55930531024933
  - 32.37608627974987
  - 32.94201022386551
  - 33.199806213378906
  - 32.940596133470535
  - 33.19849890470505
  - 32.28046065568924
  - 32.565640449523926
  - 32.291297659277916
  - 33.34316158294678
  - 33.10467527806759
  - 33.46421980857849
  - 32.082807555794716
  - 33.40185487270355
  - 32.644910246133804
  - 32.39066784083843
  - 33.08166626095772
  - 33.13287341594696
  - 32.44619098305702
  - 32.01806539297104
  - 33.26073621213436
  - 32.40498612821102
  - 32.34490819275379
  - 33.013980448246
  - 32.80821692943573
  - 33.89405182003975
  - 32.71833348274231
  - 32.34136641025543
  - 32.16634799540043
  - 32.59458653628826
  - 32.719111412763596
  - 33.644100204110146
  - 33.24703657627106
  - 33.32248343527317
  - 33.111964486539364
  - 32.202650874853134
  - 32.1258339881897
  - 34.238684833049774
  - 32.85062754154205
  validation_losses:
  - 326254166016.0
  - 1776088907776.0
  - 464202399744.0
  - 129577082880.0
  - 1932078088192.0
  - 911592325120.0
  - 675299590144.0
  - 899751870464.0
  - 825579208704.0
  - 1410486239232.0
  - 5852983459840.0
  - 814046576640.0
  - 337557913600.0
  - 580563304448.0
  - 1138482216960.0
  - 1421874954240.0
  - 2038081650688.0
  - 1516516278272.0
  - 1214562959360.0
  - 4218471055360.0
  - 1520325492736.0
  - 1167685320704.0
  - 540745596928.0
  - 928486522880.0
  - 2399942213632.0
  - 1404032385024.0
  - 81116381184.0
  - 823204642816.0
  - 1108000505856.0
  - 2223021752320.0
  - 494942879744.0
  - 1388810862592.0
  - 3767482187776.0
  - 1834304536576.0
  - 608249511936.0
  - 212493647872.0
  - 485061263360.0
  - 2110858199040.0
  - 718996635648.0
  - 1865822633984.0
  - 368436740096.0
  - 948697563136.0
  - 467543359488.0
  - 2821541330944.0
  - 8369024270336.0
  - 624440639488.0
  - 2484887093248.0
  - 1694726225920.0
  - 514837643264.0
  - 0.40991172194480896
  - 1654266003456.0
  - 717133578240.0
  - 6110150918144.0
  - 1651109396480.0
  - 604462841856.0
  - 353681244160.0
  - 3063571283968.0
  - 832731545600.0
  - 563855753216.0
  - 1104412016640.0
  - 526558527488.0
  - 1476750737408.0
  - 640187891712.0
  - 539505623040.0
  - 4951242178560.0
  - 2106828128256.0
  - 354366750720.0
  - 995361685504.0
  - 2183033913344.0
  - 647317094400.0
  - 553069182976.0
  - 4020838596608.0
  - 1333355347968.0
  - 1111636180992.0
  - 882255724544.0
  - 1491686916096.0
  - 5156449550336.0
  - 1251656728576.0
  - 530250563584.0
  - 345927319552.0
  - 578587197440.0
  - 2619685208064.0
  - 2398696505344.0
  - 693303312384.0
  - 342419996672.0
  - 2732614483968.0
  - 3035590295552.0
  - 1714595299328.0
  - 1395056443392.0
  - 1228874973184.0
  - 2098249728000.0
  - 1067007410176.0
  - 4249500778496.0
  - 3256628019200.0
  - 1213108715520.0
  - 480881278976.0
  - 1509705515008.0
  - 431702835200.0
  - 1544896118784.0
  - 2399716769792.0
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 98 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:21:34.854186'
