config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:56:36.174742'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_77fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_77fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_77fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_77fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_77fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_77fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_77fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_77fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_77fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_77fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 34.82690359950066
  - 12.717933297157288
  - 9.708107620477676
  - 18.086707013845444
  - 19.597718060016632
  - 7.6193320751190186
  - 13.197491335868836
  - 10.233124732971191
  - 11.55056780576706
  - 14.252124631404877
  - 7.9598124563694
  - 8.365591830015182
  - 8.637776505947114
  - 5.096182107925415
  - 6.508664447069169
  - 14.744472765922547
  - 6.543084448575974
  - 5.611119636893273
  - 3.859791034460068
  - 3.8534931361675264
  - 6.310847699642181
  - 3.6137289464473725
  - 3.5238618135452273
  - 3.887156176567078
  - 5.289309874176979
  - 3.50876944065094
  - 5.347788405418396
  - 4.911946803331375
  - 4.831191277503968
  - 3.3486242771148684
  - 3.871146684885025
  - 3.712345850467682
  - 4.9667231202125555
  - 4.263711708784103
  - 4.121174305677414
  - 3.319823104143143
  - 4.6414251863956455
  - 3.3048612475395203
  - 3.472774103283882
  - 4.4873719900846485
  - 4.6113510727882385
  - 3.4980505019426347
  - 3.2265014052391052
  - 3.753152072429657
  - 3.425920003652573
  - 3.5268494784832
  - 3.3195231497287754
  - 3.99744813144207
  - 3.381593954563141
  - 3.722883659601212
  - 3.511235880851746
  - 4.006470599770546
  - 3.8765674561262133
  - 3.194668221473694
  - 3.2666617274284366
  - 3.242502582073212
  - 3.2246907472610475
  - 3.787093847990036
  - 3.4570003688335422
  - 3.288329464197159
  - 3.2606447339057922
  - 3.5382244229316715
  - 3.6536439776420595
  - 3.440131032466889
  - 3.3655295014381412
  - 3.370357179641724
  - 3.238210189342499
  - 3.2221279680728916
  - 3.1825607419013977
  - 3.313962095975876
  - 3.3494254380464556
  - 3.250781863927841
  - 3.1549958825111393
  - 3.2375615358352663
  - 3.4857687383890155
  - 3.4588462650775913
  - 3.557126295566559
  - 3.1381985723972323
  - 3.128157612681389
  - 3.400713968276978
  - 3.3801538884639744
  - 3.12348964214325
  - 3.2891540586948396
  - 3.318733221292496
  - 3.9118122428655626
  - 3.5188266545534135
  - 3.392008012533188
  - 3.4314527571201325
  - 3.2840823709964755
  - 3.207604718208313
  - 3.521610671281815
  - 3.6536367475986484
  - 3.2627693295478823
  - 3.4482619881629946
  - 3.218035423755646
  - 3.293059873580933
  - 3.2905600488185884
  - 3.1991494208574296
  - 3.3706776022911074
  - 3.2776730209589005
  validation_losses:
  - 2.099651575088501
  - 1.3710981607437134
  - 0.5322477221488953
  - 0.9818517565727234
  - 0.6775866150856018
  - 0.5969468951225281
  - 0.5759642124176025
  - 0.49893468618392944
  - 0.7313270568847656
  - 0.9710903167724609
  - 0.43971869349479675
  - 0.4271925091743469
  - 0.4787392020225525
  - 0.4019589126110077
  - 0.4215388000011444
  - 0.4216616451740265
  - 0.40572085976600647
  - 0.40531793236732483
  - 0.4092128872871399
  - 0.44424936175346375
  - 0.41193559765815735
  - 0.4318505823612213
  - 0.5390795469284058
  - 0.3942922353744507
  - 0.3928220272064209
  - 0.4197971522808075
  - 0.41486844420433044
  - 0.4253886640071869
  - 0.44173121452331543
  - 0.554521381855011
  - 0.3967995047569275
  - 0.39319995045661926
  - 0.40849488973617554
  - 0.42865845561027527
  - 0.4040495753288269
  - 0.4070679545402527
  - 0.44190144538879395
  - 0.406087726354599
  - 0.3999088704586029
  - 0.4090321958065033
  - 0.4452911615371704
  - 0.39890798926353455
  - 0.39987713098526
  - 0.41229528188705444
  - 0.4123459458351135
  - 0.3929421007633209
  - 0.40504470467567444
  - 0.4339502155780792
  - 0.4009615480899811
  - 0.4614643156528473
  - 0.4026232957839966
  - 0.4090110659599304
  - 0.4240700602531433
  - 0.4372784197330475
  - 0.42597702145576477
  - 0.40296855568885803
  - 0.39697176218032837
  - 0.4084506928920746
  - 0.4190302789211273
  - 0.39685025811195374
  - 0.4057602286338806
  - 0.40635713934898376
  - 0.45171868801116943
  - 0.42812368273735046
  - 0.42092955112457275
  - 0.3990062177181244
  - 0.4035462439060211
  - 0.42301449179649353
  - 0.4015607237815857
  - 0.40365302562713623
  - 0.40764370560646057
  - 0.4026278555393219
  - 0.41975924372673035
  - 0.416181743144989
  - 0.40020516514778137
  - 0.442045658826828
  - 0.4255653917789459
  - 0.4058248996734619
  - 0.43317851424217224
  - 0.42094194889068604
  - 0.4097042679786682
  - 0.4001959264278412
  - 0.39326611161231995
  - 9.286273002624512
  - 0.5624083280563354
  - 0.4608277976512909
  - 0.3972392976284027
  - 0.48908907175064087
  - 0.41313934326171875
  - 0.40439364314079285
  - 0.505083441734314
  - 0.40911179780960083
  - 0.40548327565193176
  - 0.4231524169445038
  - 0.41510602831840515
  - 0.3995509445667267
  - 0.4130288362503052
  - 0.42368555068969727
  - 0.4613315165042877
  - 0.4218648076057434
loss_records_fold1:
  train_losses:
  - 3.8859360814094543
  - 3.271110224723816
  - 3.2207849830389024
  - 3.379215434193611
  - 3.3144290626049044
  - 3.275587791204453
  - 3.7141109645366672
  - 3.217318806052208
  - 5.016453313827515
  - 3.7749780595302584
  - 3.461457091569901
  - 3.2671862602233888
  - 3.2723511159420013
  - 3.202076143026352
  - 3.403228348493576
  - 3.2979546755552294
  - 3.2630527019500732
  - 3.381155288219452
  - 3.1987982898950578
  - 3.1153153479099274
  - 3.1951590657234195
  - 3.208138489723206
  - 3.0973902583122257
  - 3.3163741528987885
  - 3.170042419433594
  - 3.2097257137298585
  - 3.1328264236450196
  - 3.1763388931751253
  - 3.1788437545299533
  - 3.2280591905117038
  - 3.1635267376899723
  - 3.1926177084445957
  - 3.1969019293785097
  - 3.1213478446006775
  - 3.1848340332508087
  - 3.190786135196686
  - 3.3234613060951235
  - 3.752519422769547
  - 3.3744127690792087
  - 3.14928335249424
  - 3.1910047709941867
  - 3.269879245758057
  - 3.295132702589035
  - 3.2297577172517777
  - 3.2322558939456942
  - 3.2838292121887207
  - 3.2849154353141787
  - 3.0584644436836244
  - 3.193494912981987
  - 3.1975366473197937
  - 3.194199323654175
  - 3.207278156280518
  - 3.1758101999759676
  - 3.208691132068634
  - 3.1767309546470646
  - 3.111237144470215
  - 3.118211615085602
  - 3.305073189735413
  - 3.1617828547954563
  - 3.364054834842682
  - 3.122778254747391
  - 3.0869700849056247
  - 3.2041014790534974
  - 3.2426291286945346
  - 3.094116544723511
  - 3.2429846823215485
  - 3.372381490468979
  - 3.2000180780887604
  - 3.110269558429718
  - 3.2512896895408634
  - 3.4174397826194767
  - 3.216313296556473
  - 3.205255824327469
  - 3.1782163918018345
  - 3.110920423269272
  - 3.223604154586792
  - 3.2282704889774325
  - 3.3783490657806396
  - 3.3871319353580476
  - 3.1423379242420197
  - 3.2106041312217712
  - 3.1661347568035128
  - 3.292920655012131
  - 3.2563846141099932
  - 3.179485833644867
  - 3.4019240498542787
  - 3.287258851528168
  - 3.2543835759162905
  - 3.317862200737
  - 3.1678170025348664
  - 3.119053575396538
  - 3.250681179761887
  - 3.2418420910835266
  - 3.2051917493343356
  - 3.2080163896083835
  - 3.2891366481781006
  - 3.3663525581359863
  - 3.2823882877826693
  - 3.138109838962555
  - 3.1327203810214996
  validation_losses:
  - 0.421496719121933
  - 0.4206414222717285
  - 0.4244754910469055
  - 0.4203956425189972
  - 0.42498090863227844
  - 0.4164677560329437
  - 0.4141615331172943
  - 0.42850762605667114
  - 0.6773838400840759
  - 0.44779548048973083
  - 0.4340716600418091
  - 0.47796082496643066
  - 0.4478708505630493
  - 0.4812302887439728
  - 0.42261308431625366
  - 0.434169203042984
  - 0.4234950840473175
  - 0.41363027691841125
  - 0.4159657061100006
  - 0.4114667475223541
  - 0.4272989332675934
  - 0.42003315687179565
  - 0.4309336543083191
  - 0.5094140768051147
  - 0.43130093812942505
  - 0.4094001352787018
  - 0.41167232394218445
  - 0.41987332701683044
  - 0.4691856801509857
  - 0.45848900079727173
  - 0.4316100776195526
  - 0.4289325773715973
  - 0.4195704758167267
  - 0.4539843797683716
  - 0.411578506231308
  - 0.41734549403190613
  - 0.45117393136024475
  - 0.5952580571174622
  - 0.45970261096954346
  - 0.4211777150630951
  - 0.40296095609664917
  - 0.4484642446041107
  - 0.4255443513393402
  - 0.5060153007507324
  - 0.4438454210758209
  - 0.42082029581069946
  - 0.45421406626701355
  - 0.4207129180431366
  - 0.44277697801589966
  - 0.42717471718788147
  - 0.40958285331726074
  - 0.4118877649307251
  - 0.4482329785823822
  - 0.42424851655960083
  - 0.4263855814933777
  - 0.4295113682746887
  - 0.41460371017456055
  - 0.42610111832618713
  - 0.5365075469017029
  - 9.801680564880371
  - 7.019593715667725
  - 0.45745834708213806
  - 0.6751901507377625
  - 0.41655877232551575
  - 0.4118383526802063
  - 27.583248138427734
  - 22.188093185424805
  - 0.41945338249206543
  - 0.4321255087852478
  - 0.4705341160297394
  - 2.453795909881592
  - 0.4265368580818176
  - 0.4355456829071045
  - 0.4464980363845825
  - 0.41312092542648315
  - 0.4218786656856537
  - 0.47242113947868347
  - 0.40806853771209717
  - 0.4594755470752716
  - 0.4512541890144348
  - 0.4318113923072815
  - 0.4515218138694763
  - 0.43610915541648865
  - 0.41790905594825745
  - 0.4896046817302704
  - 9.733969688415527
  - 1.420791745185852
  - 0.46665719151496887
  - 0.46088945865631104
  - 0.4237672984600067
  - 0.4111802875995636
  - 0.4285551607608795
  - 1.795783281326294
  - 0.4232708513736725
  - 27.227460861206055
  - 2.0282511711120605
  - 0.4485029876232147
  - 2505.270263671875
  - 29438.474609375
  - 45999.53515625
loss_records_fold2:
  train_losses:
  - 3.19885967373848
  - 3.391591775417328
  - 3.2816951274871826
  - 3.194889795780182
  - 3.225608152151108
  - 3.158083760738373
  - 3.260440742969513
  - 3.2900334477424624
  - 3.4098884701728824
  - 3.1687078058719638
  - 3.286850076913834
  - 3.3372538298368455
  - 3.3364953339099888
  - 3.218661516904831
  - 3.222383522987366
  - 3.262723845243454
  - 3.210800135135651
  - 3.240325999259949
  - 3.1965747952461245
  - 3.1755725860595705
  - 3.396402394771576
  - 3.8495401203632356
  - 3.5996636688709263
  - 3.6047148764133454
  - 3.4515987873077396
  - 3.2411309242248536
  - 3.639221388101578
  - 3.397735369205475
  - 3.2391095936298373
  - 3.331556648015976
  - 3.336745256185532
  - 3.262777614593506
  - 3.647866851091385
  - 3.218645316362381
  - 3.15010125041008
  - 3.999766939878464
  - 3.80746483206749
  - 3.5012562036514283
  - 3.546103256940842
  - 3.289641982316971
  - 3.444850826263428
  - 3.2952671110630036
  - 3.236689674854279
  - 3.391562432050705
  - 3.406770020723343
  - 3.2143075942993167
  - 3.284108424186707
  - 3.35189208984375
  - 3.4972424626350405
  - 3.2968753367662433
  - 3.2521351873874664
  - 3.3695642888545994
  - 3.303193300962448
  - 3.22042407989502
  - 3.2053283751010895
  - 3.3325840651988985
  - 3.2064836025238037
  - 3.2819616854190827
  - 3.217730993032456
  - 3.3801276952028276
  - 3.2756100177764895
  - 3.1961779952049256
  - 3.3045572698116303
  - 3.309860956668854
  - 3.2829023182392123
  - 3.318845877051354
  - 3.437243288755417
  - 3.2154168784618378
  - 3.118720203638077
  validation_losses:
  - 10031.0361328125
  - 796.8284912109375
  - 1686.099365234375
  - 19875.787109375
  - 745.90234375
  - 6311.341796875
  - 20510.365234375
  - 14900.875
  - 54345.8125
  - 3979.350830078125
  - 1703.5631103515625
  - 5039.27734375
  - 1134.625732421875
  - 10508.8828125
  - 40920.13671875
  - 17323.900390625
  - 38688.08203125
  - 9016.5166015625
  - 444.5180358886719
  - 14586.259765625
  - 0.6317770481109619
  - 23.319358825683594
  - 0.5284169912338257
  - 0.5405962467193604
  - 0.4082428812980652
  - 0.40589621663093567
  - 0.4309934079647064
  - 0.418514609336853
  - 0.421292781829834
  - 0.44099512696266174
  - 0.4743479788303375
  - 0.4192289412021637
  - 0.40535250306129456
  - 0.396855890750885
  - 0.415668785572052
  - 1.341772198677063
  - 0.3963964581489563
  - 0.4275486469268799
  - 0.3841593563556671
  - 0.4115431606769562
  - 0.4209369122982025
  - 0.42300426959991455
  - 0.3847768306732178
  - 0.4024204909801483
  - 0.39511093497276306
  - 0.38924068212509155
  - 0.3917681574821472
  - 0.3944506347179413
  - 0.4390317499637604
  - 0.43184757232666016
  - 0.4033162295818329
  - 0.395151823759079
  - 0.39996999502182007
  - 0.4335944652557373
  - 0.40213024616241455
  - 0.39884117245674133
  - 0.4021621644496918
  - 0.3952661454677582
  - 0.44875434041023254
  - 0.42421194911003113
  - 0.42806053161621094
  - 0.39563310146331787
  - 0.4448753297328949
  - 0.41082295775413513
  - 0.38825809955596924
  - 0.3899363875389099
  - 0.3893289566040039
  - 0.3938853442668915
  - 0.39505571126937866
loss_records_fold3:
  train_losses:
  - 3.1628134548664093
  - 3.214813339710236
  - 3.123066222667694
  - 3.2520141661167146
  - 3.2071702718734745
  - 3.2853159576654436
  - 3.259422916173935
  - 3.253543356060982
  - 3.3595492601394654
  - 3.20314616560936
  - 3.1119700849056247
  - 3.1274004161357882
  - 3.290082120895386
  - 3.2258773475885394
  - 3.1464070260524752
  - 3.244496238231659
  - 3.2893887519836427
  - 3.2535044044256214
  - 3.281854730844498
  - 3.193381351232529
  - 3.062023097276688
  - 3.1308175146579744
  validation_losses:
  - 0.4083205461502075
  - 0.4614676833152771
  - 0.43616434931755066
  - 0.4199334979057312
  - 0.4475055932998657
  - 0.4066387414932251
  - 0.41724875569343567
  - 0.41343751549720764
  - 0.4303937554359436
  - 0.4151516258716583
  - 0.4153498411178589
  - 0.42688503861427307
  - 0.4449082911014557
  - 0.4151962697505951
  - 0.4225119650363922
  - 0.47134190797805786
  - 0.4452749192714691
  - 0.44702255725860596
  - 0.4341081380844116
  - 0.4286898672580719
  - 0.4117192327976227
  - 0.4085789918899536
loss_records_fold4:
  train_losses:
  - 3.2185107231140138
  - 3.2276159644126894
  - 3.1983618974685672
  - 3.177609622478485
  - 3.108983552455902
  - 3.202404922246933
  - 3.2029056549072266
  - 3.2396003723144533
  - 3.1803767383098602
  - 3.327523881196976
  - 3.2908423066139223
  - 3.2298859477043154
  - 3.23219929933548
  - 3.156354981660843
  - 3.155240935087204
  - 3.2122328102588655
  - 3.402953749895096
  - 3.3400259554386142
  - 3.352506947517395
  - 3.211577528715134
  - 3.24993292093277
  - 3.290896874666214
  - 3.2845413923263553
  - 3.2286059856414795
  validation_losses:
  - 0.3988133668899536
  - 0.4219914674758911
  - 0.4079139530658722
  - 0.4075741171836853
  - 0.4063126742839813
  - 0.4000236392021179
  - 0.3989814221858978
  - 0.41051292419433594
  - 0.41521763801574707
  - 0.4055895209312439
  - 0.4063449501991272
  - 0.3985651731491089
  - 0.429538369178772
  - 0.4044986665248871
  - 0.40814998745918274
  - 0.4306883215904236
  - 0.4109046161174774
  - 0.4593740701675415
  - 0.410898894071579
  - 0.40418168902397156
  - 0.4025050103664398
  - 0.4056379795074463
  - 0.4088149964809418
  - 0.3992139399051666
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 69 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:28:44.290094'
