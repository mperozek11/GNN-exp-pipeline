config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 17:55:29.062756'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/44/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/44/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/44/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/44/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/44/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/44/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/44/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/44/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/44/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/44/fold_4_state_dict.pt
loss_records_fold3:
  train_losses:
  - 7.786031663417816
  - 7.850730270147324
  - 7.927321404218674
  - 8.203933119773865
  - 7.970148950815201
  - 7.891901433467865
  - 7.922915607690811
  - 7.930799663066864
  - 8.047068506479263
  - 7.840576142072678
  - 7.876161605119705
  - 7.707827299833298
  - 7.638706728816032
  - 7.815229624509811
  - 7.571867495775223
  - 7.672108203172684
  - 7.506454199552536
  - 7.827775716781616
  - 7.8262268006801605
  - 7.7459059953689575
  - 7.747094303369522
  - 7.947967201471329
  - 7.561170309782028
  - 7.727461010217667
  - 7.579336553812027
  - 7.614121809601784
  - 7.61457222700119
  - 7.839961886405945
  - 7.665779322385788
  - 7.708139032125473
  - 7.590554863214493
  - 7.6227914690971375
  - 7.5187153816223145
  - 7.455142572522163
  - 7.816405385732651
  - 7.812313973903656
  - 7.764417588710785
  - 8.073760271072388
  - 7.879417181015015
  - 7.796113550662994
  - 8.009850054979324
  - 8.070688843727112
  - 7.977078437805176
  - 7.5800208151340485
  - 7.727074980735779
  - 7.780373066663742
  - 7.592210203409195
  - 7.697140008211136
  - 7.843534499406815
  - 7.97783088684082
  - 7.686498790979385
  - 7.814023554325104
  - 7.936690419912338
  - 8.03542947769165
  - 7.77821472287178
  - 7.710727125406265
  - 7.7604504227638245
  - 8.085255086421967
  - 7.729050636291504
  - 7.771067410707474
  - 7.6761170625686646
  - 7.6760430335998535
  - 7.575109362602234
  - 7.5516417324543
  - 7.551539242267609
  - 7.706365913152695
  - 7.480282962322235
  - 7.893632024526596
  - 8.191887021064758
  - 7.63395793735981
  - 7.60893252491951
  - 7.635920017957687
  - 7.697514891624451
  - 7.687207400798798
  - 7.699965834617615
  - 7.653252646327019
  - 7.737407743930817
  - 7.76351010799408
  - 7.699299901723862
  - 7.74149477481842
  - 7.575587153434753
  - 7.667919218540192
  - 8.006772071123123
  - 7.8106637597084045
  - 7.429086208343506
  - 7.683914333581924
  - 7.86118671298027
  - 7.580850005149841
  - 7.627056986093521
  - 7.550201088190079
  - 7.634674042463303
  - 7.634991437196732
  - 7.5909508764743805
  - 7.651210814714432
  - 7.83175790309906
  - 7.529401570558548
  - 7.765817373991013
  - 7.493831366300583
  - 7.541203111410141
  - 7.813010603189468
  validation_losses:
  - 0.39027953147888184
  - 0.3808283805847168
  - 0.38606786727905273
  - 0.38962262868881226
  - 0.3785838484764099
  - 0.36931949853897095
  - 0.38826224207878113
  - 0.38072171807289124
  - 0.37758806347846985
  - 0.37037500739097595
  - 0.3978478014469147
  - 0.37800267338752747
  - 0.38590267300605774
  - 0.4141311049461365
  - 0.41087237000465393
  - 0.38018327951431274
  - 0.4439072906970978
  - 0.4050114154815674
  - 0.38177889585494995
  - 0.38306066393852234
  - 0.3819235861301422
  - 0.4143196940422058
  - 0.3813856840133667
  - 0.3944627046585083
  - 0.3868924379348755
  - 0.38796862959861755
  - 0.38957563042640686
  - 0.38387590646743774
  - 0.41943565011024475
  - 0.39264488220214844
  - 0.4005793035030365
  - 0.3824790418148041
  - 0.36999475955963135
  - 0.44834238290786743
  - 0.4242839217185974
  - 0.38495105504989624
  - 0.3752680718898773
  - 0.39281943440437317
  - 0.3808565139770508
  - 0.48165738582611084
  - 0.380656898021698
  - 0.4162394404411316
  - 0.38112953305244446
  - 0.3874574899673462
  - 0.38879162073135376
  - 0.39843127131462097
  - 0.38683825731277466
  - 0.3988741934299469
  - 0.45372268557548523
  - 0.4518011212348938
  - 0.3838173449039459
  - 0.3912382125854492
  - 0.4127176105976105
  - 0.3911135494709015
  - 0.37562859058380127
  - 0.7421818375587463
  - 0.38270440697669983
  - 0.38570436835289
  - 0.37325015664100647
  - 0.3732315003871918
  - 0.3966921865940094
  - 0.4955558478832245
  - 0.3945772349834442
  - 0.42863729596138
  - 0.37478962540626526
  - 0.3682202100753784
  - 0.38101229071617126
  - 0.4073033034801483
  - 0.3808242380619049
  - 0.3812251091003418
  - 0.38301587104797363
  - 0.37578797340393066
  - 0.39199620485305786
  - 0.5042868256568909
  - 0.3993600904941559
  - 0.5240572094917297
  - 0.49877670407295227
  - 0.8245096206665039
  - 0.39086639881134033
  - 1.1256308555603027
  - 0.9443644285202026
  - 1.3327807188034058
  - 1.0670918226242065
  - 0.49351537227630615
  - 0.4087972044944763
  - 3.714538097381592
  - 0.3820817172527313
  - 0.5089333653450012
  - 0.393674373626709
  - 0.40672558546066284
  - 0.44870802760124207
  - 0.4183620810508728
  - 0.4752140939235687
  - 0.5066712498664856
  - 0.5378159880638123
  - 0.5550715923309326
  - 0.6589287519454956
  - 0.5725023150444031
  - 0.7361542582511902
  - 0.4025545120239258
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8576329331046312, 0.855917667238422,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.023255813953488372, 0.0]'
  mean_eval_accuracy: 0.8572409565407038
  mean_f1_accuracy: 0.004651162790697674
  total_train_time: '0:02:27.954515'
