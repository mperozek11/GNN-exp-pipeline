config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:16:30.007447'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_121fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_121fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_121fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_121fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_121fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_121fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_121fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_121fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_121fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_121fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 47.17531967163086
  - 30.015690803527832
  - 17.38373151421547
  - 7.993189072608948
  - 9.28310830295086
  - 12.99296835064888
  - 10.912417441606522
  - 9.546219208836556
  - 15.819040942192078
  - 7.363142150640488
  - 7.913967269659043
  - 8.248318576812744
  - 14.681880712509155
  - 18.600483745336533
  - 14.570375567674638
  - 7.0418661236763
  - 14.548234248161316
  - 6.1903522491455085
  - 7.693056511878968
  - 6.73228509426117
  - 7.829655352234841
  - 11.940739709138871
  - 8.516514086723328
  - 4.263597798347473
  - 4.4597142279148105
  - 8.589836445450784
  - 5.650266760587693
  - 9.074665495753289
  - 5.298531067371369
  - 4.681861186027527
  - 5.835492253303528
  - 13.797292190790177
  - 5.383870467543602
  - 6.079206043481827
  - 6.320207414031029
  - 4.538813361525536
  - 7.537616702914239
  - 10.550157308578491
  - 4.318620976805687
  - 5.015750235319138
  - 4.123856455087662
  - 4.552007496356964
  - 6.58804415166378
  - 3.4939077109098435
  - 5.6691825747489935
  - 5.275550371408463
  - 4.313155525922776
  - 4.011542820930481
  - 4.168650245666504
  - 4.588987571001053
  - 4.143907582759858
  validation_losses:
  - 0.7675292491912842
  - 3.191560745239258
  - 0.6314206719398499
  - 3.069895029067993
  - 2.5057849884033203
  - 5.884241580963135
  - 0.6191794872283936
  - 1.2733010053634644
  - 0.5813054442405701
  - 0.49914583563804626
  - 1.5804795026779175
  - 0.7094477415084839
  - 0.6581465005874634
  - 0.7890676259994507
  - 0.5303138494491577
  - 0.6181173920631409
  - 1.1191779375076294
  - 0.4220564365386963
  - 0.40237897634506226
  - 0.42773500084877014
  - 0.4566708207130432
  - 0.4901736080646515
  - 0.7549957633018494
  - 0.40120136737823486
  - 0.39168214797973633
  - 0.4084978997707367
  - 0.39165109395980835
  - 0.39039888978004456
  - 0.49488818645477295
  - 0.4435681402683258
  - 0.5042951107025146
  - 0.407346248626709
  - 0.42372947931289673
  - 0.5911266803741455
  - 0.41575396060943604
  - 0.4388066232204437
  - 0.3982279300689697
  - 0.4001829922199249
  - 0.38374510407447815
  - 0.4275561571121216
  - 0.3844487965106964
  - 0.7871618866920471
  - 0.42886656522750854
  - 0.41678401827812195
  - 0.48394420742988586
  - 0.4209190607070923
  - 0.37719660997390747
  - 0.38433629274368286
  - 0.38832157850265503
  - 0.3921802043914795
  - 0.3779540956020355
loss_records_fold1:
  train_losses:
  - 4.053566992282867
  - 3.6004898071289064
  - 4.951071584224701
  - 3.2880019173026085
  - 3.7031969815492634
  - 6.5075611799955375
  - 3.6420090883970264
  - 3.5844826221466066
  - 3.6556657493114475
  - 3.729789969325066
  - 4.549100503325462
  - 3.192260390520096
  - 3.0941026687622073
  - 2.901112818717957
  - 2.9992945164442064
  - 3.034158790111542
  - 3.356687703728676
  - 2.8745183408260346
  - 3.410769283771515
  - 3.7269404709339145
  - 3.925689196586609
  - 2.8721054941415787
  - 2.932437899708748
  - 3.3302048563957216
  - 3.128869152069092
  - 3.085951915383339
  - 3.2079473942518235
  - 3.11982119679451
  - 3.595563864707947
  - 2.9884513676166535
  - 3.556301599740982
  - 2.983418488502503
  - 3.105000871419907
  - 3.160377684235573
  - 3.2257964819669724
  - 2.882851177453995
  - 3.0304607510566712
  - 2.983682644367218
  - 3.0976274728775026
  - 3.4539711803197863
  - 3.550696069002152
  - 3.151667740941048
  - 3.279512083530426
  - 3.1968849033117297
  - 2.9501541316509248
  - 2.941219460964203
  - 3.07062059044838
  - 2.895478713512421
  - 2.9178338646888733
  - 2.9226988136768344
  - 3.077099472284317
  - 3.318932390213013
  - 2.910616570711136
  - 3.038441035151482
  - 3.0389264404773715
  - 2.842999213933945
  - 3.018677291274071
  - 2.886155050992966
  - 2.81593034863472
  - 2.943473914265633
  - 3.1119439154863358
  - 2.8046751022338867
  - 2.930053949356079
  - 2.8618279308080674
  - 2.929143875837326
  - 2.90381093621254
  - 2.8304597377777103
  - 3.000798547267914
  - 2.939115956425667
  - 2.8906922340393066
  - 2.947027260065079
  - 2.803888440132141
  - 3.270800951123238
  - 2.9759196579456333
  - 2.8390829086303713
  - 2.9529572963714603
  - 2.9883378744125366
  - 2.888945329189301
  - 2.9208566874265673
  - 2.981084257364273
  - 2.855342310667038
  - 3.0132100075483326
  - 3.062831711769104
  - 3.1991619229316712
  - 3.0241429150104526
  - 3.227258762717247
  - 2.969472658634186
  - 2.9454840898513797
  - 2.9505275726318363
  - 2.904768624901772
  - 2.831168913841248
  - 2.9600272297859194
  - 2.765389943122864
  - 2.819641643762589
  - 3.0348645925521853
  - 2.90511417388916
  - 2.8813257932662966
  validation_losses:
  - 0.4154752790927887
  - 0.41716793179512024
  - 0.44210466742515564
  - 0.41638943552970886
  - 0.45808205008506775
  - 0.4373972713947296
  - 0.3969711661338806
  - 0.3991343379020691
  - 0.4067685008049011
  - 0.41798922419548035
  - 0.4185165464878082
  - 0.39712437987327576
  - 0.4131976068019867
  - 0.39876848459243774
  - 0.40936630964279175
  - 0.40166354179382324
  - 0.4028621017932892
  - 0.4199422001838684
  - 0.39219218492507935
  - 0.4266607165336609
  - 0.40773746371269226
  - 0.41307318210601807
  - 0.39991000294685364
  - 0.44677358865737915
  - 0.4109361171722412
  - 0.49561747908592224
  - 0.42579880356788635
  - 0.3953710198402405
  - 0.41708633303642273
  - 0.3980269134044647
  - 0.4084678292274475
  - 0.4372785985469818
  - 0.394843727350235
  - 0.47273698449134827
  - 0.5248656868934631
  - 0.42734813690185547
  - 0.3964860737323761
  - 0.4429781138896942
  - 0.40099671483039856
  - 0.39822837710380554
  - 0.41447991132736206
  - 0.4344605803489685
  - 0.41698718070983887
  - 0.5151028633117676
  - 0.4195081293582916
  - 0.4178798496723175
  - 0.40369337797164917
  - 0.4628838300704956
  - 0.40397191047668457
  - 0.5179246068000793
  - 0.5458499193191528
  - 0.39447689056396484
  - 0.4003264009952545
  - 0.4047699272632599
  - 0.4148353338241577
  - 0.4389232099056244
  - 0.4287431538105011
  - 0.404519259929657
  - 0.4193699359893799
  - 0.40135082602500916
  - 0.5258252620697021
  - 0.4014516770839691
  - 0.4009760320186615
  - 0.40203654766082764
  - 0.4037892818450928
  - 0.39974093437194824
  - 0.4200453758239746
  - 0.4380061626434326
  - 0.4380251169204712
  - 0.4723663330078125
  - 0.39765477180480957
  - 0.46198317408561707
  - 0.4305761456489563
  - 0.438467413187027
  - 0.40281757712364197
  - 0.43623828887939453
  - 0.4829249382019043
  - 0.438245564699173
  - 0.4100927412509918
  - 0.40621280670166016
  - 0.4023100435733795
  - 0.4511418342590332
  - 0.4018472731113434
  - 0.4581228792667389
  - 0.411632776260376
  - 0.42189815640449524
  - 0.40076664090156555
  - 0.40006592869758606
  - 0.4833815395832062
  - 0.3978475332260132
  - 0.4672846496105194
  - 0.40856775641441345
  - 0.41052570939064026
  - 0.4053107798099518
  - 0.4121001660823822
  - 0.40409964323043823
  - 0.39927494525909424
loss_records_fold2:
  train_losses:
  - 2.8664939582347873
  - 2.9764816105365757
  - 3.293954473733902
  - 3.5821994960308077
  - 3.1306891649961472
  - 2.9100089848041537
  - 2.959177476167679
  - 3.0024663627147676
  - 2.9044508010149004
  - 2.8633846223354342
  - 2.7815008878707888
  - 2.840726411342621
  - 2.951715350151062
  - 2.949425405263901
  - 3.0125613033771517
  - 2.904902219772339
  - 2.8595462501049043
  - 2.844892513751984
  - 2.8618848085403443
  validation_losses:
  - 0.5323893427848816
  - 0.3989029824733734
  - 0.45964717864990234
  - 0.3950897753238678
  - 0.38361555337905884
  - 0.3812168836593628
  - 0.3809735178947449
  - 0.37611567974090576
  - 0.42255666851997375
  - 0.40567734837532043
  - 0.37275564670562744
  - 0.3744438886642456
  - 0.4087338149547577
  - 0.3910420536994934
  - 0.3821827173233032
  - 0.3739987909793854
  - 0.3813992142677307
  - 0.3719153106212616
  - 0.3741602897644043
loss_records_fold3:
  train_losses:
  - 2.8527341812849047
  - 2.968273413181305
  - 2.8898179203271868
  - 2.9349187433719637
  - 2.8660400569438935
  - 2.8320165634155274
  - 2.833598366379738
  - 2.9482144564390182
  - 2.9610636085271835
  - 2.9514959812164308
  - 2.7998515367507935
  - 2.809888678789139
  - 2.909667631983757
  - 2.747621637582779
  - 2.801213586330414
  - 2.850237086415291
  - 2.8143511503934864
  - 2.8249014228582383
  validation_losses:
  - 0.40308254957199097
  - 0.4769267439842224
  - 0.3747539222240448
  - 0.3865841329097748
  - 0.3756064474582672
  - 0.3718952238559723
  - 0.37564510107040405
  - 0.37869176268577576
  - 0.3717327117919922
  - 0.7340191602706909
  - 0.3621616065502167
  - 0.38358426094055176
  - 0.3687779903411865
  - 0.3753412961959839
  - 0.3798803687095642
  - 0.372761070728302
  - 0.3802250921726227
  - 0.3873003125190735
loss_records_fold4:
  train_losses:
  - 2.825540685653687
  - 2.8940024435520173
  - 2.8576041728258135
  - 2.8868440598249436
  - 2.8637691676616672
  - 2.8913514733314516
  - 2.777893304824829
  - 2.808849209547043
  - 2.872178736329079
  - 4.738273394107819
  - 3.4756114661693576
  - 3.674722817540169
  - 3.482878836989403
  - 3.3959816783666614
  - 3.29139184653759
  - 3.254334929585457
  - 3.1750017493963245
  - 3.1136377930641177
  - 3.1824817627668383
  - 3.0962960898876193
  - 3.1662182003259662
  - 2.9528724253177643
  - 3.0266168445348742
  - 2.9378987133502963
  - 2.98752019405365
  validation_losses:
  - 0.38203179836273193
  - 0.37103354930877686
  - 0.3807677626609802
  - 0.4626830518245697
  - 0.3736630976200104
  - 0.36313191056251526
  - 0.37021198868751526
  - 0.3729730546474457
  - 0.3820206820964813
  - 0.42125511169433594
  - 0.3935147821903229
  - 0.42638781666755676
  - 0.41300222277641296
  - 0.42003828287124634
  - 0.4195878505706787
  - 0.39425691962242126
  - 0.4109528064727783
  - 0.41124778985977173
  - 0.5026881694793701
  - 0.41633427143096924
  - 0.3917922377586365
  - 0.3917868733406067
  - 0.3985030949115753
  - 0.38946786522865295
  - 0.39007824659347534
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 97 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:17:39.328593'
