config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:01:55.703659'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_30fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_30fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_30fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_30fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_30fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_30fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_30fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_30fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_30fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_30fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 40.73075065612793
  - 9.142730140686036
  - 5.182529532909394
  - 4.656124395132065
  - 3.1587388157844547
  - 3.766252535581589
  - 3.392590254545212
  - 4.7885782122612
  - 3.660685932636261
  - 2.5506617784500123
  - 2.486997050046921
  - 1.9985072433948519
  - 1.907515174150467
  - 8.036654448509216
  - 4.066436922550202
  - 3.8263325810432436
  - 4.644703817367554
  - 2.141399985551834
  - 2.2658114314079287
  - 2.828313183784485
  - 2.4472052395343784
  - 3.272208654880524
  - 1.959420156478882
  - 2.2845552980899813
  - 6.972342622280121
  - 4.311982142925262
  - 6.7029526174068454
  - 5.5843681037426
  - 2.981849777698517
  - 2.063714733719826
  - 2.6173410773277284
  - 2.9821525394916537
  - 2.2706962525844574
  - 1.9945495903491974
  - 2.057683926820755
  - 1.7333396017551423
  - 8.638239926099777
  - 5.789857530593872
  - 5.623824632167817
  - 6.786193007230759
  - 4.7189666628837585
  - 3.828851866722107
  - 10.497942328453064
  - 3.754728549718857
  - 3.7101195812225343
  - 2.053386089205742
  - 1.8889022648334504
  - 2.1106391072273256
  - 1.7428654670715333
  - 2.5888352364301683
  - 2.1050476491451264
  - 1.9821193218231201
  - 2.1511270672082903
  - 1.8086111903190614
  - 2.1985343515872957
  - 2.1220909297466277
  - 2.0346339881420135
  - 3.257953315973282
  - 1.9531173646450044
  - 2.090284872055054
  - 1.9230690956115724
  - 1.8843636572360993
  - 1.5783817231655122
  - 2.4300345897674562
  - 1.895614355802536
  - 3.2213536977767947
  - 2.3918715476989747
  - 2.20876282453537
  - 2.0596078217029574
  - 1.643221551179886
  - 1.615808689594269
  - 1.9799865186214447
  - 1.8194382190704346
  - 2.7783005058765413
  - 1.6763593912124635
  - 1.6575867354869844
  - 1.7248403519392015
  - 1.5906912326812745
  - 1.6003688275814056
  - 1.7228196680545809
  - 1.6049022257328034
  - 1.533388587832451
  - 1.734649193286896
  - 1.8038284242153169
  - 1.6794523298740387
  - 1.9998362720012666
  - 1.889464980363846
  - 1.6251620769500734
  - 1.8111563324928284
  - 1.6457127690315247
  - 1.5765205085277558
  - 1.9250353455543519
  - 1.7525122582912447
  - 1.9290744602680208
  - 2.074785488843918
  - 1.7508328676223757
  - 1.6063361406326295
  - 1.560005271434784
  - 1.8679230690002442
  - 1.8566380083560945
  validation_losses:
  - 5.2093329429626465
  - 2.0151355266571045
  - 1.7702962160110474
  - 0.6833916306495667
  - 2.4330432415008545
  - 0.4997323751449585
  - 1.191483974456787
  - 0.5526224970817566
  - 0.5356035232543945
  - 0.7093742489814758
  - 0.6825405955314636
  - 0.39220136404037476
  - 0.4243759214878082
  - 0.5289340019226074
  - 0.46284714341163635
  - 0.5379230380058289
  - 0.45426860451698303
  - 0.47166118025779724
  - 0.4349231421947479
  - 0.564441442489624
  - 0.48678386211395264
  - 0.41150352358818054
  - 0.5910183191299438
  - 0.4945089519023895
  - 0.3872739374637604
  - 0.4912091791629791
  - 0.7248764038085938
  - 0.5663956999778748
  - 0.4300459325313568
  - 0.5353379845619202
  - 0.441650927066803
  - 0.49795642495155334
  - 0.4832325279712677
  - 0.3981304466724396
  - 0.38855665922164917
  - 0.46616610884666443
  - 0.4778768718242645
  - 0.5223066210746765
  - 0.45554348826408386
  - 0.6188468933105469
  - 0.5028569102287292
  - 0.8079385161399841
  - 0.402234822511673
  - 0.9207046031951904
  - 0.3949596583843231
  - 0.4253123104572296
  - 0.3932725489139557
  - 0.39533087611198425
  - 0.44453591108322144
  - 0.41845446825027466
  - 0.4046589732170105
  - 0.45007702708244324
  - 0.4315437376499176
  - 0.40323787927627563
  - 0.5275743007659912
  - 0.4355604648590088
  - 0.4236994981765747
  - 0.43285438418388367
  - 0.403084397315979
  - 0.38724231719970703
  - 0.5753998160362244
  - 0.3901687264442444
  - 0.40036115050315857
  - 0.4906790256500244
  - 0.41137972474098206
  - 0.41074106097221375
  - 0.40597137808799744
  - 0.4604930877685547
  - 0.4410818815231323
  - 0.38814476132392883
  - 0.7358536124229431
  - 0.5738523602485657
  - 0.38241180777549744
  - 0.3788236081600189
  - 0.38911014795303345
  - 0.3932815194129944
  - 0.3947409987449646
  - 0.3866569697856903
  - 0.38799503445625305
  - 0.41499873995780945
  - 0.383328378200531
  - 0.38811957836151123
  - 0.43682318925857544
  - 0.3891414701938629
  - 0.4262518882751465
  - 0.39682626724243164
  - 0.4141998589038849
  - 0.43689483404159546
  - 0.40157753229141235
  - 0.3880574405193329
  - 0.3889274001121521
  - 0.42319512367248535
  - 0.39314353466033936
  - 0.41030603647232056
  - 0.5155409574508667
  - 0.410468727350235
  - 0.3801690936088562
  - 0.38266417384147644
  - 0.3949553668498993
  - 0.5819849967956543
loss_records_fold1:
  train_losses:
  - 2.0607576429843903
  - 1.694675087928772
  - 2.0299444794654846
  - 1.9312238454818726
  - 2.3131668984889986
  - 1.6518842458724976
  - 1.6922168612480164
  - 1.6564679503440858
  - 1.675837254524231
  - 1.5889540851116182
  - 1.5798092246055604
  - 1.6925521850585938
  - 2.0895074546337127
  - 1.7203224539756776
  - 2.0033698201179506
  - 1.8594987273216248
  - 1.6460510969161988
  - 2.107521373033524
  - 1.6596614718437195
  - 1.7738225281238558
  - 1.5452693045139314
  - 1.5853575587272646
  - 1.6249222338199616
  - 2.290674698352814
  - 1.9010082304477693
  - 1.7542799949645997
  - 1.6109337210655212
  - 1.6246307313442232
  - 2.6447952032089237
  - 2.046566069126129
  - 1.765634047985077
  - 2.081142008304596
  - 2.350270503759384
  - 1.8395407110452653
  - 1.614173260331154
  - 1.9543108344078064
  - 1.686356419324875
  - 1.7551767051219942
  - 1.6513256669044496
  - 2.148622560501099
  - 1.5938447058200838
  - 1.565275663137436
  - 1.568981581926346
  - 1.6082994878292085
  - 1.5641440093517305
  - 1.623372232913971
  - 1.598570775985718
  - 1.7813057363033296
  - 1.7826622128486633
  - 1.6599390447139741
  - 1.6912302732467652
  - 2.3815045058727264
  - 1.8548347473144533
  - 1.6428255140781403
  - 1.6025180339813234
  - 1.7816257059574128
  - 1.8396263301372529
  - 1.851980870962143
  - 1.695711535215378
  - 1.7295820474624635
  - 1.5370163679122926
  - 1.6947733938694
  - 1.6536507606506348
  - 1.607101172208786
  - 1.6343732237815858
  - 1.86243394613266
  - 1.6275857806205751
  - 1.9210990250110627
  - 1.7385946333408357
  - 1.616068321466446
  - 1.5801298916339874
  - 1.5997055590152742
  - 1.6514810144901277
  - 1.6405423641204835
  - 1.573364943265915
  - 1.4900054037570953
  - 1.5389214277267458
  - 1.5930063903331757
  - 1.599762135744095
  - 1.691027593612671
  - 1.6640908420085907
  - 1.6678719878196717
  - 1.6840338468551637
  - 1.6046301007270813
  - 1.5494543731212618
  - 1.5277017951011658
  - 1.5685575127601625
  - 1.5958994448184969
  - 1.582103204727173
  - 1.6271405160427095
  - 1.5003407627344132
  - 1.5496466219425202
  - 1.5737333953380586
  - 1.614268958568573
  - 1.519120353460312
  validation_losses:
  - 0.4124586284160614
  - 0.4696102738380432
  - 0.38918575644493103
  - 0.42088115215301514
  - 0.396932989358902
  - 0.42398780584335327
  - 0.40762025117874146
  - 0.4052913784980774
  - 0.42043977975845337
  - 0.4057861864566803
  - 0.40876662731170654
  - 0.4695144593715668
  - 0.4141712486743927
  - 0.6312090754508972
  - 0.43310824036598206
  - 0.40951380133628845
  - 0.4011342227458954
  - 0.4164985716342926
  - 0.42533430457115173
  - 0.43822377920150757
  - 0.4096217453479767
  - 0.394124835729599
  - 0.41548749804496765
  - 0.5410295724868774
  - 0.3978723883628845
  - 0.3926021158695221
  - 0.4067029356956482
  - 0.39973488450050354
  - 0.4136355221271515
  - 0.4708694815635681
  - 0.5190335512161255
  - 0.39317503571510315
  - 0.4244072437286377
  - 0.41127175092697144
  - 0.3989017605781555
  - 0.42099109292030334
  - 0.41499611735343933
  - 0.4140922725200653
  - 0.41487228870391846
  - 0.436420738697052
  - 0.40410688519477844
  - 0.39667147397994995
  - 0.41552600264549255
  - 0.412069171667099
  - 0.40015870332717896
  - 0.412151575088501
  - 0.41701698303222656
  - 0.41660258173942566
  - 0.4466586410999298
  - 0.4306904673576355
  - 0.41484418511390686
  - 0.4249711036682129
  - 0.42059022188186646
  - 0.39887574315071106
  - 0.4112728536128998
  - 0.41165691614151
  - 0.4434090256690979
  - 0.42328760027885437
  - 0.40472787618637085
  - 0.4453343451023102
  - 0.4150981307029724
  - 0.3935956656932831
  - 0.4208608567714691
  - 0.4390638470649719
  - 0.40401843190193176
  - 0.3948418200016022
  - 0.41220757365226746
  - 0.45342594385147095
  - 0.39522236585617065
  - 0.40032657980918884
  - 0.4175383746623993
  - 0.45404887199401855
  - 0.46976932883262634
  - 0.4460480213165283
  - 0.3919953405857086
  - 0.38969528675079346
  - 0.4230988621711731
  - 0.39131680130958557
  - 0.41686153411865234
  - 0.4219651520252228
  - 0.39672040939331055
  - 0.44065138697624207
  - 0.41393548250198364
  - 0.3990902304649353
  - 0.3886060416698456
  - 0.40235915780067444
  - 0.41432639956474304
  - 0.39629727602005005
  - 0.43952620029449463
  - 0.40192294120788574
  - 0.40523800253868103
  - 0.4086023271083832
  - 0.41174986958503723
  - 0.41579777002334595
  - 0.3939351439476013
loss_records_fold2:
  train_losses:
  - 1.552747517824173
  - 1.5626334965229036
  - 1.6510508954524994
  - 1.654412752389908
  - 1.6356814324855806
  - 1.7158689618110659
  - 1.5828463196754456
  - 1.5572737097740175
  - 1.6784523725509644
  - 1.6143217861652375
  - 1.5937978863716127
  - 1.671934574842453
  - 1.6973863363265993
  - 1.6263517260551454
  - 1.9420689284801484
  - 1.7561942100524903
  - 1.585307651758194
  - 1.6191353797912598
  - 1.6825495839118958
  - 1.6217544853687287
  - 1.5078096926212312
  - 1.640509343147278
  - 1.5303962528705597
  - 1.564115047454834
  - 1.7376400411128998
  - 1.6720292747020722
  - 1.5802949309349061
  - 1.5821568191051485
  - 1.622559839487076
  - 1.5956975400447846
  - 1.5857908308506012
  - 1.5798004627227784
  - 1.580535101890564
  - 1.5934987723827363
  - 1.6011168241500855
  - 1.7072556257247926
  - 1.5652888417243958
  - 1.579691642522812
  - 1.5668456494808198
  - 1.5973815977573396
  - 1.6224181354045868
  - 1.7453307867050172
  - 1.9177555203437806
  - 1.68848574757576
  - 1.6161232054233552
  - 1.617473977804184
  - 1.7192292451858522
  - 1.679054927825928
  - 1.559661090373993
  - 1.554211473464966
  - 1.5587359905242921
  - 1.5304416060447694
  - 1.5960282981395721
  validation_losses:
  - 0.4210754334926605
  - 0.3818591833114624
  - 0.40860092639923096
  - 0.3799460530281067
  - 0.38286930322647095
  - 0.39019715785980225
  - 0.3979828357696533
  - 0.38852956891059875
  - 0.40010038018226624
  - 0.3826555013656616
  - 0.3773958086967468
  - 0.39196616411209106
  - 0.44788068532943726
  - 0.393134206533432
  - 0.4546199142932892
  - 0.3978244364261627
  - 0.3863442838191986
  - 0.46037980914115906
  - 0.40629932284355164
  - 0.3824218511581421
  - 0.37170952558517456
  - 0.3823045492172241
  - 0.37737441062927246
  - 0.4424402415752411
  - 0.3790442645549774
  - 0.42427465319633484
  - 0.37656456232070923
  - 0.38605552911758423
  - 0.3917008340358734
  - 0.38260531425476074
  - 0.40249571204185486
  - 0.37183958292007446
  - 0.38303399085998535
  - 0.408657968044281
  - 0.3827531933784485
  - 0.40359261631965637
  - 0.4021027088165283
  - 0.38795894384384155
  - 0.3888506591320038
  - 0.40535521507263184
  - 0.3825090527534485
  - 0.38903966546058655
  - 0.4622024893760681
  - 0.4044204354286194
  - 0.38244885206222534
  - 0.38218849897384644
  - 0.4475628435611725
  - 0.38587242364883423
  - 0.38336181640625
  - 0.38820621371269226
  - 0.37991461157798767
  - 0.3841976523399353
  - 0.376919150352478
loss_records_fold3:
  train_losses:
  - 1.6389638602733614
  - 1.606601506471634
  - 1.6406498551368713
  - 1.556489384174347
  - 1.551034677028656
  - 1.6334049224853517
  - 1.6327833712100983
  - 1.628484982252121
  - 1.6506962835788728
  - 1.5810432702302935
  - 1.6797510981559753
  - 1.676792871952057
  - 1.6552307784557343
  - 1.6180165886878968
  - 1.6215691387653353
  - 1.6245458841323854
  - 1.6528530299663544
  - 1.635964494943619
  validation_losses:
  - 0.39772677421569824
  - 0.4434768557548523
  - 0.3897283375263214
  - 0.3920566439628601
  - 0.38411810994148254
  - 0.3965255320072174
  - 0.41190212965011597
  - 0.406083345413208
  - 0.43218278884887695
  - 0.41590309143066406
  - 0.41217517852783203
  - 0.42965349555015564
  - 0.42149004340171814
  - 0.413545697927475
  - 0.41098034381866455
  - 0.4065788686275482
  - 0.4149624705314636
  - 0.40485379099845886
loss_records_fold4:
  train_losses:
  - 1.6693404495716095
  - 1.6158973932266236
  - 1.631635481119156
  - 1.6600656628608705
  - 1.6126697361469269
  - 1.69778550863266
  - 1.7290829420089722
  - 1.6355516970157624
  - 1.655974566936493
  - 1.729364413022995
  - 1.6014031887054445
  - 1.6355100810527803
  - 1.6052481830120087
  validation_losses:
  - 0.42482292652130127
  - 0.4078763723373413
  - 0.405563622713089
  - 0.40212956070899963
  - 0.43481430411338806
  - 0.3984803557395935
  - 0.40923967957496643
  - 0.40946123003959656
  - 0.4189916253089905
  - 0.40987786650657654
  - 0.40401971340179443
  - 0.41022124886512756
  - 0.41168537735939026
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 95 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 53 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
training_metrics:
  fold_eval_accs: '[0.6809605488850772, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.12264150943396225, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8232792818282022
  mean_f1_accuracy: 0.02452830188679245
  total_train_time: '0:23:25.923007'
