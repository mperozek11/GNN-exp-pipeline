config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 00:01:52.040325'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/92/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/92/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/92/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/92/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/92/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/92/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/92/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/92/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/92/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/92/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 190.09217607975006
  - 62.84632587432861
  - 76.37376576662064
  - 35.9295559823513
  - 30.05100506544113
  - 15.173128366470337
  - 26.957750886678696
  - 14.775053679943085
  - 11.835667073726654
  - 13.457271546125412
  - 9.930541336536407
  - 10.064707577228546
  - 11.544735133647919
  - 10.89257636666298
  - 9.393653616309166
  - 8.867955774068832
  - 33.01096738874912
  - 15.097484827041626
  - 13.696944296360016
  - 13.860345840454102
  - 14.977078586816788
  - 32.40840592980385
  - 26.41299057006836
  - 39.30362594127655
  - 48.67095956206322
  - 31.70031313598156
  - 21.952147886157036
  - 16.256530702114105
  - 12.88855466246605
  - 9.931802600622177
  - 11.483016967773438
  - 14.737983375787735
  - 13.2033970952034
  - 10.483515352010727
  - 11.005561411380768
  - 9.741589099168777
  - 10.973410546779633
  - 11.610789597034454
  - 14.295837074518204
  - 13.128574669361115
  - 18.414050668478012
  - 10.831051081418991
  - 24.089031368494034
  - 10.453458815813065
  - 8.445672303438187
  - 11.958769232034683
  - 10.18129289150238
  - 8.530015975236893
  - 10.059237778186798
  - 9.289900243282318
  - 9.00840750336647
  - 9.056744813919067
  - 8.981858938932419
  - 9.429224461317062
  - 9.258841574192047
  - 10.345957845449448
  - 9.089762896299362
  - 8.644506126642227
  - 10.416855603456497
  - 8.209543824195862
  - 7.892351031303406
  - 11.048899501562119
  - 10.383945167064667
  - 14.503820985555649
  - 17.254535526037216
  - 13.27679318189621
  - 10.795115798711777
  - 8.605232268571854
  - 10.301195979118347
  - 10.593594998121262
  - 14.062683671712875
  - 12.795732349157333
  - 31.81560629606247
  - 15.777600556612015
  - 9.322715401649475
  - 20.924686446785927
  - 8.899203449487686
  - 8.659994214773178
  - 8.579028248786926
  - 10.259227722883224
  - 15.351385027170181
  - 18.661906123161316
  - 10.330250173807144
  - 10.205312103033066
  - 20.813097715377808
  - 8.522685319185257
  - 8.251862645149231
  - 8.565075635910034
  - 9.725368529558182
  - 8.074631750583649
  - 10.047983825206757
  - 8.648042351007462
  - 10.257486283779144
  - 11.191094905138016
  - 11.065307050943375
  - 15.040204226970673
  - 8.561045289039612
  - 8.344090908765793
  - 8.520717233419418
  - 8.212897807359695
  validation_losses:
  - 1.5078339576721191
  - 7.731158256530762
  - 1.7133564949035645
  - 0.9145997762680054
  - 0.5413200855255127
  - 0.7739179730415344
  - 0.47675126791000366
  - 0.6384072303771973
  - 0.508354902267456
  - 0.4389370381832123
  - 0.4519096910953522
  - 0.5021034479141235
  - 0.49343863129615784
  - 0.4209098219871521
  - 0.415539026260376
  - 0.4705495536327362
  - 0.6517005562782288
  - 0.5589091777801514
  - 0.6260150074958801
  - 0.47057169675827026
  - 0.6589797735214233
  - 0.5459945201873779
  - 0.5431583523750305
  - 0.6242262125015259
  - 0.7057782411575317
  - 0.565243124961853
  - 0.4234403967857361
  - 0.49929314851760864
  - 0.46896892786026
  - 0.3933020830154419
  - 0.3928207457065582
  - 0.4454461634159088
  - 0.45554742217063904
  - 0.45783162117004395
  - 0.42054882645606995
  - 0.4998707175254822
  - 0.4101565182209015
  - 0.44417861104011536
  - 0.4398213326931
  - 0.4635695517063141
  - 0.4342857301235199
  - 0.3964364230632782
  - 0.4706994593143463
  - 0.4134073555469513
  - 0.5027173757553101
  - 0.3973374664783478
  - 0.45513486862182617
  - 0.39982089400291443
  - 0.40319475531578064
  - 0.4315904974937439
  - 0.39944028854370117
  - 0.4188685417175293
  - 0.6309673190116882
  - 0.5314210653305054
  - 0.4302719235420227
  - 0.4260248839855194
  - 0.38466307520866394
  - 0.39025190472602844
  - 0.42002734541893005
  - 0.3996560275554657
  - 0.40335944294929504
  - 0.40304845571517944
  - 0.5043408274650574
  - 0.4555027186870575
  - 0.5014533400535583
  - 0.5028201341629028
  - 0.3983726501464844
  - 0.628791868686676
  - 0.40989983081817627
  - 0.40581637620925903
  - 0.4490792155265808
  - 0.44626155495643616
  - 0.5163413286209106
  - 0.6911893486976624
  - 0.4292077422142029
  - 0.412815123796463
  - 0.41309472918510437
  - 0.38633012771606445
  - 0.4116787314414978
  - 0.3896411061286926
  - 0.5628474950790405
  - 0.465677410364151
  - 0.4547230303287506
  - 0.4161960780620575
  - 0.4542769193649292
  - 0.41330915689468384
  - 0.3957080841064453
  - 0.4056411683559418
  - 0.39639902114868164
  - 0.4611755907535553
  - 0.43725961446762085
  - 0.5088160634040833
  - 0.6933991312980652
  - 0.44011446833610535
  - 0.3967549800872803
  - 0.40573570132255554
  - 0.40549540519714355
  - 0.4069455862045288
  - 0.41751256585121155
  - 0.3983892500400543
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 40 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:03:33.882800'
