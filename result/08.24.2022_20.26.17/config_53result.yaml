config:
  aggregation: mean
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:38:11.743830'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_53fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_53fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_53fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_53fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_53fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_53fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_53fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_53fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_53fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_53fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.551971805095673
  - 3.295644867420197
  - 3.2500066399574283
  - 3.2190243482589724
  - 3.109831061959267
  - 3.1588680386543277
  - 3.2197634816169742
  - 3.0637353628873827
  - 3.049637266993523
  - 2.9909396409988407
  - 3.0663377344608307
  - 3.070441234111786
  - 3.113954854011536
  - 3.0229874670505525
  - 3.0093043744564056
  - 3.089405632019043
  - 3.201957976818085
  - 3.0853408813476566
  - 3.0282241106033325
  - 3.050882422924042
  - 3.005441665649414
  - 3.0373427271842957
  - 2.9673704355955124
  - 2.9909514963626864
  - 3.0295549124479297
  - 3.0176591068506244
  - 2.9883147656917575
  - 3.176064908504486
  - 3.078145903348923
  - 3.001157408952713
  - 2.922726219892502
  - 2.957686918973923
  - 2.907653915882111
  - 3.032185083627701
  - 2.974751979112625
  - 2.9977146267890933
  - 2.9528329491615297
  - 3.03569872379303
  - 2.9904540419578556
  - 2.98664585351944
  - 3.0205054342746736
  - 3.055785828828812
  - 2.9762455701828006
  - 2.9277062118053436
  - 2.9485709369182587
  - 3.028113251924515
  - 3.001731216907501
  - 2.9689979672431948
  - 2.962660849094391
  - 2.999970853328705
  - 2.9773381501436234
  - 2.9827126562595367
  - 3.0177006781101228
  - 2.9916100323200228
  - 2.926901483535767
  - 3.113425755500794
  - 2.934651249647141
  - 2.914228743314743
  - 3.0019530892372135
  - 2.9328133136034014
  - 2.9742215842008592
  - 3.0291734725236896
  - 2.9371712505817413
  - 2.963501048088074
  - 2.9511397898197176
  - 3.0092758119106295
  - 3.13381364941597
  - 2.9598847806453707
  - 2.9834015667438507
  - 2.906581974029541
  - 2.9831207394599915
  - 2.952699786424637
  - 2.926107877492905
  - 2.9369091391563416
  - 3.09132077395916
  - 2.930693301558495
  - 2.9647594809532167
  - 2.898724663257599
  - 2.96548348069191
  - 3.0014413177967074
  - 2.9653994977474216
  - 2.9524832844734195
  - 3.01861065030098
  - 2.986818265914917
  - 2.976692855358124
  - 2.9641473531723026
  - 2.956160378456116
  - 2.9506509602069855
  - 2.9483126878738406
  - 3.013821065425873
  - 2.9215907752513885
  - 3.003786110877991
  - 2.9283206880092623
  - 3.0112871289253236
  - 2.9280132442712787
  - 2.9801679790019993
  - 3.0322794169187546
  - 2.922778904438019
  - 2.940547853708267
  - 2.972965741157532
  validation_losses:
  - 0.498501718044281
  - 0.4315818250179291
  - 0.40635064244270325
  - 0.4170464277267456
  - 0.44404125213623047
  - 0.6295578479766846
  - 0.3916773796081543
  - 0.39812609553337097
  - 0.3966141641139984
  - 0.397939532995224
  - 0.431767076253891
  - 0.39077284932136536
  - 0.38868969678878784
  - 0.4159407913684845
  - 0.4759545624256134
  - 0.4255867600440979
  - 0.4152236580848694
  - 0.3956702649593353
  - 0.4070286154747009
  - 0.40286436676979065
  - 0.412596195936203
  - 0.3869442939758301
  - 0.3945607841014862
  - 0.4992269277572632
  - 0.386047899723053
  - 0.46667930483818054
  - 2.421743392944336
  - 0.4118502140045166
  - 0.7246273756027222
  - 0.7397417426109314
  - 2.086012840270996
  - 0.38537099957466125
  - 0.3875565528869629
  - 0.7647745013237
  - 0.5768969655036926
  - 0.5044600963592529
  - 0.39024126529693604
  - 0.40302613377571106
  - 1.1951842308044434
  - 0.38371729850769043
  - 1.0626834630966187
  - 0.8691298961639404
  - 2.644606351852417
  - 1.293991208076477
  - 2.417022466659546
  - 1.4014782905578613
  - 0.45558515191078186
  - 1.03701913356781
  - 0.39401549100875854
  - 0.3893660306930542
  - 0.39976173639297485
  - 0.3964069187641144
  - 0.5818471908569336
  - 0.6459009647369385
  - 1.7502427101135254
  - 0.3923048973083496
  - 0.3957865536212921
  - 0.3988755941390991
  - 0.39976605772972107
  - 0.4686201810836792
  - 0.3959123492240906
  - 0.4062345623970032
  - 0.6164065003395081
  - 0.3887750208377838
  - 0.3956746459007263
  - 0.4017341434955597
  - 0.39858490228652954
  - 0.3884156048297882
  - 0.41599541902542114
  - 0.45960813760757446
  - 0.6253029704093933
  - 0.47346511483192444
  - 0.5387874245643616
  - 1.0997446775436401
  - 0.4318951964378357
  - 0.4084649682044983
  - 0.3937619626522064
  - 0.39713290333747864
  - 0.45993152260780334
  - 0.38250017166137695
  - 0.38586553931236267
  - 0.3912845253944397
  - 0.547833263874054
  - 0.39424002170562744
  - 0.5044242739677429
  - 0.611909031867981
  - 0.3879398703575134
  - 0.7395761013031006
  - 0.8347649574279785
  - 0.40706783533096313
  - 0.39665302634239197
  - 0.3819310665130615
  - 0.3886854946613312
  - 0.4076592028141022
  - 0.4053613543510437
  - 0.42462608218193054
  - 0.39937102794647217
  - 0.4300517439842224
  - 0.4090847969055176
  - 0.40497440099716187
loss_records_fold1:
  train_losses:
  - 2.982803761959076
  - 3.073331445455551
  - 2.9538871526718142
  - 2.9405068814754487
  - 3.0096826642751697
  - 2.9579233884811402
  - 2.976496839523316
  - 2.922079637646675
  - 2.903242141008377
  - 2.9344624638557435
  - 2.9815775215625764
  - 2.950382423400879
  - 2.9476733654737473
  - 2.996145886182785
  - 2.974884676933289
  - 2.9610161185264587
  - 3.002377623319626
  - 2.964086875319481
  - 2.9660642564296724
  - 2.877493712306023
  - 2.9690543651580814
  - 2.9691183626651765
  - 2.9683909982442858
  - 2.9813692748546603
  - 2.943927848339081
  - 2.9207276910543443
  - 2.9407819688320163
  - 2.9829837620258335
  - 2.8831982851028446
  - 2.9613457322120667
  - 2.9519669234752657
  - 2.9251127660274507
  - 3.0180431962013246
  - 2.942162495851517
  - 2.8835557609796525
  - 2.9311528980731967
  - 2.947420203685761
  - 2.964546555280686
  - 2.929670912027359
  - 2.911497896909714
  - 2.9007378578186036
  - 2.9765446454286577
  - 2.969774514436722
  - 2.968914285302162
  - 2.905570843815804
  - 2.9250273495912555
  - 2.988989251852036
  - 2.883145272731781
  - 2.9568364500999453
  - 2.9140881955623628
  - 2.8731197059154514
  - 2.9003990769386294
  - 2.841217055916786
  - 2.8488357692956927
  - 2.9772319138050083
  - 2.9529438793659213
  - 2.904237848520279
  - 2.963338923454285
  - 3.056343075633049
  - 2.9437931537628175
  - 2.9021778225898744
  - 2.911589354276657
  - 2.887137079238892
  - 2.990489012002945
  - 2.8545711219310763
  - 2.8737890213727955
  - 2.914278754591942
  - 2.9555001795291904
  - 2.9304245710372925
  - 2.9699894040822983
  - 2.8831497132778168
  - 2.8521421492099766
  - 2.875122433900833
  - 2.8297795176506044
  - 2.979156190156937
  - 2.92437274158001
  - 2.8498801767826083
  - 2.826448678970337
  - 2.9091365039348602
  - 2.8786491215229035
  - 2.8383530199527742
  - 2.8826127648353577
  - 2.84054526090622
  - 2.8371657371521
  - 2.8383255243301395
  - 2.834177976846695
  - 2.9203515291213993
  - 2.90722086429596
  - 2.856750297546387
  - 2.9003468930721286
  - 2.8467864811420442
  - 2.8745126217603687
  - 2.8545993477106095
  - 2.8953421115875244
  - 2.877293634414673
  - 2.825116276741028
  - 2.920314347743988
  - 2.8644921362400058
  - 2.8854301422834396
  - 2.885491681098938
  validation_losses:
  - 0.39441725611686707
  - 0.40366098284721375
  - 0.38844022154808044
  - 1.0845288038253784
  - 0.38821980357170105
  - 0.4027393162250519
  - 0.428225040435791
  - 0.4130879044532776
  - 0.4080590605735779
  - 0.45587870478630066
  - 0.44308462738990784
  - 0.43252137303352356
  - 0.9272564053535461
  - 0.40378671884536743
  - 0.4488382041454315
  - 0.4758937358856201
  - 0.40637513995170593
  - 0.4773047864437103
  - 0.37863269448280334
  - 0.6191825866699219
  - 0.8091557025909424
  - 0.8481744527816772
  - 0.3925092816352844
  - 0.3893122673034668
  - 0.42077842354774475
  - 0.39536428451538086
  - 1.3591045141220093
  - 0.4206051826477051
  - 0.4701155126094818
  - 0.6341609358787537
  - 0.39603886008262634
  - 0.48724043369293213
  - 0.673648476600647
  - 0.40168309211730957
  - 0.5491204261779785
  - 0.9321442246437073
  - 0.7223093509674072
  - 0.5615567564964294
  - 1.0250715017318726
  - 0.607753574848175
  - 0.6379976868629456
  - 0.4681055247783661
  - 0.6436061859130859
  - 0.5136915445327759
  - 0.43336915969848633
  - 0.6727964878082275
  - 0.9875338077545166
  - 0.39523664116859436
  - 0.41789156198501587
  - 0.5991328954696655
  - 0.6601018309593201
  - 0.46696627140045166
  - 0.5871860384941101
  - 0.5928800106048584
  - 0.6161486506462097
  - 0.6174463629722595
  - 0.9762406945228577
  - 0.5707788467407227
  - 0.4341542422771454
  - 0.4009378254413605
  - 0.4916873574256897
  - 0.49855783581733704
  - 0.49339959025382996
  - 0.5991857647895813
  - 0.5486632585525513
  - 1.230893850326538
  - 0.4661172330379486
  - 0.544013500213623
  - 0.9889420866966248
  - 0.4376799762248993
  - 0.5261502861976624
  - 0.6073682904243469
  - 0.6233645677566528
  - 1.7857969999313354
  - 0.4173527657985687
  - 0.5031267404556274
  - 0.5652499198913574
  - 0.6972382664680481
  - 0.8055108189582825
  - 0.5060146450996399
  - 0.48328444361686707
  - 0.6556665897369385
  - 1.2093650102615356
  - 1.17578125
  - 1.114656925201416
  - 0.7263959646224976
  - 0.679616391658783
  - 0.5275548100471497
  - 0.7670863270759583
  - 8.287008285522461
  - 0.5227683782577515
  - 0.9262880086898804
  - 0.8409323692321777
  - 1.2667194604873657
  - 0.8399093747138977
  - 2.1391024589538574
  - 0.4349849820137024
  - 0.6238875389099121
  - 0.8916047215461731
  - 0.43694064021110535
loss_records_fold2:
  train_losses:
  - 2.762068504095078
  - 2.852750763297081
  - 2.8314058870077137
  - 2.790372514724732
  - 2.792025825381279
  - 2.895550411939621
  - 2.800834220647812
  - 2.858302262425423
  - 2.897353208065033
  - 2.71326744556427
  - 2.839994251728058
  - 2.856490606069565
  - 2.872007167339325
  - 3.0336185038089756
  - 3.031211292743683
  - 3.009046745300293
  - 2.9377731382846832
  - 2.9278106153011323
  - 2.9914864122867586
  - 3.0066396266222
  validation_losses:
  - 0.5120784640312195
  - 0.5179430842399597
  - 0.5323359966278076
  - 0.7517543435096741
  - 0.5424726605415344
  - 0.5103291869163513
  - 0.5063944458961487
  - 0.47780641913414
  - 0.9500415325164795
  - 0.581271767616272
  - 0.6279963850975037
  - 0.624461829662323
  - 0.3960012197494507
  - 0.4101583659648895
  - 0.395588219165802
  - 0.39320623874664307
  - 0.3913690149784088
  - 0.39796164631843567
  - 0.4010958671569824
  - 0.3953495919704437
loss_records_fold3:
  train_losses:
  - 3.1323448061943058
  - 2.9897607028484345
  - 2.996345889568329
  - 3.0474460154771807
  - 3.0003779172897342
  - 3.0396596789360046
  - 2.9798737943172457
  - 3.018280649185181
  - 3.0730982303619387
  - 2.9329002499580383
  - 3.029452842473984
  - 2.9881765604019166
  - 2.951390588283539
  - 2.92304305434227
  - 2.940367978811264
  - 2.984135687351227
  - 2.9610995620489122
  - 2.959910571575165
  - 2.9695751130580903
  - 2.965389221906662
  - 2.993664848804474
  - 2.9776893258094788
  - 2.9030045896768573
  - 2.92548263669014
  - 2.894505053758621
  - 2.9235699236392976
  - 2.993709868192673
  - 2.9224494099617004
  - 2.9224884271621705
  - 2.8940926492214203
  - 2.912613734602928
  - 2.9328089356422424
  - 2.8995997071266175
  - 2.9180477619171143
  - 2.948931658267975
  - 2.9384018659591677
  - 2.914956659078598
  - 2.941745245456696
  - 2.872501727938652
  - 2.893781924247742
  - 2.8853811204433444
  - 2.9154300034046177
  - 2.9294547080993656
  - 2.9147150754928592
  - 2.8960668325424197
  - 2.8524961650371554
  - 2.9049654722213747
  - 2.8764705121517182
  - 2.966322803497315
  - 2.887830996513367
  - 2.90558814406395
  - 2.9047674715518954
  - 2.877854707837105
  - 2.894448766112328
  - 2.8708932995796204
  - 3.1719146192073824
  - 2.971902972459793
  - 2.946342831850052
  - 2.9253686368465424
  - 2.9209399789571764
  - 2.857065957784653
  - 2.936352246999741
  - 2.892032983899117
  - 2.9599638462066653
  - 2.8520005017519
  - 2.8988590180873874
  - 2.8665497660636903
  - 2.9354164183139804
  - 2.915924018621445
  - 2.8610875189304354
  - 2.8662869840860368
  - 2.8318888664245607
  - 2.91543020606041
  - 2.915417575836182
  - 2.8818873405456547
  - 2.813865798711777
  - 2.8790431857109073
  - 2.933778178691864
  - 2.9226547211408618
  - 2.862169533967972
  - 2.886753642559052
  - 2.85354198217392
  - 2.875276279449463
  - 2.981857192516327
  - 3.0139641761779785
  - 2.832550668716431
  - 2.912953919172287
  - 2.8947343051433565
  - 2.810392200946808
  - 2.912609446048737
  - 2.8617897868156437
  - 2.847715148329735
  - 2.829478621482849
  - 2.8655194222927096
  - 2.897504672408104
  - 2.8961247324943544
  - 2.8875672191381456
  - 2.8737072438001636
  - 2.853905546665192
  - 2.8063599139451982
  validation_losses:
  - 0.38933655619621277
  - 0.37573784589767456
  - 0.3842221796512604
  - 0.3952362835407257
  - 0.3736717402935028
  - 0.38400179147720337
  - 0.38857540488243103
  - 0.38804149627685547
  - 0.3851141035556793
  - 0.4566507637500763
  - 0.5326548218727112
  - 0.6840561628341675
  - 0.39964011311531067
  - 0.3968789279460907
  - 0.3817250430583954
  - 0.397043913602829
  - 3.45234751701355
  - 0.38044020533561707
  - 0.3808860182762146
  - 0.38183820247650146
  - 0.4308117628097534
  - 0.3928554654121399
  - 0.46361616253852844
  - 0.42288485169410706
  - 0.4263572096824646
  - 0.40683621168136597
  - 0.3970629572868347
  - 0.3868447542190552
  - 0.43844035267829895
  - 0.48120537400245667
  - 0.43641188740730286
  - 0.44789960980415344
  - 0.5229963064193726
  - 0.545446515083313
  - 0.40316465497016907
  - 0.48483753204345703
  - 0.7903336882591248
  - 0.814013659954071
  - 0.3947269916534424
  - 0.4374864995479584
  - 0.5554075837135315
  - 0.4440899193286896
  - 0.3946181833744049
  - 1.2471768856048584
  - 0.4541008472442627
  - 1.8000706434249878
  - 0.8653630614280701
  - 0.46427100896835327
  - 0.44400346279144287
  - 0.40450191497802734
  - 0.456315815448761
  - 2.411668539047241
  - 0.6215999722480774
  - 1.9474189281463623
  - 0.41072165966033936
  - 0.4578915536403656
  - 0.4588920474052429
  - 0.4981023371219635
  - 0.43688511848449707
  - 0.41456112265586853
  - 0.4404301047325134
  - 0.49303990602493286
  - 1.0828832387924194
  - 0.9315817952156067
  - 0.787886381149292
  - 0.3835429847240448
  - 0.6076431274414062
  - 0.7545022964477539
  - 1.3068097829818726
  - 1.6513742208480835
  - 0.8016061186790466
  - 0.7829787731170654
  - 0.7867302298545837
  - 0.5995112657546997
  - 1.2404675483703613
  - 2.047065019607544
  - 0.5338213443756104
  - 1.2881141901016235
  - 0.8892439603805542
  - 1.3880515098571777
  - 1.771358847618103
  - 2.093034267425537
  - 3.0876216888427734
  - 4.83705997467041
  - 2.1702613830566406
  - 2.0355582237243652
  - 2.161686420440674
  - 2.936795711517334
  - 1.182323932647705
  - 0.4704912304878235
  - 0.47653570771217346
  - 2.303738594055176
  - 0.7941153049468994
  - 0.4524275064468384
  - 3.0310537815093994
  - 7.243601322174072
  - 1.9252170324325562
  - 3.7907049655914307
  - 1.23716402053833
  - 6.4676055908203125
loss_records_fold4:
  train_losses:
  - 2.8751687467098237
  - 2.8885645210742954
  - 2.89343795478344
  - 2.8546983897686005
  - 2.82743211388588
  - 2.8494443178176883
  - 2.897412192821503
  - 2.8892112463712696
  - 2.849879962205887
  - 2.873902601003647
  - 2.877941179275513
  - 2.8514483839273455
  - 2.833876290917397
  - 2.863316106796265
  - 2.9626222908496858
  - 2.8086039185523988
  - 2.8856230825185776
  - 2.955344277620316
  - 3.0042744040489198
  - 2.901660966873169
  - 2.827992260456085
  - 2.8159113079309464
  - 2.8328188955783844
  - 2.8755709499120714
  - 2.838738790154457
  - 2.8979673981666565
  - 2.9074893414974214
  - 2.9301806509494783
  - 2.88281632065773
  - 2.877931618690491
  - 2.8368091076612476
  - 2.850883060693741
  - 2.8602793872356416
  - 2.832518234848976
  - 2.7323950827121735
  - 2.8376335322856905
  - 2.9033311903476715
  - 2.88485541343689
  - 2.8600605309009555
  - 2.782431483268738
  - 2.8632736980915072
  - 2.9046392977237705
  - 2.9197688281536105
  - 2.8714927285909653
  - 2.821501162648201
  - 2.8730697810649874
  - 2.8380182445049287
  - 2.8467432469129563
  - 2.865517368912697
  - 2.9205593049526217
  - 2.8943724155426027
  - 2.860531195998192
  - 2.8236445009708406
  - 2.8741507768630985
  - 2.7897441387176514
  - 2.83482104241848
  - 2.836189562082291
  - 2.864968311786652
  - 2.934487903118134
  - 2.955564743280411
  - 2.95060752928257
  - 3.300144308805466
  - 3.0617156565189365
  - 3.0916802465915683
  - 2.9670371532440187
  - 2.954329711198807
  - 2.9266391158103944
  - 2.875098377466202
  - 2.869537162780762
  - 2.920594960451126
  - 2.905288314819336
  - 2.9154983639717105
  - 2.892874926328659
  - 2.834582555294037
  - 2.8423762142658235
  - 2.943133914470673
  - 2.838037133216858
  - 2.9408760964870453
  - 2.824450895190239
  - 2.845433881878853
  - 2.8609230250120166
  - 2.9521454215049747
  - 2.861224034428597
  - 2.930196869373322
  - 2.775770226120949
  - 2.852571725845337
  - 2.8891441881656648
  - 2.9381922602653505
  - 2.8916893124580385
  - 2.722647207975388
  - 2.8535007685422897
  - 2.832053005695343
  - 2.866407236456871
  - 2.871447116136551
  - 2.8909458220005035
  - 2.8439772069454197
  - 2.7769600301980972
  - 2.8784470796585087
  - 2.797407400608063
  - 2.8218191862106323
  validation_losses:
  - 0.38614320755004883
  - 0.5123763084411621
  - 0.464500367641449
  - 0.5106608271598816
  - 0.6237305402755737
  - 0.48882219195365906
  - 0.5506264567375183
  - 0.5597322583198547
  - 0.5150058269500732
  - 0.6150058507919312
  - 0.5463764667510986
  - 0.4801108241081238
  - 0.4870956838130951
  - 0.5585511922836304
  - 0.526520311832428
  - 0.58299720287323
  - 0.6668573617935181
  - 0.40410974621772766
  - 0.4856643080711365
  - 0.5492172241210938
  - 0.627526581287384
  - 0.5540369153022766
  - 0.5056748986244202
  - 0.599639356136322
  - 0.6375423073768616
  - 0.5394954085350037
  - 0.5370974540710449
  - 0.5689180493354797
  - 0.523870050907135
  - 1.367878794670105
  - 0.5012977719306946
  - 0.4930095970630646
  - 0.5620695352554321
  - 0.6939803957939148
  - 0.5301433801651001
  - 0.6696791052818298
  - 0.9589277505874634
  - 0.5751525163650513
  - 0.5101155638694763
  - 0.4113636910915375
  - 0.7052133083343506
  - 0.7037851214408875
  - 0.6329411864280701
  - 0.6538507342338562
  - 0.6444304585456848
  - 0.6288080215454102
  - 0.5276937484741211
  - 0.554561972618103
  - 0.5246732831001282
  - 0.6363997459411621
  - 0.5821248888969421
  - 0.547071099281311
  - 0.5766526460647583
  - 0.5122897624969482
  - 0.5594202280044556
  - 0.6090565919876099
  - 0.5484375953674316
  - 0.4840713441371918
  - 0.4808162450790405
  - 0.5086677074432373
  - 0.4431181848049164
  - 0.4046224057674408
  - 0.39548322558403015
  - 0.39278286695480347
  - 0.3990548253059387
  - 0.44885459542274475
  - 0.39814358949661255
  - 0.4452674090862274
  - 0.4329548180103302
  - 0.7321518659591675
  - 0.4331926107406616
  - 0.3941374719142914
  - 0.4319339692592621
  - 0.41544225811958313
  - 0.43925145268440247
  - 0.43237629532814026
  - 0.46900296211242676
  - 0.6104651093482971
  - 0.4671206772327423
  - 0.4968324601650238
  - 0.3931247591972351
  - 0.4481154978275299
  - 0.4628489017486572
  - 0.4724767804145813
  - 0.5006460547447205
  - 0.4886850118637085
  - 0.49209022521972656
  - 0.4824056327342987
  - 0.47073784470558167
  - 0.599698007106781
  - 0.9586055874824524
  - 0.5525785684585571
  - 0.6080794334411621
  - 2.0826854705810547
  - 0.6025463342666626
  - 0.6359737515449524
  - 0.678054690361023
  - 0.809943437576294
  - 0.6663916110992432
  - 0.5519188046455383
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8319039451114922, 0.8593481989708405, 0.8216123499142367,
    0.8092783505154639]'
  fold_eval_f1: '[0.023529411764705882, 0.14035087719298248, 0.0, 0.26760563380281693,
    0.16541353383458648]'
  mean_eval_accuracy: 0.8359551555233329
  mean_f1_accuracy: 0.11937989131901836
  total_train_time: '0:40:03.392858'
