config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:05:30.908334'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_83fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_83fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_83fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_83fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_83fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_83fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_83fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_83fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_83fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_83fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 0.9326784908771515
  - 0.8588454723358154
  - 0.8205933094024659
  - 0.7631364941596985
  - 0.8074465215206147
  - 0.7819691359996797
  - 0.7880781173706055
  - 0.7428442388772964
  - 0.7982543051242829
  - 0.7810298204421997
  - 0.8296963334083558
  validation_losses:
  - 0.4004004895687103
  - 0.44945257902145386
  - 0.4229111969470978
  - 0.40706515312194824
  - 0.39468687772750854
  - 0.3926635682582855
  - 0.3934366703033447
  - 0.39561033248901367
  - 0.39340507984161377
  - 0.38594377040863037
  - 0.3949223458766937
loss_records_fold1:
  train_losses:
  - 0.7711033463478089
  - 0.7714943706989289
  - 0.7839659094810486
  - 0.7808232128620148
  - 0.7860744178295136
  - 0.7775179266929627
  - 0.7432093501091004
  - 0.7707123637199402
  - 0.8355371057987213
  - 0.7636830866336823
  - 0.785783088207245
  - 0.7646111488342285
  - 0.7447115957736969
  - 0.7608513474464417
  validation_losses:
  - 0.3891969621181488
  - 0.39020922780036926
  - 0.3978348672389984
  - 0.3930359482765198
  - 0.40151941776275635
  - 0.3899279832839966
  - 0.3896418809890747
  - 0.40271487832069397
  - 0.3974698781967163
  - 0.3868075907230377
  - 0.39644086360931396
  - 0.3926464915275574
  - 0.38861334323883057
  - 0.39360272884368896
loss_records_fold2:
  train_losses:
  - 0.7755947887897492
  - 0.7485562801361084
  - 0.7966119945049286
  - 0.7609716892242432
  - 0.7829209506511688
  - 0.7970467329025269
  - 0.758638471364975
  - 0.7541836440563202
  - 0.8234277069568634
  - 0.7587878704071045
  - 0.9235840678215027
  - 0.7857342422008515
  validation_losses:
  - 0.4057292342185974
  - 0.3871491253376007
  - 0.40021073818206787
  - 0.38854148983955383
  - 0.3912009596824646
  - 0.4017776548862457
  - 0.394683837890625
  - 0.38465794920921326
  - 0.38494449853897095
  - 0.39376941323280334
  - 0.39087429642677307
  - 0.38880234956741333
loss_records_fold3:
  train_losses:
  - 0.9514506518840791
  - 0.7865781873464585
  - 0.7931786417961121
  - 0.7679820716381074
  - 0.7700881659984589
  - 0.7859899044036865
  - 0.7394607752561569
  - 0.7694895923137666
  - 0.7798047065734863
  - 0.7606764614582062
  - 0.7486296057701112
  - 0.7716309845447541
  - 0.7584305346012116
  - 0.7296535879373551
  - 0.7730237901210786
  - 0.8765187084674836
  - 0.8624696433544159
  - 0.7541259855031968
  validation_losses:
  - 0.4158599376678467
  - 0.3859288692474365
  - 0.39062610268592834
  - 0.37972375750541687
  - 0.3783757984638214
  - 0.3884611129760742
  - 0.37785327434539795
  - 0.37524357438087463
  - 0.3783852458000183
  - 0.3737788796424866
  - 0.37494292855262756
  - 0.3859312832355499
  - 0.3771985173225403
  - 0.3773971498012543
  - 0.3833065927028656
  - 0.37463849782943726
  - 0.383821576833725
  - 0.38111117482185364
loss_records_fold4:
  train_losses:
  - 0.7650880813598633
  - 0.7595446646213532
  - 0.805834311246872
  - 0.746918684244156
  - 0.7774694085121155
  - 0.7806902945041657
  - 0.7239545911550522
  - 0.782817703485489
  - 0.7685864388942719
  - 0.767361468076706
  - 0.7778171539306641
  - 0.794616734981537
  - 0.7846444487571717
  - 0.7557103335857391
  - 0.7550259470939636
  - 0.7726502358913422
  - 0.800861406326294
  validation_losses:
  - 0.4183497130870819
  - 0.37914782762527466
  - 0.38431671261787415
  - 0.4124448001384735
  - 0.37869492173194885
  - 0.3832099437713623
  - 0.3794958293437958
  - 0.37645256519317627
  - 0.38045844435691833
  - 0.38395413756370544
  - 0.3942015767097473
  - 0.38392409682273865
  - 0.3873614966869354
  - 0.38895371556282043
  - 0.382614403963089
  - 0.38319161534309387
  - 0.3776363730430603
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:05:59.149029'
