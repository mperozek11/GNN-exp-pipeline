config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:00:52.732925'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_39fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_39fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_39fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_39fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_39fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_39fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_39fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_39fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_39fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_39fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.158384418487549
  - 0.8972982406616211
  - 0.9478802859783173
  - 0.8584512412548065
  - 0.8477648138999939
  - 0.8682068407535554
  - 0.9086120963096619
  - 0.8464064240455628
  - 0.786047986149788
  - 0.8663030922412873
  - 0.8828432202339173
  - 0.9086836874485016
  - 0.9851592004299164
  - 0.8894380271434784
  - 0.8380306780338288
  - 0.8744224607944489
  - 0.8361739456653595
  - 0.8225411593914033
  - 0.8208795189857483
  - 0.8245809793472291
  validation_losses:
  - 0.4084714651107788
  - 0.41692379117012024
  - 0.41559287905693054
  - 0.4144396185874939
  - 0.42215871810913086
  - 0.40997815132141113
  - 0.40646055340766907
  - 0.39780306816101074
  - 0.42257359623908997
  - 0.41072577238082886
  - 0.448871910572052
  - 0.44514092803001404
  - 0.4151481091976166
  - 0.46693137288093567
  - 0.4089835286140442
  - 0.40106773376464844
  - 0.40131875872612
  - 0.397799015045166
  - 0.3943912982940674
  - 0.39206722378730774
loss_records_fold1:
  train_losses:
  - 0.7936161816120149
  - 0.8531265735626221
  - 0.8497581541538239
  - 0.8394524991512299
  - 0.8719425857067109
  - 0.7985312581062317
  - 0.8118897259235383
  - 0.7964480817317963
  - 0.796491152048111
  - 0.8371305286884309
  - 0.8081828474998475
  - 0.854628747701645
  - 0.8282169580459595
  - 0.8048895299434662
  - 0.7840378105640412
  - 0.7659254610538483
  - 0.7822433471679688
  - 0.7834915935993195
  - 0.8513558089733124
  - 0.8118876576423646
  - 0.7876579284667969
  - 0.8012885093688965
  - 0.8567860960960388
  - 0.7868772327899933
  - 0.9401237845420838
  - 0.8187532842159272
  - 0.7737321019172669
  - 0.794364219903946
  - 0.8413259983062744
  - 0.9741204023361206
  - 0.818936288356781
  - 0.8064845204353333
  - 0.7774416089057923
  - 0.7590337812900544
  validation_losses:
  - 0.3942202627658844
  - 0.39324951171875
  - 0.410957008600235
  - 0.39952415227890015
  - 0.398433655500412
  - 0.39362069964408875
  - 0.40257737040519714
  - 0.39334264397621155
  - 0.4011217951774597
  - 0.41474008560180664
  - 0.3961260914802551
  - 0.40153709053993225
  - 0.401163786649704
  - 0.44418588280677795
  - 0.39073315262794495
  - 0.4206647276878357
  - 0.4515638053417206
  - 0.4006233811378479
  - 0.4107920527458191
  - 0.43935298919677734
  - 0.5047910213470459
  - 0.3981266915798187
  - 0.3849496841430664
  - 0.38663846254348755
  - 0.3921396732330322
  - 0.41659098863601685
  - 0.39170533418655396
  - 0.40679091215133667
  - 0.3903024196624756
  - 0.39097416400909424
  - 0.39078786969184875
  - 0.3876704275608063
  - 0.3840678632259369
  - 0.38971146941185
loss_records_fold2:
  train_losses:
  - 0.8049262404441834
  - 0.8240151286125184
  - 0.8412054121494293
  - 0.8368381559848785
  - 0.8121138572692872
  - 0.8103366494178772
  - 0.7865910232067108
  - 0.7885239779949189
  - 0.785546374320984
  - 0.8981929004192353
  - 0.8025982081890106
  - 0.8076473951339722
  - 0.8129989206790924
  - 0.8436522066593171
  - 0.8146957039833069
  - 0.9312279045581818
  - 0.7941470623016358
  - 0.7677321791648866
  - 0.8042470932006837
  validation_losses:
  - 0.394266277551651
  - 0.47822311520576477
  - 0.6188092827796936
  - 0.3851078152656555
  - 0.39424219727516174
  - 0.37848079204559326
  - 0.38843533396720886
  - 0.38307687640190125
  - 0.4075414836406708
  - 0.41391482949256897
  - 0.4268709123134613
  - 0.3821398913860321
  - 0.39796480536460876
  - 0.38912132382392883
  - 0.3891948163509369
  - 0.3877885043621063
  - 0.38937920331954956
  - 0.39050132036209106
  - 0.3918374478816986
loss_records_fold3:
  train_losses:
  - 0.7947426199913026
  - 0.8348031163215638
  - 0.8560886085033417
  - 0.8015355587005616
  - 0.871275144815445
  - 0.8191519916057587
  - 0.8335333287715913
  - 0.788399088382721
  - 0.8993439316749573
  - 0.8474941790103913
  - 0.762740296125412
  - 0.7790727615356445
  - 0.7854489505290986
  - 0.7860301375389099
  - 0.807311624288559
  - 0.7791065216064453
  - 0.7586034923791886
  - 0.8276530623435975
  - 0.8221290588378907
  - 0.790680354833603
  - 0.750821927189827
  - 0.8149975538253784
  - 0.7585669755935669
  - 0.7788902163505554
  - 0.8056513190269471
  - 0.8338498175144196
  - 0.769234311580658
  - 0.8175894439220429
  - 0.7711219549179078
  - 0.8065916419029237
  - 0.7982602715492249
  - 0.7890559017658234
  - 0.7839521825313569
  - 0.8452047646045685
  - 0.8169055223464966
  - 0.781552952528
  - 0.7851444184780121
  - 0.7761001884937286
  - 0.7792906522750855
  - 0.7709018468856812
  - 0.7691653668880463
  - 0.7792801916599275
  - 0.7778825581073762
  - 0.743866926431656
  - 0.7695411086082459
  - 0.7624674081802368
  - 0.8034986436367035
  - 0.8228334307670594
  - 0.8214291274547577
  - 0.7957295656204224
  - 0.7945614755153656
  - 0.80159672498703
  - 0.771083265542984
  - 0.8019844770431519
  - 0.7686336755752564
  - 0.7930203378200531
  - 0.7395080894231797
  - 0.8271167993545533
  - 0.785104376077652
  - 0.8473769664764405
  - 0.7954335808753967
  - 0.7851897299289704
  - 0.8213537037372589
  - 0.7850120186805726
  - 0.7986276090145111
  - 0.7763631761074067
  - 0.8160529911518097
  - 0.7851186096668243
  - 0.9345925450325012
  - 0.7848797500133515
  - 0.8026444315910339
  - 0.7772470712661743
  - 0.7964396357536316
  - 0.7618543446063996
  - 0.8453226268291474
  - 0.8245889723300934
  - 0.808040988445282
  - 0.8455011129379273
  - 0.8106595754623414
  - 0.8049512147903443
  - 0.8028253972530366
  - 0.7768321633338928
  - 0.8092380166053772
  - 0.8367313325405121
  - 0.8131180882453919
  - 0.7885502934455872
  - 0.8234431505203248
  - 0.799778163433075
  - 0.7753592312335968
  - 0.8083007693290711
  - 0.8389660716056824
  - 0.816455352306366
  - 0.777899968624115
  - 0.798033756017685
  - 0.8114751636981965
  - 0.8008808851242066
  - 0.7783350884914398
  - 0.780498132109642
  - 0.7869003474712373
  - 0.787631231546402
  validation_losses:
  - 1.1334784030914307
  - 1.1919194459915161
  - 3.177159070968628
  - 0.3638913631439209
  - 0.3679521083831787
  - 0.41115602850914
  - 0.571821391582489
  - 0.9098254442214966
  - 0.661463737487793
  - 0.7799121737480164
  - 1.3572018146514893
  - 1.111923098564148
  - 0.8465596437454224
  - 1.146576166152954
  - 0.9704812169075012
  - 0.5786471366882324
  - 0.4948872923851013
  - 0.6700341701507568
  - 0.5706669092178345
  - 0.409205824136734
  - 0.5621623992919922
  - 0.49465152621269226
  - 2.779461622238159
  - 0.3953486680984497
  - 3.747272253036499
  - 0.4292239248752594
  - 0.5363807082176208
  - 0.7889005541801453
  - 0.6280450820922852
  - 0.6450355648994446
  - 0.3796795904636383
  - 0.6580986976623535
  - 0.4840307831764221
  - 0.6877949833869934
  - 1.329616904258728
  - 2.739107608795166
  - 6.519348621368408
  - 1.547614336013794
  - 0.8572365641593933
  - 0.7932143211364746
  - 0.6408642530441284
  - 0.8433083295822144
  - 3.5187249183654785
  - 0.45218613743782043
  - 0.6236460208892822
  - 0.5002727508544922
  - 1.1020036935806274
  - 1.2318809032440186
  - 0.395174503326416
  - 0.38156139850616455
  - 0.36536887288093567
  - 0.3628781735897064
  - 0.38142144680023193
  - 0.5427348613739014
  - 0.42999202013015747
  - 2.077119827270508
  - 2.190161943435669
  - 0.6634066104888916
  - 0.6016673445701599
  - 0.5628831386566162
  - 0.44892236590385437
  - 0.7083523273468018
  - 0.39141085743904114
  - 0.39548712968826294
  - 0.39540496468544006
  - 1.0827652215957642
  - 1.335077166557312
  - 0.6703813076019287
  - 0.39050552248954773
  - 0.41174814105033875
  - 0.37820589542388916
  - 0.3743025064468384
  - 0.3776891827583313
  - 0.5553783178329468
  - 0.40519705414772034
  - 0.3893430233001709
  - 0.3979268968105316
  - 0.3792850375175476
  - 0.3928740322589874
  - 0.4023550748825073
  - 0.3771391808986664
  - 0.3915461599826813
  - 0.42105811834335327
  - 0.3754860460758209
  - 0.4208904802799225
  - 0.39167970418930054
  - 0.4357168674468994
  - 0.4472026228904724
  - 0.561816930770874
  - 0.4239387810230255
  - 0.3796760141849518
  - 0.3755413293838501
  - 0.3781893253326416
  - 0.3838840425014496
  - 0.48344242572784424
  - 0.5774364471435547
  - 0.42080381512641907
  - 0.4607570171356201
  - 0.5131301283836365
  - 0.5637680888175964
loss_records_fold4:
  train_losses:
  - 0.819525557756424
  - 0.781006121635437
  - 0.7568257689476013
  - 0.848102080821991
  - 0.80811026096344
  - 0.780213338136673
  - 0.762470144033432
  - 0.8033051133155823
  - 0.7911361694335938
  - 0.7800282806158066
  - 0.7780008852481842
  - 0.8151367604732513
  - 0.7952449440956116
  - 0.7769778788089753
  - 0.8319322168827057
  - 0.7645259499549866
  - 0.7759525775909424
  - 0.8058169484138489
  - 0.7657220840454102
  - 0.7654985308647156
  - 0.7789315462112427
  - 0.8011752068996429
  - 0.8214272320270539
  - 0.8119082510471345
  - 0.7456420630216599
  - 0.7993788063526154
  - 0.7740474998950959
  - 0.7432500988245011
  - 0.8039445817470551
  - 0.8032728135585785
  - 0.8292231142520905
  - 0.8108303010463715
  - 0.7750504732131959
  - 0.7428578168153763
  - 0.7766486883163453
  - 0.7847799003124237
  - 0.7842314004898072
  - 0.7465960681438446
  - 0.78993598818779
  - 0.7950118243694306
  - 0.8107336461544037
  - 0.8111834526062012
  - 0.7775044023990632
  - 0.7938448011875153
  - 0.8273650050163269
  - 0.7928485453128815
  - 0.7993791043758393
  - 0.7779751062393189
  - 0.7916777729988098
  - 0.7758481860160829
  - 0.7621806323528291
  validation_losses:
  - 0.3781856596469879
  - 0.3706014156341553
  - 0.3828895092010498
  - 0.38477474451065063
  - 0.4016687572002411
  - 0.4011306166648865
  - 0.4041890799999237
  - 0.40474674105644226
  - 0.3673465847969055
  - 0.4268002510070801
  - 0.3798615634441376
  - 0.3695157468318939
  - 0.36767807602882385
  - 0.3781467378139496
  - 0.39754191040992737
  - 0.37734901905059814
  - 0.39196422696113586
  - 0.4121433198451996
  - 0.3645249605178833
  - 0.3941834568977356
  - 0.3914668560028076
  - 0.3764180541038513
  - 0.3698386549949646
  - 0.39094868302345276
  - 0.37091490626335144
  - 0.37244585156440735
  - 0.3757613003253937
  - 0.366535484790802
  - 0.4144997000694275
  - 0.36905574798583984
  - 0.36750704050064087
  - 0.3714491128921509
  - 0.4014350175857544
  - 0.3727778494358063
  - 0.38464972376823425
  - 0.3972141146659851
  - 0.37137550115585327
  - 0.36359408497810364
  - 0.37425071001052856
  - 0.37335404753685
  - 0.3688611388206482
  - 0.41187191009521484
  - 0.38075876235961914
  - 0.38720542192459106
  - 0.3985307216644287
  - 0.39794278144836426
  - 0.38930991291999817
  - 0.37811166048049927
  - 0.3780750334262848
  - 0.384380966424942
  - 0.38036343455314636
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.855917667238422, 0.8507718696397941,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.0, 0.023255813953488372, 0.13861386138613863, 0.04597701149425288]'
  mean_eval_accuracy: 0.8558687438477361
  mean_f1_accuracy: 0.04156933736677597
  total_train_time: '0:18:33.522978'
