config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:01:41.886009'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_40fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_40fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_40fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_40fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_40fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_40fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_40fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_40fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_40fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_40fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 56.54937029182911
  - 41.03282095938921
  - 28.11997882127762
  - 20.697155338525775
  - 16.533152893558146
  - 13.798532021045686
  - 12.89139193147421
  - 10.368198119103909
  - 13.152754342556001
  - 11.422417637705804
  - 10.17774071097374
  - 11.277699419856072
  - 11.023955762386322
  - 10.300048416852952
  - 11.060519564151765
  - 11.840034365653992
  - 17.250298473238946
  - 8.92555492222309
  - 12.325868636369705
  - 8.975951200723648
  validation_losses:
  - 4.0580735206604
  - 2.5160133838653564
  - 0.4874754548072815
  - 0.7191170454025269
  - 0.5937378406524658
  - 0.40143120288848877
  - 0.7375020384788513
  - 0.5045859813690186
  - 0.39291495084762573
  - 1.162603497505188
  - 0.45738041400909424
  - 0.620002269744873
  - 0.5165954232215881
  - 1.2174396514892578
  - 0.46468430757522583
  - 0.39577415585517883
  - 0.3811962604522705
  - 0.386626273393631
  - 0.394361287355423
  - 0.3957023322582245
loss_records_fold1:
  train_losses:
  - 9.028363445401192
  - 6.451920427381992
  - 9.217483642697335
  - 8.53457642197609
  - 7.470074523985386
  - 8.80430483520031
  - 6.226936462521554
  - 6.189263898134232
  - 7.081778332591057
  - 6.353740265965462
  - 7.394714805483819
  - 6.3623336344957355
  - 6.166414371132851
  - 6.436895313858987
  - 5.811394050717354
  - 6.167686766386033
  - 6.587879619002343
  - 7.056616401672364
  - 6.130480188131333
  - 5.795477318763734
  - 6.049287953972817
  - 5.841351856291294
  - 5.859850892424584
  - 6.212230117619038
  - 6.044801792502404
  - 6.4708304643630985
  - 5.814005199074746
  - 5.673546773195267
  - 5.588018390536309
  - 5.644706521928311
  - 5.659439891576767
  - 5.716889026761056
  - 5.837069600820541
  - 5.995400175452232
  - 5.748750501871109
  - 6.372492387890816
  - 5.831055926531554
  - 5.623666390776634
  - 5.704483976960183
  - 5.817911049723626
  - 6.138344582915306
  - 5.992944747209549
  - 5.737048628926278
  - 5.772565057873726
  - 5.559865555167199
  - 5.539426305890084
  - 5.9010810434818275
  - 5.674537861347199
  - 6.056104764342308
  - 5.779077582061291
  - 6.010466402769089
  - 5.802166432142258
  - 5.755790269374848
  - 5.799657341837883
  - 5.8465911746025085
  - 5.712710419297219
  - 5.873302192986012
  - 5.739922207593918
  - 5.642194667458535
  - 5.905491769313812
  - 6.213308238983155
  - 6.567595431208611
  - 6.610463005304337
  - 6.893338987231255
  - 6.527124801278115
  - 6.21371800005436
  - 6.443537305295468
  - 6.269866415858269
  - 6.152522987127305
  - 5.992426565289498
  - 6.140090174973011
  - 6.191687846183777
  - 5.988737565279007
  - 6.366649568080902
  - 6.360857769846916
  - 5.986415013670921
  - 5.892340975999833
  - 6.0893046230077745
  - 6.195856688916684
  - 5.8300406485795975
  - 6.007540205121041
  - 5.863477098941804
  - 5.978067213296891
  - 5.7542821109294895
  - 5.915050527453423
  - 5.857805228233338
  - 5.9740038335323336
  - 6.136017927527428
  - 5.911384364962578
  - 5.957056465744973
  - 11.259427148103715
  - 6.654383352398873
  - 6.001957684755325
  - 6.267844209074974
  - 5.978782695531845
  - 6.122212234139443
  - 6.032040053606034
  - 5.934561428427696
  - 6.25440169274807
  - 6.653508657217026
  validation_losses:
  - 0.39874163269996643
  - 0.40093716979026794
  - 1.452195644378662
  - 0.4714331030845642
  - 0.43800047039985657
  - 0.39998549222946167
  - 0.45919686555862427
  - 0.39607876539230347
  - 0.3972102105617523
  - 0.4461546540260315
  - 7.264595031738281
  - 0.39717942476272583
  - 0.8387991786003113
  - 0.4572434723377228
  - 0.4670139253139496
  - 0.451914519071579
  - 1.2733768224716187
  - 0.4206266701221466
  - 0.4657391309738159
  - 0.616714596748352
  - 0.40086081624031067
  - 0.4090096652507782
  - 0.40035557746887207
  - 0.44502419233322144
  - 0.7884981036186218
  - 1.4430925846099854
  - 0.39684823155403137
  - 0.3973754048347473
  - 0.9728999733924866
  - 0.459509938955307
  - 0.4314345121383667
  - 0.4850451350212097
  - 0.48641324043273926
  - 0.6608862280845642
  - 0.6750279068946838
  - 0.39886659383773804
  - 2.610964775085449
  - 0.4426721930503845
  - 0.4077214300632477
  - 0.41660216450691223
  - 0.4082280695438385
  - 0.4116031527519226
  - 0.42482516169548035
  - 0.3950478136539459
  - 0.48924964666366577
  - 0.5056298971176147
  - 2.377830743789673
  - 0.4568306803703308
  - 0.4000815749168396
  - 0.4076119363307953
  - 0.408284068107605
  - 0.6497089862823486
  - 0.3964792788028717
  - 0.44829684495925903
  - 0.3964267671108246
  - 0.5764171481132507
  - 0.49494457244873047
  - 0.40150707960128784
  - 0.45013535022735596
  - 0.44471749663352966
  - 120.19589233398438
  - 0.4311678111553192
  - 0.4057728052139282
  - 0.41472458839416504
  - 0.43765491247177124
  - 0.41318240761756897
  - 44466.26953125
  - 0.4887365698814392
  - 743.5993041992188
  - 0.4103212058544159
  - 0.40501704812049866
  - 0.5333773493766785
  - 0.40562334656715393
  - 0.423179030418396
  - 0.4468505382537842
  - 1006.08984375
  - 0.4023215174674988
  - 0.41180846095085144
  - 0.42696690559387207
  - 0.41328006982803345
  - 0.4143070876598358
  - 0.4068859815597534
  - 0.41118714213371277
  - 0.43394359946250916
  - 1317.6798095703125
  - 0.4198459982872009
  - 0.44108957052230835
  - 0.43237602710723877
  - 0.48836323618888855
  - 0.45109012722969055
  - 0.41348493099212646
  - 0.4819270670413971
  - 0.40665334463119507
  - 0.44080835580825806
  - 0.42030221223831177
  - 0.41932153701782227
  - 0.41702544689178467
  - 0.4170173704624176
  - 0.4542796015739441
  - 0.41676631569862366
loss_records_fold2:
  train_losses:
  - 6.308727872371674
  - 6.21270134150982
  - 6.714247849583626
  - 6.1675047218799595
  - 5.693492552638054
  - 5.953663425147534
  - 6.40524485707283
  - 5.7923761963844305
  - 5.905499610304833
  - 5.855051824450493
  - 5.931865066289902
  - 5.899515157938004
  - 6.133892393112183
  - 6.271795991063119
  - 6.030197930335999
  - 6.38911084830761
  - 5.935563147068024
  - 5.708508631587029
  - 5.730282503366471
  - 5.83868414759636
  - 6.3890468895435335
  - 6.007173982262612
  validation_losses:
  - 0.5906425714492798
  - 0.38682469725608826
  - 0.3823959231376648
  - 0.38360580801963806
  - 0.37892788648605347
  - 0.389358252286911
  - 0.3906000256538391
  - 0.3867228031158447
  - 0.43054065108299255
  - 0.3799397647380829
  - 0.3996035158634186
  - 0.4835185408592224
  - 0.4139370918273926
  - 0.4249410629272461
  - 0.3870987296104431
  - 0.4733235538005829
  - 0.3828427791595459
  - 0.3860630393028259
  - 0.39413103461265564
  - 0.3905724287033081
  - 0.39805859327316284
  - 0.38313767313957214
loss_records_fold3:
  train_losses:
  - 5.752519124746323
  - 5.7744688421487815
  - 5.9663801252841955
  - 5.954159191250802
  - 5.87225248515606
  - 5.864633506536484
  - 5.703622898459435
  - 5.7173260957002645
  - 5.715375253558159
  - 6.339765113592148
  - 6.162160441279411
  - 5.830056369304657
  - 6.042514061927796
  - 6.5022579580545425
  - 5.941532862186432
  - 6.156376016139984
  - 6.062003871798516
  - 5.843246129155159
  - 5.875025935471058
  - 5.906219455599786
  - 5.9450347274541855
  - 5.906213921308518
  - 6.120304524898529
  - 5.832951974868775
  - 6.013878259062768
  - 6.964472937583924
  - 6.066566631197929
  - 5.9205484092235565
  - 5.956820791959763
  - 6.000546023249626
  - 5.853413873910904
  - 5.963871654868126
  - 6.02057290673256
  - 6.1273091614246376
  - 5.888148257136345
  - 5.9669047594070435
  - 5.910845950245857
  - 5.835436150431633
  - 6.011975759267807
  - 6.081060203909875
  - 5.981236031651497
  - 6.038842219114304
  - 5.933893868327141
  - 6.358021834492684
  - 6.034995764493942
  - 5.825393453240395
  - 5.964932450652123
  - 5.836850786209107
  - 5.823625341057777
  - 5.905763110518456
  - 6.165638494491578
  - 5.864810663461686
  - 5.846654200553894
  - 5.9066011846065525
  - 5.851735591888428
  - 6.140941265225411
  - 5.77804408967495
  - 5.827173730731011
  - 6.047967398166657
  - 5.947609603404999
  - 6.196249544620514
  - 5.9204115062952045
  - 5.945046818256379
  - 5.9389391362667086
  - 5.830722478032112
  - 5.974543100595475
  - 5.7938438862562185
  - 5.835592123866082
  - 6.038398450613022
  - 6.045331948995591
  - 5.7990022808313375
  - 5.988494408130646
  - 5.889988818764687
  - 5.830037385225296
  - 5.900954651832581
  - 5.878534650802613
  - 5.8644547969102865
  - 5.8780894666910175
  - 6.0539550423622135
  - 5.917633414268494
  - 5.933232896029949
  - 5.955600407719612
  - 6.2784473419189455
  - 5.889391168951988
  - 6.018495830893517
  - 6.202232809364796
  - 5.908745735883713
  - 5.95389648526907
  - 6.001271012425423
  - 6.05546864271164
  - 6.073110666871071
  - 5.898333171010018
  - 5.925261819362641
  - 5.9454114019870765
  - 5.9332002565264705
  - 5.959019382297993
  - 5.805870568752289
  - 5.838377076387406
  - 5.955255983769894
  - 5.850797072052956
  validation_losses:
  - 0.42617326974868774
  - 0.3758440911769867
  - 0.4398200809955597
  - 0.408023476600647
  - 0.4295477569103241
  - 0.41062456369400024
  - 0.3895508646965027
  - 0.3897673189640045
  - 0.4126026928424835
  - 0.5365490913391113
  - 0.4305494427680969
  - 0.4208680987358093
  - 0.6029587984085083
  - 0.3935893476009369
  - 0.4600139260292053
  - 183.0963592529297
  - 574235.8125
  - 3329310976.0
  - 3573582592.0
  - 2972488192.0
  - 3662308352.0
  - 3593967360.0
  - 3605900800.0
  - 3583625984.0
  - 3534599936.0
  - 0.40624889731407166
  - 0.38593074679374695
  - 0.4737721085548401
  - 16.890220642089844
  - 262585280.0
  - 279475290112.0
  - 26248646656.0
  - 18662664192.0
  - 186736279552.0
  - 22059936.0
  - 199852244992.0
  - 159515492352.0
  - 343345397760.0
  - 308396752896.0
  - 176084500480.0
  - 786994102272.0
  - 685164199936.0
  - 52108738560.0
  - 9605626880.0
  - 20883718144.0
  - 43564617728.0
  - 302637219840.0
  - 435601178624.0
  - 74371203072.0
  - 32694452224.0
  - 46574817280.0
  - 63068540928.0
  - 219928150016.0
  - 173320847360.0
  - 55354769408.0
  - 400457302016.0
  - 283950841856.0
  - 62198501376.0
  - 55211696128.0
  - 24540796.0
  - 23976196.0
  - 225674428416.0
  - 50188726272.0
  - 1233609728.0
  - 28023183360.0
  - 11533840384.0
  - 225207746560.0
  - 292725751808.0
  - 109345251328.0
  - 74446995456.0
  - 247067934720.0
  - 275903643648.0
  - 81462321152.0
  - 244123615232.0
  - 62977929216.0
  - 11073696.0
  - 11361235.0
  - 14972630.0
  - 15380052.0
  - 4317597.0
  - 11043679.0
  - 19365886.0
  - 11562711.0
  - 1122407.0
  - 4181203.0
  - 10058551.0
  - 23381884.0
  - 17190132.0
  - 14193687.0
  - 11532431.0
  - 19209236.0
  - 9325664.0
  - 17553596.0
  - 16026064.0
  - 18155748.0
  - 15832554.0
  - 19105080.0
  - 14816564.0
  - 14504898.0
  - 14452993.0
loss_records_fold4:
  train_losses:
  - 5.945731389522553
  - 6.005388310551644
  - 5.91319834291935
  - 6.312580865621567
  - 6.017266097664834
  - 5.89997842013836
  - 5.880349609255791
  - 5.901334598660469
  - 6.239698380231857
  - 6.003944432735444
  - 5.835677435994149
  validation_losses:
  - 4842875.0
  - 10008509.0
  - 7738803.5
  - 0.46025195717811584
  - 1276952576000.0
  - 33858120.0
  - 0.3913500905036926
  - 0.39093703031539917
  - 0.3966742157936096
  - 0.39434143900871277
  - 0.39111390709877014
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8593481989708405, 0.14065180102915953,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.24661654135338348, 0.0]'
  mean_eval_accuracy: 0.7145314259105351
  mean_f1_accuracy: 0.0493233082706767
  total_train_time: '0:26:51.132735'
