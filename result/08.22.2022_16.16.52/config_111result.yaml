config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:46:02.337245'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_111fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_111fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_111fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_111fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_111fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_111fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_111fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_111fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_111fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_111fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 11.253706455230713
  - 11.733119535446168
  - 11.42783204317093
  - 2.8562030076980593
  - 4.469566285610199
  - 4.9039830327034
  - 2.0354137420654297
  - 2.2841300398111346
  - 1.648856157064438
  - 1.6128702044487
  - 1.5424972712993623
  - 1.6458506703376772
  - 1.9617658615112306
  - 11.56911889910698
  - 3.3365564823150637
  - 1.93229056596756
  - 4.27477594614029
  - 3.450580501556397
  - 2.716760802268982
  - 1.5766094088554383
  - 1.2192363977432252
  - 5.12677441239357
  - 3.0840941190719606
  - 5.631521701812744
  - 2.35444410443306
  - 1.3602395832538605
  - 3.8052610218524934
  - 7.485826671123505
  - 3.1640048742294313
  - 1.6330853760242463
  - 1.3904072046279907
  - 1.6137332201004029
  - 1.2562420964241028
  - 1.28606134057045
  - 1.7558942914009095
  - 8.7835129737854
  - 2.935938143730164
  - 1.5674435675144196
  - 1.559781622886658
  - 1.0969967305660249
  - 1.6564451575279238
  - 13.72010169029236
  - 4.657375103235245
  - 2.6990780234336853
  - 2.0375910699367523
  - 2.302565622329712
  - 1.9663417398929597
  - 3.28399358689785
  - 1.2872422754764559
  - 1.7121182203292848
  - 10.196852278709413
  - 1.4117251574993135
  - 15.143871474266053
  - 2.558175277709961
  - 3.2291254818439485
  - 1.422363168001175
  - 5.808503466844559
  - 2.3892043471336364
  - 1.85205694437027
  - 1.7849995970726014
  - 1.461450880765915
  - 1.316586571931839
  - 1.7038999855518342
  - 1.136989802122116
  - 1.0032562792301178
  - 0.894016456604004
  - 0.9855754911899567
  - 0.9431216776371003
  - 0.8813641965389252
  - 0.8707287073135377
  - 1.0635964035987855
  - 1.1212607860565187
  validation_losses:
  - 7.143093585968018
  - 1.3335403203964233
  - 1.555567979812622
  - 1.0032899379730225
  - 0.7008394598960876
  - 0.8871555328369141
  - 0.8251599669456482
  - 0.7955561280250549
  - 0.6105440258979797
  - 0.6789939999580383
  - 0.6832571029663086
  - 0.6367087364196777
  - 0.5347058773040771
  - 2.0874881744384766
  - 1.4389400482177734
  - 1.4298839569091797
  - 1.3713048696517944
  - 2.002112627029419
  - 0.49414366483688354
  - 0.5678439736366272
  - 0.4499185085296631
  - 0.6320667862892151
  - 0.844100296497345
  - 1.0356608629226685
  - 0.5953543782234192
  - 0.4220416843891144
  - 0.4392445981502533
  - 0.6398887038230896
  - 0.5385034680366516
  - 0.7727522253990173
  - 0.5147068500518799
  - 0.4690600633621216
  - 0.4187759459018707
  - 0.6315000057220459
  - 0.43927881121635437
  - 0.5387601852416992
  - 0.4747547507286072
  - 0.4871199131011963
  - 0.4363657534122467
  - 0.38618719577789307
  - 0.4455723464488983
  - 0.5521076321601868
  - 0.5625175833702087
  - 0.5203787684440613
  - 0.43444234132766724
  - 0.531353771686554
  - 0.5852984189987183
  - 0.5903804898262024
  - 0.429599791765213
  - 0.4416085183620453
  - 0.4398805797100067
  - 0.4309544861316681
  - 0.43778157234191895
  - 0.4863470792770386
  - 0.40777379274368286
  - 0.49519965052604675
  - 0.524138867855072
  - 0.43674612045288086
  - 0.5155689120292664
  - 0.562296986579895
  - 0.5608673095703125
  - 0.42635422945022583
  - 0.4139900803565979
  - 0.4061187505722046
  - 0.3997584283351898
  - 0.4146967828273773
  - 0.4091830849647522
  - 0.39283865690231323
  - 0.3876497745513916
  - 0.3937522768974304
  - 0.38685569167137146
  - 0.395567387342453
loss_records_fold1:
  train_losses:
  - 0.823984831571579
  - 0.8417250603437424
  - 0.8221725344657899
  - 0.8947402000427247
  - 0.8409354627132416
  - 0.9004853785037995
  - 0.8349265158176422
  - 0.9850431084632874
  - 0.916196346282959
  - 0.9734741091728211
  - 0.8372033417224884
  - 0.8682881772518158
  - 0.8596988499164582
  - 0.8943494558334351
  - 0.8511039972305299
  - 0.8780429422855378
  - 1.476007503271103
  - 1.4637168765068056
  - 0.8524570286273957
  - 0.9616126298904419
  - 0.844977754354477
  - 0.9903667032718659
  - 0.9962349116802216
  - 0.9206310629844666
  - 0.9772772789001465
  - 1.1435338020324708
  - 0.8709053218364716
  - 0.8566359519958496
  - 0.8780097961425781
  validation_losses:
  - 0.48401278257369995
  - 0.41127851605415344
  - 0.4380660355091095
  - 0.4036756455898285
  - 0.3982090353965759
  - 0.4006194472312927
  - 0.42514336109161377
  - 0.4266056716442108
  - 0.39499953389167786
  - 0.40113818645477295
  - 0.4188041090965271
  - 0.43721872568130493
  - 0.4034186601638794
  - 0.400228351354599
  - 0.43207529187202454
  - 0.4077164828777313
  - 0.6901593804359436
  - 0.40888848900794983
  - 0.39999473094940186
  - 0.4099845290184021
  - 0.4119035005569458
  - 0.4061848223209381
  - 0.4330489933490753
  - 0.4050265848636627
  - 0.4052248001098633
  - 0.4031755030155182
  - 0.40717044472694397
  - 0.40786588191986084
  - 0.4060748219490051
loss_records_fold2:
  train_losses:
  - 1.981205976009369
  - 3.5597335636615757
  - 2.254584455490112
  - 1.168835461139679
  - 1.1784025311470032
  - 0.955794644355774
  - 1.13091179728508
  - 2.8345852315425875
  - 1.1790439426898958
  - 0.9411166608333588
  - 1.212120032310486
  - 0.8927999138832092
  - 0.9569634258747102
  - 0.918183720111847
  - 0.9932692646980286
  - 0.9567902743816377
  - 1.08716778755188
  - 1.416229385137558
  - 1.149080115556717
  - 1.0891867220401765
  - 0.9913689434528351
  - 1.029717755317688
  - 4.519371813535691
  - 1.0668744325637818
  - 3.9618066906929017
  - 6.87089536190033
  - 2.3847080290317537
  - 1.5534208655357362
  - 1.0494667768478394
  - 0.9936726689338684
  - 0.9067224979400635
  - 3.1568899512290955
  - 1.6721685767173768
  - 0.8936660885810852
  - 0.8676008522510529
  - 0.8451899647712708
  - 0.8470921993255616
  - 0.9426632642745972
  - 0.9313604772090912
  - 0.839823877811432
  - 1.2748634815216064
  - 0.977988064289093
  - 0.8795261800289155
  - 0.9472707867622376
  - 0.958021605014801
  - 0.9212878167629243
  - 0.9454078793525696
  - 3.0992690920829773
  - 0.978515499830246
  - 0.9569842517375946
  - 1.0634234130382538
  - 1.0956342160701753
  - 1.190858179330826
  - 0.907793664932251
  - 0.9675816595554352
  - 0.8826468169689179
  - 0.8813381552696229
  - 0.8600107491016389
  - 0.8306654632091522
  - 0.8467909455299378
  - 0.8279766023159028
  validation_losses:
  - 0.4001142978668213
  - 0.39203527569770813
  - 0.6454206109046936
  - 0.40365466475486755
  - 0.3778018355369568
  - 0.3977757692337036
  - 0.3878647983074188
  - 0.39530184864997864
  - 0.45796918869018555
  - 0.39033547043800354
  - 0.41244399547576904
  - 0.41284599900245667
  - 0.39980167150497437
  - 0.37854334712028503
  - 0.38939592242240906
  - 0.3805786073207855
  - 0.40646490454673767
  - 0.4338008463382721
  - 0.41089683771133423
  - 0.4025689661502838
  - 0.3806959390640259
  - 0.3934065103530884
  - 0.3984242379665375
  - 0.45153599977493286
  - 0.39439857006073
  - 0.3967590928077698
  - 0.5146080851554871
  - 0.4028203785419464
  - 0.39337489008903503
  - 0.4048997461795807
  - 0.3999737501144409
  - 0.4134635329246521
  - 0.41487985849380493
  - 0.40071865916252136
  - 0.38758429884910583
  - 0.3887074887752533
  - 0.4031786620616913
  - 0.3847287595272064
  - 0.4096316695213318
  - 0.3932705521583557
  - 0.41531530022621155
  - 0.3908703625202179
  - 0.41575026512145996
  - 0.3947761058807373
  - 0.4051266610622406
  - 0.39969924092292786
  - 0.39902493357658386
  - 0.39167389273643494
  - 0.3945715129375458
  - 0.418852835893631
  - 0.42403778433799744
  - 0.47149476408958435
  - 0.4014036953449249
  - 0.3847096860408783
  - 0.39788818359375
  - 0.39802372455596924
  - 0.3947289288043976
  - 0.39054179191589355
  - 0.38751882314682007
  - 0.38475343585014343
  - 0.3885190486907959
loss_records_fold3:
  train_losses:
  - 0.8645329236984254
  - 0.8593939065933228
  - 0.8599527299404145
  - 0.9219786465168
  - 1.0567418038845062
  - 0.8754294514656067
  - 0.863854169845581
  - 0.7914531856775284
  - 1.0335032224655152
  - 0.9821500420570374
  - 0.8780869603157044
  - 0.8884536921977997
  - 1.3020699977874757
  - 0.9346608042716981
  - 1.8952532887458802
  - 0.8254233539104462
  - 1.3125674664974214
  - 0.8305590510368348
  - 0.807965311408043
  - 0.8134372234344482
  - 0.8445565044879914
  - 0.8269020557403565
  - 0.8630876362323762
  - 1.1705267667770387
  - 0.8357822239398957
  - 0.8420075416564942
  validation_losses:
  - 0.3955962359905243
  - 0.43171513080596924
  - 0.41173264384269714
  - 0.40109238028526306
  - 0.4421702027320862
  - 0.39481469988822937
  - 0.39374932646751404
  - 0.4070154130458832
  - 0.41341960430145264
  - 0.41845017671585083
  - 0.43016061186790466
  - 0.4157218337059021
  - 0.4171920418739319
  - 0.44948092103004456
  - 0.43367239832878113
  - 0.3943706154823303
  - 0.4121547341346741
  - 0.40963098406791687
  - 0.398499995470047
  - 0.41104546189308167
  - 0.40692102909088135
  - 0.4025050401687622
  - 0.40761664509773254
  - 0.41654056310653687
  - 0.3957808017730713
  - 0.401284396648407
loss_records_fold4:
  train_losses:
  - 0.8593460202217102
  - 0.8003485083580018
  - 1.4836662590503693
  - 0.8861512601375581
  - 0.8347121000289918
  - 0.8418222963809967
  - 0.8257608413696289
  - 0.8567042350769043
  - 0.8022892057895661
  - 0.8419274806976319
  - 0.8808671176433563
  - 1.8164822816848756
  - 0.966117686033249
  - 0.9912664234638214
  - 0.9991456985473633
  - 0.8798439204692841
  - 0.883833372592926
  - 0.9061284005641937
  - 0.8482304275035859
  - 0.8917392313480378
  - 0.807653546333313
  - 0.8376061260700226
  validation_losses:
  - 0.408454954624176
  - 0.443620890378952
  - 0.43582385778427124
  - 0.4090766906738281
  - 0.41799718141555786
  - 0.3946762681007385
  - 0.40605804324150085
  - 0.4011482298374176
  - 0.3949824273586273
  - 0.42959773540496826
  - 0.4010981321334839
  - 0.4134581387042999
  - 0.4845196306705475
  - 0.45399922132492065
  - 0.4308280944824219
  - 0.44774091243743896
  - 0.42391711473464966
  - 0.4152916371822357
  - 0.4079512655735016
  - 0.4145565927028656
  - 0.41180619597435
  - 0.42002367973327637
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 72 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 61 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:18:23.215142'
