config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 20:07:16.989019'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/64/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/64/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/64/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/64/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/64/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/64/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/64/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/64/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/64/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/64/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 28.16054819524288
  - 28.28563031554222
  - 28.049479573965073
  - 28.093888610601425
  - 27.87221710383892
  - 28.23236818611622
  - 27.911671996116638
  - 27.995926842093468
  - 27.997108548879623
  - 28.01650659739971
  - 28.006278097629547
  - 28.445188462734222
  - 28.31626896560192
  - 27.783930629491806
  - 27.916720002889633
  - 27.690888538956642
  - 27.598475635051727
  - 27.68851725757122
  - 27.555982321500778
  - 27.909695401787758
  - 27.769556269049644
  - 28.10151097178459
  - 27.664094045758247
  - 27.537794068455696
  - 27.830616533756256
  - 27.629578083753586
  - 27.506901770830154
  - 27.512268126010895
  - 27.562346950173378
  - 27.5881737023592
  - 27.647732079029083
  - 28.14191125333309
  - 27.46275518834591
  - 27.368612810969353
  - 27.846491038799286
  - 27.637800455093384
  - 27.739067047834396
  - 27.67284271121025
  - 27.75382386147976
  - 27.875264301896095
  - 27.29533475637436
  - 27.942075297236443
  - 27.52983270585537
  - 27.404881477355957
  - 27.754270181059837
  - 28.35158309340477
  - 27.477167189121246
  - 27.583036065101624
  - 27.51480634510517
  - 27.688218414783478
  - 27.794758945703506
  - 27.4033382833004
  - 27.50691284239292
  - 27.244799718260765
  - 27.7623291015625
  - 27.53565965592861
  - 28.033625051379204
  - 27.74145831912756
  - 27.629655450582504
  - 27.581068485975266
  - 27.389687597751617
  - 27.290752582252026
  - 27.861669957637787
  - 27.485048308968544
  - 27.768637135624886
  - 27.48584233224392
  - 27.936612352728844
  - 27.733968168497086
  - 27.40353848040104
  - 27.28809331357479
  - 27.646766126155853
  - 27.57244598865509
  - 27.477733120322227
  - 27.40232615172863
  - 27.471921026706696
  - 27.274881571531296
  - 27.381841614842415
  - 27.550235137343407
  - 27.48998226225376
  - 27.230874821543694
  - 27.50228561460972
  - 27.75269377231598
  - 27.187420159578323
  - 27.02811023592949
  - 27.09003122150898
  - 27.39827811717987
  - 27.36607825756073
  - 27.75030079483986
  - 27.20267564058304
  - 28.678925022482872
  - 27.554850548505783
  - 27.551799088716507
  - 27.567613527178764
  - 27.082346834242344
  - 27.076925843954086
  - 27.58088682591915
  - 27.119503930211067
  - 27.55561037361622
  - 27.74845890700817
  - 28.191204220056534
  validation_losses:
  - 0.3919382691383362
  - 0.3848384916782379
  - 0.39399152994155884
  - 0.40159115195274353
  - 0.4169841706752777
  - 0.39010101556777954
  - 0.3889501988887787
  - 0.3990599513053894
  - 0.3967794179916382
  - 0.39689892530441284
  - 0.39413556456565857
  - 0.42466893792152405
  - 0.6197519898414612
  - 0.3904891908168793
  - 0.40516039729118347
  - 0.38941696286201477
  - 0.39606624841690063
  - 0.3969506025314331
  - 0.5449970364570618
  - 0.39566394686698914
  - 0.4458581507205963
  - 4.051218032836914
  - 1.6888315677642822
  - 0.3868260979652405
  - 12.486795425415039
  - 4.85255241394043
  - 14.74954605102539
  - 3.7695517539978027
  - 1.237198829650879
  - 1.2420231103897095
  - 3.572471857070923
  - 1.9594388008117676
  - 0.5734444260597229
  - 1.6019574403762817
  - 0.39372512698173523
  - 0.6240637898445129
  - 0.5094361305236816
  - 0.4845402240753174
  - 0.43327370285987854
  - 0.5004977583885193
  - 0.4454653561115265
  - 0.47823694348335266
  - 1.490418791770935
  - 0.9151098728179932
  - 0.5130254030227661
  - 0.3920726180076599
  - 0.4401004910469055
  - 0.4169383943080902
  - 0.5584884285926819
  - 0.457375168800354
  - 0.46127331256866455
  - 0.5014912486076355
  - 0.5210491418838501
  - 0.5939932465553284
  - 0.4731454849243164
  - 0.4111463725566864
  - 0.486410528421402
  - 0.5815482139587402
  - 0.5963862538337708
  - 0.8896365165710449
  - 0.652341902256012
  - 0.5762805938720703
  - 0.5692290663719177
  - 0.5763995051383972
  - 0.4130903482437134
  - 0.5353316068649292
  - 0.995879054069519
  - 0.5939133167266846
  - 0.5858073830604553
  - 0.699428915977478
  - 0.5656872987747192
  - 0.5256498456001282
  - 0.47840142250061035
  - 0.6114694476127625
  - 0.5873509645462036
  - 0.8517504930496216
  - 0.5160215497016907
  - 0.5477563738822937
  - 0.6344017386436462
  - 0.4985038638114929
  - 0.48380857706069946
  - 0.5980890393257141
  - 0.44410941004753113
  - 0.55419921875
  - 0.6311348080635071
  - 0.555636465549469
  - 0.4276479184627533
  - 0.6460437178611755
  - 0.6383416056632996
  - 0.494678795337677
  - 0.4081345796585083
  - 0.4724843502044678
  - 0.4762970805168152
  - 0.5835601687431335
  - 0.6229662895202637
  - 0.6010910868644714
  - 1.0545591115951538
  - 0.5731075406074524
  - 0.5658625364303589
  - 0.4516347348690033
loss_records_fold2:
  train_losses:
  - 28.303241029381752
  - 27.742903396487236
  - 27.54028195142746
  - 27.84209057688713
  - 27.59068524837494
  - 27.45388177037239
  - 27.2566130310297
  - 27.282594673335552
  - 27.847146302461624
  - 27.228973269462585
  - 27.160879865288734
  - 27.58696123957634
  - 27.495991617441177
  - 27.673156142234802
  - 27.378264546394348
  - 27.47484701871872
  - 27.63459724187851
  - 27.028121516108513
  - 27.068822026252747
  - 27.04420594871044
  - 27.65280693769455
  - 27.359921023249626
  - 27.01051191985607
  - 27.542562544345856
  - 27.520506903529167
  - 27.472714826464653
  - 27.622842743992805
  - 27.344059601426125
  - 27.023486852645874
  - 26.882337749004364
  - 27.18282875418663
  - 27.654806718230247
  - 26.942633718252182
  - 27.01514069736004
  - 27.24450795352459
  - 27.068935453891754
  - 27.318373426795006
  - 26.960686087608337
  - 27.185998894274235
  - 27.152261927723885
  - 26.968666940927505
  - 27.154961973428726
  - 26.734766647219658
  - 27.208274945616722
  - 27.27516560256481
  - 27.407505929470062
  - 27.198057368397713
  - 27.040748983621597
  - 27.615768507122993
  - 27.016747251152992
  - 26.68668295443058
  - 27.690846905112267
  - 26.98037216067314
  - 27.349012851715088
  - 27.095107570290565
  - 26.88425025343895
  - 27.551235288381577
  - 26.858928114175797
  - 26.802511498332024
  - 27.69811502099037
  - 27.35851587355137
  - 27.11636844277382
  - 27.992046013474464
  - 27.710147336125374
  - 27.100295305252075
  - 27.057920515537262
  - 26.70808720588684
  - 26.884766668081284
  - 26.74809157848358
  - 26.9991492331028
  - 26.65340542793274
  - 27.29315960407257
  - 26.838125690817833
  - 26.74021776020527
  - 27.00464601814747
  - 27.101507678627968
  - 27.2154301404953
  - 26.650187969207764
  - 26.87785679101944
  - 26.875043243169785
  - 26.774169996380806
  - 26.613973170518875
  - 26.71014977991581
  - 26.885493233799934
  - 26.832359790802002
  - 26.829153835773468
  - 26.69011400640011
  - 27.054308757185936
  - 26.65010267496109
  - 26.50610165297985
  - 26.939756467938423
  - 26.434784397482872
  - 27.155922934412956
  - 26.89389055967331
  - 26.596705213189125
  - 27.468069553375244
  - 26.77575547993183
  - 26.805698961019516
  - 26.60095578432083
  - 26.586943492293358
  validation_losses:
  - 0.5983043909072876
  - 0.4339897036552429
  - 0.4823604226112366
  - 0.38996174931526184
  - 0.39619722962379456
  - 0.45547357201576233
  - 0.5633838772773743
  - 0.7560607194900513
  - 0.6344162821769714
  - 0.5851863026618958
  - 0.5974588394165039
  - 0.6254002451896667
  - 0.576846718788147
  - 0.6020362377166748
  - 0.5224446058273315
  - 0.5368106365203857
  - 0.419216513633728
  - 0.4907078146934509
  - 0.7056664824485779
  - 0.9981855154037476
  - 0.4381157457828522
  - 0.43474501371383667
  - 0.6028652787208557
  - 0.7141249179840088
  - 0.4009637236595154
  - 0.47931143641471863
  - 0.5945191979408264
  - 0.5496785640716553
  - 0.5196400284767151
  - 0.5682308673858643
  - 0.6263079047203064
  - 0.558645486831665
  - 0.6685029864311218
  - 0.7762749791145325
  - 0.801713228225708
  - 0.41183793544769287
  - 0.42189154028892517
  - 0.6209679245948792
  - 0.8616997003555298
  - 1.2569841146469116
  - 6.700638771057129
  - 2.291616916656494
  - 2.4384708404541016
  - 1.9090763330459595
  - 1.5481971502304077
  - 2.251922845840454
  - 4.382568836212158
  - 0.7084507942199707
  - 2.823106288909912
  - 1.8420459032058716
  - 2.404121160507202
  - 2.7823646068573
  - 7.214899063110352
  - 6.889587879180908
  - 1.8939228057861328
  - 3.4914000034332275
  - 3.2777557373046875
  - 0.7615345120429993
  - 8.978392601013184
  - 4.849884033203125
  - 6.038874626159668
  - 10.352123260498047
  - 1.9480115175247192
  - 0.45110374689102173
  - 4.963098049163818
  - 1.416305422782898
  - 7.049271583557129
  - 15.365653991699219
  - 12.40854549407959
  - 9.148431777954102
  - 13.988739013671875
  - 11.373744010925293
  - 12.220410346984863
  - 11.50135326385498
  - 15.751279830932617
  - 3.675959825515747
  - 1.0627155303955078
  - 1.1282504796981812
  - 3.1091079711914062
  - 7.30640983581543
  - 11.888387680053711
  - 18.86188507080078
  - 7.097878456115723
  - 6.304006576538086
  - 6.72548246383667
  - 6.74022102355957
  - 5.406741142272949
  - 3.5121991634368896
  - 6.5550360679626465
  - 2.7516050338745117
  - 2.739654779434204
  - 2.1870596408843994
  - 3.4805712699890137
  - 7.01455020904541
  - 13.963089942932129
  - 24.103431701660156
  - 11.46689224243164
  - 4.050671100616455
  - 5.051121234893799
  - 0.42661017179489136
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 32 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8353344768439108, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.09433962264150944, 0.0, 0.0]'
  mean_eval_accuracy: 0.8534679610734852
  mean_f1_accuracy: 0.01886792452830189
  total_train_time: '0:12:20.235398'
