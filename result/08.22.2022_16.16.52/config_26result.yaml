config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:43:19.244085'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_26fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_26fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_26fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_26fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_26fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_26fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_26fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_26fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_26fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_26fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 30.86809766292572
  - 13.865174531936646
  - 7.150760459899903
  - 4.225646531581879
  - 2.596304047107697
  - 3.9812826752662662
  - 2.7374563813209534
  - 2.829999542236328
  - 3.0677850008010865
  - 2.3790115654468535
  - 2.045644021034241
  - 1.7871091306209566
  - 1.7945429980754852
  - 2.6138762116432193
  - 2.904872715473175
  - 2.6034839749336243
  - 2.2158369719982147
  - 2.049646997451782
  - 1.6954406738281251
  - 2.054083961248398
  - 2.07892387509346
  - 2.5475674450397494
  - 2.0201602220535277
  - 1.631326848268509
  - 3.0408296227455143
  - 2.0294850409030913
  - 2.497834646701813
  - 3.3096964538097384
  - 3.661459076404572
  - 2.3630043387413027
  - 2.2805247485637667
  - 2.988231760263443
  - 2.0508208692073824
  - 1.8785560488700868
  - 1.6071550965309145
  - 1.7358633160591126
  - 2.5399809896945955
  - 2.4829872190952305
  - 3.1013228178024295
  - 2.4608946681022648
  - 2.8094434380531315
  - 2.754002970457077
  - 3.862986695766449
  - 3.6197790741920475
  - 2.80941162109375
  - 1.7991443842649462
  - 1.7395390987396242
  - 1.9454573661088945
  - 1.834803366661072
  - 1.9818669229745867
  - 2.061040735244751
  - 1.8913836240768434
  - 1.7069832235574722
  - 1.9624735772609712
  - 1.7201427787542345
  - 1.8500896453857423
  - 1.899396073818207
  - 2.0874079495668414
  - 1.6565576612949373
  - 2.2246816754341125
  - 1.9100561141967773
  - 1.625802254676819
  - 1.4286767631769182
  - 2.6308173120021823
  - 1.7393016993999482
  - 2.5043934822082523
  - 1.7839887320995331
  - 1.5944649875164032
  - 1.882556277513504
  - 1.5236738979816438
  - 2.0117868721485137
  - 1.5825902700424195
  - 1.5843509316444397
  - 2.5054348170757295
  - 1.584527611732483
  - 1.5644495308399202
  - 1.510414618253708
  - 1.481757926940918
  - 1.4996407568454744
  - 1.643709421157837
  - 1.4939852058887482
  - 1.4979371190071107
  - 1.623835098743439
  - 1.599732220172882
  - 1.464639836549759
  - 1.9625580370426179
  - 1.7915674030780793
  - 1.6242882907390594
  - 1.6895693659782411
  - 1.694799292087555
  - 1.4936529219150545
  - 1.6799597382545473
  - 1.6636879205703736
  - 1.605269342660904
  validation_losses:
  - 9.3870210647583
  - 7.677985668182373
  - 1.0124905109405518
  - 1.9469311237335205
  - 2.7630813121795654
  - 0.5844161510467529
  - 0.5085207223892212
  - 0.5731008648872375
  - 0.46005725860595703
  - 0.5540333390235901
  - 0.8289334774017334
  - 0.43360474705696106
  - 0.7328041791915894
  - 0.5095366835594177
  - 0.4247701168060303
  - 0.39579641819000244
  - 0.4254155158996582
  - 0.38374456763267517
  - 0.3784354329109192
  - 0.43336570262908936
  - 0.7999512553215027
  - 0.667664110660553
  - 0.39320123195648193
  - 0.3874545693397522
  - 0.3705933392047882
  - 0.8252885341644287
  - 2.3700616359710693
  - 0.9956783652305603
  - 0.5508137345314026
  - 0.5311573147773743
  - 0.4108653962612152
  - 0.4268256723880768
  - 0.4232204854488373
  - 0.7145478129386902
  - 0.41171178221702576
  - 0.37769246101379395
  - 1.3996999263763428
  - 0.5002171397209167
  - 0.46716436743736267
  - 0.6078624725341797
  - 0.4318118095397949
  - 0.4785275161266327
  - 0.564614474773407
  - 0.4895026981830597
  - 0.3823387324810028
  - 0.39260396361351013
  - 0.4246172606945038
  - 0.38511183857917786
  - 0.6005001068115234
  - 0.3839070200920105
  - 0.37633153796195984
  - 0.4197508692741394
  - 0.381220281124115
  - 0.37572312355041504
  - 0.8326858282089233
  - 0.38672932982444763
  - 0.388884037733078
  - 0.3843621611595154
  - 0.39629918336868286
  - 0.4519117474555969
  - 0.40957769751548767
  - 0.3881227374076843
  - 0.4188139736652374
  - 0.3756774365901947
  - 0.3785458505153656
  - 0.4221995174884796
  - 0.3916967511177063
  - 0.4022960662841797
  - 0.3906208276748657
  - 0.3846006989479065
  - 0.4478101432323456
  - 0.3787197470664978
  - 0.4552724361419678
  - 0.38621455430984497
  - 0.37825101613998413
  - 0.4036320745944977
  - 0.37793806195259094
  - 0.38105717301368713
  - 0.3896605372428894
  - 0.40318718552589417
  - 0.38583898544311523
  - 0.3972330093383789
  - 0.4125387668609619
  - 0.37895968556404114
  - 0.39542150497436523
  - 0.37756630778312683
  - 0.392874151468277
  - 0.460382342338562
  - 0.39917266368865967
  - 0.3974848687648773
  - 0.38630640506744385
  - 0.3953503668308258
  - 0.37624454498291016
  - 0.37609440088272095
loss_records_fold1:
  train_losses:
  - 1.8318567216396333
  - 1.6145569443702699
  - 1.5711668968200685
  - 1.663834798336029
  - 1.6927010089159014
  - 1.5735248386859895
  - 1.6064755737781526
  - 1.5567977607250214
  - 1.4770173609256745
  - 1.5337059199810028
  - 1.6606950163841248
  - 1.5088840007781983
  - 1.4806072890758515
  - 1.440782505273819
  - 1.5471045911312105
  - 1.5461952209472658
  - 1.5648945689201357
  - 1.8868132740259171
  - 2.7332967877388
  - 1.5623565912246704
  - 1.7599110245704652
  - 1.6755365878343582
  - 1.508284115791321
  - 1.8370584309101106
  - 1.504395490884781
  - 1.5182858645915986
  - 1.4000485002994538
  - 1.4533349931240083
  - 1.4693122148513795
  validation_losses:
  - 0.39236193895339966
  - 0.40850555896759033
  - 0.39072591066360474
  - 0.4353485405445099
  - 0.4140540063381195
  - 0.4980905055999756
  - 0.402997761964798
  - 0.41275182366371155
  - 0.4118034839630127
  - 0.40010347962379456
  - 0.42968517541885376
  - 0.3951304852962494
  - 0.3979990482330322
  - 0.41491636633872986
  - 0.40671080350875854
  - 0.3948820233345032
  - 0.3946463465690613
  - 0.462052583694458
  - 0.40093421936035156
  - 0.391677588224411
  - 0.40642204880714417
  - 0.40297725796699524
  - 0.44222891330718994
  - 0.4205397367477417
  - 0.39412665367126465
  - 0.3947071135044098
  - 0.40311381220817566
  - 0.39488255977630615
  - 0.3985830247402191
loss_records_fold2:
  train_losses:
  - 1.4483272552490236
  - 1.48516104221344
  - 1.4681045293807984
  - 1.4357881426811219
  - 1.4810434460639954
  - 1.7308830678462983
  - 1.75238516330719
  - 1.6183307230472566
  - 1.588442213833332
  - 1.5770706772804262
  - 1.6006182551383974
  - 2.0586739361286166
  - 1.5970007836818696
  - 1.7866709172725679
  - 1.897781389951706
  - 1.5137281000614167
  - 1.996169763803482
  - 1.6013759076595306
  - 1.8656581044197083
  - 1.7974702775478364
  - 2.7597149431705477
  - 1.797055172920227
  - 1.6363173991441728
  - 1.5612213850021364
  - 2.0595525324344637
  - 2.5384873509407044
  - 4.622690135240555
  - 1.906492668390274
  - 1.6411586225032808
  - 1.754259389638901
  - 1.517015963792801
  validation_losses:
  - 0.37339845299720764
  - 0.37158483266830444
  - 0.38269171118736267
  - 0.37763673067092896
  - 0.383205771446228
  - 0.3807021379470825
  - 0.3823206424713135
  - 0.38448798656463623
  - 0.39281991124153137
  - 0.42540666460990906
  - 0.38525259494781494
  - 0.3807064890861511
  - 0.39129072427749634
  - 0.3982381224632263
  - 0.37457260489463806
  - 0.3860532343387604
  - 0.38639122247695923
  - 0.48636394739151
  - 0.38697439432144165
  - 0.6243475675582886
  - 0.37575840950012207
  - 0.3774392306804657
  - 0.5431641936302185
  - 0.4024305045604706
  - 1.0279203653335571
  - 0.5328329801559448
  - 0.4359253942966461
  - 0.3797459304332733
  - 0.3865448534488678
  - 0.3813822865486145
  - 0.38717249035835266
loss_records_fold3:
  train_losses:
  - 1.83935546875
  - 1.791967159509659
  - 1.520322746038437
  - 1.486490786075592
  - 1.5745216250419618
  - 1.5094180822372438
  - 1.5178466618061066
  - 2.0189120769500732
  - 2.5036489486694338
  - 1.5928710639476777
  - 1.8774808764457704
  - 1.9311691999435425
  - 1.5140891194343569
  - 1.5415602684020997
  - 1.5138806343078615
  - 1.5624938249588014
  - 1.779749405384064
  - 1.5150026381015778
  - 1.5507783055305482
  - 1.5799778640270234
  - 1.6532854199409486
  - 1.661852550506592
  - 2.3761927247047425
  - 1.7094797015190126
  - 1.5740711987018585
  - 1.5863550782203675
  - 1.598249000310898
  validation_losses:
  - 0.43175894021987915
  - 0.44153985381126404
  - 0.4001350700855255
  - 0.4197833240032196
  - 0.44144296646118164
  - 0.4252905547618866
  - 0.40309807658195496
  - 0.4089525043964386
  - 0.39169052243232727
  - 0.43175897002220154
  - 0.45244866609573364
  - 0.39287421107292175
  - 0.391784131526947
  - 0.39807963371276855
  - 0.39183151721954346
  - 0.40520501136779785
  - 0.3979346752166748
  - 0.3926548361778259
  - 0.41494104266166687
  - 0.42191609740257263
  - 0.43373316526412964
  - 0.40702593326568604
  - 0.3980819880962372
  - 0.3923804759979248
  - 0.4007413387298584
  - 0.39367642998695374
  - 0.40088582038879395
loss_records_fold4:
  train_losses:
  - 1.5785361230373383
  - 1.6563168108463289
  - 1.5125848412513734
  - 1.5133175969123842
  - 1.5007166028022767
  - 1.544115197658539
  - 1.5158912599086762
  - 1.4972631692886353
  - 1.5241559743881226
  - 1.4783263564109803
  - 1.4916161715984346
  validation_losses:
  - 0.40563085675239563
  - 0.4003733694553375
  - 0.393881618976593
  - 0.3911459445953369
  - 0.3925652801990509
  - 0.3972744345664978
  - 0.39507246017456055
  - 0.3916884660720825
  - 0.391772598028183
  - 0.3905881345272064
  - 0.39324814081192017
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 94 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 31 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:16:02.707051'
