config:
  aggregation: mean
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:41:21.558162'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_55fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_55fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_55fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_55fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_55fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_55fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_55fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_55fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_55fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_55fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.1443349182605744
  - 0.8799138426780702
  - 0.9808657050132752
  - 0.871628874540329
  - 0.8584857821464539
  - 0.8664083421230316
  - 0.9064473748207093
  - 0.8410130083560944
  - 0.789644056558609
  - 0.8582860052585602
  - 0.8754437029361726
  - 0.8799429893493653
  - 0.8958275020122528
  - 0.8523853957653046
  - 0.773874443769455
  - 0.8335754692554475
  - 0.8213773787021638
  validation_losses:
  - 0.4073372483253479
  - 0.40771955251693726
  - 0.4058517515659332
  - 0.4110650420188904
  - 0.428714781999588
  - 0.40186017751693726
  - 0.40195879340171814
  - 0.3954450190067291
  - 0.39716261625289917
  - 0.4004133939743042
  - 0.4104885160923004
  - 0.40261706709861755
  - 0.40695446729660034
  - 0.39961305260658264
  - 0.3873644471168518
  - 0.396170049905777
  - 0.40043899416923523
loss_records_fold1:
  train_losses:
  - 0.7992947340011597
  - 0.8203886032104493
  - 0.7500994324684144
  - 0.7908117473125458
  - 0.852463322877884
  - 0.8560952782630921
  - 0.8270687758922577
  - 0.8667221486568452
  - 0.7976488173007965
  - 0.8173507869243622
  - 0.7984031736850739
  - 0.7747021317481995
  - 0.8111400425434113
  - 0.809376609325409
  - 0.8480315625667573
  - 0.8359754979610443
  - 0.8098626434803009
  validation_losses:
  - 0.39429962635040283
  - 0.4066547453403473
  - 0.3958430886268616
  - 0.40262719988822937
  - 0.39808595180511475
  - 0.4063381254673004
  - 0.39643460512161255
  - 0.4028571844100952
  - 0.3959941565990448
  - 0.39770060777664185
  - 0.41521957516670227
  - 0.3958267271518707
  - 0.3935028612613678
  - 0.3927189111709595
  - 0.40261977910995483
  - 0.4031636416912079
  - 0.39108261466026306
loss_records_fold2:
  train_losses:
  - 0.8091069400310517
  - 0.7858137905597687
  - 0.8522716701030731
  - 0.781073534488678
  - 0.8126500964164735
  - 0.793820744752884
  - 0.8055824160575867
  - 0.7859418869018555
  - 0.7564798772335053
  - 0.8031634032726288
  - 0.7938831388950348
  - 0.7767986059188843
  - 0.8394382178783417
  - 0.803738009929657
  - 0.7921982407569885
  - 0.8143196702003479
  - 0.8000555574893952
  - 0.8594415843486787
  - 0.803132575750351
  - 0.8377039670944214
  - 0.8323981702327728
  - 0.8343151569366456
  - 0.8698487997055054
  - 0.8991780102252961
  - 0.8707258164882661
  - 0.8252196669578553
  - 0.7995176494121552
  - 0.8012165427207947
  - 0.7961548507213593
  validation_losses:
  - 0.386436402797699
  - 0.3920244872570038
  - 0.4047200381755829
  - 0.39087697863578796
  - 0.38769227266311646
  - 0.387999027967453
  - 0.38905051350593567
  - 0.4021737277507782
  - 0.3903041183948517
  - 0.4730481803417206
  - 0.3953096866607666
  - 0.46602582931518555
  - 0.4369153678417206
  - 0.3922256827354431
  - 0.40765172243118286
  - 0.39175945520401
  - 0.4007689356803894
  - 0.3975023031234741
  - 0.7298680543899536
  - 0.3878420889377594
  - 0.3954735994338989
  - 0.3900783061981201
  - 0.411186158657074
  - 0.40928560495376587
  - 0.3950958549976349
  - 0.3832797110080719
  - 0.3865571916103363
  - 0.3874059021472931
  - 0.39396488666534424
loss_records_fold3:
  train_losses:
  - 0.7904779493808747
  - 0.8204519748687744
  - 0.7757101386785508
  - 0.8068637907505036
  - 0.8678137362003326
  - 0.78784339427948
  - 0.8293634414672852
  - 0.8616667032241822
  - 0.809536236524582
  - 0.8082275152206422
  - 0.791101497411728
  - 0.8365000545978547
  - 0.8428637266159058
  - 0.8026910543441773
  - 0.8828673660755157
  - 0.8245379030704498
  - 0.8445934295654297
  - 0.7980142116546631
  - 0.9747432589530945
  - 0.851784360408783
  - 0.772430843114853
  - 0.7893234133720398
  - 0.7932074964046478
  - 0.7868795573711396
  - 0.8207373440265656
  - 0.7863798439502716
  validation_losses:
  - 0.37814199924468994
  - 0.3895221948623657
  - 0.3863603472709656
  - 0.3943120241165161
  - 0.3886583149433136
  - 0.3898680508136749
  - 0.38299059867858887
  - 0.4035854935646057
  - 0.39002323150634766
  - 0.38677504658699036
  - 0.37861308455467224
  - 0.3796781897544861
  - 0.39412838220596313
  - 0.38399285078048706
  - 0.38742777705192566
  - 0.4143730103969574
  - 0.38191694021224976
  - 0.4116562306880951
  - 0.38555747270584106
  - 0.39746853709220886
  - 0.38102227449417114
  - 0.3815380036830902
  - 0.3717060387134552
  - 0.3785633146762848
  - 0.38202670216560364
  - 0.3916653096675873
loss_records_fold4:
  train_losses:
  - 0.7948034644126892
  - 0.7898585796356201
  - 0.7798194944858552
  - 0.7946648955345155
  - 0.8257953047752381
  - 0.7791384935379029
  - 0.8099587380886079
  - 0.8508416235446931
  - 0.7979919254779816
  - 0.7932673513889313
  - 0.806480485200882
  - 0.7774600028991699
  - 0.8213371992111207
  - 0.7859066665172577
  - 0.8230243086814881
  - 0.7877702295780182
  - 0.8182917416095734
  - 0.7815431416034699
  - 0.8080462694168091
  - 0.8222249686717987
  - 0.7839035868644715
  - 0.8237344086170197
  validation_losses:
  - 0.3847886621952057
  - 0.382779985666275
  - 0.3982047438621521
  - 0.40624624490737915
  - 0.3840387761592865
  - 0.3806394338607788
  - 0.3944752812385559
  - 0.46362367272377014
  - 0.38928717374801636
  - 0.3841400742530823
  - 0.39114853739738464
  - 0.4313192069530487
  - 0.3937041759490967
  - 0.38682350516319275
  - 0.3802966773509979
  - 0.39707812666893005
  - 0.4055890440940857
  - 0.3929857611656189
  - 0.38122087717056274
  - 0.3890039324760437
  - 0.3776441216468811
  - 0.38197919726371765
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:09:32.262737'
