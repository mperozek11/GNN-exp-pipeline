config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 22:56:39.303362'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/88/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/88/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/88/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/88/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/88/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/88/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/88/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/88/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/88/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/88/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 33.43912161886692
  - 31.866767928004265
  - 32.43557311594486
  - 32.05519950389862
  - 32.2736104875803
  - 32.68401901423931
  - 33.13755901157856
  - 32.38428923487663
  - 32.50547108054161
  - 32.777829214930534
  - 32.55616246163845
  - 34.02844040095806
  - 34.603549271821976
  - 32.75073480606079
  - 31.441409774124622
  - 33.89866326749325
  - 32.03087739646435
  - 32.46482992172241
  - 31.869173169136047
  - 31.26020896434784
  - 33.7253065854311
  - 33.48845154047012
  - 33.536216109991074
  - 31.876966640353203
  - 32.75568697601557
  - 32.30529277771711
  - 32.228687435388565
  - 32.788969308137894
  - 32.773291140794754
  - 32.946946904063225
  - 33.081692442297935
  - 33.398607194423676
  - 32.7555238455534
  - 31.85233974456787
  - 32.689571648836136
  - 32.445786848664284
  - 32.11213676631451
  - 30.98676446080208
  - 33.016865119338036
  - 32.11869812011719
  - 33.15457338094711
  - 32.757620334625244
  - 32.96521556377411
  - 40.951007425785065
  - 32.76564446091652
  - 31.209030345082283
  - 31.796747639775276
  - 34.46709194779396
  - 31.26408389210701
  - 31.926331266760826
  - 32.61722640693188
  - 32.18902871012688
  - 33.06788159906864
  - 31.636379763484
  - 32.732310339808464
  - 31.909146934747696
  - 31.263615638017654
  - 31.535545855760574
  - 32.1418601423502
  - 32.69026930630207
  - 33.49338240921497
  - 32.69820885360241
  - 32.58881403505802
  - 31.65374617278576
  - 32.28482623398304
  - 31.794960498809814
  - 31.493098810315132
  - 32.37927158176899
  - 31.86932547390461
  - 32.933231860399246
  - 32.52237267792225
  - 33.47223411500454
  - 31.754647821187973
  - 33.766273215413094
  - 31.184970751404762
  - 32.85645678639412
  - 31.853554889559746
  - 32.78745284676552
  - 31.314819678664207
  - 31.960836693644524
  - 31.690211594104767
  - 31.799415931105614
  - 31.404913425445557
  - 31.123760133981705
  - 30.88241755962372
  - 31.932470589876175
  - 32.48315925896168
  - 32.53563928604126
  - 31.488175585865974
  - 31.864322394132614
  - 31.922661319375038
  - 31.936288222670555
  - 30.81291390955448
  - 32.21818435192108
  - 32.823144525289536
  - 32.50737938284874
  - 33.26633632928133
  - 32.030580431222916
  - 31.90913386642933
  - 32.58501152694225
  validation_losses:
  - 0.49957385659217834
  - 0.4212297201156616
  - 0.5272047519683838
  - 0.4163503348827362
  - 0.4675341248512268
  - 0.43193212151527405
  - 0.40901342034339905
  - 0.4167500436306
  - 0.42937594652175903
  - 0.45753562450408936
  - 0.500289797782898
  - 0.430081307888031
  - 0.43644970655441284
  - 0.41926151514053345
  - 0.4255416691303253
  - 0.47680580615997314
  - 0.42478808760643005
  - 0.42752566933631897
  - 0.42366427183151245
  - 0.44289085268974304
  - 0.44888925552368164
  - 0.43384265899658203
  - 0.41169923543930054
  - 0.41077712178230286
  - 0.45135876536369324
  - 0.460610032081604
  - 0.4524827301502228
  - 0.45578733086586
  - 0.4210183322429657
  - 0.4249023199081421
  - 0.520506739616394
  - 249.71884155273438
  - 7643.30712890625
  - 2238.432373046875
  - 3556.923828125
  - 6105.49951171875
  - 3653.955810546875
  - 2574.499267578125
  - 10089.490234375
  - 1296.2279052734375
  - 1608.5184326171875
  - 446.5838623046875
  - 21.007638931274414
  - 0.47297221422195435
  - 0.46259692311286926
  - 0.47093167901039124
  - 0.4921005070209503
  - 0.4338337779045105
  - 0.42575228214263916
  - 0.4274098873138428
  - 0.423743337392807
  - 0.4382666349411011
  - 0.4187413454055786
  - 88.8874282836914
  - 15814695.0
  - 4906008707072.0
  - 0.4169762432575226
  - 0.4352087378501892
  - 0.450269877910614
  - 0.4264419376850128
  - 0.45793360471725464
  - 0.4194284677505493
  - 0.4470908045768738
  - 0.4142654538154602
  - 0.457956999540329
  - 0.43669605255126953
  - 0.4352847933769226
  - 0.413564532995224
  - 0.452768474817276
  - 0.47165510058403015
  - 0.4670086205005646
  - 0.4324285387992859
  - 0.40591907501220703
  - 0.4267694652080536
  - 0.42696359753608704
  - 0.4202635586261749
  - 0.40993624925613403
  - 0.48280033469200134
  - 0.4267834722995758
  - 0.5160940289497375
  - 0.41895195841789246
  - 0.4214096963405609
  - 0.456951379776001
  - 0.42928120493888855
  - 0.47010859847068787
  - 0.410036563873291
  - 0.4373072683811188
  - 0.41775915026664734
  - 0.41952839493751526
  - 0.42743220925331116
  - 0.4395865797996521
  - 0.42302176356315613
  - 0.4233132600784302
  - 0.4322544038295746
  - 0.43792030215263367
  - 0.42211952805519104
  - 0.4656160771846771
  - 0.42351338267326355
  - 0.4185681641101837
  - 0.4248051941394806
loss_records_fold2:
  train_losses:
  - 32.277247443795204
  - 41.503460735082626
  - 68.11901718378067
  - 37.959389105439186
  - 39.891221046447754
  - 36.42493596673012
  - 34.137252762913704
  - 38.34290890395641
  - 32.63977213948965
  - 32.38755574822426
  - 32.94224725663662
  - 33.522691786289215
  - 37.40433782339096
  - 32.6636461019516
  - 36.60077507793903
  - 32.10421958565712
  - 34.26018066704273
  - 32.74743162840605
  - 33.13826687633991
  - 34.413638800382614
  - 34.82381308078766
  - 33.661467373371124
  - 34.2453151345253
  - 31.409663647413254
  - 31.77786073833704
  - 32.550454780459404
  - 33.75247083604336
  - 33.75770157575607
  - 32.22356814146042
  - 31.422771602869034
  - 31.652447670698166
  - 32.56782603263855
  - 31.798216938972473
  - 31.419506207108498
  - 33.22582522034645
  - 31.18645864725113
  - 32.169912219047546
  - 32.72862067818642
  - 32.88700585067272
  - 33.802718445658684
  - 33.76301008462906
  - 32.144447058439255
  - 32.533347845077515
  - 33.08026675879955
  - 33.46785947680473
  - 32.25062032043934
  - 34.99013042449951
  - 32.74743181467056
  - 33.16491639614105
  - 31.893514454364777
  - 32.252157747745514
  - 31.950068101286888
  - 32.51915431022644
  - 31.876991510391235
  - 31.707642912864685
  - 30.99565550684929
  - 33.042571008205414
  - 34.07097469270229
  - 32.43398359417915
  - 32.59657935798168
  - 31.98294997215271
  - 33.350451573729515
  - 31.712136790156364
  - 32.50507575273514
  - 32.18322613835335
  - 34.21231798827648
  - 32.112208276987076
  - 32.46016113460064
  - 32.55949714779854
  - 33.04726928472519
  - 32.19571575522423
  - 32.44210332632065
  - 31.97807890176773
  - 32.526202976703644
  - 32.12299758195877
  - 31.751988470554352
  - 32.287482500076294
  - 31.99009235203266
  - 32.27987265586853
  - 32.4966027289629
  - 31.69013637304306
  - 34.3483971953392
  - 32.49497476220131
  - 32.68745417147875
  - 32.05915315449238
  - 32.09665608406067
  - 32.49405361711979
  - 33.558885745704174
  - 31.73858205974102
  - 32.24452319741249
  - 32.297079384326935
  - 32.41743537783623
  - 34.36716079711914
  - 34.639224737882614
  - 31.772895738482475
  - 31.589482858777046
  - 31.113186597824097
  - 32.59662222862244
  - 32.55379302799702
  - 33.78449736535549
  validation_losses:
  - 0.4466976821422577
  - 0.4789445996284485
  - 0.4168831706047058
  - 0.3907049000263214
  - 73912608.0
  - 210855428096.0
  - 531691241472.0
  - 465016782848.0
  - 653130399744.0
  - 1077129510912.0
  - 61202579456.0
  - 475619000320.0
  - 192742227968.0
  - 94572740608.0
  - 60102447104.0
  - 206916714496.0
  - 603852767232.0
  - 809504145408.0
  - 2193728471040.0
  - 415236423680.0
  - 544753975296.0
  - 850888818688.0
  - 444932227072.0
  - 774489636864.0
  - 1117892771840.0
  - 429403373568.0
  - 633942310912.0
  - 475045068800.0
  - 107603435520.0
  - 726668541952.0
  - 448098041856.0
  - 837533958144.0
  - 694480076800.0
  - 279145185280.0
  - 1284144758784.0
  - 436833189888.0
  - 107531264000.0
  - 892618145792.0
  - 386297692160.0
  - 1129931866112.0
  - 487528628224.0
  - 116148805632.0
  - 1057796521984.0
  - 2364288008192.0
  - 442108182528.0
  - 355141681152.0
  - 463438348288.0
  - 1192356347904.0
  - 1010228527104.0
  - 498153947136.0
  - 725367324672.0
  - 244935245824.0
  - 635692974080.0
  - 894163746816.0
  - 988598566912.0
  - 421481644032.0
  - 203419402240.0
  - 0.4402501583099365
  - 0.4162675738334656
  - 0.41570940613746643
  - 307968573440.0
  - 186582925312.0
  - 139364515840.0
  - 528088137728.0
  - 162184069120.0
  - 125822320640.0
  - 262216957952.0
  - 181276262400.0
  - 159023808512.0
  - 68437991424.0
  - 41465864192.0
  - 148733886464.0
  - 30171230208.0
  - 315614724096.0
  - 180252966912.0
  - 561967857664.0
  - 77241106432.0
  - 27199332352.0
  - 298501734400.0
  - 324237754368.0
  - 144297705472.0
  - 149301952512.0
  - 129304797184.0
  - 83857260544.0
  - 228114628608.0
  - 475464237056.0
  - 93179125760.0
  - 18023821312.0
  - 654259585024.0
  - 200067104768.0
  - 0.39803236722946167
  - 0.5017490983009338
  - 0.4269612431526184
  - 0.41920197010040283
  - 0.393621563911438
  - 0.41177788376808167
  - 0.4189215898513794
  - 0.4166354537010193
  - 0.47827908396720886
  - 2494538186752.0
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 98 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 81 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 35 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:20:05.313693'
