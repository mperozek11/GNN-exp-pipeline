config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 10:15:54.088373'
fold_0_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_0fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_0fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_0fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_0fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_0fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_0fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_0fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_0fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_0fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.04.2022_10.15.30/config_0fold_4_state_dict.pt
loss_records:
  train_losses:
  - 13.161795675754547
  - 13.138230994343758
  - 13.26039206981659
  - 13.386114984750748
  - 13.00061921775341
  - 13.351476892828941
  - 13.255327820777893
  - 13.131857261061668
  - 13.014504745602608
  - 13.159777119755745
  - 13.008586660027504
  - 13.169262647628784
  - 13.142769888043404
  - 12.873974531888962
  - 12.930715382099152
  - 13.001744404435158
  - 13.070542365312576
  - 12.822147890925407
  - 12.987020820379257
  - 12.926991999149323
  - 13.005263015627861
  - 13.070702120661736
  - 13.11341567337513
  - 12.99111819267273
  - 13.185639917850494
  - 13.32574237883091
  - 13.09497320652008
  - 13.476997166872025
  - 12.903011471033096
  - 12.91682405769825
  - 13.065122336149216
  - 13.05780516564846
  - 13.403514549136162
  - 12.939085870981216
  - 13.025597527623177
  - 13.197958007454872
  - 13.29552012681961
  - 13.242398828268051
  - 13.046248465776443
  - 13.132945209741592
  - 13.13048455119133
  - 12.860465005040169
  - 13.039927214384079
  - 13.066916212439537
  - 13.21185764670372
  - 12.628745928406715
  - 13.248341500759125
  - 13.058926075696945
  - 13.129978269338608
  - 13.161919921636581
  - 13.410566866397858
  - 13.03719575703144
  - 13.081884145736694
  - 13.372359812259674
  - 13.408110067248344
  - 13.32626011967659
  - 13.520214065909386
  - 13.070538073778152
  - 13.025396823883057
  - 13.079144805669785
  - 12.966869071125984
  - 12.842627331614494
  - 12.979703903198242
  - 13.265793204307556
  - 13.006593376398087
  - 13.099932581186295
  - 13.163308888673782
  - 13.155382588505745
  - 12.953135907649994
  - 13.101346909999847
  - 13.102523773908615
  - 12.956080570816994
  - 12.862030565738678
  - 12.839026689529419
  - 13.018674448132515
  - 12.821147084236145
  - 13.184342503547668
  - 12.972871378064156
  - 13.044835984706879
  - 12.844729229807854
  - 13.168734759092331
  - 12.786058247089386
  - 12.765735998749733
  - 12.949274137616158
  - 12.936165556311607
  - 12.43916366994381
  - 13.260305166244507
  - 12.791525468230247
  - 12.964909225702286
  - 13.346021696925163
  - 13.383616268634796
  - 12.745437055826187
  - 12.781890392303467
  - 13.192588537931442
  - 12.858926951885223
  - 12.995162904262543
  - 12.87781836092472
  - 12.764083236455917
  - 12.873501598834991
  - 13.175054594874382
  validation_losses:
  - 0.6564999222755432
  - 0.8766037225723267
  - 0.9518319368362427
  - 1.128339171409607
  - 1.3500893115997314
  - 1.2246371507644653
  - 1.47157621383667
  - 1.340600609779358
  - 1.258366346359253
  - 1.6731505393981934
  - 2.0510125160217285
  - 1.442121982574463
  - 1.6210646629333496
  - 1.8495941162109375
  - 2.13185453414917
  - 1.711645245552063
  - 1.5669476985931396
  - 1.6966828107833862
  - 2.2033445835113525
  - 3.1918742656707764
  - 2.623584747314453
  - 2.1220529079437256
  - 2.289005756378174
  - 4.013600826263428
  - 1.7194249629974365
  - 2.1584432125091553
  - 1.841789960861206
  - 2.1728057861328125
  - 2.449052572250366
  - 1.224231481552124
  - 1.909724235534668
  - 2.5474843978881836
  - 2.3535499572753906
  - 3.3404688835144043
  - 2.9857616424560547
  - 3.3019168376922607
  - 3.0603959560394287
  - 4.788127422332764
  - 4.591698169708252
  - 2.8153910636901855
  - 2.480661630630493
  - 2.2481391429901123
  - 2.874180555343628
  - 2.4614508152008057
  - 3.3042867183685303
  - 4.892268180847168
  - 1.136744499206543
  - 0.5656183362007141
  - 0.9575983285903931
  - 0.8245814442634583
  - 2.592495918273926
  - 0.7376933693885803
  - 0.6026486158370972
  - 0.5399054884910583
  - 0.3520512878894806
  - 0.8278695344924927
  - 0.7710976600646973
  - 0.7772712111473083
  - 1.346895694732666
  - 3.7403552532196045
  - 1.5670040845870972
  - 0.3652712106704712
  - 1.3791186809539795
  - 1.019339680671692
  - 1.2799181938171387
  - 0.8468590974807739
  - 0.7834963798522949
  - 1.2160298824310303
  - 0.5950398445129395
  - 0.6935496926307678
  - 1.4783906936645508
  - 1.1723968982696533
  - 1.2757530212402344
  - 0.7792516946792603
  - 1.2494852542877197
  - 1.1870157718658447
  - 0.8464289903640747
  - 0.530585765838623
  - 1.1420950889587402
  - 0.6309576630592346
  - 0.737978994846344
  - 0.6453375816345215
  - 0.8483238220214844
  - 0.8815734386444092
  - 1.0694838762283325
  - 1.2158950567245483
  - 1.3169256448745728
  - 1.7654880285263062
  - 2.56231427192688
  - 1.0687506198883057
  - 1.5428448915481567
  - 2.120225429534912
  - 1.7470316886901855
  - 2.8193373680114746
  - 1.7286714315414429
  - 2.870638132095337
  - 3.1562459468841553
  - 3.4646289348602295
  - 2.743281364440918
  - 2.216320753097534
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.855917667238422, 0.8301886792452831, 0.8250428816466552, 0.8353344768439108,
    0.8247422680412371]'
  fold_eval_f1: '[0.08695652173913043, 0.18181818181818182, 0.31081081081081086, 0.25000000000000006,
    0.203125]'
  mean_eval_accuracy: 0.8342451946031015
  mean_f1_accuracy: 0.20654210287362465
  total_train_time: '0:18:54.565684'
