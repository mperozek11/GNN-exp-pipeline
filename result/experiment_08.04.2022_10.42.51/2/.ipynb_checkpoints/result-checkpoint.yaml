config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 10:52:19.419621'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/2/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/2/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/2/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/2/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/2/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/2/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/2/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/2/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/2/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_10.42.51/2/fold_4_state_dict.pt
loss_records:
  train_losses:
  - 7.188442528247833
  - 7.059264540672302
  - 6.976922363042831
  - 6.876259863376617
  - 6.773415803909302
  - 6.935271993279457
  - 6.9246401488780975
  - 7.051322162151337
  - 7.147535443305969
  - 7.299888491630554
  - 7.219197601079941
  - 6.9629397094249725
  - 7.0264420211315155
  - 6.9302032589912415
  - 6.974448978900909
  - 6.899694353342056
  - 6.900248140096664
  - 7.19758278131485
  - 6.893333256244659
  - 6.791279673576355
  - 7.06358215212822
  - 6.9272706508636475
  - 7.170631021261215
  - 7.031706929206848
  - 6.905333369970322
  - 6.887878030538559
  - 6.796764463186264
  - 6.808122426271439
  - 6.816462725400925
  - 7.229572266340256
  - 6.9206840097904205
  - 6.9317037761211395
  - 7.1816781759262085
  - 7.124603092670441
  - 6.932494699954987
  - 6.771166086196899
  - 6.95669487118721
  - 7.005104243755341
  - 6.942059546709061
  - 6.8940474689006805
  - 6.749310880899429
  - 6.718906342983246
  - 6.925212889909744
  - 6.943337053060532
  - 7.022126704454422
  - 6.898023813962936
  - 6.823608726263046
  - 6.77774865925312
  - 6.877301007509232
  - 6.932359606027603
  - 6.899288147687912
  - 7.001646056771278
  - 7.037490159273148
  - 6.852719575166702
  - 6.856519252061844
  - 6.829962834715843
  - 6.949815928936005
  - 6.877051621675491
  - 6.8340566754341125
  - 6.94634085893631
  - 6.745420053601265
  - 6.84867849946022
  - 6.831693470478058
  - 6.950524061918259
  - 6.80216982960701
  - 6.915273100137711
  - 6.760929435491562
  - 6.957771360874176
  - 6.887217491865158
  - 6.793076992034912
  - 6.745467022061348
  - 6.921952784061432
  - 6.900143921375275
  - 7.016837686300278
  - 6.798943758010864
  - 6.858220398426056
  - 6.780798971652985
  - 6.956663250923157
  - 6.7786865234375
  - 6.720040202140808
  - 6.870263934135437
  - 6.722921818494797
  - 6.815062642097473
  - 6.717471405863762
  - 6.950314998626709
  - 6.898293644189835
  - 7.072700411081314
  - 6.882502108812332
  - 6.876905411481857
  - 7.050767809152603
  - 6.7265070378780365
  - 6.971054404973984
  - 6.856877088546753
  - 6.74741992354393
  - 6.978554278612137
  - 6.978277653455734
  - 6.674588888883591
  - 6.702493995428085
  - 6.969670116901398
  - 6.963846921920776
  validation_losses:
  - 0.3893580734729767
  - 0.42237210273742676
  - 0.36549898982048035
  - 0.36033037304878235
  - 0.5375797748565674
  - 0.5527016520500183
  - 0.4313514828681946
  - 0.37906935811042786
  - 0.4783436954021454
  - 0.4070471227169037
  - 0.4467345178127289
  - 0.3603467345237732
  - 0.3732195496559143
  - 0.37949618697166443
  - 0.3931993842124939
  - 0.46722832322120667
  - 0.4005171060562134
  - 0.4112802743911743
  - 0.42009419202804565
  - 0.5372439622879028
  - 0.5001896023750305
  - 0.3657814562320709
  - 0.5799925923347473
  - 0.4355439841747284
  - 0.5430811643600464
  - 0.3722834289073944
  - 0.4049782156944275
  - 0.44376707077026367
  - 0.3913250267505646
  - 0.39217430353164673
  - 0.37692791223526
  - 0.4617367386817932
  - 0.4994795322418213
  - 0.3879002332687378
  - 0.39341121912002563
  - 0.3957462012767792
  - 0.41005054116249084
  - 0.37569859623908997
  - 0.3954688608646393
  - 0.4055575728416443
  - 0.35774847865104675
  - 0.4482801556587219
  - 0.5241672992706299
  - 0.43051204085350037
  - 0.3927576243877411
  - 0.4326256811618805
  - 0.3634239435195923
  - 0.48914822936058044
  - 0.4166286885738373
  - 0.41877904534339905
  - 0.42435720562934875
  - 0.3759233057498932
  - 0.5930808186531067
  - 0.376900315284729
  - 0.36659345030784607
  - 0.3596048653125763
  - 0.48980242013931274
  - 0.38925448060035706
  - 0.3869629502296448
  - 0.5117579698562622
  - 0.3889157176017761
  - 0.4231560230255127
  - 0.44343751668930054
  - 0.4250742197036743
  - 0.4031067490577698
  - 0.3950941264629364
  - 0.48521187901496887
  - 0.49872061610221863
  - 0.45425257086753845
  - 0.47130274772644043
  - 0.35458049178123474
  - 0.4006665349006653
  - 0.49562832713127136
  - 0.5732690095901489
  - 0.37145477533340454
  - 0.4510546326637268
  - 0.4466819763183594
  - 0.4437853693962097
  - 0.38614436984062195
  - 0.44028979539871216
  - 0.39072245359420776
  - 0.4675537347793579
  - 0.5017843842506409
  - 0.4572899639606476
  - 0.460212379693985
  - 0.3631635904312134
  - 0.36693230271339417
  - 0.47358572483062744
  - 0.4678633213043213
  - 0.49706393480300903
  - 0.35793253779411316
  - 0.36323681473731995
  - 0.5317740440368652
  - 0.3802005648612976
  - 0.3690829575061798
  - 0.41290372610092163
  - 0.355434775352478
  - 0.37277424335479736
  - 0.4353742301464081
  - 0.4684025049209595
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 66 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8439108061749572, 0.8284734133790738,
    0.8092783505154639]'
  fold_eval_f1: '[0.023529411764705882, 0.0, 0.08080808080808081, 0.1935483870967742,
    0.2649006622516557]'
  mean_eval_accuracy: 0.8393856872557514
  mean_f1_accuracy: 0.11255730838424331
  total_train_time: '0:04:20.560166'
