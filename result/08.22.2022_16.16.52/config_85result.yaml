config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:08:22.217807'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_85fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_85fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_85fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_85fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_85fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_85fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_85fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_85fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_85fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_85fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.3787920057773593
  - 3.189722663164139
  - 3.182182586193085
  - 3.4284545660018924
  - 3.2013206422328953
  - 3.124417108297348
  - 3.1419899821281434
  - 3.0303044736385347
  - 3.2653832733631134
  - 3.1248554527759556
  - 3.1874345660209658
  - 3.1335784137248996
  - 3.226357936859131
  - 3.07277646958828
  - 3.273831784725189
  - 3.3508055210113525
  - 3.1102914571762086
  - 3.1180717378854754
  - 3.1321258485317234
  - 3.0465035229921344
  - 3.10513328909874
  - 3.0420728206634524
  - 3.040033131837845
  - 3.0526016116142274
  - 3.029300880432129
  - 3.01142538189888
  - 3.189085251092911
  - 3.039177578687668
  - 3.0023393034934998
  - 3.048079228401184
  - 3.060232910513878
  - 2.9746938884258274
  - 3.0209955811500553
  - 2.991744774580002
  - 3.1680612534284593
  - 3.1074844479560855
  - 3.123996296525002
  - 3.019703578948975
  validation_losses:
  - 0.39535704255104065
  - 0.4094013571739197
  - 0.40035122632980347
  - 0.4105382263660431
  - 0.400021493434906
  - 0.40715235471725464
  - 0.4088924825191498
  - 0.3987356424331665
  - 0.3955589234828949
  - 0.4666217267513275
  - 0.40166476368904114
  - 0.4254266619682312
  - 0.3938908576965332
  - 0.3967359662055969
  - 0.4150978922843933
  - 0.41341206431388855
  - 0.4062942564487457
  - 0.39276716113090515
  - 0.3928166329860687
  - 0.3887287378311157
  - 0.4008004367351532
  - 0.3877715766429901
  - 0.39224952459335327
  - 0.40060505270957947
  - 0.384910523891449
  - 3.189967393875122
  - 0.39221101999282837
  - 0.40319353342056274
  - 0.3928731679916382
  - 0.38398477435112
  - 0.3863425552845001
  - 0.40108370780944824
  - 0.3932725489139557
  - 0.3986005485057831
  - 0.39487501978874207
  - 0.3949712812900543
  - 0.39160045981407166
  - 0.39177197217941284
loss_records_fold1:
  train_losses:
  - 3.093999141454697
  - 2.9752503365278247
  - 2.96768017411232
  - 3.096541339159012
  - 3.044739979505539
  - 3.0337654411792756
  - 3.084291297197342
  - 2.985791176557541
  - 2.9856156706809998
  - 3.0552094250917436
  - 2.9860489040613176
  - 3.1225795626640322
  - 3.0500355243682864
  - 3.036268949508667
  - 3.010075491666794
  - 3.057002115249634
  - 3.0078744113445284
  - 2.9570736676454548
  - 3.06434942483902
  - 3.0931496381759644
  - 3.026049989461899
  - 2.969056248664856
  - 3.0884944260120393
  - 3.036263430118561
  - 3.0710134685039523
  - 2.9865958392620087
  - 3.102786821126938
  - 3.0166086792945865
  - 2.9640807241201403
  - 2.979147493839264
  - 2.986290675401688
  - 2.9107134222984317
  - 2.9537818014621737
  - 2.9859185218811035
  - 2.978365617990494
  - 2.9531911611557007
  - 2.908479961752892
  - 2.9020267128944397
  - 2.9449142992496493
  - 2.889693939685822
  - 2.9924166709184647
  - 2.984819954633713
  - 2.9510799944400787
  - 3.0233690798282624
  - 2.9445015490055084
  - 2.918271800875664
  - 2.964150467514992
  - 2.9602269887924195
  - 2.958674567937851
  - 2.996287590265274
  - 2.96629596054554
  - 2.8713773757219316
  - 2.8817807137966156
  - 3.0018196344375614
  - 3.0190530717372894
  - 2.9866133451461794
  - 2.911942571401596
  - 2.9965435564517975
  - 2.9458778977394107
  - 2.943332076072693
  - 2.8998552560806274
  - 2.872446072101593
  - 2.903418031334877
  - 3.055373948812485
  - 3.035073459148407
  - 3.165722519159317
  - 3.019416266679764
  - 3.043613159656525
  - 2.9981716960668567
  - 2.997367972135544
  - 2.9628715932369234
  - 2.897674649953842
  - 2.987856698036194
  - 2.981126779317856
  - 2.906913232803345
  - 2.8882892906665805
  - 3.053079700469971
  - 3.020853561162949
  - 2.925075197219849
  - 2.9868361324071886
  - 2.960185039043427
  - 2.900916463136673
  - 2.995441067218781
  - 2.929668027162552
  - 2.8930860400199894
  - 2.949360263347626
  - 3.0140104174613955
  - 2.9768072426319123
  - 2.9478388130664825
  - 2.866658091545105
  - 2.9205485165119174
  - 2.916612982749939
  - 2.907812601327896
  - 3.013739687204361
  - 2.9935186147689823
  - 2.904639309644699
  - 2.9462587982416153
  - 2.9457762420177462
  - 3.031010311841965
  - 3.006343138217926
  validation_losses:
  - 0.3957078754901886
  - 0.3952508270740509
  - 0.3965257406234741
  - 0.39462384581565857
  - 0.39385125041007996
  - 0.3885504901409149
  - 0.4028095602989197
  - 0.4247635304927826
  - 0.4942624866962433
  - 0.4319465160369873
  - 0.3866555392742157
  - 0.39705950021743774
  - 0.3953145146369934
  - 0.42469102144241333
  - 0.39211195707321167
  - 0.40094494819641113
  - 0.42163512110710144
  - 0.4609423279762268
  - 0.39211875200271606
  - 0.39313605427742004
  - 0.3951572775840759
  - 0.4053323268890381
  - 0.7206915616989136
  - 0.4196537137031555
  - 0.4099218249320984
  - 0.4555290937423706
  - 0.46735432744026184
  - 0.41977906227111816
  - 0.41054901480674744
  - 0.4205842614173889
  - 0.41963523626327515
  - 0.48557430505752563
  - 0.5356199145317078
  - 0.4721028506755829
  - 0.40304112434387207
  - 0.49224650859832764
  - 0.4250566363334656
  - 0.4899114668369293
  - 0.44388654828071594
  - 0.5839678049087524
  - 0.40995463728904724
  - 0.5155214667320251
  - 0.4937669336795807
  - 0.39862149953842163
  - 0.8801063299179077
  - 0.7339439988136292
  - 0.6020388603210449
  - 0.5370156168937683
  - 0.710697591304779
  - 0.4752686321735382
  - 0.5622225999832153
  - 0.8769861459732056
  - 0.6012461185455322
  - 0.5240259766578674
  - 0.6086069941520691
  - 0.6170079112052917
  - 2.1180295944213867
  - 0.3863067030906677
  - 0.6872608065605164
  - 0.6734204888343811
  - 0.7898051142692566
  - 0.74485182762146
  - 1.643433928489685
  - 0.39424338936805725
  - 0.40407291054725647
  - 0.39684075117111206
  - 0.38425302505493164
  - 0.3970204293727875
  - 0.4078805148601532
  - 0.5508169531822205
  - 0.43946191668510437
  - 0.65430748462677
  - 0.39014938473701477
  - 0.4155375361442566
  - 0.39690858125686646
  - 0.46117180585861206
  - 0.3898850977420807
  - 0.41078832745552063
  - 0.4282536506652832
  - 0.44101157784461975
  - 0.44921138882637024
  - 0.5767056345939636
  - 0.40612494945526123
  - 0.5074055790901184
  - 0.4318041205406189
  - 0.5514628291130066
  - 0.4057313799858093
  - 0.42091894149780273
  - 0.4011169970035553
  - 0.38679927587509155
  - 0.40490421652793884
  - 0.45739302039146423
  - 0.4399162828922272
  - 0.5295236706733704
  - 0.7662616968154907
  - 0.44510531425476074
  - 0.44994688034057617
  - 0.4858742952346802
  - 0.38317379355430603
  - 0.39049261808395386
loss_records_fold2:
  train_losses:
  - 2.913267755508423
  - 2.953020226955414
  - 2.9935856789350512
  - 2.9782459437847137
  - 2.8790338873863224
  - 2.8832650184631348
  - 2.8451655954122543
  - 2.8991849303245547
  - 2.9381196022033693
  - 2.9468771785497667
  - 2.9374429613351825
  - 2.9148879885673527
  - 2.859883287549019
  - 2.88196359872818
  - 3.028904873132706
  - 2.8984925210475923
  - 2.963145706057549
  - 2.9081841111183167
  - 2.9095235586166384
  - 2.913332223892212
  - 2.851674255728722
  - 2.866964340209961
  - 2.8463139951229097
  - 2.9324114680290223
  - 2.973766523599625
  - 2.8349477767944338
  - 2.828668209910393
  - 2.816312173008919
  - 2.9315028697252274
  - 2.892649704217911
  - 2.922098606824875
  - 2.9782254219055178
  - 2.8809103608131412
  - 2.887211573123932
  - 2.8617344468832018
  - 2.9205399394035343
  - 2.8322959750890733
  - 2.9181789726018907
  - 2.903040099143982
  - 2.8399517774581913
  - 2.881316548585892
  - 2.9108974605798723
  - 2.900900083780289
  - 2.8809335052967073
  - 2.9244839489459995
  - 2.929897160828114
  - 2.7845309317111973
  - 2.8502643913030625
  - 2.8791255801916122
  - 2.9416682332754136
  - 2.8385597944259646
  - 2.9107400059700015
  - 2.8215778321027756
  - 2.8306723624467853
  - 2.8584370702505115
  - 2.8539927303791046
  - 2.932060641050339
  - 2.935113799571991
  - 3.008563807606697
  - 2.9624962627887728
  - 3.0256645739078523
  - 2.888287216424942
  - 2.884471148252487
  - 2.9523115098476413
  - 2.952972462773323
  - 2.84877445101738
  - 2.878125619888306
  - 2.8984895884990696
  - 2.9235641807317734
  - 2.959647899866104
  - 2.8494433164596558
  - 2.808369839191437
  - 2.917303389310837
  - 2.8891941905021667
  - 2.829674565792084
  - 2.889310157299042
  - 2.8089902460575105
  - 2.901688498258591
  - 2.8729539990425113
  - 2.8971339881420137
  - 2.793320292234421
  - 2.859453684091568
  - 2.856711894273758
  - 2.927999046444893
  - 2.9015514045953754
  - 2.9141057938337327
  - 2.8687072813510897
  - 2.834982490539551
  - 2.9526803314685823
  - 2.8325222074985508
  - 2.7775371313095096
  - 2.9821266293525697
  - 2.888450556993485
  - 2.880679050087929
  - 2.9052784681320194
  - 2.830318033695221
  - 2.7710335284471515
  - 2.942433071136475
  - 2.866696518659592
  - 2.832786732912064
  validation_losses:
  - 0.39348161220550537
  - 0.4414103329181671
  - 0.40845048427581787
  - 0.5131019353866577
  - 0.4286401867866516
  - 0.5522902607917786
  - 0.4930097460746765
  - 0.5660991668701172
  - 0.38437768816947937
  - 0.5415274500846863
  - 0.44713059067726135
  - 0.528734564781189
  - 0.4449017643928528
  - 0.5508711338043213
  - 0.6310166120529175
  - 0.3802759349346161
  - 0.4693300127983093
  - 0.4713210165500641
  - 0.5528787970542908
  - 0.43331462144851685
  - 0.4935401380062103
  - 0.5496079325675964
  - 0.4376525580883026
  - 0.5110605359077454
  - 0.7748957872390747
  - 0.6059844493865967
  - 0.5419527292251587
  - 0.6157209873199463
  - 0.5622471570968628
  - 0.5655431747436523
  - 0.7391899228096008
  - 0.40403157472610474
  - 0.6092457175254822
  - 0.7382230758666992
  - 0.5069014430046082
  - 0.443525105714798
  - 0.7696515917778015
  - 1.1902121305465698
  - 0.48907607793807983
  - 0.776149332523346
  - 1.2027842998504639
  - 0.4294147491455078
  - 0.44569337368011475
  - 0.4046250283718109
  - 0.3940697908401489
  - 0.7540326714515686
  - 0.8885459303855896
  - 0.5995882153511047
  - 0.5629003643989563
  - 0.4330379068851471
  - 0.4577310085296631
  - 0.468828946352005
  - 0.5032352209091187
  - 0.46697747707366943
  - 0.5181149244308472
  - 0.5783851742744446
  - 0.5132525563240051
  - 0.6200882792472839
  - 0.659421443939209
  - 0.4139317572116852
  - 0.48048415780067444
  - 0.3856181800365448
  - 0.5184587836265564
  - 0.5126500725746155
  - 0.4489233195781708
  - 0.5309428572654724
  - 0.5959140658378601
  - 0.5460147261619568
  - 0.4775724709033966
  - 0.5194280743598938
  - 0.6085317134857178
  - 0.6871780157089233
  - 0.5031721591949463
  - 0.5580472350120544
  - 0.6350429654121399
  - 0.5058981776237488
  - 0.7812124490737915
  - 0.42706364393234253
  - 0.5099336504936218
  - 0.512725293636322
  - 0.6629910469055176
  - 0.6614884734153748
  - 0.6030331254005432
  - 0.4934830665588379
  - 0.4751214385032654
  - 0.5926945805549622
  - 0.4894447922706604
  - 0.6900952458381653
  - 0.7683583498001099
  - 0.6601687073707581
  - 0.7295619249343872
  - 0.3981388509273529
  - 0.4151216447353363
  - 0.4789464473724365
  - 0.6075494885444641
  - 0.45863428711891174
  - 0.5949309468269348
  - 0.48557236790657043
  - 0.6553319692611694
  - 0.5402557849884033
loss_records_fold3:
  train_losses:
  - 2.9689555346965792
  - 2.885846269130707
  - 2.93026357293129
  - 2.831580287218094
  - 2.8350605726242066
  - 2.8943504929542545
  - 2.890770965814591
  - 2.9356804221868518
  - 2.930220812559128
  - 2.8753770083189014
  - 2.8840623140335087
  - 2.9710227191448215
  - 2.9620851159095767
  - 2.899849253892899
  - 2.9173446148633957
  - 2.9386669218540193
  - 2.914791661500931
  - 2.9275911033153537
  - 2.918503165245056
  - 2.8529197067022327
  - 2.8886848330497745
  - 2.9191850364208225
  - 2.998814880847931
  - 2.8749074727296833
  - 2.9213838040828706
  - 2.9035764336586
  - 2.864916199445725
  - 2.8737495422363284
  - 2.8610495775938034
  - 2.8319711416959765
  - 2.8638799250125886
  - 2.8128855466842655
  - 2.894782930612564
  - 2.9297154486179355
  - 2.9552674651145936
  - 2.9640475988388064
  - 2.8046790599823
  - 2.8450285971164706
  - 2.8929616779088976
  - 2.9788417458534244
  - 2.8022439181804657
  - 2.848871332406998
  - 2.9089055180549623
  - 2.9156526505947116
  - 2.7762228548526764
  - 2.8400704085826876
  - 2.891393095254898
  - 2.915675324201584
  - 2.8871921539306644
  - 2.816340628266335
  - 2.9451954603195194
  - 2.826747274398804
  - 2.851677912473679
  - 2.890329656004906
  - 2.876402661204338
  - 2.844985854625702
  - 2.869643211364746
  - 2.8304259866476063
  - 2.9114689886569978
  - 2.8278072386980058
  - 2.805503726005554
  - 2.7952464252710345
  - 2.827364778518677
  - 2.7739248067140583
  - 2.8417873561382296
  - 2.8192117750644687
  - 2.8727101266384127
  - 2.8340603679418566
  - 2.8168238878250125
  - 2.9489267706871036
  - 2.793128448724747
  - 3.07145813703537
  - 3.0598676025867464
  - 3.0455786883831024
  - 2.9645047158002855
  - 2.9480734646320346
  - 2.918119263648987
  - 2.9289399564266207
  - 2.8514874547719957
  - 2.8352661311626437
  - 2.8758346796035767
  - 2.7628918796777726
  - 2.851025086641312
  - 2.824264937639237
  - 2.873973098397255
  - 2.939593505859375
  - 2.9188360512256626
  - 2.919274640083313
  - 3.0303264379501345
  - 2.8650034308433536
  - 2.795560133457184
  - 2.9282552242279056
  - 2.8758178114891053
  - 2.8705604493618013
  - 2.89488120675087
  - 2.853681218624115
  - 3.009957391023636
  - 2.8665077447891236
  - 2.788158613443375
  - 2.8444836616516116
  validation_losses:
  - 0.5477319359779358
  - 0.5152689814567566
  - 0.437059223651886
  - 0.5673577785491943
  - 0.4378502666950226
  - 0.7051166296005249
  - 1.0352293252944946
  - 0.9465044140815735
  - 0.7502944469451904
  - 0.38300079107284546
  - 0.7868144512176514
  - 0.7135394811630249
  - 0.3874005973339081
  - 0.5956625938415527
  - 0.48391860723495483
  - 0.3736797869205475
  - 0.4027567207813263
  - 0.4104471206665039
  - 0.41362205147743225
  - 0.4428141415119171
  - 0.4312300384044647
  - 0.4802967607975006
  - 0.48076027631759644
  - 0.44472891092300415
  - 0.4694211781024933
  - 0.46723636984825134
  - 0.5025823712348938
  - 0.45132285356521606
  - 0.47827333211898804
  - 0.6010680794715881
  - 0.5198626518249512
  - 0.5530440807342529
  - 0.43776896595954895
  - 0.5995082855224609
  - 0.681837260723114
  - 0.4611828029155731
  - 2.6087825298309326
  - 0.8142658472061157
  - 0.6295626759529114
  - 2.326253652572632
  - 1.5471268892288208
  - 1.4540268182754517
  - 2.2866322994232178
  - 2.2606329917907715
  - 5.044319152832031
  - 3.685093402862549
  - 0.5911287665367126
  - 0.5037614107131958
  - 0.527449905872345
  - 0.6287341713905334
  - 0.44428664445877075
  - 0.6647111773490906
  - 0.686022162437439
  - 0.5925294160842896
  - 0.7699652314186096
  - 0.7579046487808228
  - 0.5953414440155029
  - 0.6842382550239563
  - 1.3536756038665771
  - 1.7526278495788574
  - 0.6076791882514954
  - 0.5953141450881958
  - 0.5324456691741943
  - 0.5282096266746521
  - 0.5965247750282288
  - 0.5694342851638794
  - 0.48965907096862793
  - 0.49935197830200195
  - 0.5695008039474487
  - 0.7048563957214355
  - 0.6051966547966003
  - 0.49365246295928955
  - 0.4451218545436859
  - 0.3979150950908661
  - 0.38360634446144104
  - 1.3443995714187622
  - 1.6477649211883545
  - 2.608537435531616
  - 2.7945353984832764
  - 0.4140074849128723
  - 0.8385603427886963
  - 1.1122764348983765
  - 1.7197705507278442
  - 1.176058292388916
  - 0.7559707760810852
  - 0.4145011603832245
  - 0.5252412557601929
  - 0.66070157289505
  - 1.1751552820205688
  - 0.6470871567726135
  - 0.716404378414154
  - 0.5766097903251648
  - 0.5879479050636292
  - 0.5039076209068298
  - 0.5353652834892273
  - 1.1127979755401611
  - 0.5055873394012451
  - 0.5026720762252808
  - 0.6656901836395264
  - 0.4637589454650879
loss_records_fold4:
  train_losses:
  - 2.870935070514679
  - 2.908462768793106
  - 2.8611264526844025
  - 2.927804642915726
  - 2.795364058017731
  - 2.827185109257698
  - 2.8459359407424927
  - 2.886083519458771
  - 2.8447729378938678
  - 2.8543714344501496
  - 2.8913113653659823
  - 2.9457555264234543
  - 2.9615913152694704
  - 2.892344206571579
  - 2.9310631304979324
  - 2.9123129516839983
  - 2.8807473063468936
  - 2.8917558908462526
  - 2.8646975576877596
  - 2.8380390465259553
  - 2.880342918634415
  - 2.763382697105408
  - 2.8519328087568283
  - 2.8893608510494233
  - 2.7713367223739627
  - 2.8969240814447406
  - 2.8956456899642946
  - 2.9589017629623413
  - 2.9084354728460315
  - 2.8725019633769993
  - 2.8827322959899906
  - 2.898141008615494
  - 2.7987828314304353
  - 2.830002611875534
  - 2.765110003948212
  - 2.834447303414345
  - 2.8241820573806766
  - 2.906120622158051
  - 2.7880595088005067
  - 2.9082495868206024
  - 2.8150037586688996
  - 2.701724198460579
  - 2.83571250140667
  - 2.8427643120288852
  - 2.8585961341857913
  - 2.836032021045685
  - 2.8700289070606235
  - 2.811124908924103
  - 2.7992735445499424
  - 2.913389053940773
  - 2.829069694876671
  - 2.7950369119644165
  - 2.7600870788097382
  - 2.904216301441193
  - 2.850621515512467
  - 2.9851744174957275
  - 2.832576233148575
  - 2.8298538058996203
  - 2.816425406932831
  - 2.862545609474182
  - 2.8119466543197635
  - 2.840476852655411
  - 2.750604343414307
  - 2.829533725976944
  - 2.7006260812282563
  - 2.7820566594600677
  - 2.891349342465401
  - 2.8256315380334858
  - 2.805997517704964
  - 2.8477378249168397
  - 2.7882197290658954
  - 2.7992712289094928
  - 2.8160831511020663
  - 2.83525385260582
  - 2.872854042053223
  - 2.8510946393013
  - 2.7465097159147263
  - 2.858721047639847
  - 2.7670186311006546
  - 2.7921726226806642
  - 2.8555858910083773
  - 2.861538663506508
  - 2.8027930557727814
  - 2.871622452139855
  - 2.8255772560834886
  - 2.8367630511522295
  - 2.738458216190338
  - 2.841123378276825
  - 2.791350203752518
  - 2.7223326474428178
  - 2.8175828218460084
  - 2.815855845808983
  - 2.795897921919823
  - 2.731986197829247
  - 2.7430867910385133
  - 2.9204444974660877
  - 2.814496928453446
  - 2.842467537522316
  - 2.7564041018486023
  - 2.863935106992722
  validation_losses:
  - 0.5468533039093018
  - 0.5095467567443848
  - 0.47096332907676697
  - 0.5214497447013855
  - 0.5292222499847412
  - 0.4762006103992462
  - 0.5218993425369263
  - 0.5636255145072937
  - 0.537445068359375
  - 0.5274935364723206
  - 0.5006396174430847
  - 0.6090966463088989
  - 0.455064594745636
  - 0.5078543424606323
  - 0.4458242654800415
  - 0.4999843239784241
  - 0.5526556968688965
  - 0.5135391354560852
  - 0.5184876322746277
  - 0.5365287065505981
  - 0.8305666446685791
  - 0.6207370162010193
  - 0.5996546745300293
  - 0.5612407922744751
  - 0.5985739827156067
  - 0.402953177690506
  - 0.5427528023719788
  - 0.5985183119773865
  - 0.5996913909912109
  - 0.5412859320640564
  - 0.5118808746337891
  - 0.6200761795043945
  - 0.5348808765411377
  - 0.5344254970550537
  - 0.5484159588813782
  - 0.5703325271606445
  - 0.49683645367622375
  - 0.749081552028656
  - 0.5529702305793762
  - 0.6238977909088135
  - 0.5781415700912476
  - 0.6377021670341492
  - 0.5225576162338257
  - 0.5574444532394409
  - 0.638640820980072
  - 0.6129127144813538
  - 0.5785560607910156
  - 0.6262694001197815
  - 0.552266001701355
  - 0.7421204447746277
  - 0.6979449391365051
  - 0.6058871150016785
  - 0.5455597043037415
  - 0.4666174352169037
  - 0.471360981464386
  - 0.49198704957962036
  - 0.6754974722862244
  - 0.6155580878257751
  - 0.5777024030685425
  - 0.6591628193855286
  - 0.7199803590774536
  - 0.6350827813148499
  - 0.6170018911361694
  - 0.9050040245056152
  - 0.7701733112335205
  - 0.659614086151123
  - 0.5920674800872803
  - 0.6389463543891907
  - 0.5759856104850769
  - 0.6064944267272949
  - 0.6678198575973511
  - 0.8318692445755005
  - 0.5721529722213745
  - 0.8290870785713196
  - 0.8888172507286072
  - 0.7775970101356506
  - 0.7453514933586121
  - 0.5619732141494751
  - 0.7045520544052124
  - 0.68895423412323
  - 0.6127527356147766
  - 0.5456979274749756
  - 0.5407363176345825
  - 0.7411563396453857
  - 0.5572733283042908
  - 0.7474156618118286
  - 0.6671143770217896
  - 0.657803475856781
  - 0.682202160358429
  - 0.7714407444000244
  - 0.6716242432594299
  - 0.5913603901863098
  - 0.56717848777771
  - 0.7188336849212646
  - 0.5345920324325562
  - 0.4426117539405823
  - 0.4695150554180145
  - 0.7533825635910034
  - 0.620093047618866
  - 0.7558413743972778
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 38 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8044596912521441, 0.8250428816466552,
    0.8109965635738832]'
  fold_eval_f1: '[0.0, 0.045454545454545456, 0.35227272727272724, 0.2608695652173913,
    0.21428571428571427]'
  mean_eval_accuracy: 0.8308099473631471
  mean_f1_accuracy: 0.17457651044607567
  total_train_time: '0:40:34.374867'
