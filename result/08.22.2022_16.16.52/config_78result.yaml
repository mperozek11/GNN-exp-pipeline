config:
  aggregation: sum
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:57:15.675672'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_78fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_78fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_78fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_78fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_78fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_78fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_78fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_78fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_78fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_78fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 18.450477838516235
  - 6.649205756187439
  - 3.660329329967499
  - 5.24493392109871
  - 4.875416165590287
  - 5.995684492588044
  - 5.318675708770752
  - 3.315962272882462
  - 4.515932852029801
  - 2.8970749318599704
  - 3.208269661664963
  - 3.615957421064377
  - 2.0897311687469484
  - 2.466318041086197
  - 3.3671131134033203
  - 9.068141347169876
  - 10.805905038118363
  - 4.366057085990906
  - 2.091775837540627
  - 1.7502448201179506
  - 2.2476727962493896
  - 1.9116541206836701
  - 5.254961180686951
  - 4.6686212658882145
  - 3.2265818536281587
  - 1.8550627768039705
  - 4.209607774019242
  - 3.5653088986873627
  - 5.60916485786438
  - 1.9990851044654847
  - 2.639118140935898
  - 2.405958116054535
  - 1.729538232088089
  - 7.57166605591774
  - 2.7084721684455872
  - 3.2177777469158175
  - 1.7654816508293152
  - 1.5546121656894685
  - 1.8843064844608308
  - 1.720834130048752
  - 4.116366213560105
  - 6.848603245615959
  - 4.0004548072814945
  - 4.648652243614197
  - 2.4652500450611115
  - 1.6952546149492265
  - 1.9336105436086655
  - 3.4707351595163347
  - 2.3661827087402343
  - 2.163890016078949
  - 1.888810920715332
  - 2.990758937597275
  - 1.948456621170044
  - 1.6228401482105257
  - 1.6022002935409547
  - 1.694047212600708
  - 3.727208358049393
  - 4.1032891333103185
  validation_losses:
  - 13.122785568237305
  - 0.7196306586265564
  - 0.9334526658058167
  - 0.7022075057029724
  - 1.2163546085357666
  - 1.3873368501663208
  - 1.4119819402694702
  - 0.6711733937263489
  - 0.5941002368927002
  - 0.8280507922172546
  - 0.6024426817893982
  - 0.4526054859161377
  - 0.4643876254558563
  - 0.3952590227127075
  - 0.4056869149208069
  - 0.5292165279388428
  - 0.5895836353302002
  - 0.5394021272659302
  - 0.3824153542518616
  - 0.5239720940589905
  - 0.41511109471321106
  - 0.42721933126449585
  - 0.5246132016181946
  - 0.4161088764667511
  - 0.4084564447402954
  - 0.38344934582710266
  - 0.420993834733963
  - 0.44062933325767517
  - 0.5042800903320312
  - 0.43018442392349243
  - 0.48040443658828735
  - 0.45772096514701843
  - 0.4574451744556427
  - 0.4148090183734894
  - 0.43527573347091675
  - 0.3982384204864502
  - 0.40323179960250854
  - 0.41331446170806885
  - 0.39430105686187744
  - 0.41612508893013
  - 0.4440976083278656
  - 1.1534385681152344
  - 0.6980774998664856
  - 0.44359534978866577
  - 0.4172433912754059
  - 0.4583221673965454
  - 0.42919450998306274
  - 0.46893954277038574
  - 0.43470990657806396
  - 0.3960108757019043
  - 0.3895583748817444
  - 0.4306339919567108
  - 0.40417811274528503
  - 0.40896812081336975
  - 0.3911369740962982
  - 0.3951900899410248
  - 0.3956339359283447
  - 0.4005405008792877
loss_records_fold1:
  train_losses:
  - 1.8673693418502808
  - 2.057118254899979
  - 1.6578745663166048
  - 1.7906235456466675
  - 3.099076062440872
  - 2.9455931544303895
  - 1.7812493741512299
  - 1.6322702944278717
  - 1.6171714007854463
  - 2.1229923605918883
  - 1.569133734703064
  - 1.7219282567501069
  - 1.6312283277511597
  - 1.5715441286563874
  - 1.683019530773163
  - 1.720649516582489
  - 1.6239066898822785
  - 1.6265168845653535
  - 1.6474900364875795
  - 1.682824069261551
  - 1.6980735957622528
  - 1.713500726222992
  - 1.6804952323436737
  - 1.6176567375659943
  - 2.1645902276039126
  - 1.9061422407627107
  - 2.120550537109375
  - 1.6931709349155426
  - 1.9770315229892732
  - 1.7511998534202577
  - 1.7396743535995485
  - 1.6573456406593323
  - 1.6061866015195847
  - 1.6004201114177705
  - 1.6119013845920565
  - 1.6640024065971375
  - 1.5947650730609895
  - 1.6849725484848024
  - 1.7580894887447358
  - 1.6463095903396607
  - 1.936748093366623
  - 1.7686585009098055
  - 1.6693460643291473
  - 1.6755631387233736
  - 1.6024730980396271
  - 2.0089459359645843
  - 1.6792690932750702
  - 2.136743062734604
  validation_losses:
  - 0.4135104715824127
  - 0.45887240767478943
  - 0.4298762083053589
  - 0.4426701068878174
  - 0.4124997854232788
  - 0.4087434411048889
  - 0.41964927315711975
  - 0.42836305499076843
  - 0.4475356936454773
  - 0.4153955578804016
  - 0.4362446963787079
  - 0.41338416934013367
  - 0.41494491696357727
  - 0.41985437273979187
  - 0.45740312337875366
  - 0.4296676218509674
  - 0.4217967391014099
  - 0.4649824798107147
  - 0.42073163390159607
  - 0.4452037811279297
  - 0.4145839810371399
  - 0.39821675419807434
  - 0.40254053473472595
  - 0.42127564549446106
  - 0.4178108870983124
  - 0.41948094964027405
  - 0.4309863746166229
  - 0.4169444739818573
  - 0.40458858013153076
  - 0.43851661682128906
  - 0.40690699219703674
  - 0.4070885181427002
  - 0.4081851541996002
  - 0.40317243337631226
  - 0.41912680864334106
  - 0.4089106023311615
  - 0.43037521839141846
  - 0.48283082246780396
  - 0.4745232164859772
  - 0.41048312187194824
  - 0.41333165764808655
  - 0.4265466630458832
  - 0.43585383892059326
  - 0.43543151021003723
  - 0.4383988380432129
  - 0.43832045793533325
  - 0.42840319871902466
  - 0.4260099232196808
loss_records_fold2:
  train_losses:
  - 1.8618128657341004
  - 2.3487462937831878
  - 1.8279805004596712
  - 1.7273007035255432
  - 1.6303440868854524
  - 1.6983476459980011
  - 1.6219161212444306
  - 1.6129627346992494
  - 1.9496178150177004
  - 1.7393170535564424
  - 1.6507133662700655
  - 1.6734640300273895
  - 1.7060576498508455
  - 1.7052532345056535
  - 1.6621663331985475
  - 1.6693276584148409
  - 1.6360510170459748
  - 1.6302094459533691
  - 1.6041740715503694
  - 1.6369285762310029
  - 1.913431602716446
  - 1.6749515354633333
  - 1.6163445115089417
  - 1.6481399953365328
  - 1.6995067834854127
  - 1.8537244737148286
  - 1.6777487397193909
  - 1.7156385719776155
  - 2.243188405036926
  - 1.7926129877567292
  - 1.5872255831956865
  - 1.755180674791336
  - 1.7292235732078554
  - 1.6768769025802612
  - 1.9543396592140199
  - 1.728494942188263
  - 1.681076842546463
  - 1.617693728208542
  - 1.6341969132423402
  - 1.6084954023361206
  - 1.6320658385753632
  - 1.7182400584220887
  - 1.6261149585247041
  - 1.680360382795334
  - 1.5901586383581163
  - 1.642032587528229
  - 1.5811861515045167
  - 1.6180741369724274
  - 1.7463746011257173
  - 1.6126485884189607
  validation_losses:
  - 0.402328759431839
  - 0.4304186701774597
  - 0.4117733836174011
  - 0.3958638310432434
  - 0.388639897108078
  - 0.39156073331832886
  - 0.39128348231315613
  - 0.3974989950656891
  - 0.39315176010131836
  - 0.4193507432937622
  - 0.3966725766658783
  - 0.39666712284088135
  - 0.3965628743171692
  - 0.387117862701416
  - 0.39038124680519104
  - 0.4027162492275238
  - 0.38778117299079895
  - 0.3941882252693176
  - 0.39338645339012146
  - 0.38810375332832336
  - 0.4242671728134155
  - 0.3891655504703522
  - 0.3811019957065582
  - 0.4207862615585327
  - 0.39749807119369507
  - 0.38404572010040283
  - 0.4044683873653412
  - 0.3971640169620514
  - 0.39619916677474976
  - 0.3972841799259186
  - 0.39219868183135986
  - 0.4024232029914856
  - 0.4190728962421417
  - 0.396677166223526
  - 0.39270877838134766
  - 0.41397708654403687
  - 0.40432220697402954
  - 0.40086331963539124
  - 0.3897263705730438
  - 0.41254723072052
  - 0.39832380414009094
  - 0.4189158082008362
  - 0.3865649998188019
  - 0.42170441150665283
  - 0.40752214193344116
  - 0.39321357011795044
  - 0.3955445885658264
  - 0.39324626326560974
  - 0.3924722373485565
  - 0.3937978446483612
loss_records_fold3:
  train_losses:
  - 1.604169172048569
  - 1.9236962139606477
  - 1.7655053079128267
  - 1.7203352928161622
  - 1.5672390997409822
  - 1.591378778219223
  - 1.6390579640865326
  - 1.840011501312256
  - 1.6239693820476533
  - 1.6290215253829956
  - 1.6742198944091797
  - 1.8172123730182648
  - 1.931225973367691
  - 1.8673619091510774
  - 1.7708433151245118
  - 1.6831009984016418
  - 1.6288632154464722
  - 1.7132877528667452
  - 1.8780536711215974
  - 1.5848794519901277
  - 1.81716548204422
  - 1.6235951840877534
  - 1.6152895092964172
  - 1.6348541378974915
  - 1.6510841310024262
  - 1.5722131222486497
  - 1.642920196056366
  - 1.6488955795764924
  - 1.877028924226761
  - 1.707603520154953
  - 1.6541459441184998
  - 1.6509636342525482
  - 1.6641118228435516
  - 1.5886010706424714
  - 1.5781873166561127
  - 1.6246672093868257
  validation_losses:
  - 0.4339756369590759
  - 0.4083578288555145
  - 0.44134464859962463
  - 0.3908601403236389
  - 0.42210501432418823
  - 0.40193793177604675
  - 0.4026685059070587
  - 0.41676172614097595
  - 0.40811675786972046
  - 0.4073181748390198
  - 0.4088665843009949
  - 0.4229995608329773
  - 0.4102504551410675
  - 0.40932145714759827
  - 0.41706523299217224
  - 0.4204142391681671
  - 0.4193306863307953
  - 0.43736544251441956
  - 0.4145469069480896
  - 0.43314117193222046
  - 0.41037192940711975
  - 0.44000244140625
  - 0.4020652174949646
  - 0.4425807297229767
  - 0.4061726927757263
  - 0.4473465085029602
  - 0.40084993839263916
  - 0.41144657135009766
  - 0.40983903408050537
  - 0.42748039960861206
  - 0.4220731258392334
  - 0.42335864901542664
  - 0.4051884710788727
  - 0.40093427896499634
  - 0.40416550636291504
  - 0.4120226204395294
loss_records_fold4:
  train_losses:
  - 1.6044926881790162
  - 1.580688762664795
  - 1.631878983974457
  - 1.6797921717166902
  - 1.6734281957149506
  - 1.6759183585643769
  - 1.6054284751415253
  - 1.5852483093738556
  - 1.6426049768924713
  - 1.720605480670929
  - 1.6122362405061723
  - 1.6518420696258547
  - 1.6435349822044374
  - 1.6659562826156618
  - 1.650798285007477
  - 1.7543050944805145
  - 1.6653610706329347
  validation_losses:
  - 0.41225746273994446
  - 0.41545432806015015
  - 0.41909900307655334
  - 0.4036465287208557
  - 0.449754536151886
  - 0.39870890974998474
  - 0.39771178364753723
  - 0.40804436802864075
  - 0.4379926323890686
  - 0.39253702759742737
  - 0.4065386652946472
  - 0.4044080972671509
  - 0.3889036774635315
  - 0.3955271542072296
  - 0.3991294801235199
  - 0.408646821975708
  - 0.40317049622535706
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 58 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 48 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 50 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 36 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.855917667238422, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:17:47.006104'
