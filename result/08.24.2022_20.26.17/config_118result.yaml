config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:14:25.940485'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_118fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_118fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_118fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_118fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_118fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_118fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_118fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_118fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_118fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_118fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.8638237118721008
  - 1.6896100699901582
  - 1.6004827618598938
  - 1.5777200102806093
  - 1.6298516035079957
  - 1.75576428771019
  - 1.675593364238739
  - 1.5561631560325624
  - 1.5624542117118836
  - 1.5777267217636108
  - 1.5828821122646333
  - 1.5818150520324707
  - 1.5699358761310578
  - 1.7046350061893465
  - 1.649341768026352
  - 1.5933223009109498
  - 1.6156471133232118
  - 1.5861384570598602
  - 1.5851627469062806
  - 1.6266916751861573
  - 1.6678595066070558
  - 1.5839969098567963
  - 1.56546967625618
  - 1.5487777113914492
  - 1.6799922943115235
  - 1.585822120308876
  validation_losses:
  - 0.5862051248550415
  - 0.4226304888725281
  - 0.39523863792419434
  - 0.3925272524356842
  - 0.40877410769462585
  - 0.4134019613265991
  - 0.4016452729701996
  - 0.3940751850605011
  - 0.4054185450077057
  - 0.4113098680973053
  - 0.4124174118041992
  - 0.3994542360305786
  - 0.39573436975479126
  - 0.4069918096065521
  - 0.3917810618877411
  - 0.39489004015922546
  - 0.3919107913970947
  - 0.40655773878097534
  - 0.3913349211215973
  - 0.4318203926086426
  - 0.41537579894065857
  - 0.3940640091896057
  - 0.39713209867477417
  - 0.4065077006816864
  - 0.4032769799232483
  - 0.39742738008499146
loss_records_fold1:
  train_losses:
  - 1.6654357314109802
  - 1.5843896567821503
  - 1.64990553855896
  - 1.6213570535182953
  - 1.5286433577537537
  - 1.5342286586761475
  - 1.5424707353115084
  - 1.734172910451889
  - 1.6316729426383974
  - 1.5343572020530702
  - 1.5499301671981813
  - 1.585176020860672
  - 1.5711919188499452
  - 1.6027746737003328
  - 1.5531771302223207
  - 1.4970518171787264
  - 1.5649222970008851
  - 1.5098705291748047
  - 1.5868036568164827
  - 1.5245428830385208
  - 1.5456789493560792
  - 1.550932449102402
  - 1.5424726605415344
  - 1.5542643010616304
  - 1.5682888567447664
  - 1.5285249710083009
  - 1.5234819471836092
  - 1.5154308497905733
  - 1.545083224773407
  - 1.541914874315262
  - 1.5188073337078096
  - 1.5651971995830536
  - 1.569688642024994
  - 1.5617665946483612
  - 1.5491695642471315
  - 1.5208914279937744
  - 1.4954597413539887
  - 1.4676985323429108
  - 1.5259434282779694
  - 1.5499677896499635
  - 1.4848400712013246
  - 1.529499876499176
  - 1.508868157863617
  - 1.5668557047843934
  - 1.5473574340343477
  - 1.5247975915670395
  - 1.6041254520416262
  - 1.5048879593610764
  validation_losses:
  - 0.3949674069881439
  - 0.4073052704334259
  - 0.39216503500938416
  - 0.4116349220275879
  - 0.40093910694122314
  - 0.40991833806037903
  - 0.40774062275886536
  - 0.40711838006973267
  - 0.40774625539779663
  - 0.395090788602829
  - 0.40512803196907043
  - 0.410884827375412
  - 0.6183593273162842
  - 0.4742894470691681
  - 0.5383629202842712
  - 0.4800657629966736
  - 0.41938480734825134
  - 0.3980320692062378
  - 0.4086083471775055
  - 0.41087549924850464
  - 0.391733855009079
  - 0.4670264720916748
  - 0.8616312742233276
  - 0.822693407535553
  - 0.3941759765148163
  - 0.3965928554534912
  - 0.40308231115341187
  - 0.4305877685546875
  - 0.3877193033695221
  - 0.38825803995132446
  - 0.4266619384288788
  - 0.4716109037399292
  - 0.538790225982666
  - 0.467038631439209
  - 0.6647886037826538
  - 0.6946535110473633
  - 0.6120426654815674
  - 0.5634945034980774
  - 0.7584645748138428
  - 0.7325189709663391
  - 0.5159299969673157
  - 1.3378636837005615
  - 1.064629077911377
  - 0.40285784006118774
  - 0.39925697445869446
  - 0.4039037525653839
  - 0.4128897190093994
  - 0.38942086696624756
loss_records_fold2:
  train_losses:
  - 1.56718829870224
  - 1.604243987798691
  - 1.5202785670757295
  - 1.505693817138672
  - 1.5034927427768707
  - 1.548481994867325
  - 1.5045773416757584
  - 1.5548879981040955
  - 1.520251303911209
  - 1.5827468663454056
  - 1.5188700914382935
  - 1.6074604451656342
  validation_losses:
  - 0.38904163241386414
  - 0.3871462643146515
  - 0.38160187005996704
  - 0.39527350664138794
  - 0.3952959477901459
  - 0.4055403470993042
  - 0.39008334279060364
  - 0.3897359371185303
  - 0.39782652258872986
  - 0.3838454484939575
  - 0.39145341515541077
  - 0.3876504898071289
loss_records_fold3:
  train_losses:
  - 1.519694769382477
  - 1.4908244431018831
  - 1.6228948771953584
  - 1.5625388503074646
  - 1.5620690047740937
  - 1.565209072828293
  - 1.6035943746566774
  - 1.5367447674274446
  - 1.5443471014499666
  - 1.59452241063118
  - 1.524783092737198
  - 1.512259477376938
  - 1.5330381512641909
  - 1.5554904758930208
  - 1.5266452193260194
  - 1.5042904496192933
  - 1.5338541269302368
  - 1.542398589849472
  - 1.531393998861313
  - 1.4908132553100586
  - 1.5269739866256715
  - 1.523670953512192
  - 1.50147442817688
  - 1.5854351997375489
  - 1.510842326283455
  - 1.499592661857605
  - 1.6051119446754456
  - 1.5005585134029389
  - 1.5629306137561798
  - 1.6365363895893097
  - 1.506876826286316
  - 1.5479536056518555
  - 1.5177751898765566
  - 1.505907988548279
  - 1.5197911620140077
  - 1.5488083600997926
  - 1.5007545769214632
  - 1.5006085097789765
  - 1.4800936698913576
  - 1.638425201177597
  - 1.519599288702011
  - 1.547794133424759
  - 1.573436963558197
  - 1.4937458395957948
  - 1.4779262185096742
  - 1.4788042426109316
  - 1.52929904460907
  - 1.5453399658203126
  - 1.5273808896541596
  - 1.5204674720764162
  - 1.483466309309006
  - 1.5048215091228485
  - 1.4922132015228273
  - 1.5308325707912447
  - 1.5474933028221132
  - 1.4732556104660035
  - 1.462929004430771
  - 1.4997792422771454
  - 1.4788618385791779
  - 1.5517437160015106
  - 1.5435992032289505
  - 1.5177476942539216
  - 1.5129634976387025
  - 1.5086913645267488
  - 1.4935729682445527
  - 1.5048151195049286
  - 1.516942310333252
  - 1.5725655853748322
  - 1.4959571897983552
  - 1.5355918288230896
  - 1.5569960176944733
  - 1.4599596917629243
  - 1.5347138941287994
  - 1.5319364547729493
  - 1.5365486204624177
  - 1.5205421328544617
  - 1.5428717553615572
  - 1.5132153749465944
  - 1.552216786146164
  - 1.5485113263130188
  - 1.509089356660843
  - 1.5091149270534516
  - 1.520505452156067
  - 1.465740329027176
  - 1.5113278567790986
  - 1.5772554218769075
  - 1.5051345854997635
  validation_losses:
  - 0.3906596004962921
  - 0.38240158557891846
  - 0.3933129608631134
  - 0.3744068443775177
  - 0.3826334476470947
  - 0.37237632274627686
  - 0.3717758357524872
  - 0.37650448083877563
  - 0.3876912295818329
  - 0.3782236576080322
  - 0.38998132944107056
  - 0.3764018714427948
  - 0.37519770860671997
  - 0.3645229935646057
  - 0.3766513764858246
  - 0.4007143974304199
  - 0.3758772611618042
  - 0.36973974108695984
  - 0.38556668162345886
  - 0.4198615849018097
  - 0.391415536403656
  - 0.37692052125930786
  - 0.3798450827598572
  - 0.4258878231048584
  - 0.3814609944820404
  - 0.39539480209350586
  - 0.4057181477546692
  - 0.39849042892456055
  - 0.37405118346214294
  - 0.3967529833316803
  - 0.3796601891517639
  - 0.39033517241477966
  - 0.3805314004421234
  - 0.3775862455368042
  - 0.39460429549217224
  - 0.3712184727191925
  - 0.3851924538612366
  - 0.3971356153488159
  - 3.1186091899871826
  - 0.3899591267108917
  - 0.39858806133270264
  - 0.43015256524086
  - 0.383146733045578
  - 0.39572444558143616
  - 0.42508530616760254
  - 0.6729158759117126
  - 0.38326317071914673
  - 0.4018610417842865
  - 0.4164346754550934
  - 0.49642491340637207
  - 0.4247679114341736
  - 1.6635926961898804
  - 4.120261192321777
  - 0.46876445412635803
  - 0.6320137977600098
  - 0.4749350845813751
  - 0.5049291849136353
  - 0.6235423684120178
  - 3.4872658252716064
  - 22.50640869140625
  - 0.40571075677871704
  - 0.3741671144962311
  - 0.46193814277648926
  - 0.4108986258506775
  - 2.2131292819976807
  - 0.42062562704086304
  - 0.41490957140922546
  - 0.4576484262943268
  - 0.9041480422019958
  - 2.4531431198120117
  - 0.8701440095901489
  - 0.45264092087745667
  - 0.5032562613487244
  - 0.4980492889881134
  - 1.593151569366455
  - 0.3737719655036926
  - 0.40230217576026917
  - 0.38961708545684814
  - 0.6782068610191345
  - 0.47637149691581726
  - 0.8740140795707703
  - 0.8392431735992432
  - 0.7544430494308472
  - 0.6651158332824707
  - 0.6136508584022522
  - 0.40790730714797974
  - 0.40802714228630066
loss_records_fold4:
  train_losses:
  - 1.4927751898765564
  - 1.5898345053195955
  - 1.5306331396102906
  - 1.486613827943802
  - 1.557794523239136
  - 1.5507672846317293
  - 1.515888971090317
  - 1.4702659964561464
  - 1.5309620678424836
  - 1.4691390573978425
  - 1.4824749886989594
  - 1.4609317392110825
  - 1.5050130188465118
  - 1.4677117824554444
  - 1.483043557405472
  - 1.5030316948890687
  - 1.47393998503685
  - 1.4649220705032349
  - 1.4779630839824678
  - 1.4734358429908754
  - 1.4923382580280304
  - 1.49307599067688
  - 1.5242493331432343
  - 1.469496437907219
  - 1.4536163032054903
  - 1.505014991760254
  - 1.5536158055067064
  - 1.4513226687908174
  - 1.4587990909814836
  - 1.4656952857971193
  - 1.4737996935844422
  - 1.4388042390346527
  - 1.4837492644786836
  - 1.4597298443317415
  - 1.534978097677231
  - 1.4545091569423676
  - 1.4775020837783814
  - 1.4935753613710405
  - 1.556896436214447
  - 1.5044443547725679
  - 1.496712112426758
  - 1.4679110169410707
  - 1.5357660710811616
  - 1.4709008872509004
  - 1.5162924230098724
  - 1.4704420268535614
  - 1.4582049429416657
  - 1.4863695800304413
  - 1.4348213255405426
  validation_losses:
  - 0.3812274932861328
  - 0.3769829273223877
  - 0.3816448748111725
  - 0.37003958225250244
  - 0.38199183344841003
  - 0.3641209900379181
  - 0.3694842457771301
  - 0.3852846920490265
  - 0.4311990737915039
  - 0.38258224725723267
  - 0.3805859386920929
  - 0.5082297325134277
  - 0.3713107109069824
  - 0.36513981223106384
  - 0.373495876789093
  - 0.4065444767475128
  - 0.36952343583106995
  - 1.1497676372528076
  - 0.37382611632347107
  - 0.37615713477134705
  - 2.752206563949585
  - 0.37335100769996643
  - 0.384109228849411
  - 0.5564936995506287
  - 0.3714851438999176
  - 0.3517625629901886
  - 0.37183496356010437
  - 0.37129920721054077
  - 0.4879568815231323
  - 0.36931365728378296
  - 0.38184836506843567
  - 0.4462338089942932
  - 0.4705297648906708
  - 0.76859450340271
  - 0.4358806610107422
  - 0.37296006083488464
  - 0.36816999316215515
  - 0.3842895030975342
  - 0.3803309202194214
  - 0.37111324071884155
  - 0.3616320490837097
  - 0.38455772399902344
  - 0.4020702540874481
  - 0.3928346633911133
  - 0.4021279811859131
  - 0.38182416558265686
  - 0.3794182240962982
  - 0.3808327317237854
  - 0.381740927696228
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 48 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 87 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 49 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8576329331046312,
    0.8505154639175257]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.08421052631578949]'
  mean_eval_accuracy: 0.856552492440452
  mean_f1_accuracy: 0.016842105263157898
  total_train_time: '0:18:03.952203'
