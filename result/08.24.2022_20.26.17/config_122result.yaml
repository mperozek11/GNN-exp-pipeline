config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:16:35.205931'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_122fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_122fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_122fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_122fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_122fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_122fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_122fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_122fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_122fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_122fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 20.13573569059372
  - 14.42558697462082
  - 11.6271493434906
  - 6.255423974990845
  - 8.72855839729309
  - 13.270677149295807
  - 13.216876721382143
  - 6.626541286706924
  - 4.277983802556991
  - 2.6078335762023928
  - 2.9552639186382295
  - 2.1404614806175233
  - 2.3184762001037598
  - 9.597186315059663
  - 7.401094812154771
  - 4.029247760772705
  - 2.925371354818344
  - 1.803000780940056
  - 2.621975374221802
  - 2.450921416282654
  - 2.69039558172226
  - 3.447024416923523
  - 3.7024563133716586
  - 2.9864916980266574
  - 4.9517534434795385
  - 2.3420779705047607
  - 2.0957847595214845
  - 4.1356186985969545
  - 3.5139388144016266
  - 2.897878992557526
  - 1.6712326288223267
  - 2.3658359706401826
  - 1.8564918339252472
  - 1.6288057923316956
  - 2.6724159717559814
  - 3.0477338612079623
  - 3.5605273187160495
  - 4.609713971614838
  - 2.7545105129480363
  - 2.782991409301758
  - 2.167668968439102
  - 4.5977463066577915
  - 5.651902520656586
  - 2.832638680934906
  - 2.2838160872459414
  - 2.3457592964172362
  - 4.893236351013184
  - 4.1184790432453156
  - 1.6277878403663637
  - 1.575342744588852
  - 4.908110088109971
  - 1.9088210821151734
  - 3.1771637558937074
  - 1.713111585378647
  - 1.6111467778682709
  - 1.737655794620514
  - 4.44120199084282
  - 3.5299425780773164
  - 2.8200253546237946
  - 3.973605972528458
  - 2.46809898018837
  - 2.920851469039917
  - 1.8795936584472657
  - 2.135981982946396
  - 1.5534653186798097
  validation_losses:
  - 11.011143684387207
  - 3.7082197666168213
  - 1.2886115312576294
  - 5.590961933135986
  - 1.4680216312408447
  - 6.898401737213135
  - 1.5697213411331177
  - 1.650088906288147
  - 0.5029476881027222
  - 0.4708021879196167
  - 0.5997174978256226
  - 0.4746595323085785
  - 0.545349657535553
  - 0.631260871887207
  - 0.5682995319366455
  - 0.4743373394012451
  - 1.0578984022140503
  - 0.4051945209503174
  - 0.4144054055213928
  - 0.40300115942955017
  - 0.518150269985199
  - 0.41807472705841064
  - 0.9068532586097717
  - 0.495425820350647
  - 0.6718600392341614
  - 0.4412517249584198
  - 0.38443905115127563
  - 0.421210914850235
  - 0.5017561912536621
  - 0.5342671275138855
  - 0.39090654253959656
  - 0.44806724786758423
  - 0.4158582389354706
  - 0.39111456274986267
  - 0.393011212348938
  - 0.43104755878448486
  - 0.5743564367294312
  - 0.5075556635856628
  - 0.47826719284057617
  - 0.44760870933532715
  - 0.39995071291923523
  - 0.44577309489250183
  - 0.4231376051902771
  - 0.46848008036613464
  - 0.38985133171081543
  - 0.39834269881248474
  - 0.4193163812160492
  - 0.3944805860519409
  - 0.40675070881843567
  - 0.3945907652378082
  - 0.38886913657188416
  - 0.40052881836891174
  - 0.39599907398223877
  - 0.3942105174064636
  - 0.39056679606437683
  - 0.4027532637119293
  - 0.38752779364585876
  - 0.39337295293807983
  - 0.54044508934021
  - 0.39043962955474854
  - 0.38821709156036377
  - 0.3909284174442291
  - 0.3903246521949768
  - 0.3994596004486084
  - 0.3944602608680725
loss_records_fold1:
  train_losses:
  - 1.6864632725715638
  - 1.5438233226537705
  - 1.4637320578098298
  - 1.4303770869970323
  - 1.6559848308563234
  - 1.4960463047027588
  - 2.319464296102524
  - 1.6126867592334748
  - 1.6149132311344148
  - 1.5614617049694062
  - 1.6623749494552613
  validation_losses:
  - 0.4001578986644745
  - 0.44463086128234863
  - 0.4005296528339386
  - 0.3971734344959259
  - 0.4788815379142761
  - 0.4382016062736511
  - 0.42683544754981995
  - 0.40823894739151
  - 0.3973110020160675
  - 0.39271149039268494
  - 0.3951853811740875
loss_records_fold2:
  train_losses:
  - 1.5319505751132967
  - 2.068235820531845
  - 2.446089428663254
  - 2.2587700724601745
  - 1.476788690686226
  - 3.1439443409442904
  - 1.6885740220546723
  - 2.0458512157201767
  - 1.7567773044109345
  - 3.134677988290787
  - 2.124674028158188
  - 2.403223502635956
  - 1.5681825518608095
  - 1.5351049065589906
  - 2.447666257619858
  - 1.5426513254642487
  - 1.6222883284091951
  - 1.574130976200104
  - 1.5162704408168795
  - 1.482081651687622
  - 1.7651355504989625
  - 2.5319276392459873
  - 3.1057701051235203
  - 1.6010307729244233
  - 3.6193653106689454
  - 2.595674502849579
  - 3.5905917525291446
  - 2.2582543373107913
  - 1.555943912267685
  - 2.117880940437317
  - 1.527298629283905
  - 1.6227935373783113
  - 1.9340511322021485
  - 1.5641351401805879
  - 2.2547885656356814
  - 1.6035256862640381
  - 1.514717549085617
  - 1.5329543471336367
  - 1.5743643283843995
  validation_losses:
  - 0.472534716129303
  - 0.38283824920654297
  - 0.425396203994751
  - 0.46365636587142944
  - 1.4945793151855469
  - 0.9759501814842224
  - 0.37525513768196106
  - 0.4620729982852936
  - 0.4085986018180847
  - 0.46946558356285095
  - 0.40161457657814026
  - 0.38062962889671326
  - 0.38096264004707336
  - 0.3929925262928009
  - 0.39670634269714355
  - 0.3875584304332733
  - 0.38106659054756165
  - 0.3912954330444336
  - 0.38214319944381714
  - 0.387370765209198
  - 0.3803602159023285
  - 0.4082527458667755
  - 0.38152605295181274
  - 0.3828389048576355
  - 0.38460850715637207
  - 0.380503386259079
  - 0.39371541142463684
  - 0.3905300498008728
  - 0.38203272223472595
  - 0.38398584723472595
  - 0.38077986240386963
  - 0.38701844215393066
  - 0.44117066264152527
  - 0.38157084584236145
  - 0.38285180926322937
  - 0.38719671964645386
  - 0.3858855068683624
  - 0.3882693946361542
  - 0.3829546570777893
loss_records_fold3:
  train_losses:
  - 1.6003367185592652
  - 1.566674268245697
  - 1.7588741898536684
  - 1.8433489441871644
  - 1.5537643551826479
  - 1.5107942044734957
  - 2.0971133410930634
  - 1.968743544816971
  - 1.750401568412781
  - 1.8624952077865602
  - 3.036513385176659
  - 1.5741383552551271
  - 1.5409206569194795
  - 1.5949814140796663
  - 1.5358251929283142
  - 1.5291882753372192
  - 1.5071684122085571
  validation_losses:
  - 0.3960062265396118
  - 0.40094897150993347
  - 0.5119619369506836
  - 0.4229510724544525
  - 0.39324480295181274
  - 0.392715185880661
  - 0.3961590826511383
  - 0.40974828600883484
  - 0.42889952659606934
  - 0.4366171360015869
  - 0.47215378284454346
  - 0.3986402750015259
  - 0.39322423934936523
  - 0.3940667510032654
  - 0.39850035309791565
  - 0.40542539954185486
  - 0.3980860710144043
loss_records_fold4:
  train_losses:
  - 1.5252957403659821
  - 1.4955742359161377
  - 1.48506361246109
  - 1.4924563974142075
  - 1.4967461109161377
  - 1.5175447821617127
  - 1.471389788389206
  - 1.5424866259098053
  - 1.5918411910533905
  - 1.4696156442165376
  - 1.5450650930404664
  - 1.6540385127067567
  validation_losses:
  - 0.4120180308818817
  - 0.3943701684474945
  - 0.3970738351345062
  - 0.3925178349018097
  - 0.3925970196723938
  - 0.40905138850212097
  - 0.3942027986049652
  - 0.3927638530731201
  - 0.39137211441993713
  - 0.4005257487297058
  - 0.3948243260383606
  - 0.39736810326576233
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 65 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 39 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:12:27.654724'
