config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:11:38.794665'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_87fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_87fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_87fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_87fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_87fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_87fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_87fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_87fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_87fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_87fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 0.9619135439395905
  - 0.9471165895462037
  - 0.8662152647972108
  - 0.8403665184974671
  - 0.8925118386745453
  - 0.8495535969734193
  - 0.8900171577930451
  - 0.8186184883117676
  - 0.8548575222492218
  - 0.8204811990261078
  - 0.9004938840866089
  validation_losses:
  - 0.4460165500640869
  - 0.49295929074287415
  - 0.5054969191551208
  - 0.4348160922527313
  - 0.40353670716285706
  - 0.4022675156593323
  - 0.4078523516654968
  - 0.4142554700374603
  - 0.4003288447856903
  - 0.3995124101638794
  - 0.39027613401412964
loss_records_fold1:
  train_losses:
  - 0.8080748796463013
  - 0.8368315279483796
  - 0.8453609824180603
  - 0.8760654032230377
  - 0.8519188106060028
  - 0.82693030834198
  - 0.845483809709549
  - 0.8180313289165497
  - 0.8956795632839203
  - 0.8247386455535889
  - 0.8458016514778137
  validation_losses:
  - 0.39476898312568665
  - 0.3995323181152344
  - 0.3963564932346344
  - 0.398368239402771
  - 0.40146952867507935
  - 0.396614670753479
  - 0.40650680661201477
  - 0.4024698734283447
  - 0.41176217794418335
  - 0.41159874200820923
  - 0.39538317918777466
loss_records_fold2:
  train_losses:
  - 0.817986798286438
  - 0.778339296579361
  - 0.7972681522369385
  - 0.8255297303199769
  - 0.7999113440513611
  - 0.8684081792831422
  - 0.8279060959815979
  - 0.8364152073860169
  - 0.8285291194915771
  - 0.7909481287002564
  - 0.7948214888572693
  - 0.8342919290065766
  - 0.8495182752609254
  - 0.98717080950737
  - 0.843065506219864
  - 0.8788713157176972
  - 0.9104895830154419
  - 0.8284008085727692
  - 0.8351481199264527
  - 0.8458215355873109
  validation_losses:
  - 0.3906974196434021
  - 0.3860712945461273
  - 0.41263914108276367
  - 0.4152398109436035
  - 0.4207533597946167
  - 0.42661723494529724
  - 0.43861231207847595
  - 0.46633827686309814
  - 0.448054701089859
  - 0.41932374238967896
  - 0.4103308320045471
  - 0.4103526771068573
  - 0.5580801367759705
  - 0.7131780385971069
  - 0.39652082324028015
  - 0.4012260437011719
  - 0.3932749032974243
  - 0.3896936774253845
  - 0.39589932560920715
  - 0.40435755252838135
loss_records_fold3:
  train_losses:
  - 0.8266769766807557
  - 0.7965810388326645
  - 0.8236865401268005
  - 0.7856123983860016
  - 0.7950290322303772
  - 0.8004930794239045
  - 0.809571933746338
  - 0.7885602593421936
  - 0.7679423063993455
  - 0.8483884096145631
  - 0.9972450375556946
  - 0.8914438962936402
  - 0.8259128272533417
  - 0.8246205151081085
  - 0.8225167334079743
  - 0.8197946190834046
  - 0.8119771659374238
  - 0.7983909904956819
  - 0.7865493297576904
  - 0.7914670199155808
  - 0.7927775472402573
  - 0.7824747443199158
  - 0.8065624535083771
  - 0.786179256439209
  - 0.7818533062934876
  - 0.8814593374729157
  - 0.8090909838676453
  - 0.8665544033050537
  - 0.8545174598693848
  - 0.8627848267555237
  - 0.8703588485717774
  - 0.7815992534160614
  - 0.8690161585807801
  - 0.8127167403697968
  - 0.8132566094398499
  - 0.8396985590457917
  - 0.8599423289299012
  - 0.8399738311767578
  - 0.8724192559719086
  - 0.8205514430999756
  - 0.8763091802597046
  - 0.8369771718978882
  - 0.8950883984565735
  - 0.8423761546611787
  - 0.9382022321224213
  validation_losses:
  - 0.41247066855430603
  - 0.4080965220928192
  - 0.40649309754371643
  - 0.42181888222694397
  - 0.5411819815635681
  - 0.6897315382957458
  - 0.7706103324890137
  - 0.6199531555175781
  - 0.5633449554443359
  - 0.6047174334526062
  - 0.39052844047546387
  - 0.4162558913230896
  - 0.3792400658130646
  - 0.3804870843887329
  - 0.5470535159111023
  - 0.9886020421981812
  - 1.045069694519043
  - 1.5017726421356201
  - 1.6823710203170776
  - 1.8748358488082886
  - 0.556291401386261
  - 1.5067365169525146
  - 1.1921148300170898
  - 1.2826932668685913
  - 1.470988392829895
  - 2.7813241481781006
  - 1.9248074293136597
  - 0.38078218698501587
  - 0.3931104838848114
  - 0.3871258497238159
  - 0.3778742849826813
  - 0.37083497643470764
  - 0.38092491030693054
  - 0.3878796696662903
  - 0.3905952274799347
  - 0.4251072108745575
  - 0.4898996949195862
  - 0.3789609968662262
  - 0.39671432971954346
  - 0.3859442472457886
  - 0.37994903326034546
  - 0.3792198598384857
  - 0.3839494585990906
  - 0.3780030310153961
  - 0.37797147035598755
loss_records_fold4:
  train_losses:
  - 0.8489956378936768
  - 0.8133523225784303
  - 0.8679238855838776
  - 0.8418179333209992
  - 0.8698510110378266
  - 0.8569018363952637
  - 0.8743216693401337
  - 0.8256179511547089
  - 0.8897091805934907
  - 0.8387106776237488
  - 0.8117424070835114
  - 0.8013909816741944
  - 0.8077243745326996
  - 0.9272526621818543
  - 0.7956543028354646
  - 0.8274761855602265
  - 0.8253842473030091
  - 0.8220199882984162
  - 0.7980806708335877
  - 0.8042758405208588
  - 0.814717721939087
  - 0.7994821310043335
  - 0.8251156210899353
  - 0.8315766751766205
  - 0.8162542700767518
  - 0.8284357309341431
  validation_losses:
  - 0.38201460242271423
  - 0.3770017623901367
  - 0.3897286355495453
  - 0.38541075587272644
  - 0.38410869240760803
  - 0.4199022352695465
  - 0.42227619886398315
  - 0.38910794258117676
  - 0.39081597328186035
  - 0.3895106017589569
  - 0.3878132998943329
  - 0.49407923221588135
  - 0.3819275498390198
  - 0.5125446319580078
  - 0.3797057867050171
  - 0.44889500737190247
  - 0.41042277216911316
  - 0.3988338112831116
  - 0.37766730785369873
  - 0.3950778543949127
  - 0.38845744729042053
  - 0.39008477330207825
  - 0.37998443841934204
  - 0.3826061189174652
  - 0.3795192837715149
  - 0.37732845544815063
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8576329331046312, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:09:17.836770'
