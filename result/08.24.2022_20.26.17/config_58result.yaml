config:
  aggregation: sum
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:44:58.099696'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_58fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_58fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_58fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_58fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_58fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_58fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_58fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_58fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_58fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_58fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 28.094085133075716
  - 8.260982966423034
  - 7.356954896450043
  - 3.218131786584854
  - 2.4367836296558383
  - 2.6840077698230744
  - 4.114012050628662
  - 2.7083637803792957
  - 1.6066443353891373
  - 1.8394551634788514
  - 1.8746702134609223
  - 1.607165116071701
  - 2.0065947115421294
  - 1.9029496788978577
  - 1.7678287088871003
  - 2.100382387638092
  - 4.560202345252037
  - 4.419960683584214
  - 3.3991989076137545
  - 3.483371558785439
  - 2.6375852942466738
  - 4.307951021194458
  - 4.513112556934357
  - 6.083893179893494
  - 2.7729240715503694
  - 2.343791875243187
  - 2.0273082315921784
  - 2.0896947145462037
  - 1.8886760950088501
  - 1.6028705298900605
  - 1.6480384349822998
  - 2.3821051120758057
  - 2.292024624347687
  - 1.8414056599140167
  - 1.7500532269477844
  - 1.5645755887031556
  - 1.6912196397781374
  - 2.106963962316513
  - 1.7011034816503525
  - 1.542018097639084
  - 1.9823057532310486
  - 2.1246490836143495
  - 2.1494536578655246
  - 1.870459645986557
  - 1.513693517446518
  - 1.9464832901954652
  - 1.5996759474277498
  - 1.4368740856647493
  - 1.4731120288372042
  - 1.4562344253063202
  - 1.4774057924747468
  - 1.4650019228458406
  - 1.5119925320148468
  - 1.5381717741489411
  - 1.618886297941208
  - 2.068155008554459
  - 2.3777357935905457
  - 1.824005365371704
  - 1.7242927432060242
  - 1.5115942895412446
  - 1.4474806040525436
  - 1.7893560707569123
  - 3.1965721964836122
  - 1.8826823890209199
  - 2.2226893663406373
  - 1.6276987493038177
  - 1.7748912513256074
  - 1.5307132005691528
  - 1.7889394342899323
  - 1.5687845945358276
  - 2.4044509112834933
  - 3.2169444620609284
  - 3.2706116497516633
  - 1.6833187937736511
  - 1.5974140524864198
  - 2.850286251306534
  - 1.5583827793598175
  - 1.464453274011612
  - 1.4685328841209413
  - 1.4210782855749131
  - 1.5059238731861115
  - 1.6859495520591736
  - 1.5989383339881897
  - 1.49487019777298
  - 1.9369044542312623
  - 1.477879410982132
  - 1.5095598220825197
  - 1.4624818861484528
  - 1.738013356924057
  - 1.4602899491786958
  - 1.569009953737259
  - 1.4392937541007997
  - 1.995888525247574
  - 1.6106237173080444
  - 1.6373797774314882
  - 2.303880774974823
  - 1.4983795315027237
  - 1.540856009721756
  - 1.517881590127945
  - 1.4355562627315521
  validation_losses:
  - 5.54684591293335
  - 1.1265337467193604
  - 2.599045515060425
  - 0.5026274919509888
  - 0.4561333656311035
  - 0.44681859016418457
  - 1.1065932512283325
  - 0.44694066047668457
  - 0.48943978548049927
  - 0.38471490144729614
  - 0.4934152364730835
  - 0.4981372356414795
  - 0.4148600101470947
  - 0.4412235617637634
  - 0.46747711300849915
  - 0.5065873265266418
  - 0.7128835320472717
  - 0.4723284840583801
  - 0.9386996626853943
  - 1.0469943284988403
  - 0.6247044205665588
  - 0.4069228768348694
  - 0.502916157245636
  - 0.3985544741153717
  - 0.4199365973472595
  - 0.40872734785079956
  - 0.4057444632053375
  - 0.41076940298080444
  - 0.40174034237861633
  - 0.3783898949623108
  - 0.41643232107162476
  - 0.4354976713657379
  - 0.3954699635505676
  - 0.3826531767845154
  - 0.5034827589988708
  - 0.3985062837600708
  - 0.4497614800930023
  - 0.42908334732055664
  - 0.37793442606925964
  - 0.38338175415992737
  - 0.6532652378082275
  - 0.3845541477203369
  - 0.5714817047119141
  - 0.4154277443885803
  - 0.4767872989177704
  - 0.42682963609695435
  - 0.3803359270095825
  - 0.3932209312915802
  - 0.3881876766681671
  - 0.43880268931388855
  - 0.5587919354438782
  - 0.37408339977264404
  - 0.42469078302383423
  - 0.3977854251861572
  - 0.3727054297924042
  - 0.5179212689399719
  - 0.5952746272087097
  - 0.3999257981777191
  - 0.3937087655067444
  - 0.38054758310317993
  - 0.3806757926940918
  - 0.5027238726615906
  - 0.513414204120636
  - 0.4563749432563782
  - 0.38231223821640015
  - 0.46709537506103516
  - 0.3770284652709961
  - 0.493220716714859
  - 0.42308753728866577
  - 0.3846690356731415
  - 0.39634326100349426
  - 0.3799978196620941
  - 0.4380649924278259
  - 0.4188956916332245
  - 0.4298403561115265
  - 0.5073883533477783
  - 0.38974523544311523
  - 0.38046562671661377
  - 0.37709105014801025
  - 0.37757018208503723
  - 0.39910629391670227
  - 0.38403794169425964
  - 0.37682420015335083
  - 0.37821754813194275
  - 0.4156259298324585
  - 0.39811503887176514
  - 0.3803968131542206
  - 0.3718118667602539
  - 0.37066230177879333
  - 0.38641583919525146
  - 0.3710249662399292
  - 0.3923133313655853
  - 0.41996094584465027
  - 0.41937074065208435
  - 0.3799501061439514
  - 0.41231781244277954
  - 0.3717288374900818
  - 0.3710622489452362
  - 0.37196221947669983
  - 0.37095022201538086
loss_records_fold1:
  train_losses:
  - 1.6619568169116974
  - 1.8232187628746033
  - 1.659944361448288
  - 1.5602445363998414
  - 1.5294506430625916
  - 1.5380001604557039
  - 1.5409390330314636
  - 1.9485191226005556
  - 1.7788244158029558
  - 1.4856952369213106
  - 1.5623323947191239
  - 2.481851148605347
  - 2.3731730401515962
  - 1.779908859729767
  - 1.7933333098888398
  - 1.6706669867038728
  - 1.568812072277069
  - 2.334126091003418
  - 1.4995078563690187
  - 1.6679937094449997
  - 1.3884475886821748
  - 1.461254781484604
  - 1.4193181663751604
  - 1.7620671808719637
  - 1.5500066220760347
  - 1.90058434009552
  - 2.1209779918193816
  - 2.1763295471668243
  - 4.997698211669922
  - 2.109725734591484
  - 1.9810039162635804
  - 1.5547243714332581
  - 1.4839907169342041
  - 1.41519011259079
  - 1.5489130020141602
  - 1.6704892694950104
  - 1.5829631686210632
  - 1.5364460408687592
  - 1.5807152152061463
  - 1.5135541319847108
  - 1.4700700402259828
  - 1.4111146301031114
  - 1.5079745054244995
  - 1.5119485795497896
  - 1.4403412878513338
  - 1.4016867697238924
  - 1.4047918677330018
  - 1.4780655086040497
  - 1.5462480306625368
  - 1.4977338671684266
  - 1.4803340673446657
  - 1.4855203926563263
  - 1.4369415998458863
  - 1.3925378561019899
  - 1.4650829613208771
  - 1.4472694754600526
  - 1.4426214814186098
  - 1.4552315533161164
  - 1.4198863863945008
  - 1.4319089442491533
  - 1.4671698212623596
  - 1.424801516532898
  - 1.552419948577881
  - 1.471370905637741
  - 1.4264268636703492
  - 1.4489537358283997
  - 1.4185013711452485
  - 1.4472048938274384
  - 1.4800358176231385
  - 1.4595511913299561
  - 1.4159518092870713
  - 1.4356255114078522
  - 1.4147103667259218
  - 1.5756219923496246
  - 1.4767650246620179
  - 1.5944183945655823
  - 1.6039583563804627
  - 1.511298441886902
  - 1.4838085591793062
  - 1.4310536146163941
  - 1.6858827710151674
  - 1.452573174238205
  - 1.475577861070633
  - 1.4503802239894867
  - 1.4369862198829653
  - 1.4412047803401948
  - 1.5130706429481506
  - 1.4391491115093231
  - 1.4298410773277284
  - 1.4139836013317109
  - 1.4304091155529024
  - 1.4278992652893068
  - 1.529104858636856
  - 1.4122773468494416
  - 1.474090924859047
  - 1.5389203310012818
  - 1.4374697744846345
  validation_losses:
  - 0.4296623766422272
  - 0.4302952289581299
  - 0.45339107513427734
  - 0.39192995429039
  - 0.4091729521751404
  - 0.40809306502342224
  - 0.4578469693660736
  - 0.45903944969177246
  - 0.3947596848011017
  - 0.38557887077331543
  - 0.4613218903541565
  - 0.4102068841457367
  - 0.41703909635543823
  - 0.4151764214038849
  - 0.5132127404212952
  - 0.42356160283088684
  - 0.4001699388027191
  - 0.4423108994960785
  - 0.41072195768356323
  - 0.39063379168510437
  - 0.38619667291641235
  - 0.39021921157836914
  - 0.38237234950065613
  - 0.4492553472518921
  - 0.39774566888809204
  - 0.4742676615715027
  - 0.39327070116996765
  - 0.6507855653762817
  - 0.4162311255931854
  - 0.48801153898239136
  - 0.41877481341362
  - 0.4032198190689087
  - 0.39719128608703613
  - 0.4245802164077759
  - 0.3930418789386749
  - 0.42019161581993103
  - 0.5058593153953552
  - 0.3996070325374603
  - 0.4531934857368469
  - 0.3938957452774048
  - 0.39398089051246643
  - 0.39862683415412903
  - 0.45214787125587463
  - 0.41147086024284363
  - 0.4054611027240753
  - 0.3964349925518036
  - 0.47273901104927063
  - 0.4146343469619751
  - 0.4038163125514984
  - 0.41127297282218933
  - 0.4461483657360077
  - 0.3970513939857483
  - 0.3942073881626129
  - 0.42439255118370056
  - 0.3936907947063446
  - 0.4102368950843811
  - 0.394037127494812
  - 0.3994717597961426
  - 0.40188533067703247
  - 0.3916662931442261
  - 0.4117089509963989
  - 0.40068376064300537
  - 0.4091472923755646
  - 0.42464253306388855
  - 0.3985416293144226
  - 0.47261297702789307
  - 0.4815863072872162
  - 0.39654645323753357
  - 0.39586198329925537
  - 0.4069916009902954
  - 0.4014686346054077
  - 0.39511412382125854
  - 0.3956776559352875
  - 0.39963021874427795
  - 0.4329761266708374
  - 0.41388195753097534
  - 0.4038335084915161
  - 0.406053751707077
  - 0.3971098065376282
  - 0.41124674677848816
  - 0.4012221097946167
  - 0.39975640177726746
  - 0.4041192829608917
  - 0.4021783769130707
  - 0.4185639023780823
  - 0.41006705164909363
  - 0.39937084913253784
  - 0.41050755977630615
  - 0.4025709331035614
  - 0.398911714553833
  - 0.43552297353744507
  - 0.4014833867549896
  - 0.4076431393623352
  - 0.41177958250045776
  - 0.408780038356781
  - 0.40825819969177246
  - 0.40274569392204285
loss_records_fold2:
  train_losses:
  - 1.4448423981666565
  - 1.4624281585216523
  - 1.5044020384550096
  - 1.5199199855327608
  - 1.4727587282657624
  - 1.6679507315158846
  - 1.4957097291946413
  - 1.422643768787384
  - 1.5273854911327363
  - 1.450804352760315
  - 1.4621308833360673
  - 1.4443169295787812
  - 1.4701015293598176
  - 1.4227700889110566
  - 1.4812481045722963
  - 1.5097472667694092
  - 1.448104625940323
  - 1.4692839324474336
  - 1.5288225948810579
  - 1.417215567827225
  - 1.5617526948451996
  - 1.6769814491271973
  - 1.4925132095813751
  - 1.5068135201931
  - 1.4883320391178132
  - 1.47755486369133
  - 1.8396341025829317
  - 1.6088688015937807
  - 1.818641859292984
  - 1.457107883691788
  - 1.4664886623620987
  - 1.5353748738765718
  - 1.4402951657772065
  - 1.5011910617351534
  - 1.5014941900968553
  - 1.468569129705429
  - 1.4908763945102692
  - 1.5275231570005419
  - 1.6064649611711503
  - 1.6222327709198
  - 1.6138971507549287
  - 1.6191156029701235
  - 1.5407714903354646
  - 1.7223449170589449
  - 1.6385942459106446
  - 1.4691704511642456
  - 1.4209846496582033
  - 1.447458690404892
  - 1.7383041203022005
  - 1.4325506925582887
  - 1.4409465193748474
  - 1.4606608748435974
  - 1.4532422482967378
  - 1.459012043476105
  - 1.4194475054740907
  - 1.4533845484256744
  - 1.4632297486066819
  - 1.4507975935935975
  - 1.4466648161411286
  - 1.4480007231235505
  - 1.4356353640556336
  - 1.4380803346633912
  - 1.4625627458095551
  - 1.455835145711899
  - 1.5214542955160142
  - 1.6098889350891115
  - 1.5502962648868561
  - 1.46718327999115
  - 1.477986890077591
  - 1.4575537860393526
  - 1.4242802560329437
  - 1.4637579143047335
  - 1.607236695289612
  - 1.5953957557678224
  - 1.4750925719738008
  - 1.421878480911255
  - 1.4549716949462892
  - 1.5020771503448487
  - 1.547091430425644
  - 1.4527694284915924
  - 1.4344913840293885
  - 1.425940877199173
  - 1.5042734682559968
  - 1.5009429574012758
  - 1.4780311077833177
  - 1.5688740372657777
  - 1.4524653553962708
  - 1.400587570667267
  - 1.4691524803638458
  - 1.554839861392975
  - 1.4330131888389588
  - 1.4334861993789674
  - 1.4457070469856264
  - 1.4032943427562714
  - 1.4380270659923555
  - 1.4534643083810808
  - 1.4778714776039124
  - 1.4392978131771088
  - 1.4621743977069857
  - 1.4400662600994112
  validation_losses:
  - 0.36978742480278015
  - 0.39394184947013855
  - 0.38475605845451355
  - 0.6814978718757629
  - 0.44318947196006775
  - 0.3743066191673279
  - 0.4605863392353058
  - 0.38043397665023804
  - 0.38646793365478516
  - 0.4087919592857361
  - 0.3856920599937439
  - 0.37702634930610657
  - 0.3767944872379303
  - 0.3712882101535797
  - 0.3774600923061371
  - 0.6115679144859314
  - 0.3872816860675812
  - 0.3689250349998474
  - 0.38176167011260986
  - 0.3694327771663666
  - 0.40004441142082214
  - 0.38167205452919006
  - 0.36857274174690247
  - 0.40475699305534363
  - 0.3802284002304077
  - 0.3727877140045166
  - 0.5088973045349121
  - 0.40044862031936646
  - 0.3871288299560547
  - 0.4013688266277313
  - 0.37429988384246826
  - 0.38444554805755615
  - 0.37653931975364685
  - 0.38641080260276794
  - 0.39043936133384705
  - 0.3709833025932312
  - 0.38245680928230286
  - 0.4213721752166748
  - 0.4282015562057495
  - 0.38776424527168274
  - 0.37500524520874023
  - 0.38678231835365295
  - 0.44942501187324524
  - 0.37204429507255554
  - 0.5040454268455505
  - 0.3923368752002716
  - 0.393551766872406
  - 0.3781047463417053
  - 0.3844861090183258
  - 0.4047110974788666
  - 0.41010451316833496
  - 0.36882829666137695
  - 0.37293022871017456
  - 0.40628135204315186
  - 0.37783682346343994
  - 0.37702468037605286
  - 0.36933067440986633
  - 0.3810734748840332
  - 0.36804822087287903
  - 0.4861505925655365
  - 0.3746813237667084
  - 0.3707922101020813
  - 0.3703497052192688
  - 0.37102535367012024
  - 0.4043068289756775
  - 0.39578932523727417
  - 0.37346529960632324
  - 0.3693791627883911
  - 0.37056997418403625
  - 0.3977147340774536
  - 0.3702050447463989
  - 0.3706086277961731
  - 0.49086061120033264
  - 0.39714309573173523
  - 0.3705345392227173
  - 0.37024691700935364
  - 0.3681318759918213
  - 0.36931222677230835
  - 0.422900527715683
  - 0.3902294933795929
  - 0.4086923897266388
  - 0.38766077160835266
  - 0.4694778323173523
  - 0.39572322368621826
  - 0.3716573119163513
  - 0.37223318219184875
  - 0.3710355758666992
  - 0.40400049090385437
  - 0.43780431151390076
  - 0.3693256378173828
  - 0.3861697018146515
  - 0.3728311061859131
  - 0.38889622688293457
  - 0.37235763669013977
  - 0.37166252732276917
  - 0.3929050862789154
  - 0.5261634588241577
  - 0.37090247869491577
  - 0.3719824254512787
  - 0.3700004518032074
loss_records_fold3:
  train_losses:
  - 1.5078414678573608
  - 1.5274704635143281
  - 1.5525322377681734
  - 1.5955474317073823
  - 1.5191848814487459
  - 1.4754388809204102
  - 1.538241344690323
  - 1.4638866901397707
  - 1.4396621346473695
  - 1.4530259251594544
  - 1.428592222929001
  - 1.4080595552921296
  - 1.4419289469718934
  - 1.4399357974529268
  - 1.4573819458484651
  - 1.4756691098213197
  - 1.4322348088026047
  - 2.1800541043281556
  - 1.6408141732215882
  - 1.503077632188797
  - 1.5309129655361176
  - 1.4783656179904938
  - 1.4372201025485993
  - 1.4559107542037966
  - 1.4101025015115738
  - 1.46872535943985
  - 1.4842089951038362
  - 1.4529460966587067
  - 1.4746088027954103
  - 1.4323639631271363
  - 1.4716608345508577
  - 1.403001379966736
  - 1.4847130358219147
  - 1.4428986430168154
  - 1.5045670330524445
  - 1.5979496598243714
  - 1.4857970118522645
  - 1.431216686964035
  - 1.4587591469287873
  - 1.4255117595195772
  - 1.472102451324463
  - 1.5016883730888368
  - 1.4754294097423555
  - 1.4590713679790497
  - 1.4409005522727967
  - 1.443396419286728
  - 1.4130024313926697
  - 1.4435069024562837
  - 1.4372409760951996
  - 1.4206501364707949
  - 1.4556373298168184
  validation_losses:
  - 0.37767744064331055
  - 0.47259292006492615
  - 0.47946634888648987
  - 0.3798743486404419
  - 0.3761477768421173
  - 0.3879527449607849
  - 0.3691266179084778
  - 0.38475531339645386
  - 0.37621670961380005
  - 0.37424522638320923
  - 0.3715178072452545
  - 0.37011104822158813
  - 0.3637291491031647
  - 0.45422396063804626
  - 0.3706991672515869
  - 0.4830268621444702
  - 0.3846214711666107
  - 0.38432663679122925
  - 0.45425012707710266
  - 0.39525678753852844
  - 0.40265172719955444
  - 0.447263240814209
  - 0.38664674758911133
  - 0.40491580963134766
  - 0.38394835591316223
  - 0.3857962191104889
  - 0.399447500705719
  - 0.398155152797699
  - 0.4677765369415283
  - 0.4048287570476532
  - 0.38398054242134094
  - 0.39375245571136475
  - 0.40685710310935974
  - 0.4050298035144806
  - 0.37644878029823303
  - 0.3954172134399414
  - 0.402707040309906
  - 0.3820803463459015
  - 0.3738412857055664
  - 0.39320650696754456
  - 0.41784432530403137
  - 0.38209742307662964
  - 0.38522714376449585
  - 0.38712555170059204
  - 0.39861801266670227
  - 0.3870198130607605
  - 0.3792751431465149
  - 0.3858013451099396
  - 0.3859068751335144
  - 0.3851228356361389
  - 0.3724011182785034
loss_records_fold4:
  train_losses:
  - 1.4376967012882234
  - 1.511112654209137
  - 1.434151077270508
  - 1.44108447432518
  - 1.46446270942688
  - 1.4861130952835084
  - 1.4435771822929384
  - 1.487413662672043
  - 1.4884912908077241
  - 1.4319595098495483
  - 1.4165611982345583
  - 1.4483421802520753
  - 1.5408477306365969
  - 1.425358271598816
  - 1.4231679022312165
  - 1.4411174654960632
  validation_losses:
  - 0.3925365209579468
  - 0.406034916639328
  - 0.3689735233783722
  - 0.4015047550201416
  - 0.3692511022090912
  - 0.3759521245956421
  - 0.3592072129249573
  - 0.3697246015071869
  - 0.3671080470085144
  - 0.3819916844367981
  - 0.3782358765602112
  - 0.37949511408805847
  - 0.3677537143230438
  - 0.3679322600364685
  - 0.36766430735588074
  - 0.36178699135780334
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 97 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8456260720411664, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.85621238645942
  mean_f1_accuracy: 0.0
  total_train_time: '0:31:49.590831'
