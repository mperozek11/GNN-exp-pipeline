config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 14:02:25.499184'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/3/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/3/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/3/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/3/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/3/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/3/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/3/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/3/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/3/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/3/fold_4_state_dict.pt
loss_records_fold2:
  train_losses:
  - 15.60863247513771
  - 15.629143089056015
  - 15.578509777784348
  - 15.155323266983032
  - 15.65664392709732
  - 19.02351263165474
  - 19.13503396511078
  - 16.510927855968475
  - 15.712925016880035
  - 15.395270943641663
  - 15.339231193065643
  - 15.635647356510162
  - 15.434187650680542
  - 15.365137904882431
  - 15.655874699354172
  - 15.402320355176926
  - 16.46981117129326
  - 15.097347378730774
  - 15.471866130828857
  - 15.296404972672462
  - 15.427385464310646
  - 15.398685470223427
  - 15.64882418513298
  - 15.352000027894974
  - 15.534353613853455
  - 15.530361905694008
  - 15.347489655017853
  - 15.776564091444016
  - 15.191267192363739
  - 15.468920111656189
  - 15.308429092168808
  - 15.311093300580978
  - 15.515445441007614
  - 15.464081838726997
  - 14.93098846077919
  - 15.372472018003464
  - 15.232194721698761
  - 15.331266969442368
  - 15.264699846506119
  - 15.42149630188942
  - 15.968046754598618
  - 15.177304714918137
  - 15.559751704335213
  - 14.993507251143456
  - 14.998800903558731
  - 14.963169246912003
  - 15.193042367696762
  - 15.078868091106415
  - 15.261019259691238
  - 15.250992953777313
  - 15.194797068834305
  - 15.38437619805336
  - 15.521475583314896
  - 15.404989123344421
  - 15.049628674983978
  - 15.648166209459305
  - 15.69691002368927
  - 15.219480007886887
  - 15.278965175151825
  - 15.130426913499832
  - 15.362894341349602
  - 15.56835651397705
  - 15.125910684466362
  - 15.353854045271873
  - 15.287082821130753
  - 15.559285402297974
  - 15.222265362739563
  - 15.23078690469265
  - 15.23092857003212
  - 15.241726636886597
  - 15.211048811674118
  - 15.284613192081451
  - 15.059863641858101
  - 15.147472739219666
  - 15.924168854951859
  - 15.429477125406265
  - 15.028727129101753
  - 15.22899842262268
  - 15.385448336601257
  - 15.42449326813221
  - 15.619834825396538
  - 15.529978930950165
  - 15.085839405655861
  - 15.137315154075623
  - 15.392801314592361
  - 15.323100745677948
  - 15.201947942376137
  - 15.596195012331009
  - 15.366840720176697
  - 15.670023143291473
  - 15.346633464097977
  - 15.21212711930275
  - 15.039119943976402
  - 15.779514580965042
  - 15.268296509981155
  - 14.974893495440483
  - 15.335210964083672
  - 15.418869078159332
  - 15.343836650252342
  - 15.143788382411003
  validation_losses:
  - 0.4016547203063965
  - 0.401084303855896
  - 0.4213981330394745
  - 0.41550010442733765
  - 2334.95849609375
  - 9.864542961120605
  - 0.4224647581577301
  - 0.41083648800849915
  - 0.47630947828292847
  - 40.06690216064453
  - 58.49518966674805
  - 21.564437866210938
  - 18.344213485717773
  - 0.42862269282341003
  - 0.41216906905174255
  - 0.4300818145275116
  - 0.8231267333030701
  - 3.449305295944214
  - 0.7179000377655029
  - 0.4049256145954132
  - 0.4215518832206726
  - 2.228522300720215
  - 5.103185176849365
  - 3.2905616760253906
  - 0.9099189639091492
  - 0.4390617609024048
  - 1.6649473905563354
  - 4.8639750480651855
  - 0.40360355377197266
  - 0.40311551094055176
  - 0.4599422216415405
  - 12614870.0
  - 0.4372691512107849
  - 1031831.9375
  - 48906.5546875
  - 0.4435926377773285
  - 0.40345585346221924
  - 0.40141358971595764
  - 0.43739446997642517
  - 0.5406417846679688
  - 0.4191478192806244
  - 0.40329688787460327
  - 0.4033301770687103
  - 0.6466872692108154
  - 0.8660271167755127
  - 0.40016818046569824
  - 0.40637290477752686
  - 0.4077807068824768
  - 0.7969266176223755
  - 1.2649227380752563
  - 0.41000762581825256
  - 6.692079544067383
  - 4.973101615905762
  - 0.40723517537117004
  - 1.4620834589004517
  - 0.5139248371124268
  - 0.5648624897003174
  - 1.8884977102279663
  - 3.1071648597717285
  - 0.4064415395259857
  - 0.8197992444038391
  - 0.43396902084350586
  - 2.1418521404266357
  - 0.4454749524593353
  - 0.4344668984413147
  - 4.530328750610352
  - 1.8793590068817139
  - 0.40491148829460144
  - 1.311930775642395
  - 0.5408422350883484
  - 0.4228938817977905
  - 0.4020717144012451
  - 0.4347319006919861
  - 0.40272200107574463
  - 3.7978343963623047
  - 0.48188596963882446
  - 0.40443912148475647
  - 7.156277656555176
  - 0.41835156083106995
  - 2.7113254070281982
  - 2.4149200916290283
  - 0.40739333629608154
  - 0.5370224714279175
  - 6.517418384552002
  - 3.889739513397217
  - 1.1658217906951904
  - 0.41030487418174744
  - 10.378398895263672
  - 1.52571439743042
  - 2.038032293319702
  - 6.0475311279296875
  - 0.4049142897129059
  - 0.4073522686958313
  - 4.938324451446533
  - 7.838651180267334
  - 3.514538049697876
  - 3.9597620964050293
  - 1.3358527421951294
  - 6.580715656280518
  - 0.4098733961582184
loss_records_fold4:
  train_losses:
  - 15.247846066951752
  - 15.166918277740479
  - 15.02989137172699
  - 15.081308424472809
  - 14.925713986158371
  - 14.977910578250885
  - 14.985106125473976
  - 15.350714638829231
  - 15.18813270330429
  - 15.28800716996193
  - 14.978873044252396
  - 15.256893962621689
  - 15.51704952120781
  - 15.123330920934677
  - 15.26858276128769
  - 14.988063097000122
  - 15.164252042770386
  - 15.215268671512604
  - 15.207274675369263
  - 15.006840646266937
  - 15.099690198898315
  - 15.225174456834793
  - 15.090151757001877
  - 15.063419848680496
  - 15.198443964123726
  - 14.899371802806854
  - 15.589476525783539
  - 15.235211104154587
  - 15.231789529323578
  - 15.570096880197525
  - 15.038465708494186
  - 15.007791012525558
  - 15.337022513151169
  - 15.075453609228134
  - 14.963283032178879
  - 15.195026159286499
  - 15.263604819774628
  - 15.118836671113968
  - 15.222305923700333
  - 15.08301554620266
  - 14.916059583425522
  - 15.178732126951218
  - 15.230640277266502
  - 15.109612822532654
  - 15.402627855539322
  - 15.207480043172836
  - 15.137497529387474
  - 15.085356086492538
  - 15.210992604494095
  - 15.162024065852165
  - 15.080112010240555
  - 15.268933266401291
  - 15.091885656118393
  - 15.125197410583496
  - 15.140803694725037
  - 15.140908420085907
  - 15.134395375847816
  - 15.112241446971893
  - 15.105091869831085
  - 15.160660713911057
  - 15.27285286784172
  - 15.111588940024376
  - 15.027959138154984
  - 15.632312640547752
  - 15.629962980747223
  - 14.993616461753845
  - 15.125923693180084
  - 15.087763592600822
  - 15.241638153791428
  - 15.765228927135468
  - 15.499537497758865
  - 15.056812167167664
  - 15.274639815092087
  - 15.30562573671341
  - 15.52532035112381
  - 15.280026972293854
  - 15.036561280488968
  - 15.007436871528625
  - 15.081483840942383
  - 15.816122189164162
  - 15.583879172801971
  - 15.333840385079384
  - 15.265988945960999
  - 15.214050680398941
  - 15.31103464961052
  - 15.21920508146286
  - 15.081597432494164
  - 15.525864988565445
  - 15.068981021642685
  - 15.172837287187576
  - 15.122228890657425
  - 15.070802837610245
  - 15.026355981826782
  - 15.41053032875061
  - 15.517543479800224
  - 16.120178043842316
  - 15.019036516547203
  - 15.093274906277657
  - 15.230273246765137
  - 15.025268703699112
  validation_losses:
  - 0.8876201510429382
  - 8.742680549621582
  - 6.096512317657471
  - 4.053889751434326
  - 0.4041719138622284
  - 0.43995803594589233
  - 0.40596622228622437
  - 2.339921712875366
  - 2.77551531791687
  - 0.4037361741065979
  - 0.40497463941574097
  - 0.4264377951622009
  - 0.40883150696754456
  - 0.4284384548664093
  - 0.40607041120529175
  - 0.4140952229499817
  - 0.40716615319252014
  - 0.42199990153312683
  - 0.4412781000137329
  - 0.41274189949035645
  - 0.40715619921684265
  - 0.4056125283241272
  - 0.4177630543708801
  - 0.4053630530834198
  - 0.45749396085739136
  - 0.4182055592536926
  - 0.43487444519996643
  - 0.7535568475723267
  - 0.4143533408641815
  - 0.41098201274871826
  - 0.40608125925064087
  - 0.4292927384376526
  - 0.40622010827064514
  - 0.40697959065437317
  - 0.41170960664749146
  - 0.4170825779438019
  - 0.4446955621242523
  - 0.4799111485481262
  - 0.40502771735191345
  - 0.40752750635147095
  - 0.41761431097984314
  - 0.4223335385322571
  - 0.41983556747436523
  - 0.4079183042049408
  - 0.4107920527458191
  - 0.437071830034256
  - 0.4057494103908539
  - 1.1371781826019287
  - 0.4042796790599823
  - 0.407804936170578
  - 0.41633176803588867
  - 0.44416379928588867
  - 0.4301108121871948
  - 0.40808969736099243
  - 0.40540173649787903
  - 0.4975399374961853
  - 0.413504034280777
  - 0.40539246797561646
  - 0.4063015878200531
  - 2.6080923080444336
  - 0.4127737283706665
  - 0.4047359824180603
  - 1.090867519378662
  - 0.4089653789997101
  - 0.41091787815093994
  - 0.4055578410625458
  - 0.40618616342544556
  - 0.41806015372276306
  - 0.4124225080013275
  - 0.4182933270931244
  - 0.42046698927879333
  - 2.8479156494140625
  - 0.416829913854599
  - 0.43981993198394775
  - 0.4064110815525055
  - 0.4119569957256317
  - 0.42461541295051575
  - 0.4072501063346863
  - 0.41635289788246155
  - 0.4298986792564392
  - 0.42951032519340515
  - 0.408103883266449
  - 0.42380765080451965
  - 0.4053647220134735
  - 0.41930854320526123
  - 0.40510082244873047
  - 0.4369974434375763
  - 0.40709710121154785
  - 0.4256199896335602
  - 0.40634799003601074
  - 0.40792542695999146
  - 0.4140811264514923
  - 0.4158133864402771
  - 1.1534146070480347
  - 1.0151877403259277
  - 0.40482860803604126
  - 1.4858291149139404
  - 0.4059053957462311
  - 0.47561341524124146
  - 0.412801593542099
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 99 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:08:07.557765'
