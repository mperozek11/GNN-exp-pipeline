config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 05:48:50.920460'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/136/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/136/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/136/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/136/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/136/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/136/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/136/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/136/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/136/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/136/fold_4_state_dict.pt
loss_records_fold1:
  train_losses:
  - 31.708145201206207
  - 30.87561757862568
  - 31.227035135030746
  - 31.898716896772385
  - 30.089828968048096
  - 30.45077534019947
  - 30.11803087592125
  - 30.874079450964928
  - 30.121089085936546
  - 30.669645845890045
  - 30.249327763915062
  - 30.05771005153656
  - 30.752096876502037
  - 29.80038097500801
  - 30.536993339657784
  - 30.03006835281849
  - 30.1912784576416
  - 30.75453907251358
  - 30.137943476438522
  - 30.05438807606697
  - 29.835535302758217
  - 29.76437219977379
  - 30.20413486659527
  - 30.51423555612564
  - 29.66225489974022
  - 29.666525438427925
  - 29.926699697971344
  - 29.7143072783947
  - 30.734918281435966
  - 29.741874888539314
  - 29.963182479143143
  - 29.612190037965775
  - 30.002655550837517
  - 29.9419027864933
  - 29.930507630109787
  - 30.336235761642456
  - 29.40561670064926
  - 29.913309305906296
  - 30.041156366467476
  - 29.547889202833176
  - 30.57192750275135
  - 29.30159917473793
  - 29.21256846189499
  - 29.090982168912888
  - 29.326698750257492
  - 31.945128872990608
  - 30.062734454870224
  - 29.73758916556835
  - 30.69013160467148
  - 29.225407242774963
  - 30.24594557285309
  - 29.813331976532936
  - 29.549170300364494
  - 30.465070828795433
  - 29.82201787829399
  - 29.546665728092194
  - 29.66272273659706
  - 30.20830711722374
  - 29.369171753525734
  - 29.190779998898506
  - 28.796616688370705
  - 29.797822773456573
  - 29.576391488313675
  - 30.59565956890583
  - 30.446273118257523
  - 29.745483741164207
  - 29.10215663909912
  - 29.115592539310455
  - 29.235166400671005
  - 29.61017969250679
  - 30.195521622896194
  - 29.383665174245834
  - 29.40144480764866
  - 29.430303633213043
  - 28.421431452035904
  - 29.236130103468895
  - 29.819098874926567
  - 29.468563228845596
  - 29.25335754454136
  - 28.883119449019432
  - 29.068470418453217
  - 29.35991933941841
  - 29.705758288502693
  - 28.78364922106266
  - 29.050637111067772
  - 28.72675034403801
  - 28.793587237596512
  - 29.535811111330986
  - 28.95365969836712
  - 28.98234511911869
  - 29.092429101467133
  - 28.755509242415428
  - 29.05292220413685
  - 30.455975964665413
  - 28.541887924075127
  - 29.66101911664009
  - 28.70915348827839
  - 29.592308819293976
  - 29.930603340268135
  - 29.165754050016403
  validation_losses:
  - 0.441780686378479
  - 0.4286048114299774
  - 0.40896153450012207
  - 0.3964175283908844
  - 0.3905404806137085
  - 0.42671066522598267
  - 0.49332526326179504
  - 0.4519970118999481
  - 0.4894396662712097
  - 0.39529168605804443
  - 0.40168651938438416
  - 0.78146892786026
  - 0.7281718850135803
  - 0.5031474232673645
  - 0.7217246294021606
  - 0.4041840732097626
  - 0.4068179130554199
  - 0.4230692982673645
  - 0.4105772376060486
  - 0.7269944548606873
  - 0.5524693131446838
  - 1.126137375831604
  - 0.6762515306472778
  - 0.3868942856788635
  - 0.40885472297668457
  - 0.4574283957481384
  - 0.5122283697128296
  - 0.6347995400428772
  - 0.4241054654121399
  - 0.43636462092399597
  - 0.47424742579460144
  - 1.0240882635116577
  - 0.396346777677536
  - 0.691214382648468
  - 0.4660547375679016
  - 0.3901715576648712
  - 0.4528754949569702
  - 0.5712303519248962
  - 0.44505026936531067
  - 1.4510184526443481
  - 0.5723868608474731
  - 0.7464643716812134
  - 0.6621701121330261
  - 1.0097745656967163
  - 0.5481472015380859
  - 0.45928892493247986
  - 0.40048083662986755
  - 0.5170916318893433
  - 0.6233604550361633
  - 0.9378484487533569
  - 0.6866502165794373
  - 0.841488242149353
  - 0.7581925392150879
  - 0.5404369235038757
  - 0.7285677790641785
  - 1.1503345966339111
  - 0.5291393399238586
  - 0.4550671875476837
  - 0.9485278129577637
  - 0.5875926613807678
  - 0.7541463375091553
  - 0.5970224142074585
  - 1.7220323085784912
  - 0.39203590154647827
  - 0.4529270529747009
  - 0.6392708420753479
  - 0.6944234371185303
  - 0.6208125352859497
  - 0.48760107159614563
  - 0.41850557923316956
  - 0.5844079852104187
  - 0.38857248425483704
  - 0.49366435408592224
  - 0.48147520422935486
  - 0.8315988183021545
  - 0.7376765608787537
  - 0.5048803687095642
  - 0.5764918923377991
  - 0.5720697641372681
  - 0.8651608824729919
  - 0.724031388759613
  - 0.8807412981987
  - 0.584719181060791
  - 0.5166689157485962
  - 0.6146132349967957
  - 0.7093294858932495
  - 0.9775382280349731
  - 0.5513593554496765
  - 0.7283598780632019
  - 0.544196367263794
  - 0.7773914933204651
  - 0.8504270911216736
  - 0.9503827095031738
  - 0.5363357067108154
  - 0.896007239818573
  - 0.7089256644248962
  - 0.5698118209838867
  - 0.7625928521156311
  - 0.7142882943153381
  - 0.7214295864105225
loss_records_fold2:
  train_losses:
  - 29.82604105770588
  - 28.695834383368492
  - 28.356581404805183
  - 28.826158478856087
  - 29.870290264487267
  - 28.933907568454742
  - 29.575589880347252
  - 30.368479624390602
  - 30.14270029962063
  - 30.57936353981495
  - 29.154642567038536
  - 29.921870976686478
  - 30.064373403787613
  - 29.711537092924118
  - 29.91354775428772
  - 28.673314467072487
  - 30.3131432980299
  - 30.174379095435143
  - 29.751807317137718
  - 29.281772270798683
  - 29.530400708317757
  - 30.038956314325333
  - 29.523497760295868
  - 29.390305399894714
  - 29.346306800842285
  - 29.545467033982277
  - 29.402784943580627
  - 29.895010232925415
  - 29.084095239639282
  - 29.956811100244522
  - 29.324030965566635
  - 29.138574331998825
  - 29.39397993683815
  - 29.221664413809776
  - 29.71202100813389
  - 29.503371849656105
  - 29.747533604502678
  - 30.52897609770298
  - 29.13539783656597
  - 29.42425388097763
  - 29.098255291581154
  - 29.67520371079445
  - 28.949128478765488
  - 28.37468147277832
  - 30.319761127233505
  - 29.851860463619232
  - 29.80600881576538
  - 29.353718504309654
  - 29.671768099069595
  - 29.312009319663048
  - 29.72458328306675
  - 29.76959127187729
  - 29.43459191918373
  - 30.159128665924072
  - 29.239375293254852
  - 29.057942003011703
  - 29.642530173063278
  - 28.54915514588356
  - 29.038431406021118
  - 29.313715040683746
  - 29.35135117173195
  - 28.854043200612068
  - 29.80566531419754
  - 29.49427704513073
  - 29.148892790079117
  - 29.88485437631607
  - 28.67129610478878
  - 28.869222298264503
  - 29.682902693748474
  - 28.673620209097862
  - 29.601160779595375
  - 28.999685972929
  - 28.95805151760578
  - 29.804254114627838
  - 29.67891103029251
  - 29.599746137857437
  - 29.468718945980072
  - 29.210370048880577
  - 29.319457784295082
  - 28.326356202363968
  - 29.006757006049156
  - 28.76791936159134
  - 29.177021205425262
  - 28.633211389183998
  - 29.02332265675068
  - 28.29457986354828
  - 29.79427006840706
  - 29.21110798418522
  - 28.599608689546585
  - 28.792578250169754
  - 29.560152232646942
  - 29.175590872764587
  - 28.765262380242348
  - 29.297304913401604
  - 29.692558243870735
  - 28.897086024284363
  - 29.424931704998016
  - 28.88368183374405
  - 28.416616201400757
  - 28.838636800646782
  validation_losses:
  - 0.6629517674446106
  - 0.7285680174827576
  - 0.6052293181419373
  - 0.5289137363433838
  - 0.5899583101272583
  - 0.6032499074935913
  - 0.47939610481262207
  - 0.3895716965198517
  - 0.39594998955726624
  - 0.38469037413597107
  - 0.3893144428730011
  - 0.4013757109642029
  - 0.38963666558265686
  - 0.39128610491752625
  - 0.40144816040992737
  - 0.39041924476623535
  - 0.7770071029663086
  - 0.5238038301467896
  - 0.39704594016075134
  - 0.39969804883003235
  - 0.49851664900779724
  - 0.40046411752700806
  - 0.3938281834125519
  - 0.7422338724136353
  - 0.43990299105644226
  - 0.4633713662624359
  - 0.6788690090179443
  - 0.9982982873916626
  - 2.1943485736846924
  - 0.3896106481552124
  - 0.4052305817604065
  - 0.6454415321350098
  - 0.39477378129959106
  - 1.2866277694702148
  - 1.8836718797683716
  - 1.695098638534546
  - 0.38604527711868286
  - 2.0187857151031494
  - 1.2789452075958252
  - 0.6270146369934082
  - 0.5015672445297241
  - 0.5089845061302185
  - 0.9481247663497925
  - 2.6768434047698975
  - 1.8399790525436401
  - 0.38472530245780945
  - 0.4255901873111725
  - 0.3920731544494629
  - 0.37901243567466736
  - 0.39406314492225647
  - 0.38671374320983887
  - 0.3887155055999756
  - 0.3834382891654968
  - 0.38977518677711487
  - 0.40972861647605896
  - 0.4295384883880615
  - 0.44551289081573486
  - 0.4001399874687195
  - 0.5535723567008972
  - 0.591873824596405
  - 0.6247714757919312
  - 0.390381395816803
  - 0.3953215777873993
  - 0.9151256680488586
  - 0.429243803024292
  - 0.4083769917488098
  - 0.4216688871383667
  - 0.5656829476356506
  - 0.7698889970779419
  - 0.47924691438674927
  - 0.48128053545951843
  - 0.3813472092151642
  - 0.38895225524902344
  - 0.39739131927490234
  - 0.5757912397384644
  - 0.808432400226593
  - 0.5212761759757996
  - 1.258108377456665
  - 1.0474016666412354
  - 0.6865504384040833
  - 0.5424498915672302
  - 0.9813094735145569
  - 0.4993213415145874
  - 0.8504915833473206
  - 1.0661675930023193
  - 3.684190273284912
  - 3.3534295558929443
  - 4.842776298522949
  - 0.8861454725265503
  - 2.2056007385253906
  - 1.1505976915359497
  - 0.9526327848434448
  - 1.3904763460159302
  - 0.4111362397670746
  - 0.6835806965827942
  - 0.42524421215057373
  - 0.5786523818969727
  - 0.5742398500442505
  - 0.6283037066459656
  - 0.5057730674743652
loss_records_fold3:
  train_losses:
  - 29.69694635272026
  - 29.477683767676353
  - 29.241587102413177
  - 28.50888927280903
  - 28.875259563326836
  - 28.988675758242607
  - 28.42859362065792
  - 28.597669631242752
  - 28.861136257648468
  - 29.537165835499763
  - 29.369700014591217
  - 28.754551351070404
  - 29.654984936118126
  - 29.321045219898224
  - 28.823069989681244
  - 28.691567733883858
  - 28.830383345484734
  - 29.946565210819244
  - 29.53145742416382
  - 29.060763746500015
  - 28.542309626936913
  - 29.497458517551422
  - 29.762421220541
  - 29.639719486236572
  - 29.827824011445045
  - 29.45974037051201
  - 28.755349531769753
  - 29.202936813235283
  - 28.812229946255684
  - 29.304375156760216
  - 29.391486078500748
  - 28.90361650288105
  - 28.713781625032425
  - 28.860053956508636
  - 28.631436951458454
  - 29.718451127409935
  - 29.500788435339928
  - 28.670943647623062
  - 29.465275540947914
  - 29.601734444499016
  - 29.465181902050972
  - 29.71013456583023
  - 27.699051290750504
  - 28.332393392920494
  - 29.73978616297245
  - 28.91462540626526
  - 28.94756330549717
  - 29.447403267025948
  - 29.306218564510345
  - 29.09543463587761
  - 29.63995948433876
  - 28.996121555566788
  - 29.099145963788033
  - 29.689064159989357
  - 29.336861833930016
  - 29.24962316453457
  - 28.769474148750305
  - 30.761643677949905
  - 29.624468594789505
  - 29.511754378676414
  - 28.553798988461494
  - 28.675309658050537
  - 29.53756108880043
  - 29.011979892849922
  - 29.232837036252022
  - 28.656745091080666
  - 28.743406519293785
  - 28.29687711596489
  - 28.880087219178677
  - 29.287682101130486
  - 28.994050353765488
  - 28.3173848092556
  - 29.219412833452225
  - 28.857563868165016
  - 29.225940093398094
  - 28.423509538173676
  - 29.074228554964066
  - 29.068426981568336
  - 28.406788006424904
  - 28.269466504454613
  - 28.51868237555027
  - 29.06635609269142
  - 28.355240046977997
  - 28.965175360441208
  - 28.20073528587818
  - 28.996726095676422
  - 29.0338281840086
  - 29.447964042425156
  - 28.77962003648281
  - 28.596977323293686
  - 28.56805655360222
  - 29.353577733039856
  - 28.6568294018507
  - 28.51657970249653
  - 28.52016717195511
  - 29.004229724407196
  - 28.9052442163229
  - 28.855283737182617
  - 28.3863113373518
  - 28.963565185666084
  validation_losses:
  - 0.5315223932266235
  - 0.4250636696815491
  - 0.4971819221973419
  - 1.277449607849121
  - 3.6810405254364014
  - 0.631298840045929
  - 3.3505444526672363
  - 0.6424935460090637
  - 0.6766135692596436
  - 0.5222890377044678
  - 0.4303051233291626
  - 0.6316894888877869
  - 1.2575099468231201
  - 0.5981281399726868
  - 0.7916744947433472
  - 0.7176250219345093
  - 2.398988723754883
  - 0.4411002993583679
  - 0.5176301002502441
  - 0.5042341351509094
  - 0.627589762210846
  - 0.7460507154464722
  - 0.7732803821563721
  - 0.5480900406837463
  - 0.5157502293586731
  - 0.5279797911643982
  - 0.6567113995552063
  - 0.9816563725471497
  - 0.5036234855651855
  - 0.7412241101264954
  - 0.5816630125045776
  - 0.7581787109375
  - 0.64785236120224
  - 0.5497069954872131
  - 0.5681160092353821
  - 0.7271562814712524
  - 0.9440423250198364
  - 1.1030858755111694
  - 0.6180430054664612
  - 0.6715178489685059
  - 1.093885064125061
  - 1.6591839790344238
  - 1.2689603567123413
  - 6.7025346755981445
  - 1.8844300508499146
  - 1.8702538013458252
  - 1.2912180423736572
  - 2.1236062049865723
  - 2.1287589073181152
  - 1.7766178846359253
  - 3.0020203590393066
  - 2.6000475883483887
  - 0.7467256784439087
  - 5.166637420654297
  - 2.826119899749756
  - 2.4092066287994385
  - 3.061748743057251
  - 15.57296085357666
  - 3.583221435546875
  - 0.9584081768989563
  - 1.9315799474716187
  - 0.9236512184143066
  - 1.8209253549575806
  - 3.564897060394287
  - 1.6278138160705566
  - 2.15511417388916
  - 2.127612352371216
  - 2.7756471633911133
  - 0.9814178943634033
  - 0.6246376037597656
  - 0.7415586113929749
  - 1.7317911386489868
  - 1.1675307750701904
  - 3.4761006832122803
  - 0.6648889780044556
  - 3.686274290084839
  - 0.9191884398460388
  - 1.2069138288497925
  - 1.331369400024414
  - 2.222902297973633
  - 0.3966350555419922
  - 1.1655478477478027
  - 2.3318228721618652
  - 3.2703959941864014
  - 0.5136339068412781
  - 0.40931788086891174
  - 0.5083111524581909
  - 0.8770632743835449
  - 0.9491857886314392
  - 0.5501112341880798
  - 0.9294084310531616
  - 0.5340762734413147
  - 0.8659045696258545
  - 0.5788668394088745
  - 14.62949275970459
  - 0.6895430684089661
  - 4.611532688140869
  - 1.2154614925384521
  - 0.4856349229812622
  - 0.722528874874115
loss_records_fold4:
  train_losses:
  - 29.43841291964054
  - 29.046634033322334
  - 28.28469569981098
  - 28.56492878496647
  - 28.261464938521385
  - 29.291091427206993
  - 29.305168002843857
  - 29.265931606292725
  - 28.489317923784256
  - 28.568491116166115
  - 28.383179366588593
  - 28.724114432930946
  - 28.33754074573517
  - 28.806979179382324
  - 28.957203462719917
  - 28.51863729953766
  - 28.396694511175156
  - 29.15207713842392
  - 28.39740166068077
  - 28.81217712163925
  - 28.8045811355114
  - 29.23159646987915
  - 29.161531761288643
  - 29.450887337327003
  - 29.486172631382942
  - 29.58813425898552
  - 29.367476522922516
  - 28.54551813006401
  - 29.125805035233498
  - 28.85470500588417
  - 28.590696215629578
  - 29.42653913795948
  - 28.612856194376945
  - 30.40072150528431
  - 29.983298420906067
  - 29.278696537017822
  - 29.452056780457497
  - 29.494011908769608
  - 29.360989570617676
  - 28.880615189671516
  - 29.340651527047157
  - 29.373337626457214
  - 29.119228437542915
  - 28.781574949622154
  - 29.43924880027771
  - 28.778737604618073
  - 28.91293428838253
  - 29.05286404490471
  - 28.489145889878273
  - 29.055355027318
  - 29.045400619506836
  - 28.61428713798523
  - 28.87530541419983
  - 29.068058758974075
  - 28.34701243042946
  - 28.55716423690319
  - 29.11859232187271
  - 28.98210921883583
  - 28.709083169698715
  - 28.701989501714706
  - 29.158799529075623
  - 28.796251848340034
  - 29.22072207927704
  - 28.31061278283596
  - 29.25787526369095
  - 28.939498707652092
  - 28.48953041434288
  - 28.754796355962753
  - 28.7166498452425
  - 29.48060481250286
  - 29.320100277662277
  - 28.719310611486435
  - 28.80863457918167
  - 29.00571958720684
  - 29.269526943564415
  - 28.467895686626434
  - 29.042637214064598
  - 28.735541209578514
  - 28.141744390130043
  - 28.85181199014187
  - 28.722509875893593
  - 28.922760993242264
  - 28.89035053551197
  - 29.666558146476746
  - 29.012140527367592
  - 29.06487026810646
  - 28.409092500805855
  - 28.83790895342827
  - 28.694901555776596
  - 28.47756616771221
  - 29.88186751306057
  - 28.46404477953911
  - 28.862753301858902
  - 28.03442245721817
  - 28.309258356690407
  - 28.927358135581017
  - 28.812455356121063
  - 27.95893381536007
  - 28.131126582622528
  - 27.782798662781715
  validation_losses:
  - 0.3872877359390259
  - 0.42047956585884094
  - 0.4835008978843689
  - 0.5587188601493835
  - 0.46149972081184387
  - 0.4670557677745819
  - 0.4641624093055725
  - 0.5704723596572876
  - 0.4645097851753235
  - 0.5154940485954285
  - 0.9066663384437561
  - 0.5270867347717285
  - 0.49469709396362305
  - 0.48825913667678833
  - 0.6419445276260376
  - 0.5016317963600159
  - 1.4471794366836548
  - 3.259580135345459
  - 0.5906829237937927
  - 0.7359623312950134
  - 0.5844247937202454
  - 0.38353967666625977
  - 0.4534431993961334
  - 0.48133477568626404
  - 0.44564831256866455
  - 0.4682360291481018
  - 0.453505277633667
  - 0.47410091757774353
  - 0.3890184760093689
  - 0.6116442084312439
  - 0.5271605253219604
  - 0.6369682550430298
  - 0.7325294017791748
  - 0.37668728828430176
  - 0.37642282247543335
  - 0.36957818269729614
  - 0.3787839114665985
  - 0.3960866928100586
  - 1.6096580028533936
  - 0.8239301443099976
  - 0.5594142079353333
  - 0.5486389994621277
  - 0.4473549425601959
  - 0.5986269116401672
  - 0.5136019587516785
  - 0.49726182222366333
  - 0.4419035017490387
  - 0.5140582323074341
  - 0.5521126985549927
  - 1.0671993494033813
  - 1.7083587646484375
  - 0.40257954597473145
  - 0.61704021692276
  - 0.6500797867774963
  - 0.5859292149543762
  - 0.5482884645462036
  - 0.3851996958255768
  - 0.42513853311538696
  - 0.5128135085105896
  - 0.4133518934249878
  - 0.4388361871242523
  - 0.9250083565711975
  - 0.730100691318512
  - 0.5540457367897034
  - 0.42317909002304077
  - 0.5960030555725098
  - 0.4458402097225189
  - 0.46775415539741516
  - 0.6719615459442139
  - 0.5909069180488586
  - 0.5062208771705627
  - 0.48462387919425964
  - 0.4885793626308441
  - 0.4864260256290436
  - 0.7175130844116211
  - 0.4983733892440796
  - 0.47098350524902344
  - 0.46781525015830994
  - 0.5325894355773926
  - 0.507676899433136
  - 0.5811397433280945
  - 1.0320407152175903
  - 0.6813176274299622
  - 0.46136170625686646
  - 0.6673194169998169
  - 0.4565308094024658
  - 0.5342968106269836
  - 0.5352290272712708
  - 0.5600600242614746
  - 0.8448452353477478
  - 0.5034146308898926
  - 0.6051377654075623
  - 0.6298335194587708
  - 0.6739407777786255
  - 0.44922715425491333
  - 1.671101689338684
  - 1.9629898071289062
  - 0.6040072441101074
  - 0.6129603385925293
  - 0.6834651827812195
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 8 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8353344768439108, 0.8507718696397941, 0.855917667238422,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.11111111111111109, 0.10309278350515465, 0.04545454545454545,
    0.023529411764705882]'
  mean_eval_accuracy: 0.8514090525955922
  mean_f1_accuracy: 0.056637570367103415
  total_train_time: '0:17:39.579806'
