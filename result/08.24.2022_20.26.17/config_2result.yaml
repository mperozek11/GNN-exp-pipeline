config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.781502'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_2fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_2fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_2fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_2fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_2fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_2fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_2fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_2fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_2fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_2fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.7732125341892244
  - 1.526606833934784
  - 1.5052406191825867
  - 1.4925502657890322
  - 1.5033825278282167
  - 1.4833391398191453
  - 1.4773812174797059
  - 1.5136726558208466
  - 1.458107125759125
  - 1.4466717064380648
  - 1.43548903465271
  - 1.4479549884796143
  validation_losses:
  - 0.3990340530872345
  - 0.4122966229915619
  - 0.39895907044410706
  - 0.39534544944763184
  - 0.3888360261917114
  - 0.4198724627494812
  - 0.3890579044818878
  - 0.3935067057609558
  - 0.3870776891708374
  - 0.3856181502342224
  - 0.3847735524177551
  - 0.38720703125
loss_records_fold1:
  train_losses:
  - 1.4959655165672303
  - 1.4291693449020386
  - 1.4690097272396088
  - 1.4422868549823762
  - 1.42798710167408
  - 1.468066930770874
  - 1.4195013314485552
  - 1.4254142582416536
  - 1.4243144214153292
  - 1.488551598787308
  - 1.420401692390442
  - 1.4151109576225283
  validation_losses:
  - 0.39061588048934937
  - 0.3861205279827118
  - 0.3935083746910095
  - 0.3853423595428467
  - 0.38678571581840515
  - 0.40173467993736267
  - 0.3988777995109558
  - 0.3857598304748535
  - 0.386644572019577
  - 0.38811686635017395
  - 0.3864387571811676
  - 0.3868366479873657
loss_records_fold2:
  train_losses:
  - 1.5085890948772431
  - 1.4655147910118105
  - 1.4317784011363983
  - 1.4284843802452087
  - 1.4201938629150392
  - 1.4051972106099129
  - 1.422434574365616
  - 1.4445255219936373
  - 1.429477745294571
  - 1.4533499658107758
  - 1.440703421831131
  - 1.477464348077774
  - 1.4272958278656007
  - 1.4397212266921997
  - 1.455024379491806
  - 1.4249647557735443
  - 1.417639046907425
  - 1.4273742079734804
  - 1.4901018083095552
  - 1.417695587873459
  - 1.4512148082256318
  - 1.449871200323105
  - 1.43109130859375
  - 1.450233978033066
  - 1.4113912612199784
  - 1.4745399951934814
  - 1.454027557373047
  - 1.4464938998222352
  - 1.4363126397132875
  - 1.401594027876854
  - 1.453756880760193
  - 1.4405722677707673
  - 1.410114872455597
  - 1.4275392174720765
  - 1.4594610035419464
  - 1.4557293295860292
  - 1.4325725615024567
  - 1.4671252071857452
  - 1.4367794513702394
  - 1.422683596611023
  - 1.3963116228580477
  - 1.3890708953142168
  - 1.4188013970851898
  - 1.4151793003082276
  - 1.3813796758651735
  - 1.38285493850708
  - 1.4052200078964234
  - 1.3773268163204193
  - 1.4090223133563997
  - 1.4486932873725893
  - 1.3819978773593904
  - 1.3940302938222886
  - 1.4078295588493348
  - 1.4199714601039888
  - 1.408314907550812
  - 1.4654357731342316
  - 1.3958645999431611
  - 1.4117621123790742
  - 1.4301858603954316
  - 1.4412507891654969
  - 1.4024662196636202
  - 1.372048056125641
  - 1.4377332448959352
  - 1.4639395594596865
  - 1.4054667592048646
  - 1.4629104703664781
  - 1.4038845300674438
  - 1.3830039322376253
  - 1.3936599850654603
  - 1.4260862946510315
  - 1.426452511548996
  - 1.377067518234253
  - 1.4040526390075685
  - 1.3815985381603242
  - 1.412738698720932
  - 1.3980285942554476
  - 1.408454442024231
  - 1.378615552186966
  - 1.3980932116508484
  - 1.3508258014917374
  - 1.4013187706470491
  - 1.4434795737266541
  - 1.3829010844230654
  - 1.4068861484527588
  - 1.3847414910793305
  - 1.386173576116562
  - 1.4573767721652986
  - 1.494572526216507
  - 1.4111358761787416
  - 1.397895920276642
  - 1.4247888147830965
  - 1.394889685511589
  validation_losses:
  - 0.3828825354576111
  - 0.39265701174736023
  - 0.3841162919998169
  - 0.38804131746292114
  - 0.38597843050956726
  - 0.3829689621925354
  - 0.38565871119499207
  - 0.454048752784729
  - 0.391954630613327
  - 0.42176294326782227
  - 0.3885751962661743
  - 0.38392946124076843
  - 0.383374959230423
  - 0.40320250391960144
  - 0.47749122977256775
  - 0.3853311836719513
  - 0.38504719734191895
  - 0.41520553827285767
  - 0.6198936700820923
  - 0.46886008977890015
  - 0.40224599838256836
  - 0.5034973621368408
  - 0.38878583908081055
  - 0.4273194372653961
  - 0.3895735740661621
  - 0.4184750020503998
  - 0.38475003838539124
  - 0.3848140239715576
  - 0.3984502851963043
  - 0.4114190638065338
  - 0.4104609489440918
  - 0.4027245342731476
  - 0.4202418625354767
  - 0.43482476472854614
  - 0.6899403929710388
  - 0.39401140809059143
  - 0.4440051317214966
  - 0.4606126546859741
  - 0.5887767672538757
  - 0.4188285768032074
  - 0.46869394183158875
  - 0.4569675028324127
  - 0.5396555662155151
  - 0.39069053530693054
  - 0.39270317554473877
  - 0.4251071810722351
  - 1.1848347187042236
  - 0.4846654236316681
  - 0.7170609831809998
  - 0.48648110032081604
  - 0.44946417212486267
  - 1.430753469467163
  - 0.7273980379104614
  - 0.9940398335456848
  - 0.4157843589782715
  - 0.49563008546829224
  - 0.46439024806022644
  - 0.4584697186946869
  - 0.4201691150665283
  - 0.3819693624973297
  - 0.39291754364967346
  - 0.3841609060764313
  - 0.5136869549751282
  - 0.40648961067199707
  - 0.4266214668750763
  - 0.5261050462722778
  - 0.7394043803215027
  - 0.46246010065078735
  - 0.5683757066726685
  - 1.7123745679855347
  - 0.39490237832069397
  - 0.4984300136566162
  - 0.4368087649345398
  - 1.0441820621490479
  - 0.4241374135017395
  - 0.433071494102478
  - 0.5786260962486267
  - 0.554699182510376
  - 0.4318237900733948
  - 0.5264990925788879
  - 0.5800220966339111
  - 0.5538116693496704
  - 0.7208036780357361
  - 0.39455586671829224
  - 0.41730839014053345
  - 0.4972817003726959
  - 0.41567668318748474
  - 0.39731961488723755
  - 0.3881698250770569
  - 0.3902893364429474
  - 0.3924177289009094
  - 0.38789990544319153
loss_records_fold3:
  train_losses:
  - 1.4605856180191041
  - 1.4678533554077149
  - 1.4297548413276673
  - 1.4303997814655305
  - 1.4229293942451477
  - 1.4240982085466385
  - 1.4281750082969666
  - 1.4429783761501314
  - 1.4163397252559662
  - 1.4049959599971773
  - 1.4660716950893402
  - 1.451795119047165
  - 1.439154499769211
  - 1.4105156600475313
  - 1.4214321434497834
  - 1.477958154678345
  - 1.4374906301498414
  - 1.4215176105499268
  - 1.4139706134796144
  - 1.4307429999113084
  - 1.4242980062961579
  - 1.4162219166755676
  - 1.400415289402008
  - 1.4179586112499238
  - 1.4231345802545547
  - 1.3826293230056763
  - 1.3935506224632264
  - 1.3997472763061525
  - 1.4250156164169312
  - 1.4381708920001985
  - 1.4074553728103638
  - 1.381792598962784
  - 1.412506276369095
  - 1.4564933300018312
  - 1.4263329565525056
  - 1.419390344619751
  - 1.3789281696081161
  - 1.4410019457340242
  - 1.3903272718191149
  - 1.408791869878769
  - 1.396975564956665
  - 1.3720919489860535
  - 1.4325781106948854
  - 1.400680834054947
  - 1.3908520609140398
  - 1.391465425491333
  - 1.445676028728485
  - 1.365496498346329
  - 1.3930666089057924
  - 1.4021625518798828
  - 1.3751998275518418
  - 1.3995068728923798
  - 1.390853887796402
  - 1.4294454395771028
  - 1.416564154624939
  - 1.3756771087646484
  - 1.3754483580589296
  - 1.4172551751136782
  - 1.379655307531357
  - 1.419322317838669
  - 1.3978323936462402
  - 1.3883440494537354
  - 1.4102240562438966
  - 1.4257481575012207
  - 1.410320144891739
  - 1.3789950579404833
  - 1.415973997116089
  - 1.3772264927625657
  - 1.3968304693698883
  - 1.3873072504997255
  - 1.4384400308132173
  - 1.370713758468628
  - 1.3818284809589387
  - 1.3729478478431703
  - 1.448089474439621
  - 1.4266030937433243
  - 1.3869552791118622
  - 1.4064442932605745
  - 1.3885713040828707
  - 1.4017847597599031
  - 1.3820505380630494
  - 1.3678160905838013
  - 1.4195820152759553
  - 1.3985369861125947
  - 1.398025232553482
  - 1.3788340598344804
  - 1.36745223402977
  - 1.3813103854656221
  - 1.4379014432430268
  - 1.4263756453990937
  - 1.3704455733299257
  - 1.3831399142742158
  - 1.3894480407238008
  - 1.464901813864708
  - 1.384579634666443
  - 1.366172942519188
  - 1.382465445995331
  - 1.389022582769394
  - 1.3740748345851899
  - 1.4068108320236208
  validation_losses:
  - 0.37364551424980164
  - 0.3681075870990753
  - 0.3644953966140747
  - 0.36611127853393555
  - 0.36757659912109375
  - 0.3663698732852936
  - 0.37640953063964844
  - 0.37146493792533875
  - 0.3902634084224701
  - 0.4669078588485718
  - 0.5918336510658264
  - 0.3754708766937256
  - 0.38012707233428955
  - 0.38103824853897095
  - 0.4120597243309021
  - 0.3798786997795105
  - 0.4411121904850006
  - 0.4063548743724823
  - 0.38963380455970764
  - 0.3965047299861908
  - 0.5069324970245361
  - 0.37028810381889343
  - 0.37473785877227783
  - 0.4241101145744324
  - 0.43647241592407227
  - 0.5119396448135376
  - 0.5269052386283875
  - 0.565483808517456
  - 0.5974088311195374
  - 0.6217663884162903
  - 0.4122491478919983
  - 0.5411885976791382
  - 0.45800137519836426
  - 0.4254700243473053
  - 0.4731701910495758
  - 0.5277132391929626
  - 0.48446232080459595
  - 0.5148820281028748
  - 0.4568059742450714
  - 0.560576856136322
  - 0.5090779662132263
  - 0.5776299834251404
  - 0.5827867388725281
  - 0.7642219066619873
  - 0.4548496901988983
  - 0.5490811467170715
  - 0.7582886219024658
  - 0.7457705736160278
  - 0.4122259020805359
  - 0.7089362740516663
  - 0.44126880168914795
  - 0.501126229763031
  - 0.3998369872570038
  - 0.5047853589057922
  - 0.5718125104904175
  - 0.4277738630771637
  - 0.5632689595222473
  - 0.8421590328216553
  - 0.7560120224952698
  - 0.685065507888794
  - 0.5570785999298096
  - 0.5063925981521606
  - 0.5657761096954346
  - 0.44237789511680603
  - 0.49441248178482056
  - 0.514641523361206
  - 0.4450407922267914
  - 0.5182382464408875
  - 1.1472089290618896
  - 1.332133412361145
  - 0.6666489243507385
  - 0.7859514951705933
  - 1.8136638402938843
  - 1.026692509651184
  - 0.6727043986320496
  - 0.8663332462310791
  - 0.44034960865974426
  - 0.6860297322273254
  - 0.4461418688297272
  - 0.36964142322540283
  - 0.3728097677230835
  - 0.386147677898407
  - 0.38971030712127686
  - 0.5464146733283997
  - 0.7423179745674133
  - 1.0169936418533325
  - 0.41066500544548035
  - 0.4506090581417084
  - 0.3921056389808655
  - 0.4095521569252014
  - 0.4432229697704315
  - 0.4301019608974457
  - 0.5736960172653198
  - 0.3750024735927582
  - 0.40362846851348877
  - 0.43072015047073364
  - 0.461935430765152
  - 0.4481436312198639
  - 0.522972047328949
  - 0.3952311873435974
loss_records_fold4:
  train_losses:
  - 1.3850266635417938
  - 1.409607005119324
  - 1.4230432868003846
  - 1.423092293739319
  - 1.3971755027770998
  - 1.3639101535081863
  - 1.3860603630542756
  - 1.3897839903831484
  - 1.3836117744445802
  - 1.3955795228481294
  - 1.3535378307104111
  - 1.383627063035965
  - 1.392208057641983
  - 1.390079528093338
  - 1.3979823172092438
  - 1.3860474884510041
  - 1.3777281820774079
  - 1.3518425554037095
  - 1.3701416671276094
  - 1.3649476140737535
  - 1.3619612812995912
  - 1.4121382892131806
  - 1.3999910891056062
  - 1.373837822675705
  - 1.368694865703583
  - 1.36985182762146
  - 1.376680138707161
  - 1.3797056555747986
  - 1.4026187598705293
  - 1.3889304757118226
  - 1.3584319233894349
  - 1.3908334195613863
  - 1.3839446783065796
  - 1.3999292790889741
  - 1.3532051503658296
  - 1.3791874289512636
  - 1.3520234465599061
  - 1.3704494059085848
  - 1.3752402722835542
  - 1.339874005317688
  - 1.3480339527130127
  - 1.3673027426004412
  - 1.3591145992279055
  - 1.3993566274642946
  - 1.3768059313297272
  - 1.362933224439621
  - 1.34177727997303
  - 1.3973272442817688
  - 1.4048212945461274
  - 1.40037100315094
  - 1.4462623655796052
  - 1.37673779129982
  - 1.4004713237285615
  - 1.3518616497516633
  - 1.3778601169586182
  - 1.3819115519523621
  - 1.4135165929794313
  - 1.3778985142707825
  - 1.3684879899024964
  - 1.3844397068023682
  - 1.3542429208755493
  - 1.4032877564430237
  - 1.3996728777885439
  - 1.391815569996834
  - 1.3717262387275697
  - 1.408419591188431
  - 1.3419139802455904
  - 1.3587984800338746
  - 1.4106821715831757
  - 1.3995242953300477
  - 1.3878008604049683
  - 1.383035981655121
  - 1.3514666736125946
  - 1.3687175273895265
  - 1.3280876368284227
  - 1.3296271085739138
  - 1.3290927410125732
  - 1.3565605938434602
  - 1.373599034547806
  - 1.3789065480232239
  - 1.3625919103622437
  - 1.4623437821865082
  - 1.3780707359313966
  - 1.341199564933777
  - 1.3708433210849762
  - 1.39508216381073
  - 1.3832937717437745
  - 1.364347678422928
  - 1.3436775028705599
  - 1.3513808071613314
  - 1.3518210232257843
  - 1.3463728070259096
  - 1.334234791994095
  - 1.3664459109306337
  - 1.4974712908267975
  - 1.4842459738254548
  - 1.3732533842325212
  - 1.4047322750091553
  - 1.3820006787776948
  - 1.422075006365776
  validation_losses:
  - 0.34945496916770935
  - 0.47721439599990845
  - 0.4068964719772339
  - 0.35707569122314453
  - 0.38110172748565674
  - 0.3848874270915985
  - 0.4572511613368988
  - 0.36196884512901306
  - 0.38153594732284546
  - 0.36100783944129944
  - 0.3584352433681488
  - 0.4068300127983093
  - 0.3974989652633667
  - 0.36696356534957886
  - 0.3776240050792694
  - 0.4548049867153168
  - 0.3589487075805664
  - 0.4664727747440338
  - 0.3948843777179718
  - 0.38260480761528015
  - 0.42541950941085815
  - 0.37337538599967957
  - 0.47435957193374634
  - 0.39540746808052063
  - 0.37632155418395996
  - 0.36315447092056274
  - 0.416360467672348
  - 0.4095548689365387
  - 0.35902169346809387
  - 0.4438754618167877
  - 0.37182945013046265
  - 0.36410123109817505
  - 0.4078022539615631
  - 0.4000702500343323
  - 0.3775627613067627
  - 0.3835224211215973
  - 0.4638464152812958
  - 0.41343745589256287
  - 0.4187646806240082
  - 0.4701869487762451
  - 0.40251773595809937
  - 0.4258503019809723
  - 0.5081174969673157
  - 0.43832337856292725
  - 0.3917108476161957
  - 0.41904619336128235
  - 0.4531997740268707
  - 0.42111635208129883
  - 0.37127485871315
  - 0.3737846910953522
  - 0.3741092085838318
  - 0.38037440180778503
  - 0.422763854265213
  - 0.49260368943214417
  - 0.46235358715057373
  - 0.35820895433425903
  - 0.3626440465450287
  - 0.45264092087745667
  - 0.4700656235218048
  - 0.5496989488601685
  - 0.36132922768592834
  - 0.41749632358551025
  - 0.3666384518146515
  - 0.36024850606918335
  - 0.3584235608577728
  - 0.4363984763622284
  - 0.4836825430393219
  - 0.43964794278144836
  - 0.626555860042572
  - 1.0135691165924072
  - 0.7154121398925781
  - 0.4902840852737427
  - 0.36521780490875244
  - 0.3555886149406433
  - 0.492813378572464
  - 0.4312405586242676
  - 0.4534923732280731
  - 0.7707997560501099
  - 0.5765509009361267
  - 1.4492937326431274
  - 0.5450708270072937
  - 0.530769944190979
  - 1.3791919946670532
  - 1.5629316568374634
  - 0.8561567664146423
  - 1.1373815536499023
  - 1.3265256881713867
  - 1.5246610641479492
  - 1.009967565536499
  - 0.6959514617919922
  - 0.6336886286735535
  - 0.983076810836792
  - 1.2328509092330933
  - 1.451389193534851
  - 0.6677248477935791
  - 0.683173418045044
  - 0.773184061050415
  - 0.5451309680938721
  - 0.5774861574172974
  - 0.5165921449661255
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 92 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8576329331046312, 0.8576329331046312,
    0.8505154639175257]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.06741573033707865, 0.02247191011235955]'
  mean_eval_accuracy: 0.8562094392672102
  mean_f1_accuracy: 0.01797752808988764
  total_train_time: '0:26:20.618434'
