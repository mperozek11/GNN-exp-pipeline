config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:20:43.167924'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_12fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_12fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_12fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_12fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_12fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_12fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_12fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_12fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_12fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_12fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 80.98150389250368
  - 26.430552691221237
  - 15.770102414488793
  - 20.946297907829287
  - 23.66532856076956
  - 21.731871682405473
  - 18.14527800679207
  - 25.007378605008128
  - 14.10285535156727
  - 14.016886129975319
  - 14.602757996320726
  - 13.196970361471177
  - 8.566798183321954
  - 8.81726150214672
  - 8.650691571831704
  - 12.103031778335572
  - 8.608594854176046
  - 12.245301762223244
  - 10.64591841697693
  - 8.55438286960125
  - 11.788443717360497
  - 8.817700010538102
  - 8.359687069058419
  - 7.94868383705616
  - 7.284758514165879
  - 7.83764917254448
  - 6.732344454526902
  - 6.78187775015831
  - 6.579216665029526
  - 6.876710930466652
  - 6.268798074126244
  - 7.018208280205727
  - 7.1818919569253925
  - 6.497385931015015
  - 6.522539103031159
  - 7.580998873710633
  - 7.858719190955163
  - 6.799731898307801
  - 6.610272780060768
  - 6.550119295716286
  - 6.5537006765604024
  - 6.595385566353798
  - 6.4051917701959615
  - 6.489138051867485
  - 6.3772375851869585
  - 6.422874653339386
  - 6.445098796486855
  - 6.357091039419174
  - 6.884473419189454
  - 6.560386660695077
  - 7.102302360534669
  - 6.739963325858117
  - 6.7007921010255815
  - 6.487296071648598
  - 6.151400008797646
  - 6.34205414056778
  - 6.267260086536408
  - 6.39099304676056
  - 6.628226241469384
  - 7.34133834540844
  - 6.806231892108918
  - 6.562118598818779
  - 6.404211339354515
  - 7.033326983451843
  - 8.261857107281685
  - 6.965725278854371
  - 6.380472087860108
  - 6.507761758565903
  - 6.519179260730744
  - 6.906979143619537
  - 6.551866906881333
  - 6.481124210357667
  - 6.655550003051758
  - 6.9978192538023
  - 6.456823191046715
  - 6.99270619750023
  - 6.396223187446594
  - 6.495715135335923
  - 6.403718492388726
  - 6.428860449790955
  - 6.379418668150902
  - 6.428577288985252
  - 6.211705821752549
  - 6.405754798650742
  - 6.339425933361054
  - 6.455441957712174
  - 6.51802768111229
  - 6.5278216212987905
  - 6.420983377099038
  - 9.350505837798119
  - 6.716417166590691
  - 6.910252821445465
  - 6.547846034169197
  - 6.9911448746919636
  - 6.940787738561631
  - 6.833123606443405
  - 6.8224411666393285
  - 7.0359231561422355
  - 6.7941018730402
  - 6.561481282114983
  validation_losses:
  - 4.538915634155273
  - 0.695265531539917
  - 0.41129255294799805
  - 0.6776835918426514
  - 0.5269412398338318
  - 0.40128293633461
  - 0.49907028675079346
  - 0.4328323304653168
  - 0.4714786410331726
  - 0.4279957413673401
  - 0.4261350631713867
  - 0.41002118587493896
  - 0.40726637840270996
  - 0.5285922884941101
  - 0.43185168504714966
  - 0.41020479798316956
  - 0.6975378394126892
  - 0.3954398036003113
  - 0.5418969392776489
  - 0.4233360290527344
  - 0.4680314064025879
  - 0.4894779324531555
  - 0.4093662202358246
  - 0.4011618196964264
  - 0.5756445527076721
  - 0.3865310549736023
  - 0.3867376148700714
  - 0.4019133150577545
  - 0.38861513137817383
  - 0.3978700041770935
  - 0.42960524559020996
  - 0.40352317690849304
  - 0.39995673298835754
  - 0.40300410985946655
  - 0.40634962916374207
  - 0.3975137174129486
  - 0.441705584526062
  - 0.42820680141448975
  - 0.4420585334300995
  - 0.38598868250846863
  - 0.39415085315704346
  - 0.44370225071907043
  - 0.39882758259773254
  - 0.3917296528816223
  - 0.43435707688331604
  - 0.4219517111778259
  - 0.43602386116981506
  - 0.39241498708724976
  - 0.3989444971084595
  - 0.40588903427124023
  - 0.6481083035469055
  - 0.4766835868358612
  - 0.39338257908821106
  - 0.4416649639606476
  - 0.44376543164253235
  - 0.39397525787353516
  - 0.40852370858192444
  - 0.505112886428833
  - 0.4640582203865051
  - 0.4384312033653259
  - 0.42126479744911194
  - 0.40546882152557373
  - 0.4012867510318756
  - 0.8487809896469116
  - 0.40966060757637024
  - 0.41028058528900146
  - 0.4104457497596741
  - 0.48562273383140564
  - 0.41007286310195923
  - 0.4687764048576355
  - 0.4100378453731537
  - 0.4262325167655945
  - 0.45807304978370667
  - 0.3975941836833954
  - 0.41920921206474304
  - 0.41403383016586304
  - 0.3994216322898865
  - 0.4223388433456421
  - 0.4144967198371887
  - 0.4113936424255371
  - 0.4467640519142151
  - 0.4061584174633026
  - 0.44760385155677795
  - 2.6833505630493164
  - 0.41663870215415955
  - 0.41165077686309814
  - 0.41308069229125977
  - 0.4202308654785156
  - 0.41876739263534546
  - 0.4574145972728729
  - 0.4541696310043335
  - 0.43434658646583557
  - 0.45300817489624023
  - 0.4700769782066345
  - 0.41518884897232056
  - 0.4500807821750641
  - 0.4582531750202179
  - 0.3954417109489441
  - 0.48262184858322144
  - 0.3994130492210388
loss_records_fold1:
  train_losses:
  - 6.3516323387622835
  - 6.435841348767281
  - 6.271369150280953
  - 6.391536703705788
  - 6.399731671810151
  - 6.65150785446167
  - 6.831640166044235
  - 6.276799994707108
  - 6.550878673791885
  - 6.546993935108185
  - 6.775069603323937
  - 6.561748200654984
  - 6.352564108371735
  - 6.5115718156099325
  - 6.365647429227829
  - 6.381423503160477
  - 6.373801535367966
  - 6.331397432088853
  - 6.425306552648545
  - 6.893842566013337
  - 6.810558566451073
  - 6.724632722139359
  - 6.683999609947205
  - 6.397048681974411
  - 6.9288903176784515
  - 6.5715749889612205
  - 6.443567869067192
  - 6.8981663405895235
  - 6.4631165087223055
  - 6.632496818900108
  - 6.4792414069175726
  - 6.438126736879349
  - 6.29721362888813
  - 6.5447960019111635
  - 6.546720254421235
  - 6.421595937013627
  - 6.513009321689606
  - 6.419386145472527
  - 6.435700604319573
  - 6.442702496051789
  - 6.459694346785546
  - 6.477507948875427
  - 6.460716956853867
  - 6.683655506372452
  - 6.395567008852959
  - 6.730009451508522
  - 6.297175204753876
  - 6.584956234693528
  - 6.286310860514641
  - 6.603117886185647
  - 6.27982414662838
  - 6.499624347686768
  - 6.263999912142754
  - 6.848629821836949
  - 6.502885031700135
  - 6.483660185337067
  - 6.287998661398888
  - 6.484568360447884
  - 6.418002223968506
  - 6.619819736480713
  - 6.362330606579781
  - 6.628926217556
  - 6.58729556798935
  - 6.429116693139076
  - 6.646203416585923
  - 6.793584281206131
  - 6.727474561333657
  - 6.802778366208077
  - 6.5690198898315435
  - 6.424959737062455
  - 6.363299730420113
  - 6.657896575331688
  - 6.539044889807702
  - 6.355916273593903
  - 6.498576569557191
  - 6.436882898211479
  - 6.9322352617979055
  - 6.600338706374169
  - 6.3982452869415285
  - 6.327793538570404
  - 6.529052734375
  - 6.765805295109749
  - 6.4536276251077656
  - 6.307146146893501
  - 6.203628143668175
  - 6.644054353237152
  - 6.568992912769318
  - 6.5249635756015785
  - 6.5168327569961555
  - 6.628821018338204
  - 6.404841789603234
  - 6.390461573004723
  - 6.262925589084626
  - 6.4703816980123525
  - 6.6047264546155935
  - 6.385556241869927
  - 6.827919971942902
  - 6.444762283563614
  - 6.749413987994195
  - 6.693185412883759
  validation_losses:
  - 0.423581063747406
  - 0.4197860360145569
  - 0.5439291000366211
  - 0.42879408597946167
  - 0.4616762697696686
  - 0.4322471022605896
  - 0.4172378182411194
  - 0.4123384654521942
  - 0.45826205611228943
  - 0.42754825949668884
  - 0.4357947111129761
  - 0.4251368045806885
  - 0.42434489727020264
  - 0.4182402491569519
  - 0.4339754283428192
  - 0.4499424993991852
  - 0.4220510423183441
  - 0.4119406044483185
  - 0.4236679673194885
  - 0.42329201102256775
  - 0.4071917235851288
  - 0.4121077060699463
  - 0.45702287554740906
  - 0.41492506861686707
  - 0.4611910283565521
  - 0.4598510265350342
  - 0.5134518146514893
  - 0.41608938574790955
  - 0.5227998495101929
  - 0.43639394640922546
  - 0.4187230169773102
  - 0.4195195436477661
  - 0.43277356028556824
  - 0.49577796459198
  - 0.4090590178966522
  - 0.5433464050292969
  - 0.42629238963127136
  - 0.40548446774482727
  - 0.4247649610042572
  - 0.47748062014579773
  - 0.4439498484134674
  - 0.4240204095840454
  - 0.43826326727867126
  - 0.41389191150665283
  - 0.5133969783782959
  - 0.42204785346984863
  - 0.4836418628692627
  - 0.46050283312797546
  - 0.41276517510414124
  - 0.44931560754776
  - 0.4176390469074249
  - 0.4194757044315338
  - 0.41997474431991577
  - 0.4615159332752228
  - 0.4200360178947449
  - 0.42910438776016235
  - 0.41789400577545166
  - 0.4178837537765503
  - 0.40878647565841675
  - 0.44778338074684143
  - 0.45337167382240295
  - 0.4301633834838867
  - 0.4169498085975647
  - 0.5441696643829346
  - 0.46766728162765503
  - 0.4599611461162567
  - 0.5055888891220093
  - 0.481787770986557
  - 0.43181294202804565
  - 0.4387611448764801
  - 0.4945797622203827
  - 0.43396270275115967
  - 0.4053258001804352
  - 0.4271816909313202
  - 0.42365649342536926
  - 0.4321305751800537
  - 0.48763740062713623
  - 0.465366393327713
  - 0.4147375524044037
  - 0.4499019682407379
  - 0.49378421902656555
  - 0.409250408411026
  - 0.44175660610198975
  - 0.4150494933128357
  - 0.43205195665359497
  - 0.45380347967147827
  - 0.4931814670562744
  - 0.4397062361240387
  - 0.5196334719657898
  - 0.4225556254386902
  - 0.406399667263031
  - 0.4374406933784485
  - 0.45293495059013367
  - 0.44164642691612244
  - 0.4376046061515808
  - 0.48082104325294495
  - 0.46848800778388977
  - 0.43971359729766846
  - 0.6083590388298035
  - 0.4411231279373169
loss_records_fold2:
  train_losses:
  - 6.423976814746857
  - 6.308470785617828
  - 6.858520287275315
  - 6.52741771042347
  - 6.500777301192284
  - 6.46931024491787
  - 6.393481260538102
  - 6.467609861493111
  - 6.663674518465996
  - 6.650168254971504
  - 6.407910838723183
  - 6.563737112283707
  - 6.424217537045479
  - 6.371447265148163
  - 6.526837924122811
  - 6.573359054327011
  - 6.418675664067269
  - 6.719887480139732
  - 6.35834738612175
  - 6.415254917740822
  - 6.636646631360055
  - 6.354181236028672
  - 6.721950295567513
  - 6.4463046312332155
  - 6.490130865573883
  - 6.875667771697045
  - 6.569857411086559
  - 6.41594906449318
  - 6.453681287169457
  - 6.455902627110482
  - 6.63636069893837
  - 6.44860320687294
  - 6.394501513242722
  - 6.438066416978836
  - 6.594407230615616
  - 6.520651054382324
  - 6.772573110461235
  - 6.498084643483162
  - 6.569404989480972
  - 6.576767328381539
  - 6.29747953414917
  - 7.1793978899717334
  - 7.54836685359478
  - 7.737191370129586
  - 6.646457961201668
  - 6.595243275165558
  - 6.820008486509323
  - 6.806614926457406
  - 6.486283367872239
  - 6.380423244833946
  - 9.46989402770996
  - 7.212877798080445
  - 6.410314682126046
  - 6.9563439220190055
  - 6.625883337855339
  - 6.602967116236687
  - 6.370875081419945
  - 6.418657577037812
  - 6.503160381317139
  - 6.605509448051453
  - 6.676945158839226
  - 6.4490390747785575
  - 6.282292899489403
  - 6.761637341976166
  - 6.864280855655671
  - 6.366605612635613
  - 6.429831665754318
  - 6.550078919529915
  - 6.927154749631882
  - 6.55628286600113
  - 6.628457495570183
  - 6.645154705643654
  - 6.526900973916054
  - 6.943767869472504
  - 6.335909748077393
  - 6.67789341211319
  - 6.6976651176810265
  - 6.680011481046677
  - 6.515623745322228
  - 6.899212458729744
  - 6.48079517185688
  - 6.30264767408371
  - 6.352230766415596
  - 7.299258895218372
  - 6.564878243207932
  - 6.742299249768258
  - 6.543205904960633
  - 6.39605682194233
  - 6.698646706342697
  - 7.6401475995779045
  - 6.591670665144921
  - 6.666719043254853
  - 6.3051147103309635
  - 6.40994702577591
  - 6.430937579274178
  - 6.620361006259919
  - 6.573286464810372
  - 6.462384590506554
  - 6.65333194732666
  - 6.536348471045494
  validation_losses:
  - 0.412076473236084
  - 0.3980156183242798
  - 0.7817845940589905
  - 16.395898818969727
  - 0.42946264147758484
  - 0.3989616632461548
  - 0.42887595295906067
  - 0.43264153599739075
  - 0.41464218497276306
  - 0.4288884997367859
  - 52.99903106689453
  - 21.839921951293945
  - 0.3893054127693176
  - 0.397090345621109
  - 0.39150986075401306
  - 0.4468807280063629
  - 0.41544023156166077
  - 0.4153417944908142
  - 0.40464162826538086
  - 0.42062613368034363
  - 0.42533135414123535
  - 0.4001927375793457
  - 0.41564446687698364
  - 0.4011148512363434
  - 0.4414229989051819
  - 0.396636039018631
  - 0.41468366980552673
  - 0.41192522644996643
  - 0.42406901717185974
  - 0.40494784712791443
  - 0.4419581890106201
  - 0.394658625125885
  - 0.39924341440200806
  - 0.3873754143714905
  - 0.40721333026885986
  - 0.4238084554672241
  - 0.5131338834762573
  - 0.4008076786994934
  - 0.4365859627723694
  - 0.39587587118148804
  - 0.40516355633735657
  - 0.41182515025138855
  - 0.4108222424983978
  - 0.3999652862548828
  - 211.62301635742188
  - 0.4334650933742523
  - 0.4327654242515564
  - 0.4343680739402771
  - 0.42858457565307617
  - 0.8943566679954529
  - 0.42071789503097534
  - 0.43556588888168335
  - 3.514012098312378
  - 2.0268335342407227
  - 0.44143757224082947
  - 0.4100646674633026
  - 0.41467559337615967
  - 0.3876035213470459
  - 0.4137151837348938
  - 0.4208239018917084
  - 0.4384475350379944
  - 0.40256592631340027
  - 0.4452131688594818
  - 0.4100380539894104
  - 0.42122533917427063
  - 0.40164506435394287
  - 0.4951232671737671
  - 0.3989171087741852
  - 0.4912818372249603
  - 0.47257760167121887
  - 0.43476730585098267
  - 0.433074027299881
  - 0.40738460421562195
  - 0.4226035475730896
  - 0.40830981731414795
  - 0.43618836998939514
  - 0.40287089347839355
  - 0.46801304817199707
  - 0.4110254645347595
  - 0.45208266377449036
  - 0.42590224742889404
  - 0.40684211254119873
  - 0.48454928398132324
  - 0.5105172395706177
  - 0.3924805521965027
  - 0.4217051863670349
  - 0.3992145359516144
  - 0.41029122471809387
  - 0.4955999255180359
  - 8.100281715393066
  - 0.41118136048316956
  - 0.4052489101886749
  - 0.41428184509277344
  - 0.4074195325374603
  - 0.39890721440315247
  - 0.42876890301704407
  - 0.4123929440975189
  - 0.42175158858299255
  - 0.4054984748363495
  - 0.4461286962032318
loss_records_fold3:
  train_losses:
  - 6.593268418312073
  - 6.536485296487808
  - 6.587643650174141
  - 6.486270540952683
  - 6.641323199868203
  - 6.4102506965398796
  - 6.286702245473862
  - 6.347349381446839
  - 6.534014570713044
  - 6.425662305951119
  - 6.593257331848145
  - 6.6209088474512106
  - 6.504097875952721
  - 6.366280102729798
  - 6.39989302456379
  - 6.418357226252557
  validation_losses:
  - 0.4315444231033325
  - 0.46760082244873047
  - 0.4241046905517578
  - 0.41104984283447266
  - 0.40805956721305847
  - 0.43470701575279236
  - 0.5010328888893127
  - 0.40118488669395447
  - 0.4307493567466736
  - 0.4970075488090515
  - 0.4288722276687622
  - 0.4253352880477905
  - 0.4162863492965698
  - 0.4091816842556
  - 0.41218408942222595
  - 0.41717952489852905
loss_records_fold4:
  train_losses:
  - 6.861063954234123
  - 6.459166237711907
  - 6.646932324767113
  - 6.427102786302567
  - 6.5860341116786
  - 6.399864691495896
  - 6.514458853006364
  - 6.52196830958128
  - 6.508363783359528
  - 6.584658628702164
  - 6.6314484059810646
  - 6.468400639295578
  - 6.435701391100884
  - 7.060982045531273
  - 6.579892194271088
  - 6.703683650493622
  - 6.473617535829544
  - 6.431531563401222
  - 6.4478748649358755
  - 6.234850469231606
  - 6.374340277910233
  - 6.458883094787598
  - 6.6136739462614065
  - 6.546529531478882
  - 6.333550798892976
  - 6.446594735980034
  validation_losses:
  - 0.4080575704574585
  - 0.43892163038253784
  - 0.42902451753616333
  - 0.41807907819747925
  - 0.43683186173439026
  - 0.43991518020629883
  - 0.4214831292629242
  - 0.41066834330558777
  - 0.44171467423439026
  - 0.4642563760280609
  - 0.4224278926849365
  - 0.41391581296920776
  - 0.4196450114250183
  - 0.43101581931114197
  - 0.46415749192237854
  - 0.4371279180049896
  - 0.43308311700820923
  - 0.41615909337997437
  - 0.4031134247779846
  - 0.45036667585372925
  - 0.4132320284843445
  - 0.4101090729236603
  - 0.4059176743030548
  - 0.4149509072303772
  - 0.40251395106315613
  - 0.4031565189361572
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:33:35.401914'
