config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:10:23.344319'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_86fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_86fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_86fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_86fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_86fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_86fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_86fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_86fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_86fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_86fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.9285894215106965
  - 1.6600942552089692
  - 1.6276820659637452
  - 1.5926259815692902
  - 1.6898567497730257
  - 1.5979358792304994
  - 1.5916503548622132
  - 1.5489021480083467
  - 1.6691574037075043
  - 1.6627953410148621
  - 1.6311012387275696
  - 1.706453162431717
  - 1.6037056922912598
  - 1.5927148282527925
  - 1.5376521229743958
  - 1.6473257660865785
  - 1.7674081921577454
  - 1.620065474510193
  - 1.558096694946289
  - 1.5520236730575563
  - 1.6145914852619172
  - 1.586327922344208
  - 1.6026312232017519
  - 1.6221843600273134
  - 1.5641143739223482
  - 1.5375459730625154
  validation_losses:
  - 0.43255895376205444
  - 0.4056377708911896
  - 0.39952704310417175
  - 0.40401196479797363
  - 0.3985000252723694
  - 0.4136408567428589
  - 0.4006028473377228
  - 0.3847593069076538
  - 0.4164259433746338
  - 0.4215717017650604
  - 0.4498074948787689
  - 0.3952297568321228
  - 0.3970179855823517
  - 0.44168567657470703
  - 0.8733805418014526
  - 1.5259075164794922
  - 1.1068286895751953
  - 0.5922201871871948
  - 0.43623870611190796
  - 0.6124159693717957
  - 0.4500683844089508
  - 0.4041420817375183
  - 0.3947034180164337
  - 0.4010252356529236
  - 0.3894199728965759
  - 0.3878893256187439
loss_records_fold1:
  train_losses:
  - 1.576047557592392
  - 1.6221810460090638
  - 1.6052500426769258
  - 1.5686910629272461
  - 1.5462524950504304
  - 1.5774894267320634
  - 1.6351860225200654
  - 1.6452382266521455
  - 1.6101755261421205
  - 1.5637802183628082
  - 1.5366965770721437
  - 1.6095539152622225
  - 1.499068796634674
  validation_losses:
  - 0.39555802941322327
  - 0.41318610310554504
  - 0.40176722407341003
  - 0.40859606862068176
  - 0.4020977020263672
  - 0.39877671003341675
  - 0.41526147723197937
  - 0.418542742729187
  - 0.3966808319091797
  - 0.39147451519966125
  - 0.40046197175979614
  - 0.39576125144958496
  - 0.39388713240623474
loss_records_fold2:
  train_losses:
  - 1.520095717906952
  - 1.5860388040542603
  - 1.551892912387848
  - 1.5710348308086397
  - 1.5372588455677034
  - 1.5404925882816316
  - 1.576706326007843
  - 1.5765099108219147
  - 1.688696950674057
  - 1.561111342906952
  - 1.6182355284690857
  - 1.605201429128647
  - 1.5514201521873474
  - 1.5490629851818085
  - 1.5492273688316347
  - 1.5974689602851868
  - 1.583252948522568
  - 1.516492021083832
  - 1.6144745111465455
  - 1.5030316025018693
  - 1.5777256011962892
  - 1.5575719475746155
  - 1.5700158774852753
  - 1.486703872680664
  - 1.549133735895157
  - 1.543132707476616
  - 1.5396296203136446
  - 1.594612902402878
  - 1.5943597972393038
  - 1.546820378303528
  - 1.5715098142623902
  - 1.634951424598694
  - 1.5237506270408632
  - 1.6223427057266235
  - 1.5486169338226319
  - 1.5842310249805451
  - 1.5392562925815583
  - 1.5735716581344605
  - 1.548933494091034
  - 1.569716203212738
  - 1.5545311450958252
  - 1.5292264580726624
  - 1.5364082336425782
  - 1.5424904823303223
  - 1.5659500658512115
  - 1.5564919233322145
  - 1.5137264370918275
  - 1.4732872486114503
  - 1.5260802417993546
  - 1.5292743742465973
  - 1.504684591293335
  - 1.5084058672189713
  - 1.5449675738811495
  - 1.533033734560013
  - 1.5851121127605439
  - 1.5364736795425415
  - 1.5342067122459413
  - 1.517746788263321
  - 1.518204927444458
  - 1.5387507379055023
  - 1.529408484697342
  - 1.5441194772720337
  - 1.497532281279564
  - 1.5124075889587403
  - 1.5347186505794526
  - 1.4979663968086243
  - 1.5048149883747102
  - 1.5336689829826355
  - 1.5102413892745972
  - 1.609593325853348
  - 1.6136738777160646
  - 1.5741179943084718
  - 1.6418547093868257
  - 1.5608244121074677
  - 1.513045358657837
  validation_losses:
  - 0.39034903049468994
  - 0.38941332697868347
  - 0.3859209716320038
  - 0.4183783531188965
  - 0.38611897826194763
  - 0.4051471948623657
  - 0.4027288258075714
  - 0.40540367364883423
  - 0.43646904826164246
  - 0.4044623374938965
  - 0.39130377769470215
  - 0.39940643310546875
  - 0.39972227811813354
  - 0.4030149579048157
  - 0.41452503204345703
  - 0.4382607936859131
  - 0.40775421261787415
  - 0.41735193133354187
  - 0.38682010769844055
  - 0.3923056721687317
  - 0.3882122039794922
  - 0.4022608995437622
  - 0.39319372177124023
  - 0.40364325046539307
  - 0.4059754014015198
  - 0.3921505808830261
  - 0.42662733793258667
  - 0.3859453499317169
  - 0.4012359380722046
  - 0.39063066244125366
  - 0.40018680691719055
  - 0.3841801583766937
  - 0.4008367657661438
  - 0.4004627764225006
  - 0.4023306369781494
  - 0.434281587600708
  - 0.3933325409889221
  - 0.39617523550987244
  - 0.40650811791419983
  - 0.8166294693946838
  - 0.4034174978733063
  - 0.4035157859325409
  - 0.4356403946876526
  - 0.409257709980011
  - 0.4133756756782532
  - 0.4963878393173218
  - 0.41352784633636475
  - 0.3894939124584198
  - 0.3910643756389618
  - 0.4067244231700897
  - 0.4441790282726288
  - 0.44192761182785034
  - 0.4335997998714447
  - 0.3869834542274475
  - 0.3919813930988312
  - 0.390573114156723
  - 0.40096592903137207
  - 0.4376894533634186
  - 0.4037947952747345
  - 0.3866945505142212
  - 0.4168786406517029
  - 0.47985243797302246
  - 0.44362321496009827
  - 0.4481910169124603
  - 0.4727884829044342
  - 0.41598355770111084
  - 0.5863772034645081
  - 0.524514377117157
  - 0.5622288584709167
  - 0.4325229525566101
  - 0.41610217094421387
  - 0.40233445167541504
  - 0.3931821882724762
  - 0.3968181014060974
  - 0.3914545774459839
loss_records_fold3:
  train_losses:
  - 1.5788779616355897
  - 1.568799901008606
  - 1.5989600002765656
  - 1.5450830698013307
  - 1.572298061847687
  - 1.5037102460861207
  - 1.6289836645126343
  - 1.5377448201179504
  - 1.5797320723533632
  - 1.5699077606201173
  - 1.5434564054012299
  - 1.6004894196987154
  - 1.534039556980133
  - 1.591455191373825
  - 1.502451553940773
  - 1.5421132743358612
  - 1.568026912212372
  - 1.545290094614029
  - 1.5414116322994234
  - 1.4935004055500032
  - 1.5699105441570282
  - 1.5259268879890442
  - 1.5343588948249818
  - 1.617194026708603
  - 1.496353852748871
  - 1.574198204278946
  - 1.5142078071832659
  validation_losses:
  - 0.3736436367034912
  - 0.38044822216033936
  - 0.37982267141342163
  - 0.4659193158149719
  - 0.4214658737182617
  - 0.40001749992370605
  - 0.38250768184661865
  - 0.3774697482585907
  - 0.38676902651786804
  - 0.39978328347206116
  - 0.37947767972946167
  - 0.39367127418518066
  - 0.3851509690284729
  - 0.41068005561828613
  - 0.3930918574333191
  - 0.38378602266311646
  - 0.4038025140762329
  - 0.3960378170013428
  - 0.40143290162086487
  - 0.3851046860218048
  - 0.4199830889701843
  - 0.39461594820022583
  - 0.38415518403053284
  - 0.3805825412273407
  - 0.3817274868488312
  - 0.37577059864997864
  - 0.3807000517845154
loss_records_fold4:
  train_losses:
  - 1.5679534375667572
  - 1.59088596701622
  - 1.5623527228832246
  - 1.5477061569690704
  - 1.4761094927787781
  - 1.5172613322734834
  - 1.5914746284484864
  - 1.5809565663337708
  - 1.5228660047054292
  - 1.531667309999466
  - 1.5866953790187837
  validation_losses:
  - 0.3810909688472748
  - 0.4066751301288605
  - 0.3895683288574219
  - 0.3764144778251648
  - 0.38014909625053406
  - 0.38673681020736694
  - 0.37796494364738464
  - 0.3739611506462097
  - 0.37633487582206726
  - 0.38367360830307007
  - 0.3880848288536072
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 75 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.855917667238422, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579276523256294
  mean_f1_accuracy: 0.0
  total_train_time: '0:12:56.323288'
