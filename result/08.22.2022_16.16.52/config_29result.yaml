config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:45:06.139277'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_29fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_29fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_29fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_29fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_29fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_29fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_29fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_29fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_29fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_29fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 46.76458348035813
  - 14.457095807790758
  - 12.826285523176194
  - 7.421796107292176
  - 6.734180784225464
  - 6.573934656381607
  - 5.129281944036484
  - 11.897020441293717
  - 7.85503997206688
  - 5.368549358844757
  - 5.555300301313401
  - 5.26142036318779
  - 6.151126116514206
  - 7.957911854982377
  - 5.728132557868958
  - 4.864394438266754
  - 5.8394006371498115
  - 8.87663535475731
  - 7.901305207610131
  - 7.4230015873909
  - 7.201756727695465
  - 4.936328548192979
  - 5.896716266870499
  - 4.972047120332718
  - 9.862100434303285
  - 7.513074332475663
  - 5.891021525859833
  - 6.599652856588364
  - 4.115472969412804
  - 3.390491157770157
  - 3.472976192831993
  - 7.113072136044503
  - 3.7972708821296695
  - 4.339761927723885
  - 3.6428199946880344
  - 3.199061018228531
  - 4.526580110192299
  - 6.234433960914612
  - 5.304078513383866
  - 3.7881852567195895
  - 4.156920170783997
  - 3.5646262705326084
  - 4.888363528251649
  - 7.159756660461426
  - 4.342064607143402
  - 3.255505079030991
  - 4.075124657154084
  - 3.4975703537464145
  - 3.2283285677433016
  - 3.737432172894478
  - 3.877571851015091
  - 3.3833823204040527
  - 3.1471881449222567
  - 3.354577648639679
  - 3.2104302644729614
  - 3.635517674684525
  - 3.694385612010956
  - 4.196382367610932
  - 3.3330586314201356
  - 4.703939974308014
  - 3.6857126712799073
  - 3.2539603888988498
  - 3.1426756739616395
  - 3.511703199148178
  - 3.510217493772507
  - 3.3908406227827075
  - 3.4578071713447573
  - 3.3403657853603366
  - 3.3919056296348575
  - 3.630212825536728
  - 3.443633896112442
  - 3.5870279699563983
  - 3.1695897936820985
  - 3.3077200233936312
  - 3.2311147511005402
  - 3.1594143331050875
  - 3.108773872256279
  - 3.2973928570747377
  - 3.1973282992839813
  - 3.355843698978424
  - 3.0725779294967652
  - 3.3428385794162754
  - 3.2753346085548403
  - 3.634066653251648
  - 3.281369858980179
  - 3.4650034189224246
  - 3.0779054641723635
  - 3.2038528978824616
  - 3.1683504164218905
  - 3.12003555893898
  - 3.08099656701088
  - 3.207276898622513
  - 3.1176081717014315
  - 3.1622334986925127
  - 3.1700220465660096
  - 3.0870790839195252
  - 3.1664547622203827
  - 3.243965202569962
  - 3.3497202157974244
  - 3.311714017391205
  validation_losses:
  - 2.680246114730835
  - 0.8881463408470154
  - 0.6214280724525452
  - 0.4686790406703949
  - 0.6563010811805725
  - 0.4913254976272583
  - 0.7630546689033508
  - 0.4728650152683258
  - 0.4618656039237976
  - 0.5011727213859558
  - 0.5868059992790222
  - 0.5153841376304626
  - 0.48735538125038147
  - 0.47461217641830444
  - 0.5027046203613281
  - 0.4066316783428192
  - 0.5246846675872803
  - 0.38972556591033936
  - 0.5656687021255493
  - 0.4496854841709137
  - 0.41313105821609497
  - 0.4421755373477936
  - 0.381374329328537
  - 0.4464605450630188
  - 0.39575448632240295
  - 0.5534579157829285
  - 0.6596277356147766
  - 0.42913976311683655
  - 0.550358772277832
  - 0.46787959337234497
  - 0.39846524596214294
  - 0.4362117350101471
  - 0.5178064703941345
  - 0.4435276389122009
  - 0.4014567732810974
  - 0.4434143006801605
  - 0.409038245677948
  - 0.39578402042388916
  - 0.4062630832195282
  - 0.47708645462989807
  - 0.3953886330127716
  - 0.4045880436897278
  - 0.43874919414520264
  - 0.40017253160476685
  - 0.4055020809173584
  - 0.4107198119163513
  - 0.4267784357070923
  - 0.38383564352989197
  - 0.3822077214717865
  - 0.40450772643089294
  - 0.4080670475959778
  - 0.39217182993888855
  - 0.4077679216861725
  - 0.3924022614955902
  - 0.41905713081359863
  - 0.42339304089546204
  - 0.4499777853488922
  - 0.43756935000419617
  - 0.4320352375507355
  - 0.4626533091068268
  - 0.39794760942459106
  - 0.40353384613990784
  - 0.4013407230377197
  - 0.3984634280204773
  - 0.40299341082572937
  - 0.4296930730342865
  - 0.4642353057861328
  - 0.4317684769630432
  - 0.4339514374732971
  - 0.3957584500312805
  - 0.6430864334106445
  - 0.4207187592983246
  - 0.43656352162361145
  - 0.3964117169380188
  - 0.41252613067626953
  - 0.4281363785266876
  - 0.42230120301246643
  - 0.40577995777130127
  - 0.39373713731765747
  - 0.4097367227077484
  - 0.4135894477367401
  - 0.41345351934432983
  - 0.41985395550727844
  - 0.4304768741130829
  - 0.4647749364376068
  - 0.4153122305870056
  - 0.40225961804389954
  - 0.4139048457145691
  - 0.44060206413269043
  - 0.41874104738235474
  - 0.442569375038147
  - 0.41386133432388306
  - 0.44481396675109863
  - 0.3872523605823517
  - 0.39699873328208923
  - 0.4520643949508667
  - 0.41685211658477783
  - 0.4185419976711273
  - 0.4044950008392334
  - 0.40194934606552124
loss_records_fold1:
  train_losses:
  - 3.2264527142047883
  - 3.554826992750168
  - 3.443770277500153
  - 3.15604430437088
  - 3.328631591796875
  - 3.303334414958954
  - 3.175158339738846
  - 3.1452774643898014
  - 3.2191295623779297
  - 3.1552366375923158
  - 3.282872581481934
  - 3.292035347223282
  - 3.400116902589798
  - 3.215571916103363
  validation_losses:
  - 0.4911116063594818
  - 0.5951835513114929
  - 0.4320906400680542
  - 0.4465731680393219
  - 0.4099464416503906
  - 0.44216641783714294
  - 0.42164480686187744
  - 0.4637031555175781
  - 0.4178514778614044
  - 0.4133288860321045
  - 0.41399887204170227
  - 0.4081592559814453
  - 0.41564345359802246
  - 0.42168697714805603
loss_records_fold2:
  train_losses:
  - 3.1423841387033464
  - 3.4124485135078433
  - 3.3042554318904878
  - 3.3662919938564304
  - 3.237717127799988
  - 3.1847487628459934
  - 3.2502885937690738
  - 3.2671207547187806
  - 3.2975440204143527
  - 3.259046357870102
  - 3.277035939693451
  - 3.4317054569721224
  - 3.1796801388263702
  - 3.279230880737305
  - 3.200719797611237
  - 3.235728704929352
  - 3.2089396834373476
  - 3.2068969011306763
  - 3.2547338187694552
  - 3.088489902019501
  - 3.2335049211978912
  - 3.340440481901169
  - 3.232716274261475
  - 3.3164946138858795
  - 3.3048403680324556
  - 3.2002540171146396
  - 3.1902593255043032
  - 3.3593396425247195
  - 3.2749449372291566
  - 3.328663343191147
  - 3.2139593362808228
  - 3.178149563074112
  - 3.1967362344264987
  - 3.2556547224521637
  - 3.298505961894989
  - 3.3033079981803897
  - 3.206653016805649
  - 3.2703730881214144
  validation_losses:
  - 0.4176180064678192
  - 0.3938042223453522
  - 0.4140963852405548
  - 1.3932387828826904
  - 0.3912801742553711
  - 0.39561328291893005
  - 0.4411466121673584
  - 0.4157584607601166
  - 0.3939822018146515
  - 0.41042232513427734
  - 0.39873382449150085
  - 0.41755759716033936
  - 0.4080987572669983
  - 0.42569053173065186
  - 0.4040360450744629
  - 0.39577123522758484
  - 0.41539430618286133
  - 0.39795511960983276
  - 0.40210533142089844
  - 0.4057782292366028
  - 0.4090079665184021
  - 0.3909837603569031
  - 0.4153212308883667
  - 0.3948361575603485
  - 0.41583001613616943
  - 0.39557749032974243
  - 0.44162338972091675
  - 0.4020785987377167
  - 0.455098956823349
  - 0.4021467864513397
  - 0.4080147445201874
  - 0.44149163365364075
  - 0.3989785611629486
  - 0.3947618305683136
  - 0.3991551697254181
  - 0.3982526361942291
  - 0.3971244692802429
  - 0.3974187970161438
loss_records_fold3:
  train_losses:
  - 3.188754910230637
  - 3.286709123849869
  - 3.1557357966899873
  - 3.25119703412056
  - 3.4622398734092714
  - 3.4512770712375644
  - 3.2981902897357944
  - 3.499477207660675
  - 3.467132842540741
  - 4.410532784461975
  - 3.9432266652584076
  - 3.3992443621158603
  - 3.288373148441315
  - 3.545512962341309
  - 3.3283547669649125
  - 3.204874473810196
  - 3.2898866415023806
  - 3.923679003119469
  - 3.6437376141548157
  - 3.2465200543403627
  validation_losses:
  - 0.5038990378379822
  - 0.4093218147754669
  - 0.49931657314300537
  - 0.4118437170982361
  - 0.5001125335693359
  - 0.41059836745262146
  - 0.41372954845428467
  - 0.40625056624412537
  - 0.40978389978408813
  - 0.4043557643890381
  - 0.4206143617630005
  - 0.44549959897994995
  - 0.41136449575424194
  - 0.4258919656276703
  - 0.4081333577632904
  - 0.4165313243865967
  - 0.4222196042537689
  - 0.4177149832248688
  - 0.40540701150894165
  - 0.40922537446022034
loss_records_fold4:
  train_losses:
  - 3.2367272436618806
  - 3.518599843978882
  - 3.2253586411476136
  - 3.4825973331928255
  - 3.740627330541611
  - 3.2783627271652223
  - 3.4031185269355775
  - 3.2719054222106934
  - 3.3291499197483065
  - 3.3750579953193665
  - 3.2421509683132173
  - 3.3284870326519016
  - 3.397407972812653
  - 3.391068029403687
  - 3.2861127316951753
  - 3.2071135997772218
  - 3.3153682827949527
  - 3.484266406297684
  - 3.2255061864852905
  - 3.129278647899628
  - 3.2358868360519413
  - 3.3444113701581957
  - 3.3500624001026154
  - 3.2980334103107456
  - 3.4596896111965183
  - 3.4229637384414673
  - 3.2406074643135074
  - 3.326351481676102
  - 3.1498236894607547
  - 3.400293415784836
  - 3.386623087525368
  - 3.270959490537644
  - 3.3658888697624207
  - 3.249930971860886
  - 3.292009633779526
  - 3.26842320561409
  - 3.3178773283958436
  - 3.219805854558945
  - 3.331167158484459
  - 3.3881012439727787
  - 3.2456493228673935
  - 3.4543393552303314
  - 3.285783582925797
  - 3.3140177547931673
  - 3.508287340402603
  - 3.2691269248723986
  - 3.3793461680412293
  - 3.2443423330783845
  - 3.2679423928260807
  - 3.232718324661255
  - 3.194606322050095
  validation_losses:
  - 0.4120388925075531
  - 0.40882715582847595
  - 0.474255234003067
  - 0.4209660589694977
  - 0.41784653067588806
  - 0.4634166657924652
  - 0.4060323238372803
  - 0.4056234657764435
  - 0.40579065680503845
  - 0.4162059724330902
  - 0.4377785325050354
  - 0.41117292642593384
  - 0.41192564368247986
  - 0.4078460931777954
  - 0.4470738172531128
  - 0.4038600027561188
  - 0.4047602117061615
  - 0.4229032099246979
  - 0.40666651725769043
  - 0.408865749835968
  - 0.39908069372177124
  - 0.42018139362335205
  - 0.40178704261779785
  - 0.4402764141559601
  - 0.4166625738143921
  - 0.4448135197162628
  - 0.41499173641204834
  - 0.4074636995792389
  - 0.4253429174423218
  - 0.4413582384586334
  - 0.4240933954715729
  - 0.40240755677223206
  - 0.46554502844810486
  - 0.42965301871299744
  - 0.40529313683509827
  - 0.4024980068206787
  - 0.4126719534397125
  - 0.40123412013053894
  - 0.4547961354255676
  - 0.40405377745628357
  - 0.40762731432914734
  - 0.45692214369773865
  - 0.40153148770332336
  - 0.4033437967300415
  - 0.43881291151046753
  - 0.42900267243385315
  - 0.41452714800834656
  - 0.408203661441803
  - 0.41379469633102417
  - 0.39901748299598694
  - 0.40411612391471863
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 38 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:20:03.688464'
