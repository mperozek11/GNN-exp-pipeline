config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:30:57.744187'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_91fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_91fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_91fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_91fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_91fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_91fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_91fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_91fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_91fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_91fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 14.787691879272462
  - 3.757851791381836
  - 3.825900608301163
  - 2.3400121450424196
  - 7.498900735378266
  - 1.4960525035858154
  - 1.8675228714942933
  - 2.180854943394661
  - 2.412625139951706
  - 1.4325301706790925
  - 1.3410568118095398
  - 1.2846315562725068
  - 0.9119692265987397
  - 0.951906168460846
  - 1.109215933084488
  - 2.4281525671482087
  - 5.221768796443939
  - 2.340945887565613
  - 1.0905642837285996
  - 1.0243758499622346
  - 1.545367693901062
  - 1.4774150550365448
  - 1.3571662664413453
  - 1.0994369864463807
  - 0.9948099255561829
  - 1.0065751791000366
  - 1.1416163682937623
  - 1.1079727292060852
  - 0.9237033665180207
  - 0.9576593399047852
  - 1.2890486001968384
  - 0.9075679600238801
  - 0.8810030400753022
  - 2.621227014064789
  - 0.8365266919136047
  - 1.0870020508766174
  - 1.0903778076171875
  - 0.8943526506423951
  - 1.0727827548980713
  - 0.8660664319992066
  - 1.3721455335617065
  - 0.809263163805008
  - 0.958876222372055
  - 1.9353899836540223
  - 0.9410971879959107
  - 0.8822827517986298
  - 0.8320595920085907
  - 1.4936445593833925
  - 1.1852417051792146
  - 1.1937111377716065
  - 1.2383040189743042
  - 1.2352039396762848
  - 1.6868654489517212
  - 0.9892773151397706
  - 1.126728492975235
  - 1.1376789450645448
  - 0.9241616368293762
  - 1.8460048854351045
  - 1.1511344909667969
  - 1.177346283197403
  - 1.6488394916057587
  - 2.6027629256248477
  - 2.057443428039551
  - 0.976617956161499
  - 0.8952525973320008
  - 1.1057109534740448
  - 0.8842521250247956
  - 0.9416664302349091
  - 0.8799074411392213
  - 0.9048408448696137
  - 1.6276871323585511
  - 1.1233679473400116
  - 2.0342510461807253
  - 0.9060391277074814
  - 3.219028055667877
  - 2.9034076511859896
  - 1.0508275568485261
  - 1.327939134836197
  - 1.0307696759700775
  - 1.2195030629634858
  - 0.9578447759151459
  - 0.9430654466152192
  - 1.8657102644443513
  validation_losses:
  - 2.1757285594940186
  - 5.869774341583252
  - 0.9003576636314392
  - 3.7710723876953125
  - 1.340240478515625
  - 0.947439432144165
  - 1.0673837661743164
  - 1.558733582496643
  - 1.1303709745407104
  - 0.4905967116355896
  - 0.9714072942733765
  - 0.7948922514915466
  - 0.6081606149673462
  - 0.3836302161216736
  - 0.4181208312511444
  - 1.8170431852340698
  - 0.5224372148513794
  - 0.635699987411499
  - 0.46306464076042175
  - 0.46358636021614075
  - 0.7091731429100037
  - 0.6401665806770325
  - 0.5930978655815125
  - 0.5373719930648804
  - 0.3960903286933899
  - 0.4286578297615051
  - 0.38754159212112427
  - 0.4541258215904236
  - 0.5095183253288269
  - 0.4041343629360199
  - 0.4875410795211792
  - 0.4222540855407715
  - 0.39345821738243103
  - 0.3979561924934387
  - 0.3948535919189453
  - 0.45408785343170166
  - 0.421630859375
  - 0.3891550898551941
  - 0.3815684914588928
  - 0.5429123044013977
  - 0.39416661858558655
  - 0.47175171971321106
  - 0.40874576568603516
  - 0.43782708048820496
  - 0.5391526818275452
  - 0.4627372622489929
  - 0.398465096950531
  - 0.42332521080970764
  - 0.46184319257736206
  - 0.3950660526752472
  - 0.3930504024028778
  - 0.5461487770080566
  - 0.4168094992637634
  - 0.3861299157142639
  - 0.3895353674888611
  - 0.388607382774353
  - 0.4129028618335724
  - 0.40556764602661133
  - 0.4262453615665436
  - 0.504555344581604
  - 0.38873130083084106
  - 0.38382959365844727
  - 0.48343315720558167
  - 0.43440955877304077
  - 0.40397173166275024
  - 0.3854142129421234
  - 0.3874911367893219
  - 0.43809816241264343
  - 0.38873326778411865
  - 0.3891124725341797
  - 0.44274789094924927
  - 0.45004594326019287
  - 0.42474591732025146
  - 0.3930566906929016
  - 0.3869558274745941
  - 0.3996741473674774
  - 0.4378037750720978
  - 0.3996928632259369
  - 0.3961448073387146
  - 0.38722002506256104
  - 0.38930588960647583
  - 0.38742998242378235
  - 0.3908796012401581
loss_records_fold1:
  train_losses:
  - 1.7450657546520234
  - 0.9243792116641999
  - 0.8118375480175019
  - 0.8050995409488678
  - 0.7736787080764771
  - 0.7870737552642822
  - 0.8573750853538513
  - 0.8567256778478622
  - 0.7792043805122376
  - 0.8603022158145905
  - 0.9991172134876252
  validation_losses:
  - 0.4129919111728668
  - 0.40193262696266174
  - 0.4033239781856537
  - 0.4087461829185486
  - 0.40307849645614624
  - 0.40287744998931885
  - 0.40424254536628723
  - 0.4072401523590088
  - 0.4024515151977539
  - 0.40402090549468994
  - 0.4042457342147827
loss_records_fold2:
  train_losses:
  - 1.2900804281234741
  - 0.8252435743808747
  - 0.8564039885997773
  - 0.846411830186844
  - 0.842270439863205
  - 0.7957505583763123
  - 0.8055904269218446
  - 0.7934370458126069
  - 0.9016732454299927
  - 0.7988570511341095
  - 0.810763555765152
  - 0.7811301410198213
  - 0.8024082839488984
  - 0.8085820972919464
  - 1.0065781891345977
  - 0.8250393807888031
  - 0.8105100452899934
  validation_losses:
  - 0.3886508047580719
  - 0.38239866495132446
  - 0.3923349380493164
  - 0.39760348200798035
  - 0.39098304510116577
  - 0.38385024666786194
  - 0.3835963308811188
  - 0.3822535574436188
  - 0.3862147629261017
  - 0.3871630132198334
  - 0.4133123755455017
  - 0.3860049545764923
  - 0.382236510515213
  - 0.38780510425567627
  - 0.38618794083595276
  - 0.3814837634563446
  - 0.38696709275245667
loss_records_fold3:
  train_losses:
  - 2.093182283639908
  - 1.2817061424255372
  - 0.8310491681098938
  - 0.9063780426979066
  - 0.8830586135387422
  - 0.9678586721420288
  - 0.7706965267658235
  - 0.8808486342430115
  - 0.8060126900672913
  - 2.416726803779602
  - 0.9061497509479524
  validation_losses:
  - 0.40340518951416016
  - 0.39076048135757446
  - 0.3951853811740875
  - 0.3938862681388855
  - 0.3927336633205414
  - 0.397061288356781
  - 0.3928425908088684
  - 0.4011078178882599
  - 0.4000513553619385
  - 0.39657771587371826
  - 0.3917379081249237
loss_records_fold4:
  train_losses:
  - 0.7873813331127167
  - 0.7928468704223633
  - 0.7694353938102723
  - 0.7444335669279099
  - 0.7480940014123917
  - 0.7643535196781159
  - 0.7434098988771439
  - 0.7892305076122285
  - 0.8974342823028565
  - 0.7443424791097641
  - 0.7920720279216766
  - 0.808561933040619
  - 0.7736732125282288
  - 0.8772353351116181
  - 0.8087380349636079
  - 0.8052961170673371
  - 0.7900382459163666
  validation_losses:
  - 0.4429287910461426
  - 0.44667279720306396
  - 0.4397328794002533
  - 0.44626808166503906
  - 0.4477010667324066
  - 0.4375232756137848
  - 0.45467814803123474
  - 0.44416719675064087
  - 0.40511125326156616
  - 0.43056848645210266
  - 0.4595068395137787
  - 0.4455469846725464
  - 0.4481911361217499
  - 0.4539037346839905
  - 0.44044002890586853
  - 0.4331955015659332
  - 0.4359651207923889
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 83 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:11:27.184585'
