config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:39:12.954220'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_98fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_98fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_98fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_98fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_98fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_98fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_98fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_98fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_98fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_98fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.7252881407737732
  - 1.5701312482357026
  - 1.5626442432403564
  - 1.5460774600505829
  - 1.6557614862918855
  - 1.5518433928489686
  - 1.5347760379314423
  - 1.4781617790460588
  - 1.490032982826233
  - 1.4567955255508425
  - 1.4804695755243302
  - 1.4656619846820833
  - 1.4505594909191133
  - 1.5274549782276154
  - 1.510295104980469
  - 1.4733120024204256
  - 1.4564608812332154
  validation_losses:
  - 0.4120156466960907
  - 0.47424840927124023
  - 0.44138452410697937
  - 0.4729825258255005
  - 0.43110570311546326
  - 0.4312225878238678
  - 0.7258388996124268
  - 0.389544814825058
  - 0.4219740629196167
  - 0.3889157772064209
  - 0.4119338095188141
  - 0.39224740862846375
  - 0.3872848451137543
  - 0.3881343901157379
  - 0.38182950019836426
  - 0.38797423243522644
  - 0.3923090398311615
loss_records_fold1:
  train_losses:
  - 1.5285727083683014
  - 1.490120017528534
  - 1.5161403954029085
  - 1.448790431022644
  - 1.4702995598316193
  - 1.4559856414794923
  - 1.4822539806365969
  - 1.4324358046054841
  - 1.4283874452114107
  - 1.4415140628814698
  - 1.4542426466941833
  - 1.4663307964801788
  - 1.4354384481906892
  - 1.4985887467861176
  - 1.433518061041832
  - 1.4480831265449525
  - 1.4946191787719727
  - 1.5406754672527314
  - 1.5569799840450287
  - 1.4727919340133668
  - 1.4709324896335603
  - 1.43424451649189
  - 1.4723507523536683
  - 1.417554968595505
  - 1.4205757141113282
  - 1.4767264425754547
  - 1.45098375082016
  - 1.4331877291202546
  - 1.403714793920517
  - 1.4312647461891175
  - 1.4422838687896729
  - 1.493045198917389
  - 1.4948113918304444
  - 1.4420854508876801
  - 1.461083471775055
  - 1.4341417074203493
  - 1.4128086626529694
  - 1.4431562542915346
  - 1.5481966078281404
  - 1.4536071956157686
  - 1.4210440993309021
  - 1.4234602928161622
  - 1.4121473491191865
  - 1.405245327949524
  - 1.4128754794597627
  - 1.3960965096950533
  - 1.3981198489665987
  - 1.394457095861435
  - 1.4203333735466004
  - 1.3836646854877472
  - 1.3905736565589906
  - 1.395119547843933
  - 1.4583273053169252
  - 1.4084091544151307
  - 1.392820265889168
  - 1.4235939502716066
  - 1.3674774497747422
  - 1.3852989226579666
  - 1.4324586331844331
  - 1.3863355219364166
  - 1.4188614547252656
  - 1.4096371173858644
  - 1.497189086675644
  - 1.4085157036781313
  - 1.423890691995621
  - 1.3821165680885317
  - 1.4022476255893708
  - 1.3513749331235887
  - 1.4551257431507112
  - 1.4058809816837312
  - 1.4053716480731966
  - 1.419087064266205
  - 1.4109054267406465
  - 1.4210716545581819
  - 1.3952445507049562
  - 1.3860870718955995
  - 1.4043629229068757
  - 1.4355268001556398
  - 1.39305636882782
  - 1.4160840451717378
  - 1.3946005642414094
  - 1.3998308837413789
  - 1.4025875568389894
  - 1.3976453751325608
  - 1.419498759508133
  - 1.43334259390831
  - 1.4041661500930787
  - 1.3605868220329285
  - 1.4230054795742035
  - 1.3737047016620636
  - 1.4098153352737428
  - 1.3855964601039887
  - 1.3936505615711212
  - 1.3966957807540894
  - 1.4010018110275269
  - 1.445398184657097
  - 1.4565517485141755
  - 1.4707991898059847
  - 1.4283697068691255
  - 1.4423561573028565
  validation_losses:
  - 0.4109219014644623
  - 0.3891253173351288
  - 0.38734152913093567
  - 0.39396730065345764
  - 0.40595534443855286
  - 0.3867129385471344
  - 0.3905923366546631
  - 0.39561885595321655
  - 0.40265363454818726
  - 0.3868732154369354
  - 0.4016721546649933
  - 0.3965871036052704
  - 0.5343565344810486
  - 0.38805466890335083
  - 0.38757142424583435
  - 0.3927019536495209
  - 0.4302109181880951
  - 0.7359597086906433
  - 0.39260217547416687
  - 0.3990935981273651
  - 0.41007769107818604
  - 0.39186999201774597
  - 0.3849148750305176
  - 0.39883872866630554
  - 0.3882143199443817
  - 0.39184728264808655
  - 0.42388948798179626
  - 0.573143482208252
  - 0.7254599928855896
  - 0.47046083211898804
  - 0.3860991299152374
  - 0.3956073224544525
  - 0.39634400606155396
  - 0.3844873309135437
  - 0.4081305265426636
  - 0.4159187972545624
  - 0.4112396836280823
  - 0.4203149676322937
  - 0.4196830689907074
  - 0.4377041459083557
  - 1.0977071523666382
  - 0.6624093055725098
  - 0.4631081819534302
  - 0.6808720231056213
  - 0.49778082966804504
  - 0.6480452418327332
  - 0.5221964716911316
  - 0.5775806307792664
  - 0.7096410989761353
  - 0.8102218508720398
  - 0.5934011936187744
  - 0.38414832949638367
  - 0.4070470631122589
  - 0.4621654450893402
  - 0.39765235781669617
  - 0.43226876854896545
  - 0.779400646686554
  - 0.4716029167175293
  - 0.47182437777519226
  - 1.013038158416748
  - 0.7217218279838562
  - 0.397960901260376
  - 0.38955751061439514
  - 1.1536293029785156
  - 0.8120200634002686
  - 0.7288870215415955
  - 0.6026222109794617
  - 0.6606199741363525
  - 0.5813016295433044
  - 0.6057127118110657
  - 0.678993284702301
  - 1.3449723720550537
  - 0.6184953451156616
  - 0.4991726279258728
  - 0.7473913431167603
  - 0.41553977131843567
  - 0.3936944603919983
  - 0.5521912574768066
  - 0.519614577293396
  - 0.5086655616760254
  - 0.5883921384811401
  - 0.466219037771225
  - 0.6536507606506348
  - 0.6637924909591675
  - 0.8737260103225708
  - 0.48000863194465637
  - 0.49456191062927246
  - 0.5554877519607544
  - 0.7902831435203552
  - 0.6229455471038818
  - 0.8392342329025269
  - 0.4229757785797119
  - 0.4324779808521271
  - 0.6117986440658569
  - 0.5284751653671265
  - 1.3872612714767456
  - 0.5278997421264648
  - 0.390586793422699
  - 0.453379362821579
  - 0.792502760887146
loss_records_fold2:
  train_losses:
  - 1.4915411889553072
  - 1.4603804528713227
  - 1.4382934212684633
  - 1.393033289909363
  - 1.401027649641037
  - 1.47137308716774
  - 1.4171992003917695
  - 1.3945570051670075
  - 1.4179417908191683
  - 1.4285136461257935
  - 1.3869654893875123
  - 1.3831011414527894
  - 1.39113627076149
  - 1.3906031131744385
  - 1.4647894263267518
  - 1.4098557531833649
  - 1.422235894203186
  - 1.4187491655349733
  - 1.3548193722963333
  - 1.3745120465755463
  - 1.4244883179664614
  - 1.4638735324144365
  - 1.4026768267154694
  - 1.3547588795423509
  - 1.3968435049057009
  - 1.3838351905345918
  - 1.3789442718029024
  - 1.4096447110176087
  - 1.4137754559516909
  - 1.3885854601860048
  - 1.3860551118850708
  - 1.3854054331779482
  - 1.3694487035274507
  - 1.3970422565937044
  - 1.430612874031067
  - 1.378598612546921
  - 1.3880235671997072
  - 1.3885430693626404
  - 1.380103874206543
  - 1.4918265998363496
  - 1.4604650020599366
  - 1.473354870080948
  - 1.4153408348560335
  - 1.3911158204078675
  - 1.3676233708858492
  - 1.3673229038715364
  - 1.3258586689829828
  - 1.3859212160110475
  - 1.386867481470108
  - 1.3712027847766877
  - 1.3596057415008547
  - 1.3863867223262787
  - 1.3633781671524048
  - 1.3658379435539247
  - 1.351450201869011
  - 1.3361058592796327
  - 1.4011055767536165
  - 1.392960685491562
  - 1.3574467480182648
  - 1.3375248968601228
  - 1.3534085154533386
  - 1.3543391048908235
  - 1.3332841098308563
  - 1.3888823807239534
  - 1.4304036855697633
  - 1.4037994116544725
  - 1.4341426193714142
  - 1.3600411325693131
  - 1.3616686016321182
  - 1.3976902782917024
  - 1.3873549222946169
  - 1.4504415094852448
  - 1.3896191686391832
  - 1.380988782644272
  - 1.3883657276630403
  - 1.3990641236305237
  - 1.3921141088008882
  - 1.3724279165267945
  - 1.3563450425863266
  - 1.3480487525463105
  - 1.3619152694940568
  - 1.3873043715953828
  - 1.3979929238557816
  - 1.3318817615509033
  - 1.3527048945426943
  - 1.3694907009601593
  - 1.3772201478481294
  - 1.373461753129959
  - 1.362670397758484
  - 1.323404023051262
  - 1.3601544916629793
  - 1.486030411720276
  - 1.4509038925170898
  - 1.340919578075409
  - 1.3660624146461489
  - 1.352913534641266
  - 1.3783489525318147
  - 1.3529060930013657
  - 1.3807115614414216
  - 1.3669915020465853
  validation_losses:
  - 0.5376343131065369
  - 0.401090145111084
  - 0.41681385040283203
  - 0.41436782479286194
  - 0.48903733491897583
  - 0.4740118086338043
  - 0.4216766655445099
  - 0.41000664234161377
  - 0.39146333932876587
  - 0.44947242736816406
  - 0.4842400550842285
  - 0.5462663769721985
  - 0.439935564994812
  - 0.4485359191894531
  - 0.5419155359268188
  - 0.5355388522148132
  - 0.40509819984436035
  - 0.4065721333026886
  - 0.4713379740715027
  - 0.419792115688324
  - 0.424294650554657
  - 0.4540446102619171
  - 0.41865238547325134
  - 0.4049600660800934
  - 0.46304699778556824
  - 0.5149393677711487
  - 0.46419697999954224
  - 0.5294081568717957
  - 0.42908018827438354
  - 0.6881737112998962
  - 0.4776504635810852
  - 0.5797017216682434
  - 0.39203524589538574
  - 0.3845774233341217
  - 0.3956761062145233
  - 0.3971981704235077
  - 0.3843385875225067
  - 0.3933742046356201
  - 0.3955332040786743
  - 0.4284857511520386
  - 0.4629654884338379
  - 0.45048844814300537
  - 0.43451711535453796
  - 0.47880375385284424
  - 0.49346715211868286
  - 0.47312790155410767
  - 0.49619635939598083
  - 0.44366979598999023
  - 0.514517068862915
  - 0.5517673492431641
  - 0.5329124927520752
  - 0.398304283618927
  - 0.5165833830833435
  - 0.545409619808197
  - 0.5579593181610107
  - 0.4942711889743805
  - 0.7387093305587769
  - 1.2619038820266724
  - 0.8310552835464478
  - 0.9132828116416931
  - 1.2690414190292358
  - 1.1773453950881958
  - 1.0240410566329956
  - 1.3166981935501099
  - 0.4007454812526703
  - 0.4259447753429413
  - 0.39881449937820435
  - 0.4252736270427704
  - 0.46032047271728516
  - 0.45918768644332886
  - 0.43497371673583984
  - 0.4836984872817993
  - 0.4119328260421753
  - 0.41146188974380493
  - 0.4551912546157837
  - 0.4469746947288513
  - 0.41632601618766785
  - 0.3986377716064453
  - 0.4159364700317383
  - 0.4187048077583313
  - 0.42213329672813416
  - 0.4246348440647125
  - 0.44204530119895935
  - 0.49620023369789124
  - 0.5036315321922302
  - 0.5227038264274597
  - 0.531572163105011
  - 0.4229045510292053
  - 0.4512771666049957
  - 0.5122370719909668
  - 0.5262190699577332
  - 0.468294233083725
  - 0.46458181738853455
  - 0.45578500628471375
  - 0.44782543182373047
  - 0.4852348864078522
  - 0.5579285621643066
  - 0.537536084651947
  - 0.5249621272087097
  - 0.7201271653175354
loss_records_fold3:
  train_losses:
  - 1.375231158733368
  - 1.4282030165195465
  - 1.3753428906202316
  - 1.4135237216949463
  - 1.3642775207757951
  - 1.3711025059223176
  - 1.4085748225450516
  - 1.4243366956710817
  - 1.4026567399501801
  - 1.383452731370926
  - 1.390034693479538
  - 1.3636686623096468
  - 1.378285312652588
  - 1.4103650450706482
  - 1.3841370940208435
  - 1.432817417383194
  - 1.3746526002883912
  - 1.389403223991394
  - 1.4105388045310976
  - 1.3648825824260713
  - 1.420929139852524
  - 1.3673999786376954
  - 1.3886831343173982
  - 1.3459594756364823
  - 1.4076270163059235
  - 1.4104116559028625
  - 1.3841822981834413
  - 1.4186256259679795
  - 1.416517895460129
  - 1.3516396939754487
  - 1.3481559604406357
  - 1.3899480104446411
  - 1.366762137413025
  - 1.3999478042125704
  - 1.3734710097312928
  - 1.3777025461196901
  - 1.376717782020569
  - 1.3561313450336456
  - 1.41025367975235
  - 1.3627851009368896
  - 1.3675213932991028
  - 1.3631822764873505
  - 1.3724043101072312
  - 1.385878324508667
  - 1.3893941342830658
  - 1.3781238436698915
  - 1.3537162303924561
  - 1.4305403470993043
  - 1.3804490119218826
  - 1.3824916720390321
  - 1.3503641486167908
  - 1.3461193561553957
  - 1.3690899252891542
  - 1.3892215311527254
  - 1.4098813831806183
  - 1.3505866646766664
  - 1.343973767757416
  - 1.393205165863037
  - 1.384482914209366
  - 1.3677735209465027
  - 1.3919826686382295
  - 1.4349552392959595
  - 1.3906422138214112
  - 1.3887082159519197
  - 1.361522549390793
  - 1.4252185463905336
  - 1.3708804905414582
  - 1.4025460600852968
  - 1.3580065369606018
  - 1.3678160607814789
  - 1.4393181204795837
  - 1.4185835242271425
  - 1.3979007959365846
  - 1.4084920346736909
  - 1.3918826043605805
  - 1.3473681509494781
  - 1.3440715789794924
  - 1.3672747731208803
  - 1.3287598669528962
  - 1.353322619199753
  - 1.3504605680704118
  - 1.3312107622623444
  - 1.3236678630113603
  - 1.3531890928745272
  - 1.3418604612350464
  - 1.3518495082855226
  - 1.3838528573513031
  - 1.4073602855205536
  - 1.3451215505599976
  - 1.3574429690837861
  - 1.3336618751287461
  - 1.3488023400306703
  - 1.328289294242859
  - 1.3581673204898834
  - 1.3281633347272874
  - 1.3430322885513306
  - 1.3670157074928284
  - 1.376286455988884
  - 1.3344496905803682
  - 1.3353460252285005
  validation_losses:
  - 0.8300606608390808
  - 0.4429415762424469
  - 0.5257574319839478
  - 0.67127525806427
  - 0.4682278633117676
  - 0.5367977023124695
  - 0.4719792306423187
  - 0.4395985007286072
  - 0.4424813389778137
  - 0.4530031383037567
  - 0.5874397158622742
  - 0.5882875919342041
  - 0.4472353756427765
  - 0.7067196369171143
  - 1.2276562452316284
  - 0.6001378893852234
  - 0.43179935216903687
  - 0.5125595331192017
  - 1.40181303024292
  - 0.7375466227531433
  - 0.491153359413147
  - 0.3880067467689514
  - 0.5065641403198242
  - 0.44907867908477783
  - 0.6652176976203918
  - 0.3723459839820862
  - 0.37983664870262146
  - 0.5003618001937866
  - 0.5785604119300842
  - 0.49696648120880127
  - 0.5344981551170349
  - 0.8530815243721008
  - 0.5912473797798157
  - 0.6831353306770325
  - 0.7420444488525391
  - 0.47497618198394775
  - 0.38020431995391846
  - 0.4439263939857483
  - 0.5928290486335754
  - 0.48054319620132446
  - 0.6643612384796143
  - 1.091256022453308
  - 5.503163814544678
  - 1.2874428033828735
  - 1.0261884927749634
  - 0.6610768437385559
  - 0.3746609389781952
  - 2.354553461074829
  - 3.597982168197632
  - 2.5522541999816895
  - 5.340493202209473
  - 3.77274751663208
  - 4.6392598152160645
  - 11.781919479370117
  - 0.7456384897232056
  - 0.6195468306541443
  - 0.43573692440986633
  - 1.711971640586853
  - 1.170412540435791
  - 1.0726720094680786
  - 1.3512725830078125
  - 0.4331151843070984
  - 0.553747832775116
  - 0.8383758068084717
  - 0.39287179708480835
  - 0.4665013551712036
  - 0.534697949886322
  - 0.5094402432441711
  - 0.48301249742507935
  - 0.6053120493888855
  - 0.4026838541030884
  - 0.6087562441825867
  - 0.5891159772872925
  - 0.6433369517326355
  - 0.6126543879508972
  - 0.7009686827659607
  - 1.0261108875274658
  - 0.6701206564903259
  - 0.39207565784454346
  - 0.6680520176887512
  - 0.7658143043518066
  - 0.7308086156845093
  - 0.39063090085983276
  - 0.6524286866188049
  - 0.4419019818305969
  - 0.540131688117981
  - 0.5767494440078735
  - 0.4504716694355011
  - 0.6696246862411499
  - 0.5168697237968445
  - 0.6728050708770752
  - 0.797039806842804
  - 0.6398537755012512
  - 0.4259636402130127
  - 0.6705185770988464
  - 0.45167309045791626
  - 0.7801777124404907
  - 0.5359194874763489
  - 0.6400317549705505
  - 0.5595649480819702
loss_records_fold4:
  train_losses:
  - 1.3878769338130952
  - 1.3680956423282624
  - 1.377281665802002
  - 1.3591115653514863
  - 1.3472513109445572
  - 1.3660625725984574
  - 1.4023428678512575
  - 1.4000744163990022
  - 1.4001955389976501
  - 1.4005040526390076
  - 1.354084450006485
  - 1.3654667079448701
  - 1.3711852371692659
  - 1.4587930977344514
  - 1.3897472441196443
  - 1.413067811727524
  - 1.366234451532364
  - 1.3606763303279878
  - 1.367809569835663
  - 1.3525617599487305
  - 1.4455181896686555
  - 1.329171234369278
  - 1.374333268404007
  - 1.352627569437027
  - 1.4441598534584046
  - 1.3382714569568634
  - 1.336666536331177
  - 1.420563778281212
  - 1.3702773302793503
  - 1.3507216691970827
  - 1.3123052597045899
  - 1.374627596139908
  - 1.3386071801185608
  - 1.334435683488846
  - 1.3512820839881898
  - 1.4179331421852113
  - 1.339447721838951
  - 1.4061566710472109
  - 1.3792072415351868
  - 1.3183855503797532
  - 1.3519404113292695
  - 1.3144385397434235
  - 1.3400651156902315
  - 1.3511355519294739
  - 1.4053062975406647
  - 1.3245661914348603
  - 1.3348949372768404
  - 1.3622810393571854
  - 1.3707972824573518
  - 1.3533645093441011
  - 1.3322758078575134
  - 1.369520789384842
  - 1.336606788635254
  - 1.3867051482200623
  - 1.374571031332016
  - 1.325063741207123
  - 1.3456368505954743
  - 1.3890755832195283
  - 1.3886390864849092
  - 1.3561941236257553
  - 1.359063082933426
  - 1.3302846372127535
  - 1.3529424607753755
  - 1.3332391351461412
  - 1.3028016328811647
  - 1.3395408332347871
  - 1.3137741446495057
  - 1.3494825422763825
  - 1.3683815509080888
  - 1.3793058156967164
  - 1.3466710567474367
  - 1.3185981452465059
  - 1.345326054096222
  - 1.4027058959007264
  - 1.3337148129940033
  - 1.3317488372325899
  - 1.320927318930626
  - 1.3730039834976198
  - 1.343224710226059
  - 1.3300681829452516
  - 1.3218745172023774
  - 1.3427503108978271
  - 1.315919703245163
  - 1.3722698330879213
  - 1.3163113355636598
  - 1.3255361378192902
  - 1.3217372477054596
  - 1.3280555963516236
  - 1.3328342854976656
  - 1.318749189376831
  - 1.345013725757599
  - 1.3612436711788178
  - 1.361169841885567
  - 1.2974641978740693
  - 1.3278401136398317
  - 1.34070662856102
  - 1.3932696163654328
  - 1.3264507710933686
  - 1.2875885576009751
  - 1.3013411164283752
  validation_losses:
  - 0.41139012575149536
  - 0.5263765454292297
  - 0.38278889656066895
  - 0.3706478774547577
  - 0.36077651381492615
  - 0.5441203713417053
  - 0.4190974235534668
  - 0.3709894120693207
  - 0.3641081154346466
  - 0.3502419888973236
  - 0.35284507274627686
  - 0.3791492283344269
  - 0.3627762794494629
  - 0.49628746509552
  - 0.35694220662117004
  - 0.39092665910720825
  - 0.35836124420166016
  - 0.4348689019680023
  - 0.40726757049560547
  - 0.370125949382782
  - 0.4546017050743103
  - 0.3813835382461548
  - 0.3847433030605316
  - 0.38005223870277405
  - 0.5607061982154846
  - 0.5080543160438538
  - 0.538851797580719
  - 0.41788995265960693
  - 0.41566792130470276
  - 0.47779127955436707
  - 0.41473981738090515
  - 0.4184557795524597
  - 0.44328755140304565
  - 0.3959125578403473
  - 0.3728320896625519
  - 0.5838006734848022
  - 0.46758678555488586
  - 0.4080241322517395
  - 0.4503648579120636
  - 0.35475000739097595
  - 0.40429866313934326
  - 0.37417471408843994
  - 0.37360548973083496
  - 0.3814811706542969
  - 0.4211662709712982
  - 0.4903496503829956
  - 0.3780253827571869
  - 0.4128383994102478
  - 0.4647120237350464
  - 0.46016329526901245
  - 0.43041327595710754
  - 0.7589264512062073
  - 0.40542522072792053
  - 0.4950449466705322
  - 0.4427486062049866
  - 0.4555734395980835
  - 0.3941017687320709
  - 0.4760010242462158
  - 0.43038588762283325
  - 0.6269320845603943
  - 0.43679866194725037
  - 0.3931242525577545
  - 0.39202526211738586
  - 0.4269177317619324
  - 0.40764376521110535
  - 0.3908754289150238
  - 0.3682900071144104
  - 0.54790198802948
  - 0.39688840508461
  - 0.3763802647590637
  - 0.3811165690422058
  - 0.3559949994087219
  - 0.5390664339065552
  - 0.36719951033592224
  - 0.4519142210483551
  - 0.6226223707199097
  - 0.38742437958717346
  - 0.37996602058410645
  - 0.4631221294403076
  - 0.6001161336898804
  - 0.46530184149742126
  - 0.3574391305446625
  - 0.3794079124927521
  - 0.6033905148506165
  - 0.4719981849193573
  - 0.42304128408432007
  - 0.5919825434684753
  - 0.5743342041969299
  - 0.5050458908081055
  - 0.43601903319358826
  - 0.4477877914905548
  - 0.529314398765564
  - 0.46137094497680664
  - 0.3606342673301697
  - 0.4585839807987213
  - 0.492108017206192
  - 0.3843560516834259
  - 0.3555692732334137
  - 0.40908798575401306
  - 0.44295427203178406
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8198970840480274, 0.8404802744425386, 0.8421955403087479,
    0.8213058419243986]'
  fold_eval_f1: '[0.0, 0.1322314049586777, 0.24390243902439024, 0.2580645161290323,
    0.2876712328767123]'
  mean_eval_accuracy: 0.8363023347656687
  mean_f1_accuracy: 0.1843739185977625
  total_train_time: '0:37:13.871956'
