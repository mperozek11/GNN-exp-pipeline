config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:44:38.794407'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_28fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_28fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_28fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_28fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_28fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_28fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_28fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_28fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_28fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_28fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 79.63343863710762
  - 20.791052162647247
  - 21.831922188401222
  - 24.474929517507555
  - 19.839454498887065
  - 17.66962142288685
  - 29.201120752096177
  - 20.24294349849224
  - 13.25102974176407
  - 11.393350592255594
  - 18.83104421645403
  - 12.842356485128404
  - 9.21065499484539
  - 9.299555668234825
  - 9.79595037996769
  - 9.962497329711915
  - 10.536048498749734
  - 13.694513151049614
  - 13.759608626365662
  - 9.96501042842865
  - 7.4469940066337585
  - 8.674622651934625
  - 8.781675013899804
  - 8.396818426251413
  - 8.25482884645462
  - 8.099941647052765
  - 7.511777085065842
  - 7.40038166642189
  - 7.230104061961175
  - 7.44442250430584
  - 6.3619734197855
  - 7.2285821646451955
  - 7.143880644440651
  - 6.569722416996957
  - 6.456841129064561
  - 6.686300194263459
  - 7.2799439281225204
  - 6.848335692286492
  - 6.700198298692704
  - 6.721146950125695
  - 6.696964365243912
  - 6.660571932792664
  - 6.608153915405274
  - 6.742137998342514
  - 6.7640249848365785
  - 6.511317601799965
  - 6.625865563750267
  - 7.031958496570588
  - 7.08918902873993
  - 6.445142877101898
  validation_losses:
  - 2.8493196964263916
  - 0.6063529849052429
  - 0.4220671057701111
  - 0.5882302522659302
  - 0.5305954813957214
  - 0.40781357884407043
  - 0.9114693403244019
  - 0.5734657049179077
  - 0.41530075669288635
  - 0.435733824968338
  - 0.41618549823760986
  - 0.4385083019733429
  - 0.41025787591934204
  - 0.5853231549263
  - 0.47130149602890015
  - 0.3922275900840759
  - 0.5438138842582703
  - 0.4284135103225708
  - 0.7930387258529663
  - 0.4316566586494446
  - 0.4196358919143677
  - 0.5276475548744202
  - 0.4132731258869171
  - 0.3977593779563904
  - 0.4021093547344208
  - 0.40436553955078125
  - 0.38698810338974
  - 0.43909505009651184
  - 0.3917251229286194
  - 0.4004848599433899
  - 0.3915194869041443
  - 0.42373740673065186
  - 0.4013138711452484
  - 0.40860456228256226
  - 0.403369665145874
  - 0.4234349727630615
  - 0.4361386001110077
  - 0.43950265645980835
  - 0.46152615547180176
  - 0.40300115942955017
  - 0.40126633644104004
  - 0.4474019408226013
  - 0.4149358570575714
  - 1.3589451313018799
  - 0.47234857082366943
  - 0.432308167219162
  - 0.4169749319553375
  - 0.40582790970802307
  - 0.412356972694397
  - 0.39910027384757996
loss_records_fold1:
  train_losses:
  - 6.736717864871025
  - 6.554418435692788
  - 7.315034428238869
  - 6.84199080169201
  - 6.557243543863297
  - 6.602938851714135
  - 6.509610876441002
  - 6.6387228041887285
  - 6.676604279875756
  - 6.92955428659916
  - 6.810361456871033
  - 6.318753504753113
  - 6.4295072883367546
  validation_losses:
  - 0.42449477314949036
  - 0.4052779972553253
  - 0.4093707203865051
  - 0.4641468822956085
  - 0.41792312264442444
  - 0.4468495845794678
  - 0.467144250869751
  - 0.4589329957962036
  - 0.44239717721939087
  - 0.4452316462993622
  - 0.4511057734489441
  - 0.4448069632053375
  - 0.4393189251422882
loss_records_fold2:
  train_losses:
  - 6.602224454283714
  - 6.409863778948784
  - 6.597093629837037
  - 6.341941878199577
  - 6.45659511089325
  - 6.54058837890625
  - 6.8865614354610445
  - 6.635438412427902
  - 6.57254838347435
  - 6.6581561237573625
  - 6.982081696391106
  - 12.052226394414902
  - 6.654604387283325
  - 6.340100258588791
  - 6.518176433444023
  - 7.161582991480827
  - 6.924618023633958
  - 7.284597957134247
  - 6.762839818000794
  - 6.557391414046288
  - 6.520767515897751
  - 6.621905997395515
  - 6.71993283033371
  - 6.431596526503563
  - 6.887277001142502
  - 6.294594925642014
  - 6.407797428965569
  - 6.642422008514405
  - 6.261763182282448
  - 6.567892718315125
  - 6.619321522116661
  - 6.693872168660164
  - 6.424055421352387
  - 6.5323216915130615
  - 6.440710854530335
  - 6.603683224320412
  - 6.734050473570824
  - 6.399654695391655
  - 6.594552356004716
  - 6.6523903727531435
  - 6.471445086598397
  - 6.486891254782677
  - 6.458371630311013
  - 6.651879912614823
  - 6.3735591053962715
  - 6.616406533122063
  - 6.774300700426102
  - 6.460568237304688
  - 6.34579436480999
  - 6.41833180487156
  - 6.54923654794693
  - 6.295977675914765
  - 6.454164722561837
  - 6.198811545968056
  - 6.745252424478531
  - 6.592579972743988
  - 6.864200174808502
  - 6.590048307180405
  - 6.233186665177346
  - 6.543015506863594
  - 6.562698638439179
  - 6.509133008122444
  validation_losses:
  - 0.4216255247592926
  - 0.432891845703125
  - 0.40390801429748535
  - 0.4008377194404602
  - 3.676553249359131
  - 0.4626563489437103
  - 0.4018137753009796
  - 0.4994620084762573
  - 0.438932865858078
  - 2.244807004928589
  - 0.47230544686317444
  - 0.4264848232269287
  - 0.40202340483665466
  - 0.4037714898586273
  - 0.44148877263069153
  - 0.4221316874027252
  - 0.4418475329875946
  - 0.4219307005405426
  - 0.47211363911628723
  - 3.4951491355895996
  - 0.4306546747684479
  - 0.4081052243709564
  - 0.40433910489082336
  - 0.4156932830810547
  - 0.39986395835876465
  - 0.3891541659832001
  - 0.3994290828704834
  - 0.43262943625450134
  - 0.4066231846809387
  - 0.39409884810447693
  - 0.4136756360530853
  - 0.3993116617202759
  - 0.3957124650478363
  - 0.4499056339263916
  - 0.39942091703414917
  - 0.39736008644104004
  - 0.3984307050704956
  - 0.3953387439250946
  - 0.452724814414978
  - 0.427267849445343
  - 0.4540556073188782
  - 0.4219178259372711
  - 0.4039067029953003
  - 0.3990941345691681
  - 0.40225446224212646
  - 0.43181249499320984
  - 0.4091869294643402
  - 0.39648741483688354
  - 0.43129265308380127
  - 0.42348363995552063
  - 0.3949575126171112
  - 0.3992357850074768
  - 0.3946663439273834
  - 0.44398072361946106
  - 0.4088398218154907
  - 0.4234132468700409
  - 0.4176838994026184
  - 0.4041289687156677
  - 0.40947043895721436
  - 0.405261754989624
  - 0.4049272835254669
  - 0.3949546217918396
loss_records_fold3:
  train_losses:
  - 6.537553471326828
  - 6.544882154464722
  - 6.473313489556313
  - 6.465605899691582
  - 6.497358837723732
  - 6.366515499353409
  - 6.635474365949631
  - 6.619712740182877
  - 6.36366215646267
  - 6.6508950948715215
  - 6.484842094779015
  - 6.56133628487587
  - 6.491965454816818
  - 6.345001006126404
  - 6.632598498463631
  - 6.4461319804191595
  - 6.44786525964737
  - 6.670838844776154
  - 6.396601462364197
  - 6.648911732435227
  - 6.599942815303803
  validation_losses:
  - 0.464824914932251
  - 0.45910656452178955
  - 0.4012320041656494
  - 0.4203433394432068
  - 0.409183144569397
  - 0.41226351261138916
  - 0.4128495454788208
  - 24096.875
  - 0.4631577134132385
  - 0.4326082170009613
  - 0.41614991426467896
  - 0.4064483344554901
  - 0.42512592673301697
  - 0.4245169758796692
  - 0.46819236874580383
  - 0.4289707541465759
  - 0.41645774245262146
  - 0.419824481010437
  - 0.42486631870269775
  - 0.422665536403656
  - 0.4109427034854889
loss_records_fold4:
  train_losses:
  - 6.436735561490059
  - 6.401606595516205
  - 6.408276632428169
  - 6.272811150550843
  - 6.348922371864319
  - 6.374891173839569
  - 6.379381263256073
  - 6.569194808602333
  - 6.459731009602547
  - 6.381016819179059
  - 7.160733783245087
  - 6.574124750494957
  - 6.7321753919124605
  - 6.583167311549187
  - 6.53971266746521
  - 6.457478466629983
  - 6.5821394354105
  - 7.163451415300369
  - 6.773736879229546
  - 6.311938628554344
  - 6.601260557770729
  - 6.392442518472672
  - 6.422819048166275
  - 6.62544002532959
  - 6.547819995880127
  - 6.489541631937027
  - 6.433743017911912
  - 6.305711564421654
  - 6.57255038022995
  - 6.454122424125671
  - 6.510401213169098
  - 6.523951426148415
  - 6.275970208644868
  - 6.582291933894158
  - 6.579016372561455
  - 6.260065466165543
  - 6.53584061563015
  - 6.641528913378716
  - 6.514619788527489
  - 6.4694593250751495
  - 6.465915700793267
  - 6.4240151137113575
  - 6.453266188502312
  - 6.370922565460205
  - 6.4924756526947025
  - 6.384019878506661
  - 6.318312239646912
  - 6.488028204441071
  - 6.325389122962952
  - 6.426594778895378
  - 6.437797713279725
  - 6.635648813843727
  - 6.567321598529816
  - 6.401624697446824
  - 6.294450488686562
  - 6.422306105494499
  - 6.878752940893174
  - 6.4239708393812185
  - 6.346510300040245
  - 6.663418993353844
  - 6.656449636816979
  - 6.426258489489555
  - 6.376626759767532
  - 6.773963761329651
  - 6.461596038937569
  - 6.604510161280633
  - 6.5549143910408025
  - 6.7058152049779896
  - 6.504856899380684
  - 6.690508818626404
  - 6.387341171503067
  - 6.477669769525528
  - 6.627354094386101
  - 6.5062266319990165
  - 6.972781908512116
  - 6.808312180638314
  - 6.579705108702183
  - 6.518652725219727
  - 6.427974051237107
  - 6.409292674064637
  - 6.340712830424309
  - 6.635116711258888
  - 6.827384388446808
  - 6.542436835169792
  - 6.568766579031944
  - 10.057617831230164
  - 9.772072777152061
  - 6.545595753192902
  - 6.574992257356644
  - 6.410077714920044
  - 6.626716911792755
  - 6.538702729344369
  - 6.357449424266815
  - 6.454973018169404
  - 6.591219663619995
  - 7.368151938915253
  - 6.474534952640534
  - 6.573229786753655
  - 6.291137367486954
  - 6.373248684406281
  validation_losses:
  - 0.41649600863456726
  - 0.420519083738327
  - 0.41626599431037903
  - 0.4269335865974426
  - 0.40103840827941895
  - 0.4071160852909088
  - 0.4122883677482605
  - 0.4079168438911438
  - 0.412743479013443
  - 0.4327600598335266
  - 0.48031526803970337
  - 0.42364639043807983
  - 0.416554719209671
  - 0.4500945806503296
  - 0.4258516728878021
  - 0.4189372956752777
  - 0.41920486092567444
  - 0.41779792308807373
  - 0.42901211977005005
  - 0.4148390293121338
  - 0.42027580738067627
  - 0.39670634269714355
  - 0.4186854362487793
  - 0.4047544002532959
  - 0.431458979845047
  - 0.4107058048248291
  - 0.41209036111831665
  - 0.4362312853336334
  - 0.4472843110561371
  - 0.41817015409469604
  - 0.4533660113811493
  - 0.41430550813674927
  - 0.4000428318977356
  - 0.4151862561702728
  - 0.40734753012657166
  - 0.425129234790802
  - 0.40283268690109253
  - 0.4264328181743622
  - 0.4262841045856476
  - 0.4005480408668518
  - 0.5022004246711731
  - 0.41142958402633667
  - 0.4081208109855652
  - 0.416779488325119
  - 0.40915369987487793
  - 0.4439571499824524
  - 0.41315698623657227
  - 0.4562210738658905
  - 0.4077729880809784
  - 0.42141783237457275
  - 0.4090309739112854
  - 0.4309927821159363
  - 0.4084840416908264
  - 0.3955945074558258
  - 0.42394590377807617
  - 0.40812698006629944
  - 0.4133377969264984
  - 0.41109105944633484
  - 0.4118480384349823
  - 0.4360826313495636
  - 0.42883238196372986
  - 0.42085784673690796
  - 0.5503242015838623
  - 0.4525220990180969
  - 0.43017423152923584
  - 0.41481369733810425
  - 0.39592215418815613
  - 0.46144330501556396
  - 0.4369525909423828
  - 0.4193710386753082
  - 0.41162481904029846
  - 0.44117751717567444
  - 0.404186874628067
  - 0.3984690308570862
  - 0.42164865136146545
  - 0.44402310252189636
  - 0.41284748911857605
  - 0.4065631926059723
  - 0.45008385181427
  - 0.4069952964782715
  - 0.40100574493408203
  - 0.5547539591789246
  - 0.4036223888397217
  - 0.41272279620170593
  - 0.42200011014938354
  - 16.092580795288086
  - 0.4216732978820801
  - 0.4003564417362213
  - 0.4211151897907257
  - 0.4190455377101898
  - 0.4311160743236542
  - 0.4412001967430115
  - 0.40747132897377014
  - 0.40352684259414673
  - 0.4225156605243683
  - 0.41382962465286255
  - 0.42322245240211487
  - 0.41978204250335693
  - 0.4049525260925293
  - 0.4443971812725067
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 50 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 62 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:24:25.465435'
