config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:18:24.124072'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_80fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_80fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_80fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_80fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_80fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_80fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_80fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_80fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_80fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_80fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.126806536316872
  - 6.177792894840241
  - 6.0592000693082815
  - 5.89116816520691
  - 6.066825972497464
  - 6.1465180128812795
  - 5.965070381760597
  - 5.906930783390999
  - 6.029918310046196
  - 5.8129714936017995
  - 5.697063964605332
  - 5.886823147535324
  - 5.685287243127823
  - 5.745236024260521
  - 5.708204501867295
  - 5.696064627170563
  validation_losses:
  - 0.3956405520439148
  - 0.4392855167388916
  - 0.3978519141674042
  - 0.39592641592025757
  - 0.3955790400505066
  - 0.38980528712272644
  - 0.4094793200492859
  - 0.40241625905036926
  - 0.39859524369239807
  - 0.4939723014831543
  - 0.4738140404224396
  - 0.4446408152580261
  - 0.39437127113342285
  - 0.3990577757358551
  - 0.4044362008571625
  - 0.3852042853832245
loss_records_fold1:
  train_losses:
  - 5.636262583732606
  - 5.646226915717126
  - 5.739446175098419
  - 5.675736913084984
  - 5.656855490803719
  - 5.5637650936841965
  - 5.581281125545502
  - 5.6263893187046055
  - 5.639050963521004
  - 5.541877859830857
  - 5.827862250804902
  - 5.641249677538872
  - 5.62768039405346
  - 5.645880109071732
  - 5.645506244897843
  - 5.568244776129723
  - 5.573500725626946
  - 5.6150067999959
  - 5.639305856823921
  - 5.704380974173546
  - 5.651233100891114
  - 5.618095886707306
  validation_losses:
  - 0.38961461186408997
  - 0.42489296197891235
  - 0.40342655777931213
  - 0.5545408129692078
  - 0.5365726351737976
  - 0.5339305400848389
  - 0.5633031129837036
  - 0.6103335022926331
  - 0.4193677604198456
  - 0.5536529421806335
  - 0.39455658197402954
  - 0.4039824306964874
  - 0.3998458683490753
  - 0.4088931977748871
  - 0.5331711173057556
  - 0.6680365800857544
  - 0.4208197593688965
  - 0.39286524057388306
  - 0.388216495513916
  - 0.39054545760154724
  - 0.3925798535346985
  - 0.3943241238594055
loss_records_fold2:
  train_losses:
  - 5.696906945109368
  - 5.630214920639992
  - 5.704310736060143
  - 5.653020256757737
  - 5.619127124547958
  - 5.638947445154191
  - 5.594321987032891
  - 5.591714423894882
  - 5.582809802889824
  - 5.598770815134049
  - 5.681292581558228
  - 5.669551140069962
  - 5.6113061815500265
  - 5.568908205628396
  - 5.553528070449829
  - 5.645685395598412
  - 5.561670202016831
  - 5.629280051589013
  - 5.603998655080796
  - 5.579534006118775
  - 5.551351311802865
  - 5.522058352828026
  - 5.536442306637764
  - 5.572254334390164
  - 5.570719936490059
  - 5.513133472204209
  - 5.543229922652245
  - 5.51101205945015
  - 5.581734445691109
  - 5.542100644111634
  - 5.534933340549469
  - 5.552902179956437
  - 5.59977442920208
  - 5.56010234951973
  - 5.566056630015374
  - 5.5580493986606605
  - 5.516225907206536
  - 5.559404355287552
  - 5.536164891719818
  - 5.65191345512867
  - 5.517809975147248
  - 5.51783431172371
  - 5.4981520652771
  - 5.5155637383461
  - 5.520935952663422
  - 5.526485127210617
  - 5.524757191538811
  - 5.6146964788436895
  - 5.4790343046188354
  - 5.509837120771408
  - 5.56385455429554
  - 5.531407260894776
  - 5.437467473745347
  - 5.500740167498589
  - 5.52230669260025
  - 5.652605238556863
  - 5.523916152119637
  - 5.528740265965462
  - 5.518907913565636
  - 5.534538218379021
  - 5.578923016786575
  - 5.594185680150986
  - 5.535380044579506
  - 5.470970961451531
  - 5.476184749603272
  - 5.538764384388924
  - 5.5066010594367985
  - 5.530116096138954
  - 5.575141108036042
  - 5.525112894177437
  - 5.545932602882385
  - 5.558005914092064
  - 5.486952367424966
  - 5.505532598495484
  - 5.461138504743577
  - 5.546778559684753
  - 5.442592024803162
  - 5.461330953240395
  - 5.521351006627083
  - 5.459170991182328
  - 5.430973595380784
  - 5.537590146064758
  - 5.665140619874001
  - 5.515568202733994
  - 5.481470081210137
  - 5.4535640478134155
  - 5.5514904230833055
  - 5.473262293636799
  - 5.40364280641079
  - 5.4626330733299255
  - 5.530809119343758
  - 5.4925213187932975
  - 5.393661466240883
  - 5.535531157255173
  - 5.375651741027832
  - 5.425381234288216
  - 5.348878705501557
  - 5.454504889249802
  - 5.332482361793518
  - 5.452047610282898
  validation_losses:
  - 0.388911634683609
  - 0.39128953218460083
  - 0.3878130614757538
  - 0.39030054211616516
  - 0.4010775089263916
  - 0.38807204365730286
  - 0.3864666521549225
  - 0.3855573534965515
  - 0.38802963495254517
  - 0.5025004744529724
  - 0.38975805044174194
  - 0.39896050095558167
  - 0.3829551935195923
  - 0.4244593381881714
  - 0.4667311906814575
  - 0.4657762050628662
  - 0.4187015891075134
  - 0.5436872839927673
  - 0.4168723523616791
  - 0.4477008283138275
  - 0.4426482319831848
  - 0.5862122774124146
  - 0.49675697088241577
  - 0.4180264174938202
  - 0.6183875203132629
  - 0.6334596276283264
  - 0.6624564528465271
  - 0.6767895221710205
  - 0.5230496525764465
  - 0.5492831468582153
  - 0.7127813696861267
  - 0.5688651204109192
  - 0.6513475775718689
  - 0.5511564612388611
  - 0.39116498827934265
  - 0.57034832239151
  - 0.5816512107849121
  - 0.39890599250793457
  - 0.5946143865585327
  - 0.4652796983718872
  - 0.41008466482162476
  - 0.6725561618804932
  - 0.653290867805481
  - 0.5119898319244385
  - 0.6962718963623047
  - 0.6836461424827576
  - 0.7831255793571472
  - 0.6021620631217957
  - 0.6050925254821777
  - 0.6863812804222107
  - 0.6766361594200134
  - 0.5893044471740723
  - 0.4530201256275177
  - 0.7000839710235596
  - 0.5333667993545532
  - 0.509056568145752
  - 0.49962615966796875
  - 0.6279876828193665
  - 0.7871583700180054
  - 0.47182366251945496
  - 0.40324637293815613
  - 0.558214545249939
  - 0.9397715926170349
  - 0.5838848352432251
  - 0.6668201684951782
  - 0.8604957461357117
  - 0.7696926593780518
  - 1.2538801431655884
  - 0.6691492199897766
  - 0.6904498338699341
  - 0.8010610342025757
  - 1.0316424369812012
  - 0.738078773021698
  - 1.0552000999450684
  - 1.1859639883041382
  - 1.2247432470321655
  - 0.9801365733146667
  - 0.9655489921569824
  - 0.44725337624549866
  - 0.8067159652709961
  - 0.6881030201911926
  - 0.399024099111557
  - 0.3992011845111847
  - 0.3964512050151825
  - 0.7232394814491272
  - 0.4008379876613617
  - 0.4561925232410431
  - 0.42808035016059875
  - 0.5165122747421265
  - 0.417989581823349
  - 0.394640177488327
  - 0.4500868022441864
  - 0.5208459496498108
  - 0.4904063940048218
  - 0.6592579483985901
  - 0.8824175000190735
  - 0.6411285996437073
  - 0.688554048538208
  - 0.9805420637130737
  - 0.6574794054031372
loss_records_fold3:
  train_losses:
  - 5.733411213755608
  - 5.533002614974976
  - 5.513367551565171
  - 5.473886555433274
  - 5.442093884944916
  - 5.501708379387856
  - 5.512965926527977
  - 5.480087170004845
  - 5.451592367887497
  - 5.531255602836609
  - 5.503228104114533
  - 5.540627375245094
  - 5.481000629067421
  - 5.566531755030155
  - 5.479402434825897
  - 5.416233816742897
  - 5.530994671583176
  - 5.445436352491379
  - 5.4092067033052444
  - 5.391083654761315
  - 5.343747146427631
  - 5.566393241286278
  - 5.517597743868828
  - 5.4294595390558245
  - 5.360545584559441
  - 5.382664021849632
  - 5.462133947014809
  - 5.479150646924973
  - 5.43757236301899
  - 5.5384039133787155
  - 5.49876444041729
  - 5.488341400027275
  - 5.4426662802696235
  - 5.3819900333881385
  - 5.451771092414856
  - 5.419444611668587
  - 5.483610963821412
  - 5.433786875009537
  - 5.507228884100915
  - 5.419157120585442
  - 5.423739910125732
  - 5.3437963873147964
  - 5.507937467098237
  - 5.450459137558937
  - 5.418963980674744
  - 5.481858360767365
  - 5.382509455084801
  - 5.383831429481507
  - 5.348251861333847
  - 5.340320399403573
  - 5.455179205536843
  - 5.436221861839295
  - 5.448651915788651
  - 5.344266590476036
  - 5.361249089241028
  - 5.41862460076809
  - 5.4388096868991855
  - 5.386004573106766
  - 5.474125248193741
  - 5.447508549690247
  - 5.400924882292748
  - 5.419238521158696
  - 5.496116510033608
  - 5.321425303816795
  - 5.41105289310217
  - 5.330094686150551
  - 5.5472372531890874
  - 5.4897183835506445
  - 5.469214025139809
  - 5.497601333260537
  - 5.4131975948810584
  - 5.332677587866783
  - 5.4413350045681
  - 5.413842052221298
  - 5.399745988845826
  - 5.4938114315271385
  - 5.376167905330658
  - 5.378797227144242
  - 5.476355823874474
  - 5.45102995634079
  - 5.444669732451439
  - 5.37227085530758
  - 5.443521589040756
  - 5.482840326428414
  - 5.469987010955811
  - 5.430234044790268
  - 5.667213195562363
  - 5.694143238663674
  - 5.943254905939103
  - 5.684119555354119
  - 5.601562023162842
  - 5.592231410741807
  - 5.533191338181496
  - 5.558997994661332
  - 5.633438757061959
  - 5.578811839222908
  - 5.5441748052835464
  - 5.53869953751564
  - 5.535506761074067
  - 5.544926619529725
  validation_losses:
  - 0.403713583946228
  - 0.4711116552352905
  - 0.40065720677375793
  - 0.4960530400276184
  - 0.6176362037658691
  - 0.45594021677970886
  - 0.44805824756622314
  - 0.4047941267490387
  - 0.503470242023468
  - 0.5972918272018433
  - 0.6020561456680298
  - 0.39018744230270386
  - 0.397033154964447
  - 0.4591854512691498
  - 0.47356706857681274
  - 0.8738259077072144
  - 0.5556046962738037
  - 0.6456528306007385
  - 0.4811972975730896
  - 0.58575838804245
  - 0.7406384348869324
  - 0.4090011417865753
  - 0.8885678648948669
  - 0.6548668742179871
  - 0.6913275122642517
  - 0.9210730791091919
  - 0.6667829155921936
  - 0.6141724586486816
  - 0.5795531272888184
  - 0.545956552028656
  - 0.5843945741653442
  - 0.6988383531570435
  - 0.5990459322929382
  - 0.5752642750740051
  - 0.7596421837806702
  - 0.552026093006134
  - 0.7211520671844482
  - 0.5242646336555481
  - 0.5956336259841919
  - 0.7407655715942383
  - 0.64913409948349
  - 0.756880521774292
  - 0.7232495546340942
  - 0.5862278938293457
  - 0.5582324862480164
  - 0.5019355416297913
  - 0.5424512624740601
  - 0.5516533255577087
  - 0.5300261378288269
  - 0.5793378949165344
  - 0.46470367908477783
  - 0.5929052829742432
  - 0.5998043417930603
  - 0.4686306118965149
  - 0.47357022762298584
  - 0.6934661865234375
  - 0.5067906975746155
  - 0.6136075854301453
  - 0.4864204227924347
  - 0.5009182095527649
  - 0.5929133296012878
  - 0.541408360004425
  - 0.46732914447784424
  - 0.4412250816822052
  - 0.6121051907539368
  - 0.5377357602119446
  - 0.4779823422431946
  - 0.41369155049324036
  - 0.7582588195800781
  - 0.6079401969909668
  - 0.6263167858123779
  - 0.5238457322120667
  - 0.7539175748825073
  - 0.9134637117385864
  - 0.6522002816200256
  - 0.5371262431144714
  - 0.6914287805557251
  - 1.0383784770965576
  - 0.730960488319397
  - 0.6818861961364746
  - 0.6292071342468262
  - 0.7017687559127808
  - 0.542612612247467
  - 0.49402809143066406
  - 0.6377618908882141
  - 0.43984296917915344
  - 0.38478344678878784
  - 0.38581979274749756
  - 1.9621044397354126
  - 0.5183007121086121
  - 0.7514351010322571
  - 0.5283423662185669
  - 0.7516005635261536
  - 0.7922916412353516
  - 0.8210260272026062
  - 0.4434528350830078
  - 0.5845203995704651
  - 0.4593459963798523
  - 0.399495005607605
  - 0.4077167212963104
loss_records_fold4:
  train_losses:
  - 5.537888139486313
  - 5.54494004547596
  - 5.4642253696918495
  - 5.506550839543343
  - 5.510454881191254
  - 5.484281292557717
  - 5.523821833729745
  - 5.474263134598733
  - 5.41807029247284
  - 5.465762370824814
  - 5.432219848036766
  - 5.52842494994402
  - 5.493092796206475
  - 5.482840198278428
  - 5.509201213717461
  - 5.4783351778984075
  - 5.439069977402688
  - 5.510391858220101
  - 5.4394921541214
  - 5.470558160543442
  - 5.447650694847107
  - 5.457321295142174
  - 5.430641815066338
  - 5.48846310377121
  - 5.507034306228161
  - 5.461400735378266
  - 5.424185115098954
  - 5.4604567974805835
  - 5.475852519273758
  - 5.484581318497658
  - 5.4077243447303776
  - 5.388732296228409
  - 5.408409896492959
  - 5.424022406339645
  - 5.572600403428078
  - 5.470158478617669
  - 5.427986818552018
  - 5.445706178247929
  - 5.408444786071778
  - 5.38261111676693
  - 5.604787120223046
  - 5.607226133346558
  - 5.497450727224351
  - 5.450141733884812
  - 5.469725975394249
  - 5.458770155906677
  - 5.401391732692719
  - 5.463081759214401
  - 5.414630296826363
  - 5.43573169708252
  - 5.4520398914814
  - 5.427508705854416
  - 5.44458136856556
  - 5.430067479610443
  - 5.407194688916206
  - 5.43471668958664
  - 5.410982820391656
  - 5.416709187626839
  - 5.479704956710339
  - 5.473923820257188
  - 5.40026350915432
  - 5.461510345339775
  - 5.501989558339119
  - 5.413376578688622
  - 5.429811561107636
  - 5.324075019359589
  - 5.3978758513927465
  - 5.459724399447442
  - 5.4576325714588165
  - 5.3833696275949485
  - 5.424362844228745
  - 5.437181521952152
  - 5.377742341160775
  - 5.381719821691513
  - 5.389449566602707
  - 5.471775457262993
  - 5.424519988894463
  - 5.4113260671496395
  - 5.406954723596574
  - 5.360591006278992
  - 5.345863264799118
  - 5.42643575668335
  - 5.413804054260254
  - 5.34702662229538
  - 5.437359014153481
  - 5.347792094945908
  - 5.3518372774124146
  - 5.383686822652817
  - 5.350602146983147
  - 5.4401574552059175
  - 5.424642208218575
  - 5.379168319702149
  - 5.328116092085839
  - 5.317657375335694
  - 5.318939900398255
  - 5.44861378967762
  - 5.362232333421708
  - 5.411745065450669
  - 5.425632733106614
  - 5.368135762214661
  validation_losses:
  - 0.38075318932533264
  - 0.37278324365615845
  - 0.37695175409317017
  - 0.373926043510437
  - 0.3753046691417694
  - 0.37266212701797485
  - 0.39566513895988464
  - 0.40639474987983704
  - 0.3901740610599518
  - 0.5133046507835388
  - 0.4209068715572357
  - 0.3800522983074188
  - 0.3698520362377167
  - 0.3820004165172577
  - 0.38641929626464844
  - 0.38065430521965027
  - 0.38627487421035767
  - 0.426775187253952
  - 0.37627604603767395
  - 0.39541923999786377
  - 0.3718710243701935
  - 0.4470227360725403
  - 0.3832840919494629
  - 0.3685237765312195
  - 0.3787109851837158
  - 0.3856476843357086
  - 0.3681209087371826
  - 0.37438175082206726
  - 0.37100476026535034
  - 0.37889179587364197
  - 0.3965288996696472
  - 0.4286491274833679
  - 0.3737083971500397
  - 0.37983405590057373
  - 0.4097437560558319
  - 0.4026474058628082
  - 0.4175155460834503
  - 0.4508548974990845
  - 0.3858584463596344
  - 0.44021403789520264
  - 0.3877960741519928
  - 0.38291293382644653
  - 0.3725339472293854
  - 0.3739442527294159
  - 0.39190590381622314
  - 0.37658950686454773
  - 0.37323758006095886
  - 0.3692992031574249
  - 0.37203487753868103
  - 0.40710341930389404
  - 0.3694038689136505
  - 0.37525373697280884
  - 0.4261365532875061
  - 0.39827394485473633
  - 0.41115567088127136
  - 0.4012894928455353
  - 0.378194123506546
  - 0.461190402507782
  - 0.3893347978591919
  - 0.4890386760234833
  - 0.6291216611862183
  - 0.6574354767799377
  - 0.3916464149951935
  - 0.39768505096435547
  - 0.420232355594635
  - 0.47839051485061646
  - 0.4891902804374695
  - 0.4175874888896942
  - 0.42836514115333557
  - 0.502776563167572
  - 0.48470190167427063
  - 0.4569423198699951
  - 0.39984315633773804
  - 0.6532907485961914
  - 0.9659789800643921
  - 0.6928091645240784
  - 0.4155222475528717
  - 0.6309455633163452
  - 0.4399339556694031
  - 0.4519292712211609
  - 0.5031535625457764
  - 0.4373989701271057
  - 0.46378129720687866
  - 0.779760479927063
  - 0.5583837032318115
  - 0.7127439379692078
  - 0.7198977470397949
  - 0.8005721569061279
  - 0.8688362240791321
  - 0.549065351486206
  - 0.5348923802375793
  - 0.6934442520141602
  - 0.670362651348114
  - 0.45150506496429443
  - 0.6666752099990845
  - 0.7740540504455566
  - 0.7276670932769775
  - 0.6621372699737549
  - 0.708894670009613
  - 0.6240620613098145
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8130360205831904, 0.8542024013722127,
    0.8350515463917526]'
  fold_eval_f1: '[0.0, 0.0, 0.12800000000000003, 0.02298850574712644, 0.07692307692307693]'
  mean_eval_accuracy: 0.8435111669112836
  mean_f1_accuracy: 0.04558231653404068
  total_train_time: '0:35:00.633694'
