config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:28:24.850526'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_86fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_86fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_86fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_86fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_86fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_86fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_86fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_86fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_86fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_86fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.7975556492805482
  - 1.6272411584854127
  - 1.6264558374881746
  - 1.5930115759372712
  - 1.6859641909599306
  - 1.5774821937084198
  - 1.592159312963486
  - 1.5160328775644303
  - 1.6352060914039612
  - 1.6188053071498871
  - 1.62533301115036
  - 1.6459911525249482
  - 1.6554362237453462
  - 1.5946568071842195
  - 1.5621487975120545
  - 1.6896035373210907
  - 1.757281631231308
  - 1.6055343925952912
  - 1.5290985107421875
  - 1.554753065109253
  - 1.6325238704681397
  - 1.5789663553237916
  - 1.6013194382190705
  - 1.5827388525009156
  - 1.5555577874183655
  - 1.5308986067771913
  - 1.5885907769203187
  - 1.5587312579154968
  - 1.5744150519371034
  - 1.5414179921150208
  - 1.5981631815433504
  - 1.5391015887260437
  - 1.5716491341590881
  - 1.640285074710846
  - 1.5954598128795625
  - 1.5833104372024538
  - 1.5860674738883973
  - 1.5315636098384857
  - 1.5998894155025483
  - 1.5726780593395233
  - 1.5652173042297364
  - 1.5339025795459749
  - 1.545497936010361
  - 1.5744668126106263
  - 1.5504661023616793
  - 1.5933246821165086
  - 1.5600357502698898
  - 1.617368221282959
  - 1.6145593464374544
  - 1.561296594142914
  - 1.5911706805229189
  - 1.6167775750160218
  - 1.5330418050289154
  - 1.5569860517978669
  - 1.5553711771965029
  - 1.581723564863205
  - 1.5998100876808168
  validation_losses:
  - 0.43103519082069397
  - 0.40182673931121826
  - 0.39985978603363037
  - 0.40134069323539734
  - 0.4046486020088196
  - 0.40603339672088623
  - 0.40339675545692444
  - 0.37755489349365234
  - 0.39738893508911133
  - 0.41525641083717346
  - 0.43892809748649597
  - 0.3885151147842407
  - 0.39626768231391907
  - 0.39187026023864746
  - 0.4346385896205902
  - 0.4003240168094635
  - 0.4514097571372986
  - 0.3952738642692566
  - 0.3779366612434387
  - 0.38512325286865234
  - 0.388641357421875
  - 0.5735388994216919
  - 0.3891524374485016
  - 0.3965914249420166
  - 0.38551145792007446
  - 0.4824196398258209
  - 0.46617060899734497
  - 0.3825012743473053
  - 0.38699647784233093
  - 0.3907631039619446
  - 0.40910011529922485
  - 0.38667571544647217
  - 0.7199774980545044
  - 0.39424723386764526
  - 0.38665589690208435
  - 0.3857351243495941
  - 0.5229047536849976
  - 0.3805083632469177
  - 0.40527716279029846
  - 0.4135998785495758
  - 0.3815160393714905
  - 0.6216840147972107
  - 0.430431604385376
  - 0.5125764012336731
  - 1.3786145448684692
  - 0.39873644709587097
  - 0.3909721076488495
  - 0.3987252116203308
  - 0.39525970816612244
  - 0.38813984394073486
  - 0.3988594710826874
  - 0.4041236340999603
  - 0.398838073015213
  - 0.3947884440422058
  - 0.3863813877105713
  - 0.38974809646606445
  - 0.3892899453639984
loss_records_fold1:
  train_losses:
  - 1.621090751886368
  - 1.560902327299118
  - 1.5257301449775698
  - 1.576870459318161
  - 1.519661331176758
  - 1.5424960553646088
  - 1.5886148512363434
  - 1.5665372252464296
  - 1.5735690116882326
  - 1.47384232878685
  - 1.5504537343978884
  - 1.5483687281608582
  - 1.5323877692222596
  - 1.5057026863098146
  - 1.4847549855709077
  - 1.525875747203827
  - 1.5608228147029877
  - 1.5225632399320603
  - 1.544225811958313
  - 1.537998080253601
  - 1.5374172389507295
  - 1.4964799404144289
  - 1.5545976281166078
  - 1.5866872489452364
  - 1.5242698907852175
  - 1.5224402964115145
  - 1.5330972969532013
  - 1.526433700323105
  - 1.5826620161533356
  - 1.5172291100025177
  - 1.5138731241226198
  - 1.5138438045978546
  - 1.527708423137665
  - 1.5080639541149141
  - 1.5232037603855133
  - 1.5234616041183473
  - 1.5324990808963777
  - 1.5215312182903291
  - 1.5746901392936707
  - 1.5288335084915161
  - 1.499275004863739
  - 1.5469231545925142
  - 1.5298108994960786
  - 1.5263642191886904
  - 1.528354185819626
  - 1.4876794755458833
  - 1.5755328238010406
  - 1.5356610834598543
  - 1.5597589492797852
  - 1.5572970271110536
  - 1.5185551404953004
  - 1.4989511728286744
  - 1.5108727633953096
  - 1.5168000519275666
  - 1.5571514010429384
  - 1.5188895583152773
  - 1.5430410981178284
  - 1.524981141090393
  - 1.5237459421157837
  - 1.5133191078901291
  - 1.4953856647014618
  - 1.5041288435459137
  - 1.536996591091156
  - 1.5130607068538666
  - 1.5329685747623445
  - 1.528107899427414
  - 1.4661647379398346
  - 1.5141270756721497
  - 1.5060703098773958
  - 1.4902632474899293
  - 1.5178867638111115
  - 1.5036645472049714
  - 1.494095814228058
  - 1.4935925364494325
  - 1.4974145472049714
  - 1.497285556793213
  - 1.4866072297096253
  - 1.5238772869110109
  - 1.5677833020687104
  - 1.485345247387886
  - 1.5078555405139924
  - 1.458111044764519
  - 1.5056575059890749
  - 1.5131406903266909
  - 1.5291773438453675
  - 1.4679897606372834
  - 1.5329190492630005
  - 1.5526059448719025
  - 1.4938925862312318
  - 1.4966304302215576
  - 1.477455496788025
  - 1.5360502064228059
  - 1.4644927263259888
  - 1.5279245853424073
  - 1.4938645005226137
  - 1.4543896555900575
  - 1.5074860990047456
  - 1.5462269127368928
  - 1.5257252871990206
  - 1.4874931812286378
  validation_losses:
  - 0.4011957347393036
  - 0.4068552255630493
  - 0.4111173748970032
  - 0.4090743362903595
  - 0.40140223503112793
  - 0.381830096244812
  - 0.4064653515815735
  - 0.40268003940582275
  - 0.392742395401001
  - 0.4493196904659271
  - 0.7474769353866577
  - 0.42074957489967346
  - 0.4458293318748474
  - 0.49697113037109375
  - 0.5907810926437378
  - 0.7090163230895996
  - 0.657715380191803
  - 0.5896442532539368
  - 0.3994491398334503
  - 0.39312246441841125
  - 0.4286496639251709
  - 0.5195833444595337
  - 0.6933065056800842
  - 0.6975712180137634
  - 0.39226284623146057
  - 0.5350853800773621
  - 0.542255163192749
  - 0.6225096583366394
  - 0.5782742500305176
  - 0.4109734892845154
  - 0.5215504765510559
  - 0.4809892475605011
  - 0.39460256695747375
  - 0.4065416157245636
  - 0.44775596261024475
  - 0.3862702548503876
  - 0.3986482620239258
  - 0.4176892340183258
  - 0.4262882173061371
  - 0.4676397442817688
  - 0.3923933506011963
  - 0.4092572331428528
  - 0.4003634452819824
  - 0.43362730741500854
  - 0.3880634009838104
  - 0.4075445532798767
  - 0.42493969202041626
  - 0.4028465449810028
  - 0.3942255675792694
  - 0.3901623785495758
  - 0.39303961396217346
  - 0.4528416693210602
  - 0.49550747871398926
  - 0.5891705751419067
  - 0.6069232821464539
  - 0.5155354142189026
  - 0.5503847599029541
  - 0.38741981983184814
  - 0.5019321441650391
  - 0.5322062373161316
  - 0.3935862183570862
  - 0.40306252241134644
  - 0.4089730679988861
  - 0.38172227144241333
  - 0.44529828429222107
  - 0.45263344049453735
  - 0.4704626500606537
  - 0.5935165882110596
  - 0.6093822121620178
  - 1.8494603633880615
  - 0.6738185882568359
  - 0.4612147808074951
  - 0.689274251461029
  - 0.5581749081611633
  - 0.6067457795143127
  - 0.8320490121841431
  - 0.5417991280555725
  - 0.6899831891059875
  - 0.7901650667190552
  - 0.39042672514915466
  - 0.41280609369277954
  - 0.403792142868042
  - 0.5557994246482849
  - 0.44113391637802124
  - 0.5822572112083435
  - 0.5242927670478821
  - 0.5363054275512695
  - 0.5469207763671875
  - 0.5113807916641235
  - 1.1274923086166382
  - 0.5393919944763184
  - 0.572256863117218
  - 0.8171326518058777
  - 0.7079833149909973
  - 0.48566266894340515
  - 0.5543053150177002
  - 0.4416828155517578
  - 0.4747157692909241
  - 0.48582446575164795
  - 0.5987263321876526
loss_records_fold2:
  train_losses:
  - 1.4777906894683839
  - 1.5237028419971468
  - 1.529768878221512
  - 1.4836243331432344
  - 1.5416870296001435
  - 1.4967395186424257
  - 1.4872249245643616
  - 1.4788195073604584
  - 1.470998027920723
  - 1.5083619952201843
  - 1.4846745431423187
  - 1.5045290172100068
  - 1.5249245882034304
  - 1.553197515010834
  - 1.516904377937317
  - 1.4812137305736544
  - 1.4940051436424255
  - 1.4709233283996583
  - 1.507180553674698
  - 1.4907112360000612
  - 1.4747999787330628
  - 1.47845576107502
  - 1.4924168288707733
  - 1.5083060264587402
  - 1.5040025472640992
  - 1.4901288598775864
  - 1.488748574256897
  - 1.5041693389415742
  - 1.468433552980423
  - 1.4765112549066544
  - 1.5163158476352692
  - 1.471571296453476
  - 1.4563634634017946
  - 1.4715659350156785
  - 1.501671725511551
  - 1.4909252405166626
  - 1.4875054180622103
  - 1.545827478170395
  - 1.5535198152065277
  - 1.54777330160141
  - 1.5166880428791047
  - 1.495709079504013
  - 1.4477835029363633
  - 1.4328351736068727
  - 1.5143270790576935
  - 1.5469004094600678
  - 1.4822404921054841
  - 1.4484482884407044
  - 1.4732526361942293
  - 1.5135968923568726
  - 1.5033926188945772
  - 1.4779624819755555
  - 1.514055782556534
  - 1.486538964509964
  - 1.420918768644333
  - 1.4981050550937653
  - 1.4877278923988344
  - 1.4981845021247864
  - 1.459972831606865
  - 1.5151593804359438
  - 1.533854925632477
  - 1.5652646899223328
  - 1.4981850862503052
  - 1.499014323949814
  - 1.4792872607707979
  - 1.4731591403484345
  - 1.4973813414573671
  - 1.4697181522846223
  - 1.513785082101822
  - 1.5266136288642884
  - 1.5153164267539978
  - 1.4425926446914674
  - 1.518346256017685
  - 1.5253553986549377
  - 1.5206784009933472
  - 1.526164734363556
  - 1.5162191629409791
  - 1.4725316584110262
  - 1.4891946911811829
  - 1.4652859687805178
  - 1.4916446805000305
  - 1.4758836150169374
  - 1.4352245151996614
  - 1.444283938407898
  - 1.4918493866920473
  - 1.4161761313676835
  - 1.4705088913440705
  - 1.454666531085968
  - 1.4951089560985567
  - 1.5105954349040986
  - 1.655026614665985
  - 1.547184443473816
  - 1.4786382436752321
  - 1.5261499404907228
  - 1.528295025229454
  - 1.4815360307693481
  - 1.47960986495018
  - 1.4653052985668182
  - 1.5039607524871828
  - 1.5190164923667908
  validation_losses:
  - 0.38484230637550354
  - 0.4879160523414612
  - 0.6014643311500549
  - 0.39116692543029785
  - 0.7637335062026978
  - 1.0160419940948486
  - 1.1072286367416382
  - 0.7303913831710815
  - 0.8875823616981506
  - 0.5174019932746887
  - 0.47965410351753235
  - 0.566094696521759
  - 0.4308737814426422
  - 0.792012095451355
  - 0.8254809975624084
  - 0.7913128137588501
  - 0.5098579525947571
  - 0.824708104133606
  - 1.3267769813537598
  - 1.172179102897644
  - 0.722204864025116
  - 0.4318142533302307
  - 0.40893858671188354
  - 0.5794928073883057
  - 0.526243269443512
  - 0.41682517528533936
  - 0.7324427962303162
  - 0.42589128017425537
  - 0.4180293679237366
  - 0.5068773031234741
  - 0.6453418731689453
  - 0.5782280564308167
  - 0.6017388105392456
  - 0.669653594493866
  - 0.6633267402648926
  - 0.6163591146469116
  - 0.5196170210838318
  - 0.8545762896537781
  - 0.424307256937027
  - 0.40665438771247864
  - 0.5105859041213989
  - 0.46503135561943054
  - 0.3907316327095032
  - 0.44958972930908203
  - 0.44774413108825684
  - 0.4420976936817169
  - 0.4142935276031494
  - 0.42145267128944397
  - 0.5003377795219421
  - 0.4122411012649536
  - 0.44925007224082947
  - 0.5261451005935669
  - 0.518825888633728
  - 0.5881348848342896
  - 0.5874119997024536
  - 0.6366222500801086
  - 0.5121962428092957
  - 0.8177192211151123
  - 0.5294108986854553
  - 0.3973004221916199
  - 0.4348583221435547
  - 0.8582898378372192
  - 0.4250936508178711
  - 0.38917025923728943
  - 0.4209412634372711
  - 0.4848211109638214
  - 0.449908584356308
  - 0.4168330132961273
  - 0.5430572628974915
  - 0.48075029253959656
  - 0.4495464563369751
  - 0.45670247077941895
  - 0.5492584109306335
  - 0.4793933629989624
  - 0.5044567584991455
  - 1.2156383991241455
  - 0.5734104514122009
  - 0.410683810710907
  - 0.511380136013031
  - 0.8603975176811218
  - 0.7499115467071533
  - 1.0347827672958374
  - 0.5860851407051086
  - 0.515487551689148
  - 0.49256420135498047
  - 0.5436170101165771
  - 0.5735445618629456
  - 0.6022342443466187
  - 2.4734461307525635
  - 1.607962965965271
  - 1.046944499015808
  - 0.5482318997383118
  - 0.43772628903388977
  - 0.4159199893474579
  - 0.4600677788257599
  - 0.4783927798271179
  - 0.4336075186729431
  - 0.5226099491119385
  - 0.47846031188964844
  - 0.588077962398529
loss_records_fold3:
  train_losses:
  - 1.5013885080814362
  - 1.5204366385936738
  - 1.5457044839859009
  - 1.5201843559741974
  - 1.4942953944206239
  - 1.5485485017299654
  - 1.5018836617469788
  - 1.518538409471512
  - 1.5440759003162385
  - 1.4914777040481568
  - 1.4874065935611727
  - 1.519450306892395
  - 1.4456872880458833
  - 1.5016698181629182
  - 1.482472848892212
  - 1.502520138025284
  - 1.5005923569202424
  - 1.4350486308336259
  - 1.4837868332862856
  - 1.4809148609638214
  - 1.5202738463878633
  - 1.44840424656868
  - 1.488238972425461
  - 1.487021154165268
  - 1.4468014299869538
  - 1.5018680095672607
  - 1.4879993855953217
  - 1.5041806757450105
  - 1.5234678208827974
  - 1.4925393283367159
  - 1.485164922475815
  - 1.466500335931778
  - 1.4871004760265352
  - 1.487114304304123
  - 1.5907532155513764
  - 1.512254387140274
  - 1.5567984998226168
  - 1.494623339176178
  - 1.4999366581439972
  - 1.4904800772666933
  - 1.4572862684726715
  - 1.4754274785518646
  - 1.4505379498004913
  - 1.485839205980301
  - 1.4801306366920473
  - 1.4739368438720704
  - 1.4708009958267212
  - 1.4702734649181366
  - 1.5246113479137422
  - 1.4568483322858812
  - 1.4903226375579834
  - 1.4790755033493044
  - 1.5368463933467866
  - 1.527538025379181
  - 1.462643164396286
  - 1.5409340977668764
  - 1.4763892889022827
  - 1.487282019853592
  - 1.4768594264984132
  - 1.5178025960922241
  - 1.4865596413612367
  - 1.5140593230724335
  - 1.454253923892975
  - 1.5085696101188661
  - 1.5043851613998414
  - 1.475313949584961
  - 1.491926145553589
  - 1.475432425737381
  - 1.4353107780218126
  - 1.4516656041145326
  - 1.5234780311584473
  - 1.4648723304271698
  - 1.4807749927043916
  - 1.5144600689411165
  - 1.4776875436306
  - 1.4814830839633943
  - 1.4631947398185732
  - 1.4454252183437348
  - 1.5090656936168672
  - 1.5177420675754547
  - 1.4728678941726685
  - 1.5750462830066683
  - 1.4298628687858583
  - 1.4699195623397827
  - 1.4931933760643006
  - 1.4446972608566284
  - 1.5803516209125519
  - 1.4690829038619997
  - 1.5268061935901642
  - 1.5064147770404817
  - 1.458283966779709
  - 1.506645464897156
  - 1.4602595627307893
  - 1.4094164669513702
  - 1.4474013268947603
  - 1.4525882750749588
  - 1.4721069514751435
  - 1.4771133959293365
  - 1.4402503073215485
  - 1.4714841783046724
  validation_losses:
  - 0.7547913193702698
  - 0.6146484613418579
  - 0.7093400359153748
  - 1.040411114692688
  - 0.9858709573745728
  - 1.3521955013275146
  - 0.662151038646698
  - 0.8547034859657288
  - 0.42446503043174744
  - 0.469336599111557
  - 0.46102556586265564
  - 0.48063135147094727
  - 0.5024576187133789
  - 0.48712149262428284
  - 0.5791260600090027
  - 0.45143428444862366
  - 0.5568415522575378
  - 0.6278356313705444
  - 0.5412940979003906
  - 0.40555068850517273
  - 0.5216739773750305
  - 0.5087860226631165
  - 0.5292119383811951
  - 0.4550016224384308
  - 0.41322803497314453
  - 0.5557740330696106
  - 1.0231077671051025
  - 1.0651830434799194
  - 0.6419732570648193
  - 0.7547823190689087
  - 1.4763339757919312
  - 0.3927459120750427
  - 0.4392295777797699
  - 0.43997302651405334
  - 0.47823259234428406
  - 0.5388070940971375
  - 0.5298941731452942
  - 0.4668561518192291
  - 0.4261872172355652
  - 0.5296140313148499
  - 0.5200193524360657
  - 0.6027214527130127
  - 0.5630779266357422
  - 0.42861801385879517
  - 0.43938446044921875
  - 0.5409780740737915
  - 0.45098876953125
  - 0.5387631058692932
  - 0.4799618422985077
  - 0.4405035078525543
  - 0.48265182971954346
  - 0.5521323084831238
  - 0.5142784714698792
  - 0.4551334083080292
  - 0.47529900074005127
  - 0.5096425414085388
  - 0.43401241302490234
  - 0.48167872428894043
  - 0.8119121789932251
  - 0.47817298769950867
  - 0.5431032776832581
  - 0.5775821805000305
  - 0.48413458466529846
  - 0.45976099371910095
  - 0.5504910349845886
  - 3.516925811767578
  - 0.44266581535339355
  - 4.359198570251465
  - 0.42462149262428284
  - 8.327547073364258
  - 6.0070343017578125
  - 5.6636505126953125
  - 0.44682589173316956
  - 13.892791748046875
  - 0.5477351546287537
  - 0.5631681680679321
  - 0.8855168223381042
  - 12.014134407043457
  - 0.5249348878860474
  - 0.4752116799354553
  - 0.45237860083580017
  - 0.4348142743110657
  - 0.39628949761390686
  - 0.4588305950164795
  - 0.3972874879837036
  - 0.4418157637119293
  - 0.4588535726070404
  - 0.4614272117614746
  - 0.38720443844795227
  - 0.5123900175094604
  - 0.45052599906921387
  - 0.496703177690506
  - 0.5273078680038452
  - 0.5463263392448425
  - 0.4594188630580902
  - 0.5308722853660583
  - 0.46926191449165344
  - 0.5517402291297913
  - 0.5044167637825012
  - 0.6115130186080933
loss_records_fold4:
  train_losses:
  - 1.443273624777794
  - 1.4471958756446839
  - 1.5299887359142303
  - 1.4652808070182801
  - 1.4051465809345247
  - 1.4346924781799317
  - 1.4608048975467682
  - 1.50320520401001
  - 1.5193096578121186
  - 1.476646649837494
  - 1.5091484904289247
  - 1.5617766380310059
  - 1.5022152304649354
  - 1.4920292675495148
  - 1.4873920500278475
  - 1.4667999804019929
  - 1.4557971179485323
  - 1.4269622236490251
  - 1.4822970747947695
  - 1.4876044213771822
  - 1.460856729745865
  - 1.4848057746887209
  - 1.458863651752472
  - 1.453682217001915
  - 1.4803994655609132
  - 1.4960650742053987
  - 1.4395053803920748
  - 1.4692098319530489
  - 1.4666136622428896
  - 1.45025092959404
  - 1.471717154979706
  - 1.466307705640793
  - 1.4980863988399507
  - 1.477184569835663
  - 1.5721788823604586
  - 1.4872648894786835
  - 1.507473921775818
  - 1.484229636192322
  - 1.4401520460844042
  - 1.468271690607071
  - 1.500209015607834
  - 1.4864401519298553
  - 1.5509369850158692
  - 1.4855004608631135
  - 1.50159569978714
  - 1.5100997149944306
  - 1.423335635662079
  - 1.4357569932937624
  - 1.45379096865654
  - 1.5125636219978333
  - 1.4606091618537904
  - 1.4786552250385285
  - 1.4101380527019503
  - 1.49067440032959
  - 1.529100400209427
  - 1.493693256378174
  - 1.4678674876689912
  - 1.4080762922763825
  - 1.4533970654010773
  - 1.4340073704719545
  - 1.4989749908447267
  - 1.479133516550064
  - 1.476156520843506
  - 1.4674968481063844
  - 1.4861512362957001
  - 1.4561305582523347
  - 1.4782978713512422
  - 1.4528217554092409
  - 1.4784292578697205
  - 1.5061252355575563
  - 1.8335847318172456
  - 1.6143100261688232
  - 1.4764895617961884
  - 1.5075193881988527
  - 1.4678782761096956
  - 1.468181985616684
  - 1.451440691947937
  - 1.4573028147220612
  - 1.474249815940857
  - 1.4117149472236634
  - 1.4945806026458741
  - 1.4771888315677644
  - 1.4389825850725175
  - 1.471907448768616
  - 1.480161261558533
  - 1.4190369844436646
  - 1.4226659059524538
  - 1.4611024916172028
  - 1.466298294067383
  - 1.4173774719238281
  - 1.476902896165848
  - 1.4419938564300538
  - 1.4710481345653534
  - 1.462043970823288
  - 1.4830854058265688
  - 1.479028367996216
  - 1.4851861000061035
  - 1.4811274111270905
  - 1.4538706779479982
  - 1.533999353647232
  validation_losses:
  - 0.4202394187450409
  - 0.4598967134952545
  - 0.4271405041217804
  - 0.43194764852523804
  - 0.41682466864585876
  - 0.45527151226997375
  - 0.42926445603370667
  - 0.3642749786376953
  - 0.45798444747924805
  - 5.223492622375488
  - 0.40585649013519287
  - 0.4217725098133087
  - 0.5328670144081116
  - 0.4574356973171234
  - 0.3937550485134125
  - 0.4003394544124603
  - 0.3725789487361908
  - 0.3691755533218384
  - 0.41536423563957214
  - 0.42610275745391846
  - 0.4049849808216095
  - 0.4301418960094452
  - 0.4718362092971802
  - 0.4609147608280182
  - 0.4065287709236145
  - 0.3899112045764923
  - 0.5609397888183594
  - 0.45414215326309204
  - 0.46712765097618103
  - 0.4110923707485199
  - 0.40727388858795166
  - 0.43983495235443115
  - 0.45321324467658997
  - 0.4122031331062317
  - 0.4390423893928528
  - 0.47840458154678345
  - 0.40050390362739563
  - 0.3677966892719269
  - 0.39700108766555786
  - 0.48770859837532043
  - 0.4482347369194031
  - 0.44055700302124023
  - 0.46708348393440247
  - 0.3778308629989624
  - 0.4825992286205292
  - 0.4447738826274872
  - 0.44275978207588196
  - 0.5001740455627441
  - 0.4396791458129883
  - 0.43212416768074036
  - 0.4267272353172302
  - 0.4353664219379425
  - 0.5107339024543762
  - 0.44882580637931824
  - 0.44003674387931824
  - 0.4730929434299469
  - 0.42084401845932007
  - 0.4557138681411743
  - 0.4247438609600067
  - 0.41750234365463257
  - 0.4274977743625641
  - 0.443238765001297
  - 0.48763924837112427
  - 0.44413265585899353
  - 0.4639344811439514
  - 0.40352168679237366
  - 0.4846023619174957
  - 0.47644945979118347
  - 0.4921538829803467
  - 0.5924955606460571
  - 0.44936326146125793
  - 0.3749820590019226
  - 0.3800247311592102
  - 0.429781049489975
  - 0.42642948031425476
  - 0.46673107147216797
  - 0.473633348941803
  - 0.42814597487449646
  - 0.4627186357975006
  - 0.46429046988487244
  - 0.40422484278678894
  - 0.4487335681915283
  - 0.5520935654640198
  - 0.5117326974868774
  - 0.34886837005615234
  - 0.40086981654167175
  - 0.376434862613678
  - 0.44160735607147217
  - 0.4420992434024811
  - 0.42776983976364136
  - 0.43722760677337646
  - 0.4385097026824951
  - 0.4509958028793335
  - 0.4221530854701996
  - 0.46079686284065247
  - 0.4848451018333435
  - 0.47297024726867676
  - 0.5471271872520447
  - 0.4660244584083557
  - 0.5976556539535522
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 57 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.7375643224699828, 0.8336192109777015, 0.8319039451114922,
    0.8264604810996563]'
  fold_eval_f1: '[0.0, 0.23115577889447236, 0.2595419847328244, 0.19672131147540983,
    0.22900763358778625]'
  mean_eval_accuracy: 0.8174361785526928
  mean_f1_accuracy: 0.18328534173809857
  total_train_time: '0:39:36.579445'
