config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 23:15:28.063530'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_120fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_120fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_120fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_120fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_120fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_120fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_120fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_120fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_120fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_120fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 85.40997049808503
  - 38.76262570470572
  - 36.77360256612301
  - 24.43816306702793
  - 20.74006569981575
  - 14.507938971370459
  - 19.244732594490053
  - 22.670848691463473
  - 21.990047717094424
  - 43.0783010005951
  - 13.261269131302834
  - 10.74199438393116
  - 15.832273688912393
  - 13.62805468440056
  - 12.454634478688241
  - 7.925216174125672
  - 8.13025614619255
  - 8.90049830675125
  - 9.35084144473076
  - 7.389978250861168
  - 8.32324905395508
  - 9.680090466141701
  - 8.534724140167237
  - 11.372249388694764
  - 6.846732565760613
  - 7.475461283326149
  - 6.40770560503006
  - 6.398665547370911
  - 6.621905085444451
  - 6.375837451219559
  - 6.036253762245178
  - 6.293992587924004
  - 6.145690488815308
  - 6.33989505469799
  - 6.063439193367959
  - 6.073193046450616
  - 6.020061331987382
  - 6.108245059847832
  - 5.843798589706421
  - 6.134005093574524
  - 6.031742170453072
  - 6.054214614629746
  - 5.856619197130204
  - 5.929831328988076
  - 6.0279755949974065
  - 5.984653973579407
  - 5.876186159253121
  - 7.062362234294415
  - 7.425709224492312
  - 5.9593195736408235
  - 10.041547027230264
  - 7.8608039110898975
  - 6.187728056311608
  - 6.120154061913491
  - 6.059064123034478
  - 5.961695528030396
  - 5.923381289839745
  - 5.975129428505898
  - 6.196521112322808
  - 6.000040382146835
  - 6.111645317077637
  - 6.061812503635884
  - 5.8763484269380575
  - 5.883435314893723
  - 5.897584500908852
  - 5.853293797373772
  - 5.957997936010361
  - 5.939736163616181
  - 5.976873588562012
  - 5.798612040281296
  - 5.842926326394082
  - 5.980813544988632
  - 5.959321767091751
  - 6.084989950060844
  - 5.94115601181984
  - 6.0918592572212225
  - 5.859213128685951
  - 6.037575276196003
  - 6.005285087227822
  - 5.9412987872958185
  - 5.890727174282074
  - 6.122299921512604
  - 6.181625857949257
  - 5.9589999437332155
  - 6.0605721354484565
  - 5.956397853791714
  - 5.812442943453789
  - 5.865845954418183
  - 6.007032042741776
  - 6.267551797628403
  - 5.9618241667747505
  - 5.983421042561531
  - 5.920130681991577
  - 5.982604682445526
  - 6.816520708799363
  - 7.13346081674099
  - 8.513606318831444
  - 6.413051825761795
  - 6.074502205848694
  - 6.9304801523685455
  validation_losses:
  - 1.2659214735031128
  - 0.5538145899772644
  - 1.174712896347046
  - 0.6600053310394287
  - 0.4107995927333832
  - 0.79241943359375
  - 9.074504852294922
  - 0.47433802485466003
  - 1.0799223184585571
  - 0.5070357322692871
  - 0.3973228633403778
  - 0.40690794587135315
  - 0.397990882396698
  - 0.4127449691295624
  - 0.4256792664527893
  - 0.38406801223754883
  - 0.9821141362190247
  - 0.41284650564193726
  - 0.545416533946991
  - 0.40520912408828735
  - 0.4655950367450714
  - 0.3829309046268463
  - 0.4621417820453644
  - 0.3863944709300995
  - 0.40439698100090027
  - 0.40262654423713684
  - 0.41762617230415344
  - 0.4574294090270996
  - 0.41061803698539734
  - 0.3905107378959656
  - 0.40706315636634827
  - 0.3926822543144226
  - 0.3947288691997528
  - 0.3907836675643921
  - 0.39467525482177734
  - 0.3892838954925537
  - 0.41021862626075745
  - 0.39389359951019287
  - 0.4422464072704315
  - 0.40773439407348633
  - 0.3967227339744568
  - 0.39510661363601685
  - 0.4149477779865265
  - 0.40784215927124023
  - 0.3942277431488037
  - 0.6665697693824768
  - 0.404681921005249
  - 0.3886660039424896
  - 0.4093968868255615
  - 0.3909198045730591
  - 0.3981855511665344
  - 0.4506168067455292
  - 0.3937080502510071
  - 0.40016672015190125
  - 0.41776779294013977
  - 0.4048987627029419
  - 0.4057188034057617
  - 0.38835573196411133
  - 0.4042946398258209
  - 0.4361836314201355
  - 0.4042275846004486
  - 0.4367063343524933
  - 0.3910936117172241
  - 0.3976685106754303
  - 0.39576753973960876
  - 0.39306575059890747
  - 0.6602843999862671
  - 0.42733100056648254
  - 0.3910069465637207
  - 0.40255874395370483
  - 0.390215128660202
  - 0.4366033673286438
  - 0.3933928310871124
  - 0.4648161828517914
  - 0.3928476870059967
  - 0.4171395003795624
  - 0.3930145800113678
  - 0.39106374979019165
  - 0.3908061981201172
  - 0.3916078209877014
  - 0.4107859134674072
  - 0.3891591727733612
  - 0.4184633195400238
  - 0.4181339144706726
  - 0.431376188993454
  - 0.38993746042251587
  - 0.39260968565940857
  - 0.3913138210773468
  - 11731.763671875
  - 0.3995454013347626
  - 0.39074140787124634
  - 0.40356430411338806
  - 0.3915806710720062
  - 0.7873263955116272
  - 6.243748188018799
  - 0.4030269682407379
  - 0.4400891363620758
  - 0.3983924388885498
  - 0.4402405321598053
  - 0.3956908881664276
loss_records_fold1:
  train_losses:
  - 7.539184215664864
  - 6.029156976938248
  - 6.025595325231553
  - 5.896866494417191
  - 6.103071376681328
  - 6.1195398598909385
  - 6.062536683678627
  - 5.8476323872804645
  - 6.005411371588707
  - 5.890321379899979
  - 6.00668649673462
  - 6.058748373389244
  - 5.874009507894517
  - 5.833974406123161
  - 5.794674670696259
  - 5.813146898150444
  - 5.909993912279607
  - 5.960806319117546
  - 5.9657653957605365
  - 5.814948505163193
  - 5.848651862144471
  - 5.975557008385659
  - 5.742875877022744
  - 5.796631571650505
  - 5.819305577874184
  - 5.832569941878319
  - 5.90080718100071
  - 6.161356753110886
  - 5.963912883400917
  - 5.843663138151169
  - 5.890738773345948
  - 6.0184796273708345
  - 5.961172014474869
  - 5.9607394427061084
  - 5.897314107418061
  - 6.005226504802704
  - 5.967445085942746
  - 5.779159396886826
  - 6.090755435824395
  - 5.865077850222588
  - 5.8711033165454865
  - 5.836334758996964
  - 5.868328168988228
  - 5.932791638374329
  - 6.0653424888849266
  - 5.94110546708107
  - 5.807693696022034
  - 5.921789133548737
  - 5.843621408939362
  - 5.990683895349503
  - 5.861289255321026
  - 6.116338098049164
  - 5.872091329097748
  - 6.126195386052132
  - 5.82588783800602
  - 5.891936618089677
  - 5.855638912320138
  - 5.813341593742371
  - 5.848728987574578
  - 6.26049744784832
  - 5.813866436481476
  - 5.933695471286774
  - 6.003916198015213
  - 5.762365418672562
  - 5.915560433268547
  - 5.980593684315682
  - 6.003573504090309
  - 5.837017199397088
  - 6.025495019555092
  validation_losses:
  - 0.40185442566871643
  - 0.4685136079788208
  - 0.42417392134666443
  - 0.4198760390281677
  - 0.44965696334838867
  - 0.404634952545166
  - 0.41638273000717163
  - 0.40478065609931946
  - 0.42828062176704407
  - 0.41393348574638367
  - 0.4175035059452057
  - 0.5423545241355896
  - 0.4290448725223541
  - 0.4667671024799347
  - 0.4082540273666382
  - 0.48022785782814026
  - 44.71399688720703
  - 237.19911193847656
  - 0.44639867544174194
  - 0.40608513355255127
  - 0.4092717468738556
  - 0.4261360168457031
  - 0.4220616817474365
  - 0.40389928221702576
  - 0.40509653091430664
  - 0.4301205575466156
  - 0.40492257475852966
  - 0.4454033374786377
  - 0.4045736491680145
  - 0.40516096353530884
  - 180.27084350585938
  - 0.42129287123680115
  - 0.5312665700912476
  - 0.4410974681377411
  - 0.4246271550655365
  - 0.4049533009529114
  - 0.4071386456489563
  - 287.51361083984375
  - 0.4186682403087616
  - 0.4021866023540497
  - 0.4025593101978302
  - 0.40483003854751587
  - 0.40861913561820984
  - 0.4484747648239136
  - 0.4161982238292694
  - 0.4178357422351837
  - 0.46514371037483215
  - 0.40125492215156555
  - 660.4723510742188
  - 0.45663097500801086
  - 0.408916711807251
  - 0.4064866006374359
  - 17899.5859375
  - 357735.40625
  - 0.4179723560810089
  - 0.40994587540626526
  - 0.4239901602268219
  - 0.4049960970878601
  - 0.43035653233528137
  - 0.4047362804412842
  - 0.40625083446502686
  - 0.40598875284194946
  - 0.4192145764827728
  - 0.4128795564174652
  - 0.40992897748947144
  - 0.41463571786880493
  - 0.42202427983283997
  - 0.41764894127845764
  - 0.40495404601097107
loss_records_fold2:
  train_losses:
  - 5.981047043204308
  - 5.8613110274076465
  - 6.166476552188397
  - 5.964227414131165
  - 5.909581729769707
  - 6.104100579023362
  - 5.9711042732000355
  - 5.889854428172112
  - 6.103738159686327
  - 5.863027742505074
  - 6.0718778520822525
  - 5.893847051262856
  - 6.122376230359078
  - 5.971262633800507
  - 5.942023316025734
  - 5.911060082912446
  - 5.9107208311557775
  - 6.000222438573838
  - 5.909695383906365
  - 6.1598599523305895
  - 6.041076329350472
  - 6.118167480826378
  - 5.943279963731766
  - 6.030217882990837
  - 5.915436288714409
  validation_losses:
  - 0.41398513317108154
  - 0.5045798420906067
  - 0.3931787610054016
  - 0.3904130160808563
  - 0.39406266808509827
  - 0.41910216212272644
  - 0.40406060218811035
  - 0.39365971088409424
  - 0.3813641369342804
  - 0.38660502433776855
  - 0.38768747448921204
  - 0.43142685294151306
  - 0.39094510674476624
  - 0.40941479802131653
  - 0.38988620042800903
  - 0.3873940110206604
  - 0.3832913041114807
  - 0.38460302352905273
  - 0.41149893403053284
  - 0.4039671719074249
  - 0.40328139066696167
  - 0.3845917582511902
  - 0.38317468762397766
  - 0.38702717423439026
  - 0.3822101056575775
loss_records_fold3:
  train_losses:
  - 5.967613607645035
  - 6.121579706668854
  - 5.869880956411362
  - 5.857584601640702
  - 5.86069764494896
  - 6.05068165063858
  - 6.076352339982987
  - 5.925788877904416
  - 6.063854467868805
  - 5.983480614423752
  - 5.940053844451905
  - 5.9409037917852405
  - 5.8479510128498084
  - 5.931084755063058
  - 5.965674927830697
  - 5.9294963657855995
  - 5.880758580565453
  - 5.983483222126961
  - 5.973822170495987
  - 5.908945947885513
  - 5.7876849681139
  - 5.909412389993668
  - 5.928821820020676
  - 5.955227255821228
  - 5.98581592142582
  - 6.079598605632782
  validation_losses:
  - 0.5338124632835388
  - 0.42749470472335815
  - 0.39680594205856323
  - 0.40305864810943604
  - 0.3981724977493286
  - 0.40880486369132996
  - 0.39940670132637024
  - 0.39789554476737976
  - 0.42718881368637085
  - 0.41166558861732483
  - 0.44180309772491455
  - 0.4143308997154236
  - 0.40288496017456055
  - 0.45245039463043213
  - 0.4088894724845886
  - 0.4117927551269531
  - 0.3974413275718689
  - 0.4007423520088196
  - 0.4102058708667755
  - 3400218502496256.0
  - 230426673152000.0
  - 841921527808.0
  - 497834229760.0
  - 394473275392.0
  - 388277993472.0
  - 228859772928.0
loss_records_fold4:
  train_losses:
  - 5.962761637568474
  - 6.2272253185510635
  - 6.122641173005104
  - 6.064602664113045
  - 10.63960250914097
  - 12.683907639980317
  - 12.970466661453248
  - 9.68582209944725
  - 13.293810668587685
  - 9.774528843164445
  - 9.200024044513702
  - 6.576854091882706
  - 7.15050288438797
  - 7.452567061781884
  - 7.627136370539666
  - 6.6483000963926315
  validation_losses:
  - 0.3965476155281067
  - 0.39142152667045593
  - 0.39197495579719543
  - 0.39406612515449524
  - 0.5881050825119019
  - 0.4921973645687103
  - 0.4853570759296417
  - 0.43657997250556946
  - 0.4050523042678833
  - 0.4516475200653076
  - 0.3979547321796417
  - 0.39912909269332886
  - 0.39777296781539917
  - 0.3951437473297119
  - 0.3958270847797394
  - 0.396393746137619
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 69 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:22:15.281092'
