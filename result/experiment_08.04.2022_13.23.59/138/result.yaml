config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 06:28:05.680139'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/138/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/138/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/138/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/138/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/138/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/138/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/138/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/138/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/138/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/138/fold_4_state_dict.pt
loss_records_fold3:
  train_losses:
  - 15.221800714731216
  - 15.242771297693253
  - 16.171956330537796
  - 15.517085462808609
  - 15.532456248998642
  - 15.20289632678032
  - 15.184435546398163
  - 15.552416145801544
  - 14.741185456514359
  - 15.544102609157562
  - 15.109247624874115
  - 15.172702714800835
  - 14.75565455853939
  - 15.17230886220932
  - 14.811001509428024
  - 14.919212073087692
  - 15.056647464632988
  - 15.45814523100853
  - 14.75234967470169
  - 14.917568802833557
  - 14.941409915685654
  - 14.953312188386917
  - 14.927699446678162
  - 15.038058340549469
  - 15.209989666938782
  - 15.173002004623413
  - 14.981047183275223
  - 14.720297574996948
  - 14.586978569626808
  - 15.11003902554512
  - 15.387057334184647
  - 15.276585340499878
  - 14.805326104164124
  - 14.918529570102692
  - 15.198387235403061
  - 15.096190959215164
  - 14.910231426358223
  - 15.244246035814285
  - 15.02186268568039
  - 14.84360545873642
  - 14.40580703318119
  - 15.233178496360779
  - 14.625122874975204
  - 15.332117974758148
  - 14.663475707173347
  - 14.944522321224213
  - 14.950840890407562
  - 14.996890872716904
  - 15.096482276916504
  - 14.581635445356369
  - 14.905234277248383
  - 14.732996761798859
  - 14.843818843364716
  - 15.521245002746582
  - 14.954874396324158
  - 15.290794610977173
  - 14.88354903459549
  - 14.621561974287033
  - 14.98511216044426
  - 15.094590663909912
  - 14.917732983827591
  - 14.706189513206482
  - 14.713336855173111
  - 14.764267027378082
  - 14.857316642999649
  - 14.90916720032692
  - 14.803507149219513
  - 14.879056692123413
  - 14.967776775360107
  - 14.841854840517044
  - 14.614554271101952
  - 14.14291426539421
  - 14.95040637254715
  - 14.634963393211365
  - 14.734281808137894
  - 14.651651546359062
  - 14.551933884620667
  - 14.561456367373466
  - 14.878852993249893
  - 14.359556525945663
  - 15.031713396310806
  - 14.962867856025696
  - 14.928069561719894
  - 14.710786819458008
  - 14.886178255081177
  - 14.307918846607208
  - 14.665745705366135
  - 14.820663571357727
  - 14.167756497859955
  - 14.585198387503624
  - 14.809215068817139
  - 14.62695261836052
  - 14.658094868063927
  - 14.625384777784348
  - 14.665037989616394
  - 14.665570229291916
  - 14.96431452035904
  - 14.888568133115768
  - 15.032076135277748
  - 14.69955013692379
  validation_losses:
  - 0.42248621582984924
  - 0.41359075903892517
  - 0.4153442978858948
  - 0.4161289632320404
  - 0.4497673511505127
  - 0.5223201513290405
  - 0.45790961384773254
  - 0.44632717967033386
  - 0.4789826571941376
  - 1.416440486907959
  - 4.37866735458374
  - 0.4606155753135681
  - 0.5608969926834106
  - 0.5490021109580994
  - 0.45104721188545227
  - 0.6149213910102844
  - 0.7015829086303711
  - 3.548529624938965
  - 2.7294163703918457
  - 0.43942341208457947
  - 0.4986550807952881
  - 0.6062681674957275
  - 0.6414269804954529
  - 0.7276726961135864
  - 0.49422261118888855
  - 0.41816237568855286
  - 0.4326245188713074
  - 0.8741490840911865
  - 1.2305928468704224
  - 0.40099626779556274
  - 0.4754810631275177
  - 0.9636496901512146
  - 0.5319303274154663
  - 0.5779233574867249
  - 0.5390212535858154
  - 0.5033374428749084
  - 0.4665697515010834
  - 0.5093315839767456
  - 0.3961920738220215
  - 0.5200666189193726
  - 0.5216906070709229
  - 0.4751957058906555
  - 0.5116007924079895
  - 0.4930324852466583
  - 0.6332362294197083
  - 0.46467891335487366
  - 0.48151808977127075
  - 0.4751843512058258
  - 1.8530126810073853
  - 0.5007908940315247
  - 2.65474796295166
  - 0.7623746395111084
  - 0.5275658965110779
  - 2.7991783618927
  - 3.9161312580108643
  - 0.4586077034473419
  - 0.546634316444397
  - 0.5402204990386963
  - 0.6693753004074097
  - 0.5109241604804993
  - 0.7721816897392273
  - 0.5740090012550354
  - 0.9233745336532593
  - 0.5975279211997986
  - 0.5856434106826782
  - 0.4801902770996094
  - 0.4803410470485687
  - 0.5051416754722595
  - 0.44521042704582214
  - 0.5574219226837158
  - 0.5557606220245361
  - 0.5855669379234314
  - 0.5012228488922119
  - 0.5844800472259521
  - 0.5462605953216553
  - 0.4059874415397644
  - 0.7217133045196533
  - 0.6011894941329956
  - 0.4294596314430237
  - 0.4916207194328308
  - 0.611851155757904
  - 0.603817343711853
  - 1.0536116361618042
  - 0.5574412941932678
  - 0.6066907644271851
  - 0.5471838712692261
  - 0.7730445265769958
  - 2.2910332679748535
  - 1.0721489191055298
  - 0.6726425290107727
  - 0.5431623458862305
  - 0.4812542498111725
  - 0.5922372937202454
  - 0.6085421442985535
  - 0.5924196243286133
  - 0.5615213513374329
  - 0.4821145236492157
  - 0.6228022575378418
  - 0.6156921982765198
  - 0.4574068486690521
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 32 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 71 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 70 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8576329331046312,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.04597701149425288, 0.046511627906976744]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.018497727880245923
  total_train_time: '0:07:28.291055'
