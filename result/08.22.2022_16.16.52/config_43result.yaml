config:
  aggregation: sum
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 17:05:18.819746'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_43fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_43fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_43fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_43fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_43fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_43fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_43fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_43fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_43fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_43fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 18.6659068107605
  - 7.064216876029969
  - 9.384355592727662
  - 3.8266014933586123
  - 3.1100023269653323
  - 2.940148425102234
  - 2.7664195537567142
  - 1.77538321018219
  - 1.5024132847785951
  - 1.942278289794922
  - 2.531191647052765
  - 4.61902596950531
  - 3.540451264381409
  - 3.758368539810181
  - 1.2667672574520112
  - 1.8518114686012268
  - 1.713248845934868
  - 2.235216510295868
  - 1.4407694756984712
  - 1.5127316296100617
  - 1.2572048723697664
  - 1.1623718440532684
  - 2.7923738300800327
  - 1.7904851973056795
  - 2.267969661951065
  - 1.2187458604574204
  - 1.2502479612827302
  - 1.6060328960418702
  - 1.5262932717800142
  - 1.0900260627269744
  - 1.0376986384391784
  - 1.1573132276535034
  - 1.4122259616851807
  - 1.0046451330184938
  - 1.1291292369365693
  - 1.9202412724494935
  - 1.518500167131424
  - 2.363307535648346
  - 1.2316465198993685
  - 0.8543972551822663
  - 0.9373666286468506
  - 0.927025979757309
  - 3.6278989911079407
  - 0.9318079411983491
  - 1.3001606881618502
  - 0.9436126589775086
  - 0.9367712616920472
  - 0.8128950715065003
  - 0.8209588944911957
  - 0.8594918608665467
  - 0.9096946835517884
  - 0.8205529391765595
  - 0.8410878539085389
  - 0.7615102112293244
  - 0.8043544650077821
  - 1.0691731691360473
  - 0.9146965980529785
  - 0.8756375789642334
  - 0.7903112292289735
  - 0.8029393255710602
  - 0.7746588438749313
  - 1.1789161264896393
  - 0.8147902667522431
  - 0.9536362886428833
  - 0.9567094087600708
  - 1.0358381748199463
  - 1.1973228335380555
  - 1.1283174753189087
  - 2.3772740185260774
  - 0.9085666000843049
  - 1.0908058643341065
  - 0.865397322177887
  - 1.2285201191902162
  - 0.8884602427482605
  - 0.8575300097465516
  - 0.9696568369865418
  - 0.8875187337398529
  - 0.8192470192909241
  - 0.8037102401256562
  - 0.7409816890954972
  - 0.7649121701717377
  - 0.81673544049263
  - 0.8272086501121522
  - 0.7652610272169114
  - 0.8471335113048554
  - 0.8262584209442139
  - 0.8058818995952607
  - 0.8071113407611847
  - 0.7874650418758393
  - 0.790375143289566
  - 0.8014492452144624
  - 0.767681211233139
  - 1.3334523499011994
  - 1.535049730539322
  - 1.191653060913086
  - 0.9178730428218842
  - 1.1081517606973648
  - 1.041705471277237
  - 1.012522214651108
  - 0.9720076143741608
  validation_losses:
  - 5.137269973754883
  - 1.8373198509216309
  - 7.416326522827148
  - 0.6974155306816101
  - 1.7834532260894775
  - 0.9394702911376953
  - 1.0876655578613281
  - 0.6854062676429749
  - 0.7222721576690674
  - 0.9212071895599365
  - 5.9269490242004395
  - 2.053584098815918
  - 1.0522618293762207
  - 0.47231024503707886
  - 0.5749610662460327
  - 0.6772036552429199
  - 1.458744764328003
  - 0.9874345064163208
  - 0.7653833031654358
  - 0.5804769396781921
  - 1.6498832702636719
  - 0.4566406011581421
  - 0.5274049043655396
  - 0.9003422856330872
  - 0.5813403129577637
  - 0.4966931641101837
  - 0.6561235785484314
  - 0.5895830988883972
  - 0.5455644726753235
  - 0.46899116039276123
  - 0.4167405068874359
  - 0.7176564335823059
  - 0.4881301522254944
  - 0.4064605236053467
  - 0.605811357498169
  - 0.7493883967399597
  - 0.4072783291339874
  - 0.47883284091949463
  - 0.38477128744125366
  - 0.38299596309661865
  - 0.42219892144203186
  - 0.38955023884773254
  - 0.39448970556259155
  - 0.4754009544849396
  - 0.4034266769886017
  - 0.3964445888996124
  - 0.4052151143550873
  - 0.4077851176261902
  - 0.39552637934684753
  - 0.47394275665283203
  - 0.39238178730010986
  - 0.3925936818122864
  - 0.4331451654434204
  - 0.3868749439716339
  - 0.40364906191825867
  - 0.39370235800743103
  - 0.5970786809921265
  - 0.39709970355033875
  - 0.3825100362300873
  - 0.4073987603187561
  - 0.382425457239151
  - 0.45232170820236206
  - 0.4037317633628845
  - 0.38776594400405884
  - 0.8403287529945374
  - 0.42258644104003906
  - 0.7497802972793579
  - 0.43561017513275146
  - 0.42446768283843994
  - 0.4037821590900421
  - 0.40065133571624756
  - 0.38838136196136475
  - 0.4461074769496918
  - 0.4020775556564331
  - 0.3810122013092041
  - 0.39355093240737915
  - 0.4058399796485901
  - 0.3945724666118622
  - 0.38354790210723877
  - 0.38508301973342896
  - 0.4099000096321106
  - 0.3860003650188446
  - 0.39075931906700134
  - 0.3987095057964325
  - 0.3813329041004181
  - 0.4866507947444916
  - 0.41701292991638184
  - 0.468357115983963
  - 0.4546669125556946
  - 0.4036681652069092
  - 0.382764607667923
  - 0.3844355344772339
  - 2.54443359375
  - 0.416225403547287
  - 0.4824122488498688
  - 0.42842793464660645
  - 0.3966628313064575
  - 0.3931623697280884
  - 0.3849824368953705
  - 0.38963788747787476
loss_records_fold1:
  train_losses:
  - 1.4453084945678711
  - 0.9055068373680115
  - 0.8436453104019166
  - 0.797561800479889
  - 0.8137490868568421
  - 0.7455920875072479
  - 0.8277317523956299
  - 0.8127439677715302
  - 0.7512872934341431
  - 0.797917115688324
  - 0.7529033303260804
  - 1.2321309089660646
  - 1.5241963088512422
  - 5.310610610246659
  - 1.0439509212970735
  - 1.1704525470733642
  - 0.9316558241844177
  - 0.9800231039524079
  - 0.9021348655223846
  - 0.8684406459331513
  - 0.80186627805233
  - 0.7643534183502197
  - 0.746720039844513
  - 2.432243448495865
  - 0.9654339849948883
  - 0.8310591191053391
  - 0.9089101821184159
  - 2.928014147281647
  - 0.9200199902057649
  - 0.8924230694770814
  - 1.5527195930480957
  validation_losses:
  - 0.4889105558395386
  - 0.41563042998313904
  - 0.40114450454711914
  - 0.4188568592071533
  - 0.3957807421684265
  - 0.4168083667755127
  - 0.4037274122238159
  - 0.4162678122520447
  - 0.40361881256103516
  - 0.4002983570098877
  - 0.44861122965812683
  - 1.1051733493804932
  - 0.4021274447441101
  - 0.47578752040863037
  - 0.65800541639328
  - 0.42584675550460815
  - 0.4318746030330658
  - 0.4678550064563751
  - 0.44220486283302307
  - 0.40186333656311035
  - 0.41044893860816956
  - 0.3924437165260315
  - 0.39590761065483093
  - 0.41770678758621216
  - 0.44021666049957275
  - 0.400112509727478
  - 0.39225924015045166
  - 0.3964536786079407
  - 0.401626855134964
  - 0.39150896668434143
  - 0.3912346363067627
loss_records_fold2:
  train_losses:
  - 0.8111786246299744
  - 2.2777372360229493
  - 0.915882334113121
  - 0.8215467095375062
  - 0.8279659569263459
  - 2.3320025980472567
  - 0.9268232464790345
  - 0.7469677954912186
  - 0.8555092215538025
  - 1.4590948104858399
  - 2.134078621864319
  - 2.436782401800156
  - 1.0003731191158296
  - 1.2508214950561525
  - 4.342902481555939
  - 1.1120468378067017
  - 0.9504258334636688
  - 1.0567263066768646
  - 0.8629924237728119
  - 0.8732004582881928
  - 0.8052332997322083
  - 0.820796275138855
  - 0.932396149635315
  - 0.9059544444084168
  - 1.3365043342113496
  - 1.0632535874843598
  - 0.7378068208694458
  - 0.7843054831027985
  - 0.8021955311298371
  - 0.840809041261673
  - 0.8445870637893678
  - 0.9493786215782166
  - 2.302516907453537
  - 1.0134264171123506
  - 2.5728632390499118
  - 0.864019113779068
  - 0.8678395748138428
  - 0.8255985140800477
  - 0.7986944228410722
  - 0.8146856367588043
  - 0.8689014494419098
  - 0.7902577549219132
  - 0.9149882674217225
  - 0.8116735816001892
  - 0.8171076595783234
  - 0.8172772467136383
  - 1.3372273564338686
  - 0.7723016411066056
  - 0.7885128617286683
  - 0.9249919533729554
  - 0.7905627965927124
  - 0.8004493534564973
  - 0.812793654203415
  - 0.9997741341590882
  - 0.8866999804973603
  - 0.8213755130767822
  - 0.8187576532363892
  - 0.880148708820343
  - 0.9175443887710572
  - 0.8133849203586578
  - 0.7868909120559693
  - 0.8053231239318848
  - 0.8021017968654633
  validation_losses:
  - 0.3733866810798645
  - 0.4302311837673187
  - 0.3728349506855011
  - 0.37642598152160645
  - 0.37309566140174866
  - 0.5311033129692078
  - 0.3819918930530548
  - 0.37597647309303284
  - 0.3844611644744873
  - 0.6642534732818604
  - 0.4589751660823822
  - 0.3782806098461151
  - 0.4420608580112457
  - 0.37337902188301086
  - 0.3755802512168884
  - 0.43695518374443054
  - 0.4745582938194275
  - 0.39106255769729614
  - 0.4473007619380951
  - 0.43460673093795776
  - 0.38075757026672363
  - 0.442623496055603
  - 0.3737756013870239
  - 0.4078845679759979
  - 0.3798684775829315
  - 0.4022276997566223
  - 0.38875460624694824
  - 0.3956288993358612
  - 0.3892779052257538
  - 0.37815433740615845
  - 0.45703262090682983
  - 0.40342679619789124
  - 0.38215699791908264
  - 0.9998156428337097
  - 2.1150946617126465
  - 0.4277418553829193
  - 0.4355495572090149
  - 0.45485377311706543
  - 0.39041632413864136
  - 0.3765105605125427
  - 0.38363194465637207
  - 0.3772229552268982
  - 0.3903171718120575
  - 0.42854800820350647
  - 0.3746187090873718
  - 0.37454381585121155
  - 0.3774717152118683
  - 0.38495492935180664
  - 0.4297237694263458
  - 0.41331788897514343
  - 0.38950517773628235
  - 0.3728716969490051
  - 0.39601629972457886
  - 0.3810557723045349
  - 0.44982755184173584
  - 0.4147814214229584
  - 0.461301326751709
  - 0.4405914545059204
  - 0.3859530985355377
  - 0.37957528233528137
  - 0.380929559469223
  - 0.3790549039840698
  - 0.3742515444755554
loss_records_fold3:
  train_losses:
  - 0.8540345788002015
  - 0.8081219255924226
  - 0.8787108421325684
  - 0.7911245405673981
  - 0.7664636075496674
  - 0.8326047539710999
  - 0.7970944702625276
  - 0.8184842646121979
  - 0.8222875118255616
  - 0.7689116537570954
  - 0.773388558626175
  - 0.803477817773819
  - 0.7611933529376984
  - 0.7357365012168885
  - 0.791876918077469
  - 0.7886859774589539
  - 0.7401543408632278
  - 0.8128879070281982
  - 1.4252374708652498
  - 0.8531190395355225
  - 0.8345693588256836
  - 0.8430163562297821
  - 0.8132564544677735
  - 1.240672218799591
  - 1.2649823725223541
  - 0.754887530207634
  - 0.8815667629241943
  - 0.813384735584259
  - 0.9155393719673157
  - 0.8899006068706513
  - 0.8316657304763795
  - 0.7975585222244264
  - 0.7750740289688111
  - 0.7750903367996216
  - 0.8496460974216462
  - 1.983721935749054
  validation_losses:
  - 0.38178354501724243
  - 0.4142096936702728
  - 0.38213860988616943
  - 0.38833796977996826
  - 0.3918723464012146
  - 0.4176211953163147
  - 0.38613829016685486
  - 0.4148712456226349
  - 0.38471096754074097
  - 0.3826523423194885
  - 0.38286682963371277
  - 0.4188937246799469
  - 0.38386037945747375
  - 0.38432687520980835
  - 0.3874163329601288
  - 0.3803093731403351
  - 0.39491480588912964
  - 0.3854503333568573
  - 0.40723636746406555
  - 0.3807697892189026
  - 0.4180174171924591
  - 0.38542529940605164
  - 0.37942934036254883
  - 0.39997461438179016
  - 0.3804764151573181
  - 0.39025458693504333
  - 0.382070928812027
  - 0.4213947057723999
  - 0.3804667294025421
  - 0.40497666597366333
  - 0.3855917453765869
  - 0.379636287689209
  - 0.38165849447250366
  - 0.3836526870727539
  - 0.38208436965942383
  - 0.378052294254303
loss_records_fold4:
  train_losses:
  - 0.7575845539569855
  - 0.7741667211055756
  - 0.7324552088975906
  - 0.7743147432804108
  - 0.733640667796135
  - 0.7763513743877412
  - 0.8233570158481598
  - 0.8170516669750214
  - 0.8183537721633911
  - 0.752389943599701
  - 0.7550551235675812
  - 0.8118525862693787
  validation_losses:
  - 0.3888057470321655
  - 0.38505876064300537
  - 0.3810316324234009
  - 0.37916189432144165
  - 0.37856343388557434
  - 0.41825586557388306
  - 0.3889113962650299
  - 0.387886643409729
  - 0.38655468821525574
  - 0.38118913769721985
  - 0.38124698400497437
  - 0.38900044560432434
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 31 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 63 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 36 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:20:04.587563'
