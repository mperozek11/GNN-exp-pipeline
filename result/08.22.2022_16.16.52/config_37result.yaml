config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:58:15.548721'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_37fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_37fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_37fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_37fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_37fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_37fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_37fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_37fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_37fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_37fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.4743309319019318
  - 3.327978438138962
  - 3.287383306026459
  - 3.258999738097191
  - 3.1205779105424885
  - 3.1851593136787417
  - 3.2626490116119387
  - 3.1455970734357837
  - 3.0284723192453384
  - 2.9796147584915165
  - 3.066476446390152
  - 3.158872586488724
  - 3.2153246045112613
  - 3.01042457818985
  - 3.0076457351446155
  - 3.1488796055316928
  - 3.2654386043548587
  - 3.177075815200806
  - 3.119863682985306
  - 3.0893346190452577
  - 3.0404656141996385
  - 3.0347316563129425
  - 2.9849019646644592
  - 3.002468079328537
  validation_losses:
  - 0.48648393154144287
  - 0.4177502691745758
  - 0.4646185636520386
  - 0.41802290081977844
  - 0.4039872884750366
  - 0.39952322840690613
  - 0.39805901050567627
  - 0.4001556634902954
  - 0.3955681622028351
  - 0.40054604411125183
  - 0.4253593385219574
  - 0.39510825276374817
  - 0.39978495240211487
  - 0.4213597774505615
  - 0.38674792647361755
  - 0.4129401743412018
  - 0.39095813035964966
  - 0.41365739703178406
  - 0.40321287512779236
  - 0.405839204788208
  - 0.40762951970100403
  - 0.38711366057395935
  - 0.39213550090789795
  - 0.3898667097091675
loss_records_fold1:
  train_losses:
  - 3.053145903348923
  - 3.022888094186783
  - 3.06232203245163
  - 3.0698023021221164
  - 2.955595701932907
  - 3.007127732038498
  - 2.9891298651695255
  - 3.02151997089386
  - 3.0259780645370484
  - 3.0059266626834873
  - 2.9992625504732136
  - 2.9775453835725787
  - 2.922575014829636
  - 2.991470301151276
  - 3.0137824028730393
  - 2.9404845714569094
  - 2.9792333245277405
  - 2.936211717128754
  - 3.021843945980072
  - 2.9719739198684696
  - 3.035771676898003
  - 2.9623310923576356
  - 2.955578210949898
  - 2.939047709107399
  - 2.9891119599342346
  - 3.0064120560884477
  - 3.004796320199967
  - 2.9394481599330904
  - 2.953231739997864
  - 2.955875664949417
  - 2.951430916786194
  - 2.912557312846184
  - 2.9501797765493394
  - 2.9749577522277835
  - 2.955329379439354
  - 2.96536300778389
  - 2.9371896326541904
  - 2.948730653524399
  - 2.9957752227783203
  - 2.9569612920284274
  - 3.0111852526664737
  - 2.9264291137456895
  - 2.8847600281238557
  - 2.939133578538895
  - 3.0091713249683383
  - 2.9064418971538544
  - 3.014367270469666
  - 3.011703333258629
  - 2.9043167322874073
  - 3.0017566800117494
  - 2.9508745759725574
  - 2.9369439065456393
  - 2.9527153968811035
  - 2.971705120801926
  - 2.828397634625435
  - 2.900878527760506
  - 2.938422954082489
  - 2.9523198008537292
  - 2.874192249774933
  - 2.9409006595611573
  - 2.9767205119132996
  - 2.919799542427063
  - 3.035702565312386
  - 2.819984892010689
  validation_losses:
  - 0.3927134573459625
  - 0.39148610830307007
  - 0.45905163884162903
  - 0.40509095788002014
  - 0.4941699504852295
  - 0.6799383759498596
  - 0.387827068567276
  - 0.3904187083244324
  - 0.38964101672172546
  - 0.3842372000217438
  - 0.3865693509578705
  - 0.6947321891784668
  - 0.868153989315033
  - 0.38837406039237976
  - 0.39540114998817444
  - 0.40231820940971375
  - 0.3847154378890991
  - 0.3862372934818268
  - 0.40007734298706055
  - 0.3842613101005554
  - 0.3837903141975403
  - 0.4008350968360901
  - 0.3827134072780609
  - 0.39057254791259766
  - 0.40080180764198303
  - 0.4037349224090576
  - 0.5260442495346069
  - 0.41134336590766907
  - 0.42947107553482056
  - 0.8099178075790405
  - 0.9539549946784973
  - 0.4533397853374481
  - 0.4227387309074402
  - 0.4000686705112457
  - 0.5748997330665588
  - 0.534871518611908
  - 0.8214619159698486
  - 0.39367416501045227
  - 0.3875521719455719
  - 0.40460091829299927
  - 0.38408467173576355
  - 0.3848974108695984
  - 0.3789849877357483
  - 0.48982545733451843
  - 0.3906347155570984
  - 0.3906203806400299
  - 0.3838992416858673
  - 0.4256942868232727
  - 0.45779457688331604
  - 1.324297547340393
  - 1.2754771709442139
  - 0.5793893337249756
  - 0.3928478956222534
  - 0.6051497459411621
  - 1.0840857028961182
  - 0.39475294947624207
  - 0.45485955476760864
  - 0.558591902256012
  - 0.5087354779243469
  - 0.4276660084724426
  - 0.38671034574508667
  - 0.3887942433357239
  - 0.3894084393978119
  - 0.38213399052619934
loss_records_fold2:
  train_losses:
  - 2.9496400356292725
  - 2.9653853654861453
  - 2.978437387943268
  - 2.964285555481911
  - 2.9221801400184635
  - 2.9192737430334095
  - 2.9799968838691715
  - 2.9863238811492923
  - 2.913636475801468
  - 2.961902713775635
  - 2.920939320325852
  - 2.926492190361023
  - 2.8837752491235733
  - 2.894345754384995
  - 2.9370893061161043
  - 2.931167659163475
  - 2.981982371211052
  - 2.9716825366020205
  - 2.8459190607070926
  - 2.888726145029068
  - 2.8871015042066577
  - 2.948388317227364
  - 2.895509511232376
  - 3.0108248710632326
  - 2.945760315656662
  - 3.0165048420429232
  - 2.940856891870499
  - 2.909715807437897
  - 2.8826372742652895
  - 2.889746806025505
  - 2.88275700211525
  - 3.009272411465645
  - 2.9317850470542908
  - 2.901656538248062
  - 2.8092714309692384
  - 2.9260101646184924
  - 2.947821962833405
  - 2.9595269858837128
  - 2.874659386277199
  - 2.8530534625053408
  - 2.8933912813663483
  - 2.9238278716802597
  - 2.880834007263184
  - 2.8640873551368715
  - 2.9535865962505343
  - 2.8077151209115985
  - 2.8563940763473514
  - 2.937756508588791
  - 2.9774595201015472
  - 3.0026165544986725
  - 3.0949486017227175
  - 3.015973746776581
  - 3.049157825112343
  - 2.9655608952045442
  - 2.962987017631531
  - 2.8892343044281006
  - 2.884615954756737
  - 2.970198625326157
  - 2.911885464191437
  - 2.9090153336524964
  - 2.8800522059202196
  - 2.917522495985031
  - 2.8520252764225007
  - 2.8505428940057755
  - 2.9438521146774295
  - 2.918377935886383
  - 2.9164921879768375
  - 2.89762806892395
  - 3.0345457673072818
  - 2.9186447948217396
  - 2.8971752315759662
  - 2.909066599607468
  - 2.865043556690216
  - 2.921306657791138
  - 3.02560133934021
  - 3.0047778248786927
  - 2.9173240065574646
  - 2.9873101592063906
  - 2.971244114637375
  - 2.8905891299247743
  - 2.9107410550117496
  - 2.8447689145803454
  - 2.913100591301918
  - 2.9337182343006134
  - 2.8232144474983216
  - 2.881972917914391
  - 2.8899885356426243
  - 2.91187037229538
  - 2.95080481171608
  - 2.9393984854221347
  - 2.9063193172216417
  - 2.8625298678874973
  - 2.8714323520660403
  - 2.908565962314606
  - 2.8353030622005466
  - 2.8984733104705813
  - 2.92007229924202
  - 2.9816199719905856
  - 2.949465125799179
  - 2.8883160412311555
  validation_losses:
  - 0.39828869700431824
  - 0.37445691227912903
  - 0.4252834916114807
  - 0.3883737623691559
  - 0.39083364605903625
  - 0.4145727753639221
  - 0.40448108315467834
  - 0.3849736750125885
  - 0.38203343749046326
  - 0.39657384157180786
  - 0.9271141886711121
  - 0.4661005735397339
  - 0.3831106722354889
  - 0.49569961428642273
  - 0.9365718364715576
  - 0.3845418095588684
  - 0.5628058910369873
  - 0.3822217285633087
  - 0.4324997067451477
  - 0.4383144974708557
  - 0.4037511348724365
  - 0.4082307517528534
  - 0.7169064283370972
  - 0.48800891637802124
  - 0.47340360283851624
  - 0.385932058095932
  - 0.5253822803497314
  - 0.4950897693634033
  - 0.9958053231239319
  - 1.579077124595642
  - 2.0204381942749023
  - 0.443805068731308
  - 0.8061830401420593
  - 1.0987590551376343
  - 0.9968335032463074
  - 1.1131640672683716
  - 0.8904709219932556
  - 0.7620833516120911
  - 0.39546847343444824
  - 0.4522779583930969
  - 1.9913512468338013
  - 0.8682678937911987
  - 0.5358039140701294
  - 0.5476769804954529
  - 0.6071045994758606
  - 0.5877506136894226
  - 0.761394739151001
  - 0.404478520154953
  - 0.6052020192146301
  - 1.0705779790878296
  - 0.567194938659668
  - 0.4952068030834198
  - 0.4273882210254669
  - 0.3913028836250305
  - 0.3982508182525635
  - 0.42174413800239563
  - 0.4370799958705902
  - 0.40569791197776794
  - 0.4303712248802185
  - 0.4271808862686157
  - 0.4115516245365143
  - 0.39815688133239746
  - 0.48420965671539307
  - 3.8707492351531982
  - 0.39039674401283264
  - 0.38285255432128906
  - 0.4570809006690979
  - 0.48460984230041504
  - 1.1194539070129395
  - 0.48843616247177124
  - 0.4794538915157318
  - 0.5179285407066345
  - 0.4345090389251709
  - 0.4539684057235718
  - 0.38928595185279846
  - 0.4342382550239563
  - 0.4576874077320099
  - 0.6839867830276489
  - 0.6123607158660889
  - 0.38828662037849426
  - 0.3928683400154114
  - 0.6536282300949097
  - 0.5252865552902222
  - 0.608891487121582
  - 1.2530384063720703
  - 0.438963383436203
  - 0.46188655495643616
  - 0.39156731963157654
  - 0.405322402715683
  - 0.43943655490875244
  - 0.7907398343086243
  - 0.43150606751441956
  - 0.6574835181236267
  - 0.8585708737373352
  - 0.7873978018760681
  - 1.3618158102035522
  - 2.515414237976074
  - 0.5695868730545044
  - 0.39340701699256897
  - 0.48006945848464966
loss_records_fold3:
  train_losses:
  - 2.9697509109973907
  - 2.9055244207382205
  - 2.9977056562900546
  - 2.963307696580887
  - 2.9483711242675783
  - 2.920759278535843
  - 2.98031080365181
  - 2.96464940905571
  - 2.934338629245758
  - 2.9343701720237734
  - 2.9426682472229007
  - 2.8430371642112733
  - 2.903371196985245
  - 2.9186139881610873
  - 2.9468607664108277
  - 2.9987429797649385
  - 2.8811926394701004
  - 2.9787120133638383
  - 2.890329843759537
  - 2.8846251130104066
  - 2.914006280899048
  - 2.9907499253749847
  - 2.8739458829164506
  - 2.9460608065128326
  - 2.8871977210044864
  - 2.89753612279892
  - 2.885033887624741
  - 2.847549468278885
  - 2.887195473909378
  - 2.9679229050874714
  - 2.9414860844612125
  - 2.8648068070411683
  - 2.9233903855085375
  - 2.8751403570175174
  - 2.916351103782654
  - 2.9558764636516575
  - 2.929701268672943
  - 2.9674460709095003
  - 2.8812198579311374
  - 2.9104082703590395
  - 3.0381668090820315
  - 2.889535200595856
  - 2.921255946159363
  - 2.896867108345032
  - 2.908007073402405
  - 2.887918472290039
  - 2.8797338843345646
  - 2.928658449649811
  - 2.910182371735573
  - 2.9016685754060747
  - 2.9385549902915957
  - 2.8841805517673493
  - 2.936097872257233
  - 2.909775269031525
  - 2.8829347729682926
  - 2.944843062758446
  - 2.8800001174211505
  - 2.912759405374527
  - 2.9495849817991258
  - 2.8905844509601595
  - 2.9314907968044284
  - 2.878056502342224
  - 3.004460805654526
  - 3.0299863636493685
  - 2.9586727738380434
  - 2.957172131538391
  - 2.953872811794281
  - 2.9140479743480685
  - 2.9416277319192887
  - 2.9905707001686097
  - 2.923116436600685
  - 2.9404601216316224
  - 2.9060492873191834
  - 2.9363634884357452
  - 2.9263190627098083
  - 2.9842229306697847
  - 2.9420011729002002
  - 2.864901602268219
  - 2.8628963232040405
  - 2.866012600064278
  - 2.9426297008991242
  - 2.9239126384258274
  - 2.9007268965244295
  - 2.985290026664734
  - 2.901834583282471
  - 2.9111589044332504
  - 2.872948017716408
  - 2.8562897861003878
  - 2.9212986856698993
  - 2.865686345100403
  - 2.870779475569725
  - 2.882697689533234
  - 2.835165536403656
  - 2.9210587382316593
  - 2.8379438638687136
  - 2.9339627563953403
  - 2.8616202771663666
  - 2.9733973622322085
  - 2.9028822213411334
  - 3.013253921270371
  validation_losses:
  - 0.6562905311584473
  - 1.3325966596603394
  - 0.4675353765487671
  - 0.49257153272628784
  - 2.0560686588287354
  - 0.5895675420761108
  - 1.161866307258606
  - 0.6782726645469666
  - 0.851877748966217
  - 0.3761451244354248
  - 0.4553452432155609
  - 0.6110036969184875
  - 0.8161750435829163
  - 0.8798535466194153
  - 0.6414813995361328
  - 1.8710355758666992
  - 0.44573715329170227
  - 0.563928484916687
  - 0.672752320766449
  - 1.6954151391983032
  - 0.46259844303131104
  - 1.4701074361801147
  - 0.38699275255203247
  - 0.773511528968811
  - 0.99042809009552
  - 1.5281965732574463
  - 2.571335554122925
  - 1.7779074907302856
  - 3.374656915664673
  - 1.4792673587799072
  - 1.1029666662216187
  - 1.120916485786438
  - 1.4509098529815674
  - 0.3809051513671875
  - 0.39168983697891235
  - 0.641708254814148
  - 2.6126418113708496
  - 0.9889488816261292
  - 0.8323506712913513
  - 5.2249860763549805
  - 3.783351421356201
  - 2.726418972015381
  - 2.7665741443634033
  - 6.71355676651001
  - 4.258011341094971
  - 2.9232804775238037
  - 7.504668712615967
  - 5.2120890617370605
  - 0.8896658420562744
  - 0.3906342089176178
  - 0.39129573106765747
  - 1.2870664596557617
  - 1.1523185968399048
  - 0.6582537293434143
  - 3.518892288208008
  - 0.6932501196861267
  - 0.4764835238456726
  - 1.5541566610336304
  - 18.032142639160156
  - 0.9687795639038086
  - 0.4869982600212097
  - 0.4640783667564392
  - 0.5463917851448059
  - 0.5552614331245422
  - 0.8328680992126465
  - 0.7182297110557556
  - 0.6254245638847351
  - 0.48465695977211
  - 0.6063811779022217
  - 0.8399425745010376
  - 0.4362315237522125
  - 0.4605773687362671
  - 1.061110258102417
  - 0.4739192724227905
  - 0.48227086663246155
  - 0.4497203528881073
  - 0.5917459726333618
  - 0.790860116481781
  - 0.42509159445762634
  - 1.7944797277450562
  - 1.438510537147522
  - 0.5183448791503906
  - 25.09087562561035
  - 1.1272578239440918
  - 1.1626496315002441
  - 2.2076377868652344
  - 7.692816257476807
  - 0.7244846820831299
  - 2.2458086013793945
  - 1.1805460453033447
  - 0.4970056414604187
  - 1.1380224227905273
  - 0.40832051634788513
  - 1.8448941707611084
  - 3.157240390777588
  - 3.225569725036621
  - 14.848142623901367
  - 0.4578548073768616
  - 0.5928760766983032
  - 0.4358283579349518
loss_records_fold4:
  train_losses:
  - 2.90676389336586
  - 3.008571869134903
  - 2.91220001578331
  - 2.9445476055145265
  - 2.924295276403427
  - 2.8982732594013214
  - 2.945848059654236
  - 2.928064161539078
  - 2.9409023195505144
  - 2.9561900913715364
  - 2.8582731246948243
  - 2.933010214567185
  - 2.9267529249191284
  - 2.9648187875747682
  - 2.9081367671489717
  - 2.9091138631105427
  - 2.8736883729696276
  - 2.8557517528533936
  - 3.206764662265778
  - 2.930579960346222
  - 2.88243682384491
  - 2.9152165889739994
  - 2.9055703520774845
  - 2.876678162813187
  - 2.9058757543563845
  - 2.8707353979349137
  - 2.899540102481842
  - 2.839157405495644
  validation_losses:
  - 0.375938355922699
  - 0.39347806572914124
  - 0.40192711353302
  - 0.3905443549156189
  - 0.37349238991737366
  - 0.38016068935394287
  - 0.3623164892196655
  - 0.37166979908943176
  - 0.37581321597099304
  - 0.36738550662994385
  - 0.37991076707839966
  - 0.44576653838157654
  - 0.4298326373100281
  - 0.4553975760936737
  - 0.5538538098335266
  - 0.4780066907405853
  - 0.3692367672920227
  - 0.386944055557251
  - 0.3795621693134308
  - 0.3948814570903778
  - 0.41049158573150635
  - 0.5295242667198181
  - 0.5062882304191589
  - 0.419511616230011
  - 0.37973469495773315
  - 0.3728947937488556
  - 0.3616892099380493
  - 0.3693089783191681
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 64 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8524871355060034, 0.8490566037735849, 0.8576329331046312,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.044444444444444446, 0.02222222222222222, 0.0, 0.023809523809523808]'
  mean_eval_accuracy: 0.8551832269396945
  mean_f1_accuracy: 0.018095238095238095
  total_train_time: '0:29:46.076777'
