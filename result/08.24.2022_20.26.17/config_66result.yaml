config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:56:31.074112'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_66fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_66fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_66fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_66fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_66fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_66fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_66fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_66fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_66fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_66fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.710287892818451
  - 1.5479689359664919
  - 1.5043630063533784
  - 1.4814375162124636
  - 1.5666490435600282
  - 1.5073857545852662
  - 1.4985905468463898
  - 1.461993157863617
  - 1.5314059138298035
  - 1.4969439804553986
  - 1.5508095741271974
  - 1.5282959282398225
  - 1.457556113600731
  - 1.4410706102848054
  - 1.4689431488513947
  - 1.4946195721626283
  - 1.5241023361682893
  - 1.4871071875095367
  - 1.4342697679996492
  - 1.442820203304291
  - 1.48022877573967
  - 1.4652248919010162
  - 1.5055636525154115
  - 1.485056233406067
  - 1.417072057723999
  - 1.4395592987537384
  - 1.4917860507965088
  validation_losses:
  - 0.41381770372390747
  - 0.40771737694740295
  - 0.3953394293785095
  - 0.38686832785606384
  - 0.3944959044456482
  - 0.3940986096858978
  - 0.3991730809211731
  - 0.39058172702789307
  - 0.3874984085559845
  - 0.4252745509147644
  - 0.38445380330085754
  - 0.3858371078968048
  - 0.3845556676387787
  - 0.3840896487236023
  - 0.4034173786640167
  - 0.4022741913795471
  - 0.3983811140060425
  - 0.3805680274963379
  - 0.3793504536151886
  - 0.3794132173061371
  - 0.556012749671936
  - 0.3813174366950989
  - 0.38848134875297546
  - 0.38466617465019226
  - 0.38365304470062256
  - 0.38289982080459595
  - 0.3828986585140228
loss_records_fold1:
  train_losses:
  - 1.4785508632659914
  - 1.4432114720344544
  - 1.4680942714214327
  - 1.436615985631943
  - 1.3991592228412628
  - 1.4540666341781616
  - 1.4920033752918245
  - 1.4413947880268099
  - 1.4289835095405579
  - 1.4180156528949739
  - 1.4283724427223206
  - 1.4085018992424012
  - 1.4416418254375458
  - 1.423233824968338
  - 1.4828633010387422
  - 1.431649088859558
  - 1.431613802909851
  - 1.4056382179260254
  - 1.4129987716674806
  - 1.4172676026821138
  - 1.397968238592148
  - 1.4023976981639863
  - 1.4270147860050202
  - 1.4176928490400316
  - 1.4568990111351015
  - 1.4378450870513917
  - 1.417251640558243
  - 1.4331967294216157
  - 1.3723440289497377
  - 1.4758048057556152
  - 1.5036700129508973
  - 1.398920750617981
  - 1.3850815236568452
  - 1.4385492444038392
  - 1.4028116762638092
  - 1.3910763263702393
  - 1.4233766436576845
  - 1.4646248817443848
  - 1.4485460221767426
  - 1.3695217788219454
  - 1.388821852207184
  - 1.3973726511001587
  - 1.402654540538788
  - 1.3913103640079498
  - 1.394879323244095
  - 1.4074969708919527
  - 1.4179980278015138
  - 1.3542716950178146
  - 1.3885069131851198
  - 1.4195773959159852
  - 1.411825031042099
  - 1.417545622587204
  - 1.4258415877819062
  - 1.443017452955246
  - 1.3980477035045624
  - 1.3920047640800477
  - 1.4253451168537141
  - 1.3977263927459718
  - 1.4221296072006226
  - 1.4268498301506043
  - 1.3518869042396546
  - 1.3817901015281677
  - 1.3743588507175446
  - 1.3464362233877183
  - 1.3706915438175202
  - 1.4049801111221314
  - 1.3986850440502168
  - 1.390837574005127
  - 1.374284902215004
  - 1.3479926764965058
  - 1.3616537749767303
  - 1.361325591802597
  - 1.3765474557876587
  - 1.3739442229270935
  - 1.4017141938209534
  - 1.3318917244672777
  - 1.4272404670715333
  - 1.3822415471076965
  - 1.3898384630680085
  - 1.36333766579628
  - 1.3681495547294618
  - 1.3701590538024904
  - 1.3680522441864014
  - 1.357729858160019
  - 1.3771218419075013
  - 1.3488437354564669
  - 1.3767561435699465
  - 1.3917782336473465
  - 1.4014512747526169
  - 1.3376848340034486
  - 1.3436550498008728
  - 1.3466863453388216
  - 1.3595953822135927
  - 1.3536967456340792
  - 1.354952257871628
  - 1.3776012420654298
  - 1.3938224852085115
  - 1.3699187099933625
  - 1.3544852852821352
  - 1.3553935050964356
  validation_losses:
  - 0.3977033793926239
  - 0.38825875520706177
  - 0.39316555857658386
  - 0.3922163248062134
  - 0.3908974826335907
  - 0.4028586447238922
  - 0.4138123393058777
  - 0.38365137577056885
  - 0.39289340376853943
  - 0.4616841673851013
  - 0.38057342171669006
  - 0.3896508514881134
  - 0.7456509470939636
  - 0.506971538066864
  - 0.39762619137763977
  - 0.3836768865585327
  - 0.39431941509246826
  - 0.4137987792491913
  - 0.6310264468193054
  - 1.3703128099441528
  - 0.5885899662971497
  - 0.6327964067459106
  - 0.38252997398376465
  - 0.3835676610469818
  - 0.43499261140823364
  - 0.391892671585083
  - 0.44624489545822144
  - 0.6118376851081848
  - 1.222113847732544
  - 0.3909718990325928
  - 0.41400644183158875
  - 0.5215977430343628
  - 0.7980523705482483
  - 0.42349934577941895
  - 0.3966938853263855
  - 0.5020909905433655
  - 0.545441746711731
  - 0.5698579549789429
  - 0.77701735496521
  - 0.5133668184280396
  - 0.7553734183311462
  - 0.7464635968208313
  - 0.4764424264431
  - 0.5065470337867737
  - 0.6651784777641296
  - 0.6362602114677429
  - 0.7071312665939331
  - 0.5337830781936646
  - 0.43034103512763977
  - 0.5287843346595764
  - 0.39373910427093506
  - 0.47446513175964355
  - 0.5573699474334717
  - 0.6370697021484375
  - 0.4563010334968567
  - 0.6487809419631958
  - 0.6629630327224731
  - 0.5354719161987305
  - 1.0076240301132202
  - 0.42316651344299316
  - 0.6786635518074036
  - 0.814637303352356
  - 0.7206136584281921
  - 0.6324294805526733
  - 0.7101750373840332
  - 0.7277100086212158
  - 0.5919398665428162
  - 0.4776253402233124
  - 0.7390106916427612
  - 1.0186315774917603
  - 0.6374217867851257
  - 1.1691168546676636
  - 0.8732067346572876
  - 0.8786229491233826
  - 1.1080796718597412
  - 1.2565428018569946
  - 0.7426439523696899
  - 0.7305499315261841
  - 0.7740190029144287
  - 0.6541571021080017
  - 0.5502527952194214
  - 0.48565927147865295
  - 1.1697394847869873
  - 1.2265444993972778
  - 0.8901865482330322
  - 0.5602849721908569
  - 0.8021355867385864
  - 0.598738968372345
  - 0.7586832642555237
  - 0.6845033764839172
  - 0.7027838826179504
  - 0.851423442363739
  - 0.6382601261138916
  - 0.5417835116386414
  - 0.9444835782051086
  - 1.0111223459243774
  - 0.8455479145050049
  - 0.6533137559890747
  - 1.1048718690872192
  - 1.0552572011947632
loss_records_fold2:
  train_losses:
  - 1.3583922803401949
  - 1.331626468896866
  - 1.3683241665363313
  - 1.391487354040146
  - 1.363108903169632
  - 1.4038812637329103
  - 1.425716543197632
  - 1.4597553372383119
  - 1.4623431861400604
  - 1.4188183546066284
  - 1.3920355916023255
  - 1.3731497824192047
  - 1.3775557816028596
  - 1.4065528929233553
  - 1.3662392854690553
  - 1.3531967818737032
  - 1.3343194782733918
  - 1.348149371147156
  - 1.3640139222145082
  - 1.3603375136852265
  - 1.3647867143154144
  - 1.3467776715755464
  - 1.3584778219461442
  - 1.3507149308919908
  - 1.3487130582332612
  - 1.3820814907550814
  - 1.374678835272789
  - 1.42912717461586
  - 1.3651603043079377
  - 1.3726907670497894
  - 1.38245387673378
  - 1.380374324321747
  - 1.375595885515213
  - 1.3372501850128176
  - 1.3752441525459291
  - 1.3347267985343934
  - 1.3569809556007386
  - 1.3648574352264404
  - 1.3343642830848694
  - 1.366565251350403
  - 1.3427423238754272
  - 1.3860051035881042
  - 1.3848682433366777
  - 1.3915589183568955
  - 1.3725530445575715
  - 1.3641677021980287
  - 1.3566109597682954
  - 1.3094058185815811
  - 1.3492009282112123
  - 1.3467508077621462
  - 1.370374244451523
  - 1.3388611048460008
  - 1.3552055835723877
  - 1.3587602853775025
  - 1.3790943861007692
  - 1.3378970921039581
  - 1.3001773804426193
  - 1.3611163794994354
  - 1.3167608380317688
  - 1.348165848851204
  - 1.3558309614658357
  - 1.3437857627868652
  - 1.308666342496872
  - 1.326490977406502
  - 1.3639711141586304
  - 1.3460144460201264
  - 1.334414929151535
  - 1.3898962438106537
  - 1.4041721045970919
  - 1.4306010365486146
  - 1.3968868374824526
  - 1.3469178974628448
  - 1.3106942772865295
  - 1.351732212305069
  - 1.3404907226562501
  - 1.40259850025177
  - 1.3516972720623017
  - 1.3432812213897707
  - 1.343881079554558
  - 1.3673124372959138
  - 1.3380268812179565
  - 1.3383643150329592
  - 1.337989765405655
  - 1.355784147977829
  - 1.314064794778824
  - 1.3260887295007706
  - 1.3195117563009262
  - 1.3443580746650696
  - 1.329785180091858
  - 1.3403188526630403
  - 1.358913314342499
  - 1.3239315509796143
  - 1.3259071052074434
  - 1.324731057882309
  - 1.3423321664333345
  - 1.3319549679756166
  - 1.3403051733970643
  - 1.3451548397541047
  - 1.3559213399887087
  - 1.348283624649048
  validation_losses:
  - 1.1151347160339355
  - 0.758563756942749
  - 1.1902722120285034
  - 0.5380544662475586
  - 0.9808018207550049
  - 0.724186360836029
  - 1.0543198585510254
  - 0.7396609783172607
  - 0.9258056879043579
  - 0.5061588883399963
  - 0.4221119284629822
  - 0.4914146661758423
  - 0.7044957280158997
  - 0.5183252692222595
  - 0.5929978489875793
  - 0.5292536616325378
  - 0.7766520977020264
  - 0.5344808101654053
  - 0.6093562841415405
  - 0.38552576303482056
  - 0.754479169845581
  - 0.7508281469345093
  - 1.068938136100769
  - 0.9589667916297913
  - 0.5568253993988037
  - 1.1806652545928955
  - 2.278352975845337
  - 1.2962678670883179
  - 0.7767555117607117
  - 0.5018588900566101
  - 0.4124014377593994
  - 0.6091791391372681
  - 0.8911903500556946
  - 0.5689136981964111
  - 0.8186207413673401
  - 0.7555223703384399
  - 3.526095390319824
  - 0.5788487792015076
  - 2.1125848293304443
  - 0.41694948077201843
  - 0.46242716908454895
  - 0.5063958168029785
  - 0.4237533211708069
  - 0.5584737062454224
  - 0.8603072166442871
  - 0.5578551292419434
  - 0.5052685141563416
  - 0.6541592478752136
  - 0.7632972002029419
  - 0.6177462339401245
  - 0.4916616380214691
  - 0.6103837490081787
  - 0.48339980840682983
  - 0.5329797267913818
  - 0.676425039768219
  - 0.5012783408164978
  - 0.9633514881134033
  - 1.0439691543579102
  - 3.2190911769866943
  - 0.4825991690158844
  - 0.6551363468170166
  - 0.6115903854370117
  - 0.70768141746521
  - 0.7076423764228821
  - 0.6899238228797913
  - 0.788304328918457
  - 0.5452826619148254
  - 0.7301106452941895
  - 0.593121349811554
  - 0.6236482262611389
  - 0.6541216969490051
  - 0.5629149675369263
  - 0.45113101601600647
  - 0.49548304080963135
  - 0.5741597414016724
  - 0.40604180097579956
  - 0.6755358576774597
  - 0.7882940769195557
  - 0.5272054076194763
  - 0.5683249235153198
  - 0.6471965909004211
  - 0.7743479013442993
  - 0.60001540184021
  - 0.7410343289375305
  - 0.5719889998435974
  - 0.7146759629249573
  - 0.4677174389362335
  - 0.9026974439620972
  - 0.7293684482574463
  - 0.7371299266815186
  - 0.6258827447891235
  - 0.6502416729927063
  - 0.7878627777099609
  - 0.5153478980064392
  - 0.5793391466140747
  - 0.7577483057975769
  - 0.6486467719078064
  - 0.5896461009979248
  - 0.8650661706924438
  - 0.6707853078842163
loss_records_fold3:
  train_losses:
  - 1.3611934244632722
  - 1.3741925776004793
  - 1.3574724674224854
  - 1.451958853006363
  - 1.4519507706165315
  - 1.408327603340149
  - 1.3885294795036316
  - 1.367982029914856
  - 1.3576163828372956
  - 1.3342626720666886
  - 1.3346914201974869
  - 1.397922080755234
  - 1.3674934536218644
  - 1.3121117413043977
  - 1.3352789759635926
  - 1.3043190807104112
  - 1.4571438968181611
  - 1.3433254301548005
  - 1.3495297849178316
  - 1.3592687487602235
  - 1.347942864894867
  - 1.3350838124752045
  - 1.3573536217212678
  - 1.3370291709899904
  - 1.4653771579265595
  - 1.3542689681053162
  - 1.375598055124283
  - 1.3956524431705475
  - 1.3911615729331972
  - 1.3383558571338654
  - 1.3342087388038637
  - 1.3656396210193635
  - 1.3929987490177156
  - 1.3937932133674622
  - 1.384985539317131
  - 1.4258442640304567
  - 1.4015941321849823
  - 1.397112274169922
  - 1.381701868772507
  - 1.4046782851219177
  - 1.3508719831705094
  - 1.3819451928138733
  - 1.3417379170656205
  - 1.3666974306106567
  - 1.359075838327408
  - 1.3773595690727234
  - 1.368127477169037
  - 1.3080001950263977
  - 1.3641490697860719
  - 1.3690090775489807
  - 1.3708403885364533
  - 1.3273724138736727
  - 1.3613431096076967
  - 1.3613574892282487
  - 1.3218725323677063
  - 1.306467941403389
  - 1.3337967574596405
  - 1.360753220319748
  - 1.383942598104477
  - 1.3682770550251009
  - 1.3581617653369904
  - 1.3247623920440674
  - 1.3207305252552033
  - 1.3513379454612733
  - 1.3676847457885744
  - 1.328560322523117
  - 1.3657676041126252
  - 1.3479538261890411
  - 1.3132339358329774
  - 1.3193277239799501
  - 1.34223729968071
  - 1.3530673623085023
  - 1.3155212521553041
  - 1.3501713395118715
  - 1.3321982473134995
  - 1.339329081773758
  - 1.3112339675426483
  - 1.3105277359485628
  - 1.3963411271572115
  - 1.340807694196701
  - 1.3837802052497865
  - 1.3338634431362153
  - 1.3461399644613268
  - 1.3732967615127565
  - 1.3177709817886354
  - 1.3602212727069856
  - 1.3482200026512148
  - 1.3412072837352753
  - 1.3422573149204255
  - 1.4117689549922945
  - 1.3536570727825166
  - 1.4247983992099762
  - 1.3388038098812105
  - 1.3423967063426971
  - 1.3212240457534792
  - 1.3946256548166276
  - 1.5749150097370148
  - 1.6863421440124513
  - 1.427184471487999
  - 1.3706513315439226
  validation_losses:
  - 3.178018093109131
  - 4.436777114868164
  - 2.162684440612793
  - 2.040867805480957
  - 1.754936933517456
  - 1.8636078834533691
  - 2.254399538040161
  - 2.719191074371338
  - 3.1803081035614014
  - 4.83414888381958
  - 8.286630630493164
  - 9.613234519958496
  - 10.009845733642578
  - 6.337898254394531
  - 16.25457763671875
  - 3.937793254852295
  - 6.343615531921387
  - 2.6127490997314453
  - 2.8512203693389893
  - 3.2993721961975098
  - 3.660452365875244
  - 2.510601758956909
  - 4.266907215118408
  - 3.1139514446258545
  - 4.331050395965576
  - 1.8855522871017456
  - 1.0609503984451294
  - 1.990010380744934
  - 2.7663962841033936
  - 2.178431749343872
  - 2.176382064819336
  - 2.8733162879943848
  - 3.3661530017852783
  - 2.1659722328186035
  - 2.8293540477752686
  - 1.305527925491333
  - 3.5550079345703125
  - 6.005460739135742
  - 1.5537469387054443
  - 3.3289554119110107
  - 2.219326972961426
  - 1.5559186935424805
  - 1.597354531288147
  - 1.4599030017852783
  - 2.01826548576355
  - 1.6461174488067627
  - 2.7517967224121094
  - 2.564164638519287
  - 4.090128421783447
  - 2.0048699378967285
  - 5.42034912109375
  - 3.6084957122802734
  - 4.3838629722595215
  - 4.227602005004883
  - 6.012082576751709
  - 6.669212341308594
  - 8.255415916442871
  - 4.308774471282959
  - 4.254377841949463
  - 5.870489597320557
  - 4.472621917724609
  - 2.198296546936035
  - 5.7856340408325195
  - 5.70949125289917
  - 5.295181751251221
  - 7.439421653747559
  - 13.040308952331543
  - 7.573625087738037
  - 6.507357120513916
  - 9.778122901916504
  - 4.91364049911499
  - 11.935704231262207
  - 7.194537162780762
  - 2.8765523433685303
  - 5.010991096496582
  - 9.348695755004883
  - 6.194459438323975
  - 7.150042533874512
  - 6.009415149688721
  - 12.312444686889648
  - 10.843269348144531
  - 11.722122192382812
  - 9.278499603271484
  - 10.709562301635742
  - 16.621017456054688
  - 13.255008697509766
  - 7.836668014526367
  - 21.1689453125
  - 17.904354095458984
  - 7.792425632476807
  - 3.310547351837158
  - 3.8269145488739014
  - 4.2708234786987305
  - 9.239497184753418
  - 6.9308366775512695
  - 18.22764015197754
  - 11.324973106384277
  - 0.5701554417610168
  - 0.5283851623535156
  - 0.5426809191703796
loss_records_fold4:
  train_losses:
  - 1.461834651231766
  - 1.4654036700725557
  - 1.3782217860221864
  - 1.450091850757599
  - 1.394965398311615
  - 1.3910155773162842
  - 1.386220771074295
  - 1.3613111853599549
  - 1.395989990234375
  - 1.382367727160454
  - 1.4132343590259553
  - 1.3610717713832856
  - 1.3424895644187929
  - 1.3558200597763062
  - 1.3866152971982957
  - 1.322313877940178
  - 1.3537670820951462
  - 1.343207234144211
  - 1.3427523344755175
  - 1.3632092356681824
  - 1.3199583232402803
  - 1.37870312333107
  - 1.364710706472397
  - 1.3361203074455261
  - 1.3236418664455414
  - 1.3253230929374695
  - 1.3285622894763947
  - 1.3379205524921418
  - 1.3229293167591096
  - 1.3462956249713898
  - 1.3211440443992615
  - 1.3158369541168213
  - 1.3784395873546602
  - 1.326391762495041
  - 1.4302586138248445
  - 1.3466907650232316
  - 1.3516320377588273
  - 1.351672822237015
  - 1.3661526560783388
  - 1.3147208213806154
  - 1.3521905839443207
  - 1.3731850564479828
  - 1.3405578672885896
  - 1.3277896344661713
  - 1.3179682552814485
  - 1.3222630560398103
  - 1.313985887169838
  - 1.2948686063289643
  - 1.3293644845485688
  - 1.3527258336544037
  - 1.344990211725235
  - 1.341291105747223
  - 1.3022779345512392
  - 1.2883590668439866
  - 1.3223830819129945
  - 1.3133522510528566
  - 1.3121708154678347
  - 1.3178978860378265
  - 1.2953636825084687
  - 1.3419686496257783
  - 1.3295429527759552
  - 1.3317991733551027
  - 1.329451283812523
  - 1.3108384042978287
  - 1.3520909219980242
  - 1.3075751662254333
  - 1.3111512064933777
  - 1.3293824553489686
  - 1.2814610779285431
  - 1.329967123270035
  - 1.3017417252063752
  - 1.3041023790836335
  - 1.3116140723228455
  - 1.3139432430267335
  - 1.3408640503883362
  - 1.3704812943935396
  - 1.3243575692176819
  - 1.3484660416841507
  - 1.3145339787006378
  - 1.3041843205690384
  - 1.3111397325992584
  - 1.3853316515684129
  - 1.2764731079339982
  - 1.3039184212684631
  - 1.3545341670513154
  - 1.338143727183342
  - 1.3073209941387178
  - 1.2878176003694535
  - 1.300463992357254
  - 1.3028755009174349
  - 1.3271081030368805
  - 1.3519503831863404
  - 1.294659435749054
  - 1.3138306558132173
  - 1.318678069114685
  - 1.3026000142097474
  - 1.329081678390503
  - 1.2910812735557557
  - 1.2832581520080568
  - 1.3057241141796112
  validation_losses:
  - 0.4121018350124359
  - 0.4394375681877136
  - 0.43189358711242676
  - 0.4882715046405792
  - 0.4349316656589508
  - 0.436113566160202
  - 0.4152665436267853
  - 0.5368384718894958
  - 0.5835207104682922
  - 0.5767354369163513
  - 0.6303147077560425
  - 0.5548723936080933
  - 0.7326191663742065
  - 0.5670353174209595
  - 0.45092833042144775
  - 0.4980984926223755
  - 0.590329110622406
  - 1.056574821472168
  - 1.246579885482788
  - 0.9749608039855957
  - 1.1226797103881836
  - 0.8021209836006165
  - 1.2531330585479736
  - 0.863452136516571
  - 0.9355265498161316
  - 0.8754938244819641
  - 1.528530240058899
  - 1.4378596544265747
  - 1.3653323650360107
  - 2.116053342819214
  - 1.172372817993164
  - 1.3762716054916382
  - 1.014172911643982
  - 0.9396569132804871
  - 0.6822798848152161
  - 0.480130136013031
  - 0.7230973243713379
  - 0.4160007834434509
  - 0.7438485026359558
  - 0.8579950332641602
  - 0.6413787007331848
  - 0.4994758665561676
  - 0.7855107188224792
  - 0.906328558921814
  - 0.762805700302124
  - 0.6279727816581726
  - 0.6391441822052002
  - 0.7220373749732971
  - 0.7685137391090393
  - 0.8794891238212585
  - 0.8669856190681458
  - 1.2134904861450195
  - 1.024227499961853
  - 0.9602442979812622
  - 1.3453243970870972
  - 0.5817936062812805
  - 0.9715084433555603
  - 1.1117439270019531
  - 0.8982548117637634
  - 1.1420843601226807
  - 0.8301321268081665
  - 0.9736279845237732
  - 0.6543865203857422
  - 0.6214655041694641
  - 0.9016717672348022
  - 0.6674060225486755
  - 0.7750959396362305
  - 0.7880951762199402
  - 0.8184456825256348
  - 1.4412399530410767
  - 1.1782557964324951
  - 1.1273033618927002
  - 1.141310214996338
  - 0.7498222589492798
  - 0.9422415494918823
  - 1.5352578163146973
  - 0.9213089346885681
  - 0.872096836566925
  - 0.6526642441749573
  - 0.749790608882904
  - 0.8294934630393982
  - 0.6879811882972717
  - 0.8404295444488525
  - 0.8456842303276062
  - 0.838765025138855
  - 0.7402026653289795
  - 0.8395505547523499
  - 1.0708346366882324
  - 0.8652154207229614
  - 0.9146330952644348
  - 1.223816156387329
  - 0.8649271130561829
  - 0.7026583552360535
  - 0.5613617897033691
  - 0.6419610977172852
  - 0.7947645783424377
  - 0.8339074850082397
  - 0.9567312002182007
  - 0.6941497325897217
  - 0.9244805574417114
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 27 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8353344768439108, 0.8078902229845626, 0.855917667238422,
    0.8006872852233677]'
  fold_eval_f1: '[0.0, 0.1724137931034483, 0.32530120481927716, 0.1764705882352941,
    0.2564102564102564]'
  mean_eval_accuracy: 0.831492517078979
  mean_f1_accuracy: 0.18611916851365518
  total_train_time: '0:36:43.447482'
