config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:59:31.392003'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_38fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_38fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_38fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_38fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_38fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_38fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_38fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_38fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_38fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_38fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.9190337777137758
  - 1.644849556684494
  - 1.7406098127365113
  - 1.6172585010528566
  - 1.5597880601882936
  - 1.5821521103382112
  - 1.6399550378322603
  - 1.5718684434890748
  - 1.5127747684717179
  - 1.6018774747848512
  - 1.5835637152194977
  - 1.5559420883655548
  - 1.51793572306633
  - 1.561957484483719
  - 1.5209691375494003
  - 1.5301851332187653
  - 1.6130992949008942
  - 1.55755832195282
  - 1.5432899296283722
  - 1.5471317946910859
  - 1.5982746064662934
  - 1.5779008865356445
  - 1.5726114451885225
  - 1.620803737640381
  - 1.5992047131061555
  - 1.572890067100525
  validation_losses:
  - 0.40231403708457947
  - 0.5016690492630005
  - 0.4303717613220215
  - 0.40622761845588684
  - 0.400356650352478
  - 0.39508476853370667
  - 0.39244621992111206
  - 0.39684218168258667
  - 0.39486750960350037
  - 0.4569505453109741
  - 0.4397760033607483
  - 0.3837149441242218
  - 0.40063807368278503
  - 0.3920643627643585
  - 0.39172235131263733
  - 0.42338982224464417
  - 0.40700557827949524
  - 0.38946533203125
  - 0.38412293791770935
  - 0.4165975749492645
  - 0.39417189359664917
  - 0.393353670835495
  - 0.3944050669670105
  - 0.3972569704055786
  - 0.38573184609413147
  - 0.39299285411834717
loss_records_fold1:
  train_losses:
  - 1.5937673151493073
  - 1.5808116674423218
  - 1.4921450912952423
  - 1.5665728628635407
  - 1.5493072211742402
  - 1.5672163605690004
  - 1.5339475095272066
  - 1.535323065519333
  - 1.5127737164497377
  - 1.537550038099289
  - 1.519359475374222
  - 1.5114784240722656
  - 1.5488696932792665
  - 1.543372565507889
  - 1.4973751544952394
  - 1.5810367405414583
  - 1.6007480144500734
  - 1.5623504519462585
  - 1.6140407383441926
  validation_losses:
  - 0.39679861068725586
  - 0.4236621856689453
  - 0.39064547419548035
  - 0.4004667103290558
  - 0.41170769929885864
  - 0.3947703242301941
  - 0.38829588890075684
  - 0.38102856278419495
  - 0.3963068723678589
  - 0.40270206332206726
  - 0.39012643694877625
  - 0.3851204216480255
  - 0.4115545153617859
  - 0.38792920112609863
  - 0.38625648617744446
  - 0.39516010880470276
  - 0.3970358073711395
  - 0.38581913709640503
  - 0.3900451958179474
loss_records_fold2:
  train_losses:
  - 1.5423206388950348
  - 1.6372254729270936
  - 1.6106024265289307
  - 1.5495313704013824
  - 1.5375973701477053
  - 1.5262859106063844
  - 1.5181489944458009
  - 1.595452755689621
  - 1.5376922369003296
  - 1.5218000054359437
  - 1.5798273205757143
  - 1.525911694765091
  - 1.534233444929123
  - 1.4883302450180054
  - 1.4869307458400727
  - 1.5285934448242189
  - 1.530345642566681
  - 1.539814031124115
  - 1.5597211360931398
  - 1.5115316212177277
  - 1.5328834772109987
  - 1.5065492689609528
  - 1.5915340423583986
  - 1.5082522809505463
  - 1.57858025431633
  - 1.4997780203819275
  - 1.5137587070465088
  - 1.5117162823677064
  - 1.525256872177124
  - 1.5401098668575288
  - 1.475635725259781
  - 1.5244043529033662
  - 1.5024525701999665
  - 1.5525375843048097
  - 1.4952289521694184
  - 1.531082969903946
  - 1.565113639831543
  - 1.4829250931739808
  - 1.4451655477285386
  - 1.5190949857234957
  - 1.5296592473983766
  - 1.4641671180725098
  - 1.4759769320487977
  - 1.5282501876354218
  - 1.510851639509201
  - 1.5316534101963044
  - 1.505454421043396
  - 1.5349058628082277
  - 1.4992190837860109
  - 1.5149896740913391
  - 1.5034616351127625
  validation_losses:
  - 0.44949817657470703
  - 0.5154378414154053
  - 0.9806995987892151
  - 0.39238011837005615
  - 0.40482574701309204
  - 0.3945360779762268
  - 0.3984937369823456
  - 0.39715519547462463
  - 0.3876141309738159
  - 0.39356815814971924
  - 0.40535786747932434
  - 0.38596877455711365
  - 0.3984457552433014
  - 0.39188140630722046
  - 0.391855388879776
  - 0.388856440782547
  - 0.40407058596611023
  - 0.4178410768508911
  - 0.3862369656562805
  - 0.3960796892642975
  - 0.3894490897655487
  - 0.3950193226337433
  - 0.3921809196472168
  - 0.4024222195148468
  - 0.38424402475357056
  - 0.38827186822891235
  - 3.0408332347869873
  - 0.4017820954322815
  - 0.39997825026512146
  - 1.3381290435791016
  - 0.523815929889679
  - 0.4017893671989441
  - 0.4383777678012848
  - 0.4548152685165405
  - 0.40021374821662903
  - 0.44751212000846863
  - 0.5694014430046082
  - 0.582237720489502
  - 0.605154812335968
  - 2.051260232925415
  - 0.41067200899124146
  - 1.3167212009429932
  - 2.2484912872314453
  - 0.39477095007896423
  - 0.4069746434688568
  - 0.40358293056488037
  - 0.39252689480781555
  - 0.39754360914230347
  - 0.39526575803756714
  - 0.38941407203674316
  - 0.3898727595806122
loss_records_fold3:
  train_losses:
  - 1.512829101085663
  - 1.5615801930427553
  - 1.5242372632026673
  - 1.4867863655090332
  - 1.5421383798122408
  - 1.5010591804981233
  - 1.5251187503337862
  - 1.4780271589756013
  - 1.465914386510849
  - 1.4974237799644472
  - 1.5029018044471742
  - 1.5315130829811097
  - 1.520221084356308
  - 1.504320001602173
  - 1.4684381365776062
  - 1.5219348311424257
  - 1.4765536904335024
  - 1.4755128264427186
  - 1.5115820467472076
  - 1.4886160850524903
  - 1.4593945503234864
  - 1.521931165456772
  - 1.5019949555397034
  - 1.5094570338726045
  - 1.4904393672943117
  - 1.5287269353866577
  - 1.5198336184024812
  - 1.4687254011631012
  - 1.4809785664081574
  - 1.5069110929965974
  - 1.5831189036369324
  validation_losses:
  - 0.3706590235233307
  - 0.42564302682876587
  - 0.37256646156311035
  - 0.3773368299007416
  - 0.38334524631500244
  - 0.37920236587524414
  - 0.39190030097961426
  - 0.375371515750885
  - 0.4764576554298401
  - 0.4187473654747009
  - 0.4699815511703491
  - 1.4244205951690674
  - 0.3797628879547119
  - 0.49255624413490295
  - 0.6595970988273621
  - 0.5844336748123169
  - 0.39027249813079834
  - 0.3884446620941162
  - 0.38149791955947876
  - 0.589387059211731
  - 0.4776565432548523
  - 0.5116525888442993
  - 0.4043595492839813
  - 0.7815179228782654
  - 1.1521316766738892
  - 1.1039422750473022
  - 0.9461697340011597
  - 0.9546273350715637
  - 0.7795556783676147
  - 0.6532923579216003
  - 0.374386191368103
loss_records_fold4:
  train_losses:
  - 1.5602017462253572
  - 1.5816926836967469
  - 1.5226574182510377
  - 1.4949565708637238
  - 1.4743832468986513
  - 1.5192254185676575
  - 1.5164272010326387
  - 1.4891132712364197
  - 1.5060742557048798
  - 1.516285765171051
  - 1.5133789002895357
  - 1.5296166837215424
  - 1.49410979449749
  - 1.4708367466926575
  - 1.4948392689228058
  - 1.494632911682129
  - 1.5022657155990602
  - 1.487857782840729
  - 1.4838410288095476
  - 1.4828286647796631
  - 1.5004704177379609
  - 1.5089745163917543
  - 1.5331511497497559
  - 1.5143815875053406
  - 1.484903645515442
  - 1.4942188382148744
  - 1.508590179681778
  - 1.4950031518936158
  - 1.47677441239357
  - 1.5166654884815216
  validation_losses:
  - 0.39827343821525574
  - 0.3833951950073242
  - 0.37649503350257874
  - 0.3805312216281891
  - 0.37796151638031006
  - 0.3708169460296631
  - 0.39224281907081604
  - 0.38248708844184875
  - 0.3968234658241272
  - 0.37473535537719727
  - 0.381314218044281
  - 0.38805779814720154
  - 0.3938179314136505
  - 0.38204556703567505
  - 1.1597458124160767
  - 0.37932994961738586
  - 0.4021991491317749
  - 0.40398547053337097
  - 0.3846624493598938
  - 0.36478349566459656
  - 0.4795459806919098
  - 0.45200467109680176
  - 0.36557716131210327
  - 0.42210838198661804
  - 0.37229588627815247
  - 0.3762798011302948
  - 0.38501647114753723
  - 0.38081806898117065
  - 0.38487711548805237
  - 0.3845156133174896
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 19 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 31 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:13:37.010640'
