config:
  aggregation: sum
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-04 21:49:53.028862'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/80/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/80/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/80/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/80/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/80/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/80/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/80/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/80/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/80/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/80/fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 330.8211313486099
  - 155.56197355687618
  - 93.27039597928524
  - 107.17096948623657
  - 96.71526241861284
  - 92.09781414270401
  - 57.73289266228676
  - 51.676016584038734
  - 51.50294563174248
  - 48.16206741333008
  - 48.38907650113106
  - 63.65759763121605
  - 42.822938933968544
  - 52.1993006169796
  - 51.23496796190739
  - 43.60023235529661
  - 69.1664270311594
  - 40.028382889926434
  - 51.87701567262411
  - 62.267526775598526
  - 36.464907586574554
  - 40.230678364634514
  - 38.47138865292072
  - 35.62254847586155
  - 31.61640015244484
  - 43.25871740281582
  - 43.4860765337944
  - 36.55231387913227
  - 42.340048775076866
  - 33.33807623386383
  - 36.289868608117104
  - 31.97670066356659
  - 33.434736385941505
  - 33.017420664429665
  - 33.18316997587681
  - 33.18796172738075
  - 30.533121407032013
  - 32.59709903597832
  - 30.05487611889839
  - 31.165220201015472
  - 31.208431243896484
  - 30.337828934192657
  - 29.321136310696602
  - 30.183907449245453
  - 30.706787422299385
  - 30.039718076586723
  - 30.867003351449966
  - 31.130015537142754
  - 30.06959769129753
  - 30.126438409090042
  - 30.482845775783062
  - 31.02433030307293
  - 30.418458729982376
  - 29.610926270484924
  - 30.171168565750122
  - 29.771178737282753
  - 30.99526159465313
  - 30.17473104596138
  - 29.46012084186077
  - 29.22705054283142
  - 30.285689428448677
  - 29.132130026817322
  - 29.84362192451954
  - 30.551894262433052
  - 30.19615648686886
  - 30.907370641827583
  - 29.452773690223694
  - 31.140531226992607
  - 29.863940373063087
  - 29.65664404630661
  - 29.55637177824974
  - 30.205640345811844
  - 33.88267129659653
  - 31.58661938458681
  - 31.142083391547203
  - 33.084164530038834
  - 30.650881465524435
  - 29.23293487727642
  - 30.097047872841358
  - 30.056017339229584
  - 29.982392206788063
  - 29.545170485973358
  - 29.180800288915634
  - 29.942688792943954
  - 30.645850762724876
  - 29.33835320174694
  - 29.598471090197563
  - 29.48238478600979
  - 29.4649099111557
  - 29.437787547707558
  - 29.678414076566696
  - 29.7973071038723
  - 29.356975063681602
  - 29.430652871727943
  - 30.90115724503994
  - 30.119824662804604
  - 30.216139256954193
  - 29.45591451227665
  - 30.346537977457047
  - 29.721041351556778
  validation_losses:
  - 1.1057566404342651
  - 3.5589234828948975
  - 0.5571670532226562
  - 0.733424186706543
  - 0.6231328248977661
  - 0.6390207409858704
  - 0.4697197675704956
  - 0.5771597623825073
  - 0.40031102299690247
  - 1.045945167541504
  - 0.3830929696559906
  - 0.4028801918029785
  - 0.39616283774375916
  - 0.38551658391952515
  - 0.5724930167198181
  - 0.5379571914672852
  - 0.3918038010597229
  - 0.39113518595695496
  - 0.5818737149238586
  - 0.4650764465332031
  - 0.4047269821166992
  - 0.39945855736732483
  - 0.3903098404407501
  - 0.38931766152381897
  - 0.8604182600975037
  - 0.39088690280914307
  - 0.3880351483821869
  - 0.39966246485710144
  - 0.43674954771995544
  - 0.39301595091819763
  - 0.39245694875717163
  - 0.42100808024406433
  - 0.3904644548892975
  - 0.43670138716697693
  - 0.43927574157714844
  - 0.3900221586227417
  - 0.39760643243789673
  - 0.38908493518829346
  - 0.38417014479637146
  - 0.4330896735191345
  - 0.3907119631767273
  - 0.3889497220516205
  - 0.9036283493041992
  - 0.39152589440345764
  - 0.3903999626636505
  - 0.38992708921432495
  - 0.39050328731536865
  - 0.3904542624950409
  - 0.4366287291049957
  - 0.6652135252952576
  - 0.710196316242218
  - 1.1523444652557373
  - 0.3967514932155609
  - 0.3913205564022064
  - 0.42440155148506165
  - 0.4092659056186676
  - 0.43924447894096375
  - 0.3905969560146332
  - 0.389591246843338
  - 0.3917306959629059
  - 1.5434330701828003
  - 1.3369742631912231
  - 0.39435067772865295
  - 0.3935450613498688
  - 0.498702734708786
  - 0.39159005880355835
  - 0.3910754919052124
  - 0.4239126443862915
  - 0.4356207847595215
  - 0.3935621976852417
  - 0.9705837965011597
  - 0.4749525785446167
  - 0.38861367106437683
  - 0.5505889654159546
  - 0.4157986640930176
  - 0.38995811343193054
  - 0.44699758291244507
  - 0.6118889451026917
  - 0.408596396446228
  - 0.4488992393016815
  - 0.40410691499710083
  - 0.42502689361572266
  - 0.45201194286346436
  - 0.3902495503425598
  - 0.39088574051856995
  - 0.429872989654541
  - 0.4067035913467407
  - 0.400321364402771
  - 0.39099016785621643
  - 0.41448044776916504
  - 0.41507089138031006
  - 0.3945389986038208
  - 4.190832614898682
  - 39704125440.0
  - 349762224128.0
  - 84696629248.0
  - 1375569.875
  - 27690392354816.0
  - 0.392493337392807
  - 0.3960437476634979
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 79 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:10:29.231513'
