config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:10:19.935517'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_34fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_34fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_34fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_34fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_34fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_34fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_34fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_34fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_34fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_34fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.735211890935898
  - 1.5235008716583254
  - 1.599908608198166
  - 1.5196776628494264
  - 1.4761196553707123
  - 1.5018425941467286
  - 1.5137509286403656
  - 1.4618820607662202
  - 1.4222324281930925
  - 1.4698175609111788
  - 1.4796356022357942
  - 1.465507858991623
  - 1.480100393295288
  - 1.5817123651504517
  - 1.444285297393799
  - 1.4882091999053957
  - 1.4857965230941774
  validation_losses:
  - 0.40673187375068665
  - 0.4429763853549957
  - 0.5082069635391235
  - 0.3963860869407654
  - 0.38582566380500793
  - 0.3912499248981476
  - 0.38840845227241516
  - 0.39491841197013855
  - 0.3906501531600952
  - 0.3972906172275543
  - 0.45669373869895935
  - 0.3849877417087555
  - 0.385051965713501
  - 0.383777916431427
  - 0.3900475800037384
  - 0.3905157148838043
  - 0.3822265565395355
loss_records_fold1:
  train_losses:
  - 1.485735821723938
  - 1.4764480829238893
  - 1.4060409337282183
  - 1.4422227680683137
  - 1.4765844583511354
  - 1.5201039075851441
  - 1.4413012504577638
  - 1.490148150920868
  - 1.475742119550705
  - 1.4312566250562668
  - 1.468226456642151
  - 1.41888085603714
  - 1.45660657286644
  - 1.457933795452118
  - 1.4670823395252228
  - 1.4734079241752625
  - 1.4376584470272065
  - 1.4188789904117585
  - 1.417909413576126
  - 1.4494811832904817
  - 1.4353684186935425
  - 1.439654463529587
  - 1.4175334334373475
  - 1.415951180458069
  - 1.4371763348579407
  - 1.470580840110779
  - 1.4184182345867158
  - 1.4995556235313416
  - 1.4339740037918092
  - 1.4239888548851014
  - 1.4150538444519043
  - 1.4230845272541046
  - 1.507363569736481
  - 1.4685019612312318
  - 1.405333787202835
  - 1.4378287374973298
  - 1.3907152086496355
  - 1.4050040006637574
  - 1.3813325464725494
  - 1.3961962789297104
  - 1.4248628199100495
  - 1.438114470243454
  - 1.4259037613868715
  - 1.3884712725877764
  - 1.412724679708481
  - 1.421254724264145
  - 1.3888252019882203
  - 1.4277440011501312
  - 1.4762478947639466
  - 1.4047053575515749
  - 1.446473389863968
  - 1.432782942056656
  - 1.3894021391868592
  - 1.3903387486934662
  - 1.423942643404007
  - 1.4044524669647218
  - 1.4717237651348114
  - 1.4210482060909273
  - 1.4776573956012726
  - 1.454439151287079
  - 1.4944117844104767
  - 1.4073031842708588
  - 1.3947008192539216
  - 1.4966516673564911
  - 1.4185730874538423
  - 1.4253242135047914
  - 1.430165457725525
  - 1.3949572652578355
  - 1.4351644575595857
  - 1.4913898766040803
  - 1.3966882258653641
  - 1.4118857800960543
  - 1.4306351959705355
  - 1.4341738998889924
  - 1.3826974987983705
  - 1.385085326433182
  - 1.3888986200094224
  - 1.40701584815979
  - 1.40178644657135
  - 1.3956967055797578
  - 1.386884480714798
  - 1.4307373881340029
  - 1.4597383618354798
  - 1.4280578672885895
  - 1.4617409110069275
  - 1.3971032857894898
  - 1.3953830003738403
  - 1.4424003034830095
  - 1.3943032503128052
  - 1.409149318933487
  - 1.418250384926796
  - 1.3683603405952454
  - 1.3997226536273957
  - 1.3845766693353654
  - 1.430527174472809
  - 1.4328129798173905
  - 1.418601703643799
  - 1.4237819552421571
  - 1.3970012784004213
  - 1.388030755519867
  validation_losses:
  - 0.41812410950660706
  - 0.3906237483024597
  - 0.3846094608306885
  - 0.3878353536128998
  - 0.3873302936553955
  - 0.38839325308799744
  - 0.382428914308548
  - 0.3851783871650696
  - 0.3871074914932251
  - 0.38302505016326904
  - 0.4096370339393616
  - 0.4058101773262024
  - 0.5369218587875366
  - 0.4046199917793274
  - 0.3839280903339386
  - 0.38740459084510803
  - 0.3809497356414795
  - 0.3900177776813507
  - 0.46630221605300903
  - 0.4314064085483551
  - 0.3845129609107971
  - 0.4066255986690521
  - 0.39200109243392944
  - 0.40746474266052246
  - 0.39481040835380554
  - 0.43113023042678833
  - 0.39323359727859497
  - 0.38835692405700684
  - 0.41695865988731384
  - 0.3964291512966156
  - 0.39068540930747986
  - 0.4039989411830902
  - 0.41293397545814514
  - 0.43709930777549744
  - 0.38702675700187683
  - 0.42321619391441345
  - 0.4902791380882263
  - 0.6901099681854248
  - 0.3762684464454651
  - 0.40757787227630615
  - 0.5336602926254272
  - 0.45166686177253723
  - 0.42324987053871155
  - 0.8971193432807922
  - 0.4364640414714813
  - 0.599700391292572
  - 0.5864761471748352
  - 0.7777618169784546
  - 0.585472583770752
  - 0.3793712556362152
  - 0.38960304856300354
  - 0.5244482159614563
  - 0.4837929308414459
  - 0.6603061556816101
  - 0.7080959677696228
  - 0.3861294388771057
  - 0.3945980668067932
  - 0.4395763576030731
  - 0.4390588104724884
  - 0.465433269739151
  - 0.42790457606315613
  - 0.4649234414100647
  - 0.5088731646537781
  - 0.4868815243244171
  - 0.4650595188140869
  - 0.6054118871688843
  - 0.5478858351707458
  - 0.45691946148872375
  - 0.40837588906288147
  - 0.4074164628982544
  - 0.4252059757709503
  - 0.4468878507614136
  - 0.3972872495651245
  - 0.4773997366428375
  - 0.47026991844177246
  - 0.4362064003944397
  - 0.5227125883102417
  - 0.4688907265663147
  - 0.4855765998363495
  - 0.47478756308555603
  - 0.54822838306427
  - 0.5045182108879089
  - 0.4650390148162842
  - 0.5043765306472778
  - 0.5417760610580444
  - 0.4060998857021332
  - 0.48726627230644226
  - 0.38603639602661133
  - 0.3839223086833954
  - 0.385787695646286
  - 0.5022506713867188
  - 0.4326544404029846
  - 0.46782201528549194
  - 0.4851478338241577
  - 0.5133121609687805
  - 0.559054434299469
  - 0.5981658101081848
  - 0.43843376636505127
  - 0.5420697927474976
  - 0.4318225085735321
loss_records_fold2:
  train_losses:
  - 1.361175149679184
  - 1.4299016773700715
  - 1.4065781950950624
  - 1.4036155611276628
  - 1.4395693004131318
  - 1.4301351249217988
  - 1.4054001569747925
  - 1.3802420854568482
  - 1.3629828810691835
  - 1.3961224615573884
  - 1.3569325327873232
  - 1.3639273345470428
  - 1.3745332837104798
  - 1.3697267174720764
  - 1.3391705483198166
  - 1.4336277663707735
  - 1.3570272773504257
  - 1.397860872745514
  - 1.3676726043224336
  - 1.4321356832981111
  - 1.392424488067627
  - 1.3545287549495697
  - 1.3783402025699616
  - 1.399517533183098
  - 1.395209863781929
  - 1.3878594994544984
  - 1.3824503034353257
  - 1.3847924768924713
  - 1.4275667667388916
  - 1.3746579706668856
  - 1.4505167841911317
  - 1.3803231477737428
  - 1.3659331798553467
  - 1.370700091123581
  - 1.3558439731597902
  - 1.3976920008659364
  - 1.3751260280609132
  - 1.3797274112701416
  - 1.4269679486751556
  - 1.3722715824842453
  - 1.3416259676218034
  - 1.3687326610088348
  - 1.362697196006775
  - 1.359289163351059
  - 1.3354155272245407
  - 1.403474920988083
  - 1.4229799538850785
  - 1.4214005768299103
  - 1.4122827589511873
  - 1.3683899462223055
  - 1.376962798833847
  - 1.3972075939178468
  - 1.350407436490059
  - 1.3637273192405701
  - 1.3740280032157899
  - 1.3805089592933655
  - 1.4245652556419373
  - 1.398865666985512
  - 1.3427437484264375
  - 1.3882562458515169
  - 1.4000468254089355
  - 1.3748746275901795
  - 1.3892183363437653
  - 1.361469876766205
  - 1.3223355770111085
  - 1.3576685667037964
  - 1.3819133996963502
  - 1.3658079147338869
  - 1.3969322472810746
  - 1.3559664964675904
  - 1.3593340486288072
  - 1.3608821213245392
  - 1.3706650018692017
  - 1.4112654387950898
  - 1.3978528916835786
  - 1.571536022424698
  - 1.4914264380931854
  - 1.4909197628498079
  - 1.4258888244628907
  - 1.4285651564598085
  - 1.387963017821312
  - 1.429932451248169
  validation_losses:
  - 0.512879490852356
  - 0.45804736018180847
  - 0.4495090842247009
  - 0.44413328170776367
  - 0.5402491092681885
  - 0.5638988018035889
  - 0.40612030029296875
  - 0.4825485646724701
  - 0.545276403427124
  - 0.395188570022583
  - 0.48080283403396606
  - 0.6507232785224915
  - 0.6220452189445496
  - 0.5173556208610535
  - 0.4939029812812805
  - 0.6462664604187012
  - 0.37574416399002075
  - 0.5002924203872681
  - 0.3919559419155121
  - 0.45332711935043335
  - 0.4619618058204651
  - 8.487921714782715
  - 0.694529116153717
  - 0.7429711818695068
  - 0.5391339063644409
  - 0.7860230207443237
  - 0.6748687624931335
  - 0.8189809322357178
  - 0.9474769234657288
  - 1.0523263216018677
  - 0.5576989054679871
  - 0.5478883385658264
  - 0.43980708718299866
  - 0.6751295924186707
  - 0.8193427920341492
  - 0.4352805018424988
  - 0.5473344326019287
  - 0.7337508201599121
  - 0.8381799459457397
  - 0.5792367458343506
  - 0.7176209092140198
  - 0.6810276508331299
  - 0.7826390266418457
  - 0.756617546081543
  - 0.6843545436859131
  - 0.37407395243644714
  - 0.44923895597457886
  - 0.5109961628913879
  - 1.0347142219543457
  - 1.323764681816101
  - 0.5095191597938538
  - 0.44754117727279663
  - 0.6343410611152649
  - 0.57848060131073
  - 1.4983091354370117
  - 4.470327377319336
  - 0.5757735371589661
  - 0.6212429404258728
  - 0.7060993313789368
  - 0.6822165250778198
  - 0.9881191849708557
  - 0.9576152563095093
  - 0.38660407066345215
  - 0.7988823652267456
  - 0.8100682497024536
  - 0.8334743976593018
  - 0.5318231582641602
  - 1.0804805755615234
  - 0.8476032018661499
  - 0.5805497765541077
  - 0.8317647576332092
  - 0.77017742395401
  - 0.5926539897918701
  - 0.39975452423095703
  - 0.41221892833709717
  - 0.43078672885894775
  - 0.3931310176849365
  - 0.3890630304813385
  - 0.38446736335754395
  - 0.3835256099700928
  - 0.39069873094558716
  - 0.3821433186531067
loss_records_fold3:
  train_losses:
  - 1.4514570474624635
  - 1.442749845981598
  - 1.4167508721351625
  - 1.4654265820980072
  - 1.4493796229362488
  - 1.4296567261219026
  - 1.4738317012786866
  - 1.4224707245826722
  - 1.3707691431045532
  - 1.4311287939548494
  - 1.4385332763195038
  - 1.3767334759235383
  - 1.4225845396518708
  - 1.482320636510849
  - 1.4163371145725252
  - 1.4383256673812868
  - 1.416093099117279
  - 1.3977498769760133
  - 1.4446540832519532
  - 1.428041857481003
  - 1.3823904544115067
  - 1.4062450855970383
  - 1.397634541988373
  - 1.4207415878772736
  - 1.3904212892055512
  - 1.4104207038879395
  - 1.4072739064693451
  - 1.3834992110729218
  - 1.4004456877708436
  - 1.4582032382488253
  - 1.3831323504447939
  - 1.4062457203865053
  - 1.3815611928701401
  - 1.3779159963130951
  - 1.4132951557636262
  - 1.3634631723165513
  - 1.4295238554477692
  - 1.40740025639534
  - 1.3965466797351838
  - 1.4407076597213746
  - 1.379901248216629
  - 1.4103965997695924
  - 1.394895923137665
  - 1.3989151477813722
  - 1.4015452325344087
  - 1.4000855922698976
  - 1.3800195157527924
  - 1.449611508846283
  - 1.402167934179306
  - 1.4002074778079987
  - 1.3521751046180726
  - 1.3869023442268373
  - 1.4005512952804566
  - 1.356336072087288
  - 1.3721902430057527
  - 1.3766300678253174
  - 1.4059666693210602
  - 1.3907585084438325
  - 1.3919990956783295
  - 1.3709975004196169
  - 1.4054111242294312
  - 1.4055877327919006
  - 1.4562203109264376
  - 1.432920050621033
  - 1.437407946586609
  validation_losses:
  - 0.3656749725341797
  - 0.3662581145763397
  - 0.36409205198287964
  - 0.3691154718399048
  - 0.36944863200187683
  - 0.3890116512775421
  - 0.383760541677475
  - 0.37187063694000244
  - 0.37170490622520447
  - 0.3755371570587158
  - 0.42101556062698364
  - 0.37754425406455994
  - 0.380053848028183
  - 0.40918925404548645
  - 0.4499909579753876
  - 0.37777554988861084
  - 0.36983171105384827
  - 0.40355730056762695
  - 0.47674089670181274
  - 0.3768484592437744
  - 0.390909343957901
  - 0.4054264724254608
  - 0.43702974915504456
  - 0.40769580006599426
  - 0.431911826133728
  - 0.45333361625671387
  - 0.4097481667995453
  - 0.48298847675323486
  - 0.45415136218070984
  - 0.5170702934265137
  - 0.5497467517852783
  - 0.6636931300163269
  - 0.7224661707878113
  - 0.6898649334907532
  - 0.6884816288948059
  - 0.5040889382362366
  - 0.6169686913490295
  - 46.74547576904297
  - 0.41011905670166016
  - 0.40740492939949036
  - 0.4832584857940674
  - 0.3785474896430969
  - 0.4886670410633087
  - 0.4703403413295746
  - 0.5118351578712463
  - 0.4111483693122864
  - 0.389501690864563
  - 0.5821954011917114
  - 0.5103529095649719
  - 0.4914760887622833
  - 0.6336708664894104
  - 0.6244939565658569
  - 0.4125535786151886
  - 0.46672379970550537
  - 0.6369618773460388
  - 0.6246654987335205
  - 0.3725039064884186
  - 0.4536527693271637
  - 0.49663376808166504
  - 0.4543347656726837
  - 0.37083926796913147
  - 0.3738304078578949
  - 0.36829379200935364
  - 0.36817997694015503
  - 0.376903772354126
loss_records_fold4:
  train_losses:
  - 1.441658079624176
  - 1.435066890716553
  - 1.4214233756065369
  - 1.4294601738452912
  - 1.4004436612129212
  - 1.4171490609645845
  - 1.4130266427993776
  - 1.3837621331214907
  - 1.3587945431470871
  - 1.4287723064422608
  - 1.4233646631240846
  - 1.3889200270175934
  - 1.3868251681327821
  - 1.3698011159896852
  - 1.347911822795868
  - 1.432599663734436
  - 1.3861471474170686
  - 1.4144445061683655
  - 1.4215474665164949
  - 1.4828046202659608
  - 1.3697378873825075
  - 1.3909717977046967
  - 1.3848053812980652
  validation_losses:
  - 0.3683537244796753
  - 0.36959293484687805
  - 0.37102973461151123
  - 0.37187719345092773
  - 0.3655391335487366
  - 0.3758084177970886
  - 0.3762146234512329
  - 0.36780357360839844
  - 0.3851294219493866
  - 0.5103130340576172
  - 0.4048829972743988
  - 0.3649304211139679
  - 0.39493897557258606
  - 0.4281829297542572
  - 0.37958797812461853
  - 0.45343858003616333
  - 0.48333388566970825
  - 0.3610245883464813
  - 0.3670116364955902
  - 0.3696320652961731
  - 0.37475916743278503
  - 0.37010905146598816
  - 0.3696240484714508
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 82 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 65 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 23 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8490566037735849, 0.8576329331046312, 0.8542024013722127,
    0.8608247422680413]'
  fold_eval_f1: '[0.0, 0.043478260869565216, 0.023529411764705882, 0.0, 0.06896551724137931]'
  mean_eval_accuracy: 0.8558699227246201
  mean_f1_accuracy: 0.02719463797513008
  total_train_time: '0:24:59.796143'
