config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 05:29:58.720226'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/130/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/130/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/130/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/130/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/130/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/130/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/130/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/130/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/130/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/130/fold_4_state_dict.pt
loss_records_fold2:
  train_losses:
  - 14.19870899617672
  - 14.086278557777405
  - 14.117443919181824
  - 14.038774013519287
  - 13.913526624441147
  - 14.112588346004486
  - 14.064561039209366
  - 14.026329815387726
  - 13.83041062951088
  - 13.921231999993324
  - 13.89365328848362
  - 14.008337050676346
  - 13.707157135009766
  - 13.676008954644203
  - 14.322228819131851
  - 13.921216249465942
  - 13.674290597438812
  - 13.941839933395386
  - 13.742597043514252
  - 13.887798205018044
  - 13.910371974110603
  - 13.804905444383621
  - 14.294388115406036
  - 14.085904911160469
  - 14.118716061115265
  - 13.86890684068203
  - 13.733515530824661
  - 13.993258565664291
  - 13.741328462958336
  - 13.72073420882225
  - 13.815860271453857
  - 13.76303905248642
  - 13.91853478550911
  - 13.795882552862167
  - 13.706748813390732
  - 13.596438184380531
  - 13.824756845831871
  - 13.857863277196884
  - 13.672790005803108
  - 14.197640538215637
  - 13.892478257417679
  - 13.854656502604485
  - 13.822330474853516
  - 13.91653522849083
  - 13.671435430645943
  - 13.738206684589386
  - 13.945479303598404
  - 13.896161064505577
  - 13.743188977241516
  - 13.69158436357975
  - 13.692640289664268
  - 13.7000952064991
  - 13.785819619894028
  - 13.757479429244995
  - 14.044690072536469
  - 13.669485852122307
  - 13.937973275780678
  - 13.674094259738922
  - 13.663998872041702
  - 13.847094982862473
  - 13.855650916695595
  - 13.707846835255623
  - 13.466469138860703
  - 13.79942287504673
  - 13.652366891503334
  - 13.698437869548798
  - 13.591224700212479
  - 13.72653503715992
  - 13.931358829140663
  - 14.137057825922966
  - 13.717274487018585
  - 13.703535556793213
  - 13.78968296945095
  - 13.75420942902565
  - 14.114338159561157
  - 13.761335492134094
  - 13.763719365000725
  - 13.842282012104988
  - 13.970452576875687
  - 13.835700005292892
  - 13.775386735796928
  - 13.912043869495392
  - 13.93104699254036
  - 13.707058444619179
  - 13.705237746238708
  - 14.139528647065163
  - 13.713720515370369
  - 13.633204609155655
  - 13.64043214917183
  - 13.715834975242615
  - 13.474751561880112
  - 13.490567162632942
  - 13.559153467416763
  - 13.73778486251831
  - 13.664369404315948
  - 13.628428757190704
  - 13.588352486491203
  - 13.822986364364624
  - 13.572851240634918
  - 13.579458370804787
  validation_losses:
  - 0.3883349895477295
  - 0.3826879560947418
  - 0.38570719957351685
  - 0.4100080132484436
  - 0.3935798406600952
  - 0.3953917622566223
  - 0.517295777797699
  - 0.40206822752952576
  - 0.4443776309490204
  - 0.3818214535713196
  - 0.4147484302520752
  - 0.42864787578582764
  - 0.4467522203922272
  - 0.6855742931365967
  - 0.43856438994407654
  - 0.3875056207180023
  - 0.4271773397922516
  - 0.3850676119327545
  - 0.4038803279399872
  - 0.3963173031806946
  - 0.4004681408405304
  - 0.5268839597702026
  - 0.40809664130210876
  - 0.41081103682518005
  - 0.5408409237861633
  - 0.4023857116699219
  - 0.38716205954551697
  - 0.43109947443008423
  - 0.7941171526908875
  - 0.4420614242553711
  - 0.4762873947620392
  - 0.4929622411727905
  - 0.40611732006073
  - 0.41611751914024353
  - 0.5609620213508606
  - 0.414082407951355
  - 0.46150484681129456
  - 0.41989120841026306
  - 0.4081779420375824
  - 0.38678449392318726
  - 0.3822630047798157
  - 0.4547958970069885
  - 0.41542956233024597
  - 0.43748655915260315
  - 0.5394726395606995
  - 0.452277809381485
  - 0.37588754296302795
  - 0.45220157504081726
  - 0.45260727405548096
  - 0.384429931640625
  - 1.8761415481567383
  - 0.6071265339851379
  - 0.48478978872299194
  - 0.420858770608902
  - 0.5593208074569702
  - 0.4756607115268707
  - 0.5235061645507812
  - 0.478536456823349
  - 0.5054473876953125
  - 0.6834231019020081
  - 0.3993968069553375
  - 0.739881694316864
  - 0.40076082944869995
  - 0.403626024723053
  - 0.4291331171989441
  - 0.5649829506874084
  - 0.44927528500556946
  - 0.3959333598613739
  - 0.4856818914413452
  - 1.5447310209274292
  - 3.382350206375122
  - 0.821534276008606
  - 0.415520578622818
  - 0.6461570262908936
  - 0.45115241408348083
  - 0.5242418050765991
  - 1.0308897495269775
  - 0.4060061275959015
  - 0.4116371273994446
  - 0.4645659029483795
  - 0.6274256706237793
  - 0.42111024260520935
  - 0.6124231219291687
  - 0.40577179193496704
  - 0.5390679240226746
  - 0.39528393745422363
  - 0.3844640254974365
  - 0.44106003642082214
  - 0.41916388273239136
  - 0.5422375202178955
  - 0.45528119802474976
  - 0.7153874039649963
  - 0.42219603061676025
  - 0.681756317615509
  - 0.5684014558792114
  - 0.565811038017273
  - 1.9598044157028198
  - 0.5484389066696167
  - 0.5449177026748657
  - 0.7259483933448792
loss_records_fold3:
  train_losses:
  - 13.988358214497566
  - 14.278162121772766
  - 13.962643131613731
  - 13.869793951511383
  - 13.872661799192429
  - 13.691521912813187
  - 13.899807065725327
  - 13.740872114896774
  - 13.8173418790102
  - 13.782281398773193
  - 13.711270913481712
  - 13.827475875616074
  - 13.60452239215374
  - 13.767686977982521
  - 13.613324329257011
  - 14.014576315879822
  - 13.46921594440937
  - 13.862666070461273
  - 13.935552060604095
  - 13.878750309348106
  - 13.826521098613739
  - 13.633502185344696
  - 13.891961395740509
  - 13.82720622420311
  - 13.846405625343323
  - 13.688292741775513
  - 13.61236360669136
  - 13.874431893229485
  - 13.924904510378838
  - 13.682093366980553
  - 13.700567603111267
  - 13.962578818202019
  - 14.028355985879898
  - 13.816029042005539
  - 13.462957456707954
  - 13.650988534092903
  - 13.658101230859756
  - 13.744383454322815
  - 13.692002445459366
  - 13.62916025519371
  - 14.102914556860924
  - 14.123750776052475
  - 13.721566289663315
  - 13.698591992259026
  - 13.720543339848518
  - 13.64676746726036
  - 13.541586607694626
  - 13.582318603992462
  - 13.66931326687336
  - 13.582519948482513
  - 13.960452601313591
  - 13.63500364124775
  - 13.902066081762314
  - 13.613970294594765
  - 13.460889771580696
  - 13.704856932163239
  - 13.661174654960632
  - 13.693442165851593
  - 13.583894565701485
  - 13.731650978326797
  - 13.551859855651855
  - 13.757316529750824
  - 13.692986205220222
  - 13.497043013572693
  - 13.578430011868477
  - 13.671071141958237
  - 13.673523515462875
  - 13.574515998363495
  - 13.619551241397858
  - 13.739910989999771
  - 13.717576622962952
  - 13.764122664928436
  - 13.555924490094185
  - 13.982252761721611
  - 13.534071058034897
  - 13.607609018683434
  - 13.369071990251541
  - 13.513822436332703
  - 13.791937708854675
  - 13.485919296741486
  - 13.758862167596817
  - 13.939940795302391
  - 13.621607080101967
  - 13.68630638718605
  - 13.415095955133438
  - 13.417494595050812
  - 13.62749183177948
  - 13.800083756446838
  - 13.887834131717682
  - 13.605526402592659
  - 13.624453336000443
  - 13.692598342895508
  - 13.663912773132324
  - 13.672763258218765
  - 13.37064716219902
  - 13.681974664330482
  - 13.578518897294998
  - 13.854502275586128
  - 13.53693713247776
  - 13.402969300746918
  validation_losses:
  - 1.063112497329712
  - 0.5347362160682678
  - 0.4279320240020752
  - 0.5546989440917969
  - 0.5877906680107117
  - 0.5169254541397095
  - 0.6802684664726257
  - 0.9026607871055603
  - 0.9447873830795288
  - 0.7618237733840942
  - 0.3750254511833191
  - 0.8244017958641052
  - 0.8080009818077087
  - 0.5737865567207336
  - 0.4608747065067291
  - 0.8530365228652954
  - 1.9810209274291992
  - 0.9811871647834778
  - 1.7832615375518799
  - 0.7933869361877441
  - 1.3608750104904175
  - 0.6881852746009827
  - 0.6754525303840637
  - 0.8055508732795715
  - 0.8532136678695679
  - 0.9124196767807007
  - 0.38372787833213806
  - 0.4884130656719208
  - 1.1396963596343994
  - 0.8584400415420532
  - 0.9881037473678589
  - 0.6475265026092529
  - 1.723259449005127
  - 1.3005977869033813
  - 2.1676199436187744
  - 3.113692045211792
  - 3.7761454582214355
  - 7.503775596618652
  - 1.5362749099731445
  - 1.0028886795043945
  - 0.931972086429596
  - 1.5787231922149658
  - 3.238509178161621
  - 0.8612332940101624
  - 1.6504859924316406
  - 1.5216811895370483
  - 1.8908252716064453
  - 0.3848241865634918
  - 3.3906548023223877
  - 0.6916713118553162
  - 0.43135109543800354
  - 0.8867001533508301
  - 0.4744908809661865
  - 0.48470354080200195
  - 0.4766555726528168
  - 0.4224114716053009
  - 0.5504922866821289
  - 1.1362277269363403
  - 0.45845144987106323
  - 1.4904171228408813
  - 3.850029945373535
  - 0.7433335781097412
  - 0.4899921715259552
  - 0.44546034932136536
  - 8.106727600097656
  - 1.770359992980957
  - 1.5885834693908691
  - 0.44022104144096375
  - 0.5933705568313599
  - 1.3073430061340332
  - 0.7539958953857422
  - 0.4256700873374939
  - 0.5296774506568909
  - 1.0834434032440186
  - 1.17778742313385
  - 1.4898518323898315
  - 1.015529751777649
  - 1.1907355785369873
  - 1.205869436264038
  - 0.9008925557136536
  - 4.9337239265441895
  - 21.575454711914062
  - 15.501912117004395
  - 0.5500956177711487
  - 0.6091784238815308
  - 1.485579252243042
  - 0.5662752985954285
  - 0.5691153407096863
  - 0.7494913339614868
  - 4.057218551635742
  - 0.7138344645500183
  - 1.491627812385559
  - 0.7203941941261292
  - 1.9349738359451294
  - 1.5882240533828735
  - 0.6359068155288696
  - 2.8973333835601807
  - 0.7212932705879211
  - 1.929327368736267
  - 0.4916231632232666
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 31 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8456260720411664, 0.8490566037735849,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.1346153846153846, 0.15384615384615385, 0.0]'
  mean_eval_accuracy: 0.8538110142467271
  mean_f1_accuracy: 0.05769230769230769
  total_train_time: '0:06:35.064590'
