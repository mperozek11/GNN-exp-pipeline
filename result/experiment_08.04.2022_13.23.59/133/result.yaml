config:
  aggregation: mean
  batch_size: 128
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.1
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-05 05:45:05.716976'
fold_0_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/133/fold_0_optim_dict.pt
fold_0_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/133/fold_0_state_dict.pt
fold_1_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/133/fold_1_optim_dict.pt
fold_1_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/133/fold_1_state_dict.pt
fold_2_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/133/fold_2_optim_dict.pt
fold_2_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/133/fold_2_state_dict.pt
fold_3_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/133/fold_3_optim_dict.pt
fold_3_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/133/fold_3_state_dict.pt
fold_4_optim_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/133/fold_4_optim_dict.pt
fold_4_state_dict: /home/maxpzk/GNN-exp-pipeline/result/experiment_08.04.2022_13.23.59/133/fold_4_state_dict.pt
loss_records_fold4:
  train_losses:
  - 7.499743163585663
  - 7.517098933458328
  - 7.362514287233353
  - 7.550654202699661
  - 7.5872833132743835
  - 7.587242245674133
  - 7.2882551699876785
  - 7.458452612161636
  - 7.588879585266113
  - 7.558831736445427
  - 7.471316158771515
  - 7.601776629686356
  - 8.04353742301464
  - 7.9500612616539
  - 8.09567540884018
  - 8.104432106018066
  - 7.981076747179031
  - 7.708785057067871
  - 7.805712431669235
  - 7.610841274261475
  - 7.64618393778801
  - 7.669430077075958
  - 7.661501407623291
  - 7.9376261830329895
  - 7.729002147912979
  - 7.759190917015076
  - 7.659214615821838
  - 7.6435187458992
  - 7.681521147489548
  - 7.763231813907623
  - 7.735363990068436
  - 7.797157198190689
  - 7.861930727958679
  - 7.83429080247879
  - 7.718629419803619
  - 7.704174429178238
  - 7.777961358428001
  - 7.66558974981308
  - 7.6649340987205505
  - 7.999916881322861
  - 7.707713603973389
  - 7.747186958789825
  - 7.787973880767822
  - 7.76338267326355
  - 7.6776324808597565
  - 7.609056130051613
  - 7.766974896192551
  - 7.7050561606884
  - 7.5263610780239105
  - 7.584988653659821
  - 7.682908266782761
  - 7.61809229850769
  - 7.6387713849544525
  - 7.729924529790878
  - 7.603715687990189
  - 7.677687644958496
  - 7.627767637372017
  - 7.649403095245361
  - 7.8775938749313354
  - 7.797860026359558
  - 7.810491561889648
  - 7.702842801809311
  - 7.713020473718643
  - 7.921395540237427
  - 7.798302248120308
  - 7.949323803186417
  - 7.611664414405823
  - 7.8350750207901
  - 7.683956414461136
  - 7.774982035160065
  - 7.943158209323883
  - 7.767561882734299
  - 7.735124558210373
  - 7.747405588626862
  - 7.885021358728409
  - 7.642192035913467
  - 7.673338711261749
  - 7.669238716363907
  - 7.753597795963287
  - 7.670801639556885
  - 7.7920946180820465
  - 7.846367180347443
  - 7.706498801708221
  - 7.841630578041077
  - 7.830868512392044
  - 7.6719576716423035
  - 7.7462286949157715
  - 7.672408908605576
  - 7.693027913570404
  - 7.742208391427994
  - 7.882894158363342
  - 8.023164853453636
  - 7.976473331451416
  - 7.865157276391983
  - 7.821362614631653
  - 7.935559064149857
  - 7.663752049207687
  - 7.504037082195282
  - 7.7801907658576965
  - 7.747505605220795
  validation_losses:
  - 0.39414963126182556
  - 0.3909326195716858
  - 0.38663071393966675
  - 0.3971863389015198
  - 0.39124253392219543
  - 0.38530153036117554
  - 0.38817262649536133
  - 0.38769960403442383
  - 0.3851358890533447
  - 0.44032371044158936
  - 0.39659157395362854
  - 0.40583541989326477
  - 0.42497268319129944
  - 0.42014992237091064
  - 0.4103640615940094
  - 0.42520251870155334
  - 0.43080782890319824
  - 0.40755167603492737
  - 0.4085642993450165
  - 0.40558871626853943
  - 0.40674179792404175
  - 9190083.0
  - 46460500.0
  - 547180480.0
  - 317762720.0
  - 997403072.0
  - 804360384.0
  - 505430432.0
  - 661252032.0
  - 660430528.0
  - 534518656.0
  - 564963072.0
  - 886365760.0
  - 588195200.0
  - 428798944.0
  - 238919680.0
  - 255972352.0
  - 473506048.0
  - 247134176.0
  - 127410744.0
  - 159168704.0
  - 517861472.0
  - 182911456.0
  - 730271040.0
  - 1062699392.0
  - 923294912.0
  - 380574624.0
  - 579053376.0
  - 583724416.0
  - 797012480.0
  - 267198304.0
  - 570610816.0
  - 514892096.0
  - 11551605.0
  - 443478080.0
  - 894536128.0
  - 536462848.0
  - 661394944.0
  - 590913152.0
  - 145665248.0
  - 424926656.0
  - 510226208.0
  - 400633664.0
  - 240384032.0
  - 50229820.0
  - 598929600.0
  - 516099392.0
  - 457037600.0
  - 744653120.0
  - 389850240.0
  - 349535968.0
  - 0.4075324237346649
  - 1147225216.0
  - 2092932480.0
  - 718420288.0
  - 592623680.0
  - 1066874752.0
  - 907270528.0
  - 168902288.0
  - 152829008.0
  - 369210816.0
  - 747181568.0
  - 838383616.0
  - 799153728.0
  - 726485056.0
  - 682954176.0
  - 980819648.0
  - 904745856.0
  - 443099264.0
  - 192437808.0
  - 740864960.0
  - 970606208.0
  - 746411328.0
  - 1052488384.0
  - 894536128.0
  - 968084608.0
  - 567257984.0
  - 671985088.0
  - 725224896.0
  - 456597664.0
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 20 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8576329331046312, 0.8593481989708405,
    0.8573883161512027]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8579270628871873
  mean_f1_accuracy: 0.0
  total_train_time: '0:02:42.138484'
