config:
  aggregation: sum
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 19:09:09.795235'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_125fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_125fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_125fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_125fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_125fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_125fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_125fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_125fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_125fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_125fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 53.83174930810929
  - 29.102833902835847
  - 18.409301471710204
  - 10.352200457453728
  - 19.433916306495668
  - 23.284684997797015
  - 11.162526166439058
  - 11.77085838019848
  - 17.00629763007164
  - 16.490992492437364
  - 16.729650390148162
  - 14.031983739137651
  - 17.19775066673756
  - 31.19863587021828
  - 18.956782543659212
  - 10.348169296979904
  - 9.0805082321167
  - 6.384400492906571
  - 7.827561962604523
  - 13.977330726385118
  - 7.070933976769448
  - 9.610076040029526
  - 11.333258515596391
  - 7.160022401809693
  - 7.488124310970306
  - 11.291630041599275
  - 14.327195608615876
  - 27.098068058490753
  - 11.275737237930299
  - 9.482828241586686
  - 11.390615528821947
  - 9.355592888593675
  - 9.062766867876054
  - 6.948030495643616
  - 6.924013859033585
  - 6.867308324575425
  - 6.1825384378433235
  - 7.7153096139431
  - 3.8791425943374636
  - 5.570055842399597
  - 7.318619039654732
  - 8.854619532823563
  - 9.124144458770752
  - 6.2544804334640505
  - 10.954202601313591
  - 8.280697983503343
  - 6.8568394780159
  - 4.285916864871979
  - 4.493183434009552
  - 4.43325492143631
  - 5.281660652160645
  - 4.83480248451233
  - 4.651734751462937
  - 5.043340551853181
  - 4.920178318023682
  - 4.979718577861786
  - 3.9337468206882478
  - 5.795033979415894
  - 5.893153446912766
  - 4.369418871402741
  - 3.8735701918601992
  - 3.3526692152023316
  - 4.086614799499512
  - 4.7917039573192595
  - 3.340733587741852
  - 3.330881649255753
  - 3.7356736719608308
  - 3.708349579572678
  - 4.353236639499665
  - 3.2420147538185122
  - 3.7940523087978364
  - 3.641138964891434
  - 3.5922551929950717
  - 3.414332532882691
  - 3.1267852008342745
  - 3.512704050540924
  - 3.5241756319999697
  - 3.3706013143062594
  - 3.3380109488964083
  - 4.450720542669297
  - 3.5447102010250093
  - 4.858479231595993
  - 3.8379978090524673
  - 3.458086109161377
  - 4.562146002054215
  - 3.8032862782478336
  - 3.5123870730400086
  - 3.9211316227912905
  - 5.479373514652252
  - 4.300311478972435
  - 3.524215239286423
  - 3.4403743863105776
  - 3.3442984580993653
  - 3.40297731757164
  - 3.293587934970856
  - 3.208197385072708
  - 3.473615747690201
  - 3.4451079666614532
  - 3.5907891750335694
  - 3.501162374019623
  validation_losses:
  - 5.448575019836426
  - 3.2889535427093506
  - 0.7388015985488892
  - 0.5234305262565613
  - 0.8137481212615967
  - 0.621926486492157
  - 1.1850950717926025
  - 0.5248921513557434
  - 0.5900924205780029
  - 0.9725863337516785
  - 1.1740328073501587
  - 0.6098279356956482
  - 0.49276697635650635
  - 0.43426188826560974
  - 0.43837428092956543
  - 0.548523485660553
  - 0.38773080706596375
  - 0.799824059009552
  - 0.5350667834281921
  - 0.6052428483963013
  - 0.43264755606651306
  - 0.4351280629634857
  - 0.5112488865852356
  - 0.5684696435928345
  - 0.4468080699443817
  - 0.6108846664428711
  - 0.40606051683425903
  - 1.0130316019058228
  - 0.9238340258598328
  - 0.463500052690506
  - 0.48797911405563354
  - 0.39737987518310547
  - 0.41477492451667786
  - 0.4513852596282959
  - 0.43656495213508606
  - 0.41299012303352356
  - 0.4173479378223419
  - 0.47768428921699524
  - 0.4427366852760315
  - 0.5039035677909851
  - 0.4772099256515503
  - 0.4184357523918152
  - 0.41714948415756226
  - 0.4680778980255127
  - 0.40168488025665283
  - 0.5077217221260071
  - 0.3968542516231537
  - 0.389619916677475
  - 0.3919660449028015
  - 0.5739380121231079
  - 0.49512389302253723
  - 0.4098529815673828
  - 0.42475858330726624
  - 0.577113151550293
  - 0.4674040973186493
  - 0.4165615737438202
  - 0.40344730019569397
  - 0.44057711958885193
  - 0.47010356187820435
  - 0.4085729718208313
  - 0.4197520911693573
  - 0.39752209186553955
  - 0.39539453387260437
  - 0.39780521392822266
  - 0.3853069245815277
  - 0.40882205963134766
  - 0.39123275876045227
  - 0.4026132822036743
  - 0.3877694010734558
  - 0.4036012887954712
  - 0.3922045826911926
  - 0.3935239613056183
  - 0.46159854531288147
  - 0.4156889319419861
  - 0.42353788018226624
  - 0.4128473997116089
  - 0.42514416575431824
  - 0.42004743218421936
  - 0.3845251500606537
  - 0.3961200714111328
  - 0.4863761365413666
  - 0.39679208397865295
  - 0.40960800647735596
  - 0.45009884238243103
  - 0.41199013590812683
  - 0.4020664393901825
  - 0.4082573354244232
  - 0.42108654975891113
  - 0.4109564423561096
  - 0.3969937860965729
  - 0.40558308362960815
  - 0.4106445908546448
  - 0.42763832211494446
  - 0.4228474795818329
  - 0.43731489777565
  - 0.4222041964530945
  - 0.42904800176620483
  - 0.41479524970054626
  - 0.4302692413330078
  - 0.42460453510284424
loss_records_fold1:
  train_losses:
  - 3.3407336950302127
  - 4.6839717686176305
  - 3.2287253737449646
  - 3.764114484190941
  - 4.307947364449501
  - 3.866003781557083
  - 4.798122853040695
  - 3.646406799554825
  - 3.547824460268021
  - 4.250626051425934
  - 3.3128128290176395
  - 3.6664990544319154
  - 3.4940775811672213
  - 3.4617686510086063
  - 3.333241730928421
  - 3.433799141645432
  - 3.201970711350441
  - 3.1767311096191406
  - 3.340535894036293
  - 3.167719042301178
  - 3.3645228087902073
  - 3.290682828426361
  - 3.2474726140499115
  - 3.269833612442017
  - 3.4086925089359283
  - 3.230794307589531
  - 3.215841361880303
  - 3.3320992171764376
  - 3.2278724998235706
  - 3.241195684671402
  - 3.1274576961994174
  - 3.1845660626888277
  - 3.30950785279274
  - 4.611860549449921
  - 3.9665838837623597
  - 3.613917750120163
  - 3.4879538118839264
  - 3.3169630825519563
  - 3.724575155973435
  - 3.403829175233841
  - 3.548912847042084
  - 3.4574360787868503
  - 3.4850723266601564
  - 3.3044570922851566
  - 3.674345090985298
  - 3.648944061994553
  - 3.166232720017433
  - 3.1952513933181765
  - 3.2883871644735336
  - 3.4172013819217684
  - 3.3147251158952713
  - 3.219853162765503
  - 3.3110810160636905
  - 3.2537837743759157
  - 3.360953325033188
  - 3.3353406310081484
  - 3.3278686285018924
  - 3.2374401807785036
  - 3.209079468250275
  - 3.235610216856003
  - 3.182736659049988
  - 3.1268079340457917
  - 3.1927362740039826
  - 3.523225575685501
  - 3.6194167613983157
  - 3.9224854290485385
  - 3.472335630655289
  - 3.7496810972690584
  - 4.557658576965332
  - 4.509374660253525
  validation_losses:
  - 0.4201588034629822
  - 0.40575554966926575
  - 0.4151259660720825
  - 0.45933768153190613
  - 0.4046632945537567
  - 0.40961942076683044
  - 0.43430009484291077
  - 0.4138661026954651
  - 1.0196696519851685
  - 0.40246665477752686
  - 0.42347195744514465
  - 0.46013668179512024
  - 0.45079725980758667
  - 0.41228243708610535
  - 0.5133935213088989
  - 0.42542532086372375
  - 0.4304925799369812
  - 0.44500038027763367
  - 0.4423030614852905
  - 0.43872565031051636
  - 0.45475438237190247
  - 0.4382626414299011
  - 0.4937208294868469
  - 0.40525731444358826
  - 0.4120987057685852
  - 0.4117083251476288
  - 0.432945191860199
  - 0.4331097900867462
  - 0.4365617036819458
  - 0.4225827157497406
  - 0.4210953116416931
  - 0.4822964668273926
  - 0.4384527802467346
  - 0.41814860701560974
  - 0.45088115334510803
  - 0.437340646982193
  - 0.4269397258758545
  - 0.4186016917228699
  - 0.4152548909187317
  - 0.415073961019516
  - 0.4847068190574646
  - 0.42455923557281494
  - 0.41427236795425415
  - 0.4157170057296753
  - 0.5008363127708435
  - 0.4180198013782501
  - 0.4026578962802887
  - 0.46993982791900635
  - 0.47239941358566284
  - 0.45166027545928955
  - 0.40958210825920105
  - 0.45658445358276367
  - 0.41735392808914185
  - 0.41771194338798523
  - 0.4329026937484741
  - 0.415683388710022
  - 0.4338839650154114
  - 15.159819602966309
  - 0.4097895622253418
  - 0.4307299852371216
  - 0.42183056473731995
  - 0.4079163372516632
  - 0.41058599948883057
  - 117759.59375
  - 52630.48828125
  - 3284.3154296875
  - 95.79993438720703
  - 2.0190908908843994
  - 0.44046497344970703
  - 0.4359971284866333
loss_records_fold2:
  train_losses:
  - 4.605259770154953
  - 3.5610494852066044
  - 3.6722750902175907
  - 4.194854736328125
  - 4.2376807749271395
  - 3.3355470955371858
  - 3.945564389228821
  - 3.3283833622932435
  - 3.5134440422058106
  - 3.5913186550140384
  - 3.177812173962593
  - 3.374797111749649
  - 3.3683930873870853
  - 3.4206256449222567
  - 3.3772445023059845
  - 3.4239017724990846
  - 3.219793581962586
  validation_losses:
  - 0.42107322812080383
  - 0.41398733854293823
  - 0.3933548927307129
  - 0.39444631338119507
  - 0.4066075384616852
  - 0.41824740171432495
  - 0.3977407217025757
  - 0.4206373393535614
  - 0.4296192228794098
  - 0.38557299971580505
  - 0.44103220105171204
  - 0.4134557843208313
  - 0.4090687334537506
  - 0.38861921429634094
  - 0.3847455382347107
  - 0.39140117168426514
  - 0.38187095522880554
loss_records_fold3:
  train_losses:
  - 3.027134293317795
  - 3.187813895940781
  - 3.0691262006759645
  - 3.2518757790327073
  - 3.287560367584229
  - 3.2345297932624817
  - 3.3249990284442905
  - 4.049989116191864
  - 5.149878889322281
  - 3.8329165101051332
  - 3.9257540047168735
  - 3.4856878161430362
  - 3.2796387493610384
  - 3.226713174581528
  - 3.3281363725662234
  - 3.2685740649700166
  - 3.336673909425736
  - 3.3229419350624085
  - 3.161385762691498
  - 3.250358521938324
  - 3.4862680912017825
  - 3.4587520301342014
  - 3.196934658288956
  - 3.4060830354690554
  - 3.258369854092598
  - 3.1345048159360887
  - 3.2198828607797623
  - 3.2494963943958286
  - 3.39299910068512
  - 3.276037719845772
  - 3.208397310972214
  - 3.1795358538627627
  - 3.159992435574532
  - 3.2516842961311343
  - 3.3032519549131396
  - 3.1666058599948883
  - 3.2740249752998354
  - 3.2485556662082673
  - 3.3318960905075077
  - 3.2170257657766346
  - 3.1943329870700836
  - 3.1799094617366794
  - 3.178296905755997
  - 3.3081467509269715
  - 3.277630233764649
  - 3.1480809569358827
  - 3.153377997875214
  - 3.369071584939957
  - 3.38507896065712
  - 3.158867448568344
  - 3.263391077518463
  - 3.1654212057590487
  - 3.1684961855411533
  - 3.1326146662235264
  - 3.2024852275848392
  - 3.117046329379082
  - 3.1927528381347656
  - 3.176846370100975
  validation_losses:
  - 0.3990863561630249
  - 0.41253241896629333
  - 0.4063596725463867
  - 0.39497092366218567
  - 0.39564135670661926
  - 0.49762991070747375
  - 0.41496187448501587
  - 55.9749641418457
  - 0.4458276033401489
  - 0.6497978568077087
  - 0.39632901549339294
  - 0.4053363800048828
  - 0.4478408098220825
  - 0.42330750823020935
  - 0.42619699239730835
  - 0.39610159397125244
  - 0.4334057867527008
  - 0.42534857988357544
  - 0.39277347922325134
  - 0.39991995692253113
  - 0.50868159532547
  - 0.42619699239730835
  - 0.4115538001060486
  - 0.40050747990608215
  - 0.5269290208816528
  - 0.4376722276210785
  - 0.41582682728767395
  - 0.42293158173561096
  - 0.44038981199264526
  - 0.42912688851356506
  - 0.397586464881897
  - 0.40740710496902466
  - 0.4256093502044678
  - 0.3938518166542053
  - 0.4095747172832489
  - 0.4502123296260834
  - 0.4179968535900116
  - 0.3960832953453064
  - 0.4092411696910858
  - 0.40914851427078247
  - 0.44474953413009644
  - 0.42048269510269165
  - 0.4066750705242157
  - 0.4141301214694977
  - 0.405122846364975
  - 0.41271647810935974
  - 0.5025246739387512
  - 0.4913733899593353
  - 0.44665077328681946
  - 0.4266907274723053
  - 0.41043365001678467
  - 0.4448673725128174
  - 0.4409016966819763
  - 0.41490963101387024
  - 0.4239993393421173
  - 0.4096928834915161
  - 0.41374096274375916
  - 0.4111330211162567
loss_records_fold4:
  train_losses:
  - 3.198533868789673
  - 3.157257759571076
  - 3.2476874828338627
  - 3.167065525054932
  - 3.4065849006175997
  - 3.4751132369041446
  - 3.2380473613739014
  - 3.347356760501862
  - 3.338656181097031
  - 3.2098836302757263
  - 3.300716555118561
  - 3.2219857156276706
  - 3.1991622865200045
  - 3.2735076010227204
  - 3.376408874988556
  - 3.2308958709239963
  - 3.2820150136947635
  - 3.372522157430649
  - 3.372133708000183
  - 3.408136621117592
  - 3.5098045945167544
  - 3.3941244900226595
  - 3.4332241982221605
  - 3.377920109033585
  - 3.3919901281595233
  - 3.0917012304067613
  - 3.134667193889618
  - 3.284599593281746
  - 3.5096573889255525
  - 3.2321494936943056
  - 3.3233948051929474
  - 3.837392407655716
  - 3.360960251092911
  - 3.8803760528564455
  - 3.4172878533601763
  - 3.1167240262031557
  - 3.1597724646329883
  - 3.408804708719254
  - 3.2464245080947878
  - 3.316705346107483
  - 3.1961787402629853
  - 3.306758809089661
  - 3.1491832733154297
  - 3.3511117070913317
  - 3.1934449672698975
  - 3.2803177475929264
  - 3.3024423956871036
  - 3.159487318992615
  - 3.1924291968345644
  - 3.2124091327190403
  - 3.3660893082618717
  - 3.1556526660919193
  - 3.1813518524169924
  - 3.2779791891574863
  - 3.2221007227897647
  - 3.35154829621315
  - 3.215790414810181
  - 3.3613228738307956
  - 3.2429563045501713
  - 3.2107277572155
  - 3.184757998585701
  - 3.2245409786701202
  - 3.272728663682938
  - 3.209360557794571
  validation_losses:
  - 0.4279106557369232
  - 0.40902724862098694
  - 0.40170586109161377
  - 0.4112967848777771
  - 0.4439028203487396
  - 0.4090917408466339
  - 0.4064699709415436
  - 0.3962385058403015
  - 0.4114113450050354
  - 0.4029598832130432
  - 0.3946535289287567
  - 0.3958486020565033
  - 0.41164883971214294
  - 0.39743414521217346
  - 0.38310399651527405
  - 0.41447994112968445
  - 0.414856493473053
  - 0.4244883954524994
  - 0.4259743392467499
  - 0.6003056764602661
  - 0.4233858585357666
  - 0.44067084789276123
  - 0.402730792760849
  - 0.401481032371521
  - 0.401096373796463
  - 0.38459861278533936
  - 0.39599454402923584
  - 0.403481662273407
  - 0.44835808873176575
  - 0.4385315179824829
  - 0.3934396803379059
  - 0.48064619302749634
  - 0.5059532523155212
  - 0.4436851143836975
  - 0.39531752467155457
  - 0.3996548652648926
  - 0.4227941632270813
  - 0.45019060373306274
  - 0.3996013104915619
  - 0.41748037934303284
  - 0.41481176018714905
  - 0.4069770574569702
  - 0.3977263271808624
  - 0.40626347064971924
  - 0.42925840616226196
  - 0.4183255434036255
  - 1.0399799346923828
  - 0.42059022188186646
  - 0.39906346797943115
  - 4.045155048370361
  - 0.4234677255153656
  - 0.4004283845424652
  - 0.44032177329063416
  - 0.39037442207336426
  - 586.3324584960938
  - 603.34130859375
  - 0.40033408999443054
  - 0.4121444821357727
  - 0.4205795228481293
  - 0.4128636419773102
  - 0.4002181589603424
  - 0.3969326615333557
  - 0.40603360533714294
  - 0.40067392587661743
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 70 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 58 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 64 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:20:17.173530'
