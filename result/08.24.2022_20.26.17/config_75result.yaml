config:
  aggregation: sum
  batch_size: 256
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:11:01.505624'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_75fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_75fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_75fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_75fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_75fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_75fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_75fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_75fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_75fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_75fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 16.13702745437622
  - 5.679261612892152
  - 3.990178942680359
  - 3.673355346918106
  - 6.834548461437226
  - 2.9672508716583255
  - 2.0256409466266634
  - 1.6369147390127183
  - 1.3447253942489625
  - 1.25976420044899
  - 1.3792541563510896
  - 1.1750316500663758
  - 0.9483396232128144
  - 1.0291938841342927
  - 1.3808408081531525
  - 4.46070100069046
  - 4.403235769271851
  - 1.5584882736206056
  - 1.0866776883602143
  - 1.0898691177368165
  - 1.274305295944214
  - 1.0815108954906465
  - 1.2947279155254365
  - 1.1638232529163361
  - 1.1804807126522066
  - 0.9061134696006775
  - 1.0975050449371337
  - 0.997459751367569
  - 0.8636372804641724
  - 0.9162697851657868
  - 1.3668264091014863
  - 0.9882255911827088
  - 0.9140287399291993
  - 2.3834202527999877
  - 0.994046700000763
  - 1.111528640985489
  - 1.2574810206890108
  - 1.0091156363487244
  - 1.1184072017669677
  - 0.9566252648830414
  - 1.4795455813407898
  - 0.9079278349876404
  - 0.9174605548381806
  - 2.512782406806946
  - 1.1228712022304534
  - 0.8749818682670594
  - 7.4165511429309845
  - 3.7929247081279756
  - 0.9139383614063263
  - 1.221229887008667
  - 0.9929351687431336
  - 0.8741284966468812
  - 1.3893949747085572
  - 0.816574215888977
  - 0.8238600432872772
  - 1.017448788881302
  - 1.4189248800277712
  - 1.9145040750503541
  - 1.7655171811580659
  - 1.635953325033188
  - 3.571881401538849
  - 3.0648272275924686
  - 4.011904978752137
  - 1.528912204504013
  - 1.0391731142997742
  - 1.3289311766624452
  - 0.9553014755249024
  - 0.9574876070022583
  - 0.9320591270923615
  - 0.8418460488319397
  - 1.0079243183135986
  - 1.0039818644523621
  - 1.3727911531925203
  - 1.0625966519117356
  - 3.539435750246048
  - 1.109985202550888
  - 1.180988597869873
  - 1.058671846985817
  - 0.910664701461792
  - 0.9185222446918488
  - 0.807231068611145
  - 0.8553618609905244
  - 0.9931370735168458
  - 0.8114296972751618
  - 0.906971824169159
  - 0.9408380806446076
  - 1.5792131066322328
  - 0.9486305356025696
  - 0.9111105442047119
  - 0.9543015539646149
  - 0.8955134272575379
  - 0.8923391401767731
  - 0.9417101681232453
  - 0.8386416494846345
  - 0.8233492195606232
  - 0.920414000749588
  - 0.812202513217926
  - 0.7858985781669617
  - 0.8597119927406311
  - 0.7561136335134506
  validation_losses:
  - 19.982900619506836
  - 2.376803159713745
  - 1.3854962587356567
  - 2.924085855484009
  - 5.465173244476318
  - 2.0995798110961914
  - 3.610853672027588
  - 0.5255947113037109
  - 0.6125456690788269
  - 0.8618472814559937
  - 0.7748687863349915
  - 0.4263356029987335
  - 0.5039678812026978
  - 0.4430040717124939
  - 0.6039646863937378
  - 2.555492401123047
  - 0.6951306462287903
  - 0.5444403290748596
  - 0.5722900032997131
  - 0.4813368022441864
  - 0.43348538875579834
  - 0.6260986328125
  - 0.4471895694732666
  - 0.5211829543113708
  - 0.48049911856651306
  - 0.48698297142982483
  - 0.4472394585609436
  - 0.4111575186252594
  - 0.41037026047706604
  - 0.4363042116165161
  - 0.43447595834732056
  - 0.39725014567375183
  - 0.40969523787498474
  - 0.48601478338241577
  - 0.444375216960907
  - 0.4780013859272003
  - 0.40515264868736267
  - 0.4010065793991089
  - 0.3955116868019104
  - 0.3882593512535095
  - 0.4695754945278168
  - 0.3916943371295929
  - 0.40865764021873474
  - 0.4019804298877716
  - 0.6512327790260315
  - 0.41569626331329346
  - 0.39678528904914856
  - 0.5763502717018127
  - 0.5028674006462097
  - 0.3945540189743042
  - 0.3823414742946625
  - 0.41050073504447937
  - 0.3907064199447632
  - 0.4190513789653778
  - 0.4024188816547394
  - 0.40110111236572266
  - 0.5899288654327393
  - 2.3174169063568115
  - 0.5254604816436768
  - 0.7253327369689941
  - 0.4514693021774292
  - 0.4366801083087921
  - 0.8674139976501465
  - 0.5161184072494507
  - 0.39699679613113403
  - 0.4041149914264679
  - 0.38203397393226624
  - 0.4920263886451721
  - 0.3866080641746521
  - 0.3834596574306488
  - 0.5963559746742249
  - 0.39074283838272095
  - 0.390088826417923
  - 0.44394993782043457
  - 0.4264838397502899
  - 0.44651663303375244
  - 0.3990638256072998
  - 0.3908616304397583
  - 0.3873758018016815
  - 0.38683032989501953
  - 0.3895743787288666
  - 0.41352716088294983
  - 0.412680059671402
  - 0.3888591229915619
  - 0.3889199495315552
  - 0.41696006059646606
  - 0.47579333186149597
  - 0.39293304085731506
  - 0.3879948854446411
  - 0.3880827724933624
  - 0.3996421992778778
  - 0.3866865634918213
  - 0.407161682844162
  - 0.3941958248615265
  - 0.38729578256607056
  - 0.38840290904045105
  - 0.38745400309562683
  - 0.38726451992988586
  - 0.3987441658973694
  - 0.3903293311595917
loss_records_fold1:
  train_losses:
  - 0.8720752596855164
  - 0.7742147982120514
  - 0.7593640625476837
  - 0.8414377450942994
  - 0.8377154231071473
  - 0.8764204382896423
  - 0.7968685448169709
  - 0.8201501071453094
  - 0.7861649632453919
  - 0.7920704782009125
  - 0.7656447023153305
  - 0.7905174314975739
  - 0.7807136356830597
  - 0.7871037065982819
  - 1.7216844320297242
  - 1.291391807794571
  - 0.8758647680282593
  - 1.1439412444829942
  - 1.0025952994823457
  - 1.420806634426117
  - 0.9071259140968323
  - 0.9555716156959534
  - 0.9026654958724976
  - 0.832350105047226
  - 0.9992988407611847
  - 1.3761803388595581
  validation_losses:
  - 0.3994225561618805
  - 0.40489259362220764
  - 0.4730473458766937
  - 0.40804290771484375
  - 0.42402738332748413
  - 0.4598272442817688
  - 0.4003766179084778
  - 0.39958426356315613
  - 0.40700051188468933
  - 0.4444805383682251
  - 0.4078907370567322
  - 0.4009232521057129
  - 0.4033557176589966
  - 0.4043092429637909
  - 0.6577388644218445
  - 0.40021342039108276
  - 0.4022509455680847
  - 0.39998990297317505
  - 0.398347944021225
  - 0.42503878474235535
  - 0.40047183632850647
  - 0.4025067687034607
  - 0.40663596987724304
  - 0.39843297004699707
  - 0.4037036895751953
  - 0.39503413438796997
loss_records_fold2:
  train_losses:
  - 0.8112042784690857
  - 0.8325340747833252
  - 0.8215865671634675
  - 0.8380909562110901
  - 0.8460114657878877
  - 2.9347628712654115
  - 2.5867393851280216
  - 0.9793201208114625
  - 3.0695467352867127
  - 1.8399453938007355
  - 0.9882090866565705
  - 1.5215391397476197
  - 1.2368702352046967
  - 1.0808036148548126
  - 10.267471015453339
  - 1.080416452884674
  - 0.8269505441188812
  - 0.7752311348915101
  - 0.7963725924491882
  - 0.7759981185197831
  - 0.7711079239845277
  - 0.802340978384018
  validation_losses:
  - 0.39318299293518066
  - 0.3820493221282959
  - 0.3926970064640045
  - 0.48122891783714294
  - 0.39566469192504883
  - 0.42863696813583374
  - 0.410799115896225
  - 0.40635213255882263
  - 0.4916057288646698
  - 0.3813345730304718
  - 0.5263950228691101
  - 0.3777197003364563
  - 0.3860940635204315
  - 0.41738244891166687
  - 0.3776053488254547
  - 0.4014248251914978
  - 0.38049760460853577
  - 0.37923938035964966
  - 0.3798935115337372
  - 0.3834441602230072
  - 0.3871050179004669
  - 0.37657269835472107
loss_records_fold3:
  train_losses:
  - 0.8325225591659546
  - 0.7664731919765473
  - 0.7567981898784638
  - 0.7735264658927918
  - 0.7566409826278687
  - 0.8581137359142303
  - 0.8296129822731019
  - 0.779644674062729
  - 0.8177266299724579
  - 0.8343993902206421
  - 0.838298922777176
  - 0.8048596084117889
  - 0.7607119798660279
  - 0.7933850884437561
  - 0.8269976317882538
  - 1.839624893665314
  - 0.7731815099716187
  - 0.8114047586917877
  - 0.7758254230022431
  - 0.8552918076515198
  - 0.7587421000003816
  - 0.9049986839294434
  - 0.9864842891693115
  - 0.8291381716728211
  - 0.8498051762580872
  - 0.7651609778404236
  - 3.5609337389469147
  - 0.824673092365265
  - 0.7719916641712189
  - 0.8538118362426759
  - 2.01665558218956
  - 1.0776769161224367
  - 0.8520641922950745
  - 0.7617398709058762
  - 0.7611417949199677
  - 0.7696292340755463
  - 0.8375859498977661
  - 0.847851300239563
  - 0.9099938988685609
  - 0.8077629148960114
  - 0.7907879948616028
  - 0.7746695995330811
  - 0.7699600756168365
  - 0.7914563655853272
  validation_losses:
  - 0.3817710280418396
  - 0.37774857878685
  - 0.3841134309768677
  - 0.3905700147151947
  - 0.3856953978538513
  - 0.40107208490371704
  - 0.3831113874912262
  - 0.41436558961868286
  - 0.3854143023490906
  - 0.40402132272720337
  - 0.3848869800567627
  - 0.38376760482788086
  - 0.3838360607624054
  - 0.3835095465183258
  - 0.3826583921909332
  - 0.4070877134799957
  - 0.3896753489971161
  - 0.38409534096717834
  - 0.3834742605686188
  - 0.39791905879974365
  - 0.3820013701915741
  - 0.41291865706443787
  - 0.40124818682670593
  - 0.3842173218727112
  - 0.3825029730796814
  - 0.3846832811832428
  - 0.3812194764614105
  - 0.39735913276672363
  - 0.3788561224937439
  - 0.3883029818534851
  - 1.9275790452957153
  - 0.458494633436203
  - 0.38408368825912476
  - 0.39469829201698303
  - 0.38284167647361755
  - 0.3806192874908447
  - 0.37828490138053894
  - 0.41126200556755066
  - 0.3944343626499176
  - 0.3940330743789673
  - 0.4008719027042389
  - 0.3814259171485901
  - 0.38964229822158813
  - 0.378217488527298
loss_records_fold4:
  train_losses:
  - 0.7767381131649018
  - 0.7896957457065583
  - 0.7891772270202637
  - 0.813800609111786
  - 0.8023540616035462
  - 0.7929183006286622
  - 0.7614031553268433
  - 0.7821903228759766
  - 0.8305557131767274
  - 0.8223450541496278
  - 0.8262394487857819
  - 0.7284953176975251
  - 0.7558492600917817
  - 0.7770679473876954
  - 0.853449696302414
  validation_losses:
  - 0.39695748686790466
  - 0.4018155634403229
  - 0.3905610144138336
  - 0.3826042711734772
  - 0.40158364176750183
  - 0.38588982820510864
  - 0.3907313048839569
  - 0.39522361755371094
  - 0.41115736961364746
  - 0.4133278727531433
  - 0.39149245619773865
  - 0.3809308111667633
  - 0.3878842890262604
  - 0.396194189786911
  - 0.3988264799118042
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 26 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 44 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:17:13.929654'
