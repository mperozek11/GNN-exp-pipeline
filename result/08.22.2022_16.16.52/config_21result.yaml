config:
  aggregation: mean
  batch_size: 64
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 16:34:46.081678'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_21fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_21fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_21fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_21fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_21fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_21fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_21fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_21fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_21fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_21fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 3.6757111668586733
  - 3.217333048582077
  - 3.2062310695648195
  - 3.0994086593389514
  - 3.247874861955643
  - 3.116601121425629
  - 3.086470746994019
  - 3.250508373975754
  - 3.1663749992847445
  - 3.106033056974411
  - 3.07559289932251
  validation_losses:
  - 0.4613899290561676
  - 0.4084416627883911
  - 0.41213706135749817
  - 0.4027080833911896
  - 0.4231550693511963
  - 0.3939812481403351
  - 0.39179158210754395
  - 0.3939652442932129
  - 0.39476293325424194
  - 0.4000392258167267
  - 0.38822051882743835
loss_records_fold1:
  train_losses:
  - 3.117313170433045
  - 3.1415888488292696
  - 3.0189749211072923
  - 3.055597722530365
  - 3.0597074151039125
  - 3.028211832046509
  - 3.0393971502780914
  - 2.9129377841949466
  - 2.9560300141572955
  - 3.0955723702907565
  - 3.1239427983760835
  - 3.105330151319504
  - 3.045884531736374
  - 3.0566974341869355
  validation_losses:
  - 0.3955914378166199
  - 0.43245455622673035
  - 0.42400994896888733
  - 0.39598673582077026
  - 0.3989286720752716
  - 0.4049408435821533
  - 0.4020935297012329
  - 0.41247129440307617
  - 0.4125981330871582
  - 0.40145498514175415
  - 0.39566466212272644
  - 0.40058645606040955
  - 0.4092269539833069
  - 0.4182039499282837
loss_records_fold2:
  train_losses:
  - 3.061093717813492
  - 3.0786235451698305
  - 3.0500331908464435
  - 3.0660861492156983
  - 3.0930805236101153
  - 3.0319394290447237
  - 2.981155997514725
  - 3.030707234144211
  - 3.1063034713268283
  - 3.0246826112270355
  - 3.0226022958755494
  - 2.996250733733177
  - 3.015765029191971
  - 3.105822652578354
  - 3.0052177190780642
  validation_losses:
  - 0.40161463618278503
  - 0.4091424345970154
  - 0.40201207995414734
  - 0.4053758382797241
  - 0.3905087113380432
  - 0.5618874430656433
  - 0.39820602536201477
  - 0.3881068527698517
  - 0.40013623237609863
  - 0.4012449383735657
  - 0.3986018896102905
  - 0.39051881432533264
  - 0.39562058448791504
  - 0.40406563878059387
  - 0.3881150484085083
loss_records_fold3:
  train_losses:
  - 3.0810613274574283
  - 3.0224696159362794
  - 3.0372002065181736
  - 3.089134979248047
  - 3.0531820297241214
  - 3.0468965947628024
  - 3.0605927467346192
  - 3.0104697555303574
  - 2.98637952208519
  - 3.108777248859406
  - 3.0514076292514805
  - 3.0415887355804445
  - 3.098859053850174
  - 2.969663441181183
  - 3.1140529394149783
  - 3.16849422454834
  - 3.0147119224071504
  - 3.119436204433441
  - 3.0926630377769473
  - 3.0640894114971164
  - 2.9724749445915224
  - 2.973277205228806
  - 3.0155916154384617
  - 2.99751877784729
  - 3.016911727190018
  - 3.042918193340302
  - 3.0411674976348877
  - 3.0580480456352235
  - 3.0007721334695816
  - 2.935288143157959
  - 2.9506249964237217
  - 3.07122694849968
  - 3.0464934885501864
  - 2.923762208223343
  - 2.96262047290802
  - 2.97666974067688
  - 3.01245174407959
  - 3.0277336180210117
  - 3.014509040117264
  - 2.970318865776062
  - 3.0198811471462252
  - 3.038151586055756
  - 2.9480483382940292
  - 2.991670101881027
  - 3.0041542649269104
  - 2.918219953775406
  - 2.9778835833072663
  - 3.071155732870102
  - 3.0917908251285553
  - 3.0625128000974655
  - 2.995398473739624
  - 3.0293163061141968
  - 3.008172106742859
  - 2.95253221988678
  - 3.0324333846569065
  - 3.107172000408173
  - 3.0788000345230104
  - 2.987245383858681
  - 2.980822503566742
  - 3.054296976327896
  - 3.0365691900253298
  - 3.009405934810639
  - 3.0630923092365268
  - 2.970911401510239
  - 3.0197603046894077
  - 2.998661440610886
  - 3.0391695350408554
  - 3.0190436840057373
  - 3.0075686246156694
  - 3.0092544555664062
  - 2.9797849655151367
  - 2.949756866693497
  - 2.965232023596764
  - 2.9239440083503725
  - 2.878747355937958
  - 3.004363065958023
  - 2.963005006313324
  - 2.986992335319519
  - 2.9850038528442386
  - 2.977371293306351
  - 3.0610311329364777
  - 2.9964970499277115
  - 3.0261200666427612
  - 2.979339444637299
  - 2.9750601649284363
  - 2.9181891679763794
  - 3.008640632033348
  - 2.972130769491196
  - 2.9530908644199374
  - 2.930973613262177
  - 2.949461889266968
  - 3.0237103402614594
  - 2.9816789031028748
  - 2.88102098107338
  - 2.9413598775863647
  - 2.9674469232559204
  - 2.9389258444309236
  - 2.9886372089385986
  - 2.9398804664611817
  - 2.9380518019199373
  validation_losses:
  - 0.3824976980686188
  - 0.39864063262939453
  - 0.37986376881599426
  - 0.39016568660736084
  - 0.39646539092063904
  - 0.418711394071579
  - 0.3822454512119293
  - 0.4153413772583008
  - 0.47290605306625366
  - 0.4493430256843567
  - 0.3900887370109558
  - 0.3970055878162384
  - 0.5716279745101929
  - 1.2016801834106445
  - 0.3875247836112976
  - 0.5333921909332275
  - 1.5806750059127808
  - 0.44224902987480164
  - 0.4764527380466461
  - 0.9764758348464966
  - 0.4145026206970215
  - 0.4260823428630829
  - 0.7610116600990295
  - 0.4109114408493042
  - 0.5211264491081238
  - 0.5917211771011353
  - 0.8093592524528503
  - 0.5210123062133789
  - 0.7140882015228271
  - 0.42580798268318176
  - 0.7781903147697449
  - 0.5418300032615662
  - 0.6132669448852539
  - 0.7979479432106018
  - 0.7139023542404175
  - 0.46782323718070984
  - 0.6398902535438538
  - 0.894500732421875
  - 0.5472595691680908
  - 0.566746175289154
  - 0.5144760012626648
  - 0.5466488003730774
  - 0.766281008720398
  - 0.5503793358802795
  - 0.8890217542648315
  - 1.11158287525177
  - 0.3838990032672882
  - 0.40556570887565613
  - 0.38316792249679565
  - 0.40841802954673767
  - 0.4124113917350769
  - 0.49448633193969727
  - 0.6493748426437378
  - 0.41055378317832947
  - 0.3980920910835266
  - 0.44082897901535034
  - 0.7892425060272217
  - 0.5606434941291809
  - 0.4070286154747009
  - 0.527562141418457
  - 0.425378680229187
  - 0.5219843983650208
  - 0.4427030384540558
  - 0.47440242767333984
  - 0.4833301901817322
  - 1.4522428512573242
  - 0.4371792674064636
  - 0.5202847123146057
  - 0.5067301988601685
  - 0.9620361328125
  - 0.3862340450286865
  - 0.4564811587333679
  - 0.4518178403377533
  - 0.5322991013526917
  - 0.6461603045463562
  - 0.424604207277298
  - 0.5303248167037964
  - 0.9927351474761963
  - 0.5254627466201782
  - 0.4195577800273895
  - 0.4012707769870758
  - 0.42768898606300354
  - 0.8080698847770691
  - 2.1006522178649902
  - 6.191928386688232
  - 5.03115177154541
  - 1.0622066259384155
  - 1.6350020170211792
  - 2.7221601009368896
  - 0.5063690543174744
  - 0.6855818629264832
  - 0.43006017804145813
  - 0.45688846707344055
  - 0.5354810357093811
  - 0.44472455978393555
  - 0.4211483597755432
  - 0.49956533312797546
  - 0.5520411133766174
  - 0.48243412375450134
  - 0.4513435363769531
loss_records_fold4:
  train_losses:
  - 2.953061187267304
  - 2.9894285798072815
  - 3.1569698393344883
  - 3.020089137554169
  - 3.0405926018953324
  - 2.9927541613578796
  - 2.9908713310956956
  - 3.0188244700431826
  - 3.0214751154184345
  - 2.9375684142112735
  - 3.003198426961899
  - 2.8785398662090302
  - 3.0450293242931368
  - 2.9639767289161685
  - 3.0144089460372925
  - 3.030387538671494
  - 2.9909435689449313
  - 2.865662154555321
  - 2.9248373210430145
  - 2.9712601959705354
  - 2.899259328842163
  - 3.024836724996567
  - 2.9600355446338655
  - 2.9404894888401034
  - 2.9307588905096056
  - 2.936416167020798
  - 2.9074423134326937
  - 2.9628106474876406
  - 2.9510822713375093
  - 2.9689636647701265
  - 2.992947444319725
  - 3.0065504133701326
  - 2.980554783344269
  - 2.9967408895492555
  - 2.956901562213898
  - 2.913456267118454
  - 2.987827664613724
  - 2.901870322227478
  - 2.910442763566971
  - 3.1253565788269047
  - 3.0850723803043367
  - 3.0648516446352008
  - 3.002269893884659
  - 2.974848741292954
  validation_losses:
  - 0.428899347782135
  - 0.45906543731689453
  - 0.42316916584968567
  - 0.3910880982875824
  - 0.4019615948200226
  - 0.4052921533584595
  - 0.38356414437294006
  - 0.4435863792896271
  - 0.4362151622772217
  - 0.4154236316680908
  - 0.40713077783584595
  - 0.4101211726665497
  - 0.42305850982666016
  - 0.5072889924049377
  - 0.43652334809303284
  - 0.3899103105068207
  - 0.44003191590309143
  - 0.4414379298686981
  - 0.5269551873207092
  - 0.4452736973762512
  - 0.4886322617530823
  - 0.45371532440185547
  - 0.4037743806838989
  - 0.4182903468608856
  - 0.47425752878189087
  - 0.4880712926387787
  - 0.4391478896141052
  - 0.41715359687805176
  - 0.4035937786102295
  - 0.3932514786720276
  - 0.4381079375743866
  - 0.4203685224056244
  - 0.46803662180900574
  - 0.484839528799057
  - 0.4038558006286621
  - 0.4674849510192871
  - 0.3980063199996948
  - 0.4525285065174103
  - 0.44660037755966187
  - 0.3742983639240265
  - 0.38131478428840637
  - 0.38412532210350037
  - 0.3909810483455658
  - 0.3792392611503601
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 14 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 15 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 44 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.823327615780446,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.21374045801526717, 0.0]'
  mean_eval_accuracy: 0.851409642034034
  mean_f1_accuracy: 0.042748091603053436
  total_train_time: '0:16:24.363692'
