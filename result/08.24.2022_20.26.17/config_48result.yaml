config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:31:17.763812'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_48fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_48fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_48fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_48fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_48fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_48fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_48fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_48fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_48fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_48fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.130672910809517
  - 5.892178851366044
  - 5.936330771446229
  - 5.825666826963425
  - 5.8741113394498825
  - 5.936963823437691
  - 5.836138847470284
  - 5.7448929280042655
  - 6.117777028679848
  - 5.884508416056633
  - 5.731805974245072
  - 5.871006035804749
  - 5.723381073772908
  - 5.8136831521987915
  - 5.7743768990039825
  - 5.7312016725540165
  - 5.762486872076988
  - 5.688917297124863
  - 5.686246874928475
  - 5.622340312600136
  - 5.852192762494088
  - 5.7243878751993185
  - 5.621339666843415
  - 5.656017211079598
  - 5.60945130288601
  - 5.674292051792145
  - 5.610228523612022
  - 5.587865635752678
  - 5.672685861587524
  - 5.6048934936523445
  - 5.624487587809563
  - 5.638591071963311
  - 5.627824592590333
  - 5.657861810922623
  - 5.627533993124962
  - 5.609878528118134
  - 5.579668888449669
  - 5.5917612522840505
  - 5.563080045580865
  - 5.60311882942915
  - 5.577095657587051
  - 5.576962700486184
  - 5.499643230438233
  - 5.519476628303528
  - 5.557508873939515
  - 5.552254679799081
  - 5.531983026862145
  - 5.728342831134796
  - 5.6773605972528465
  - 5.538667994737626
  - 5.542528960108758
  - 5.5297678232192995
  - 5.540992310643197
  - 5.612444041669369
  - 5.512786620855332
  - 5.600925990939141
  - 5.542568382620812
  - 5.55767267048359
  - 5.481957548856736
  - 5.46332260966301
  - 5.547570136189461
  - 5.544279459118844
  - 5.461490643024445
  - 5.55344387292862
  - 5.508613321185113
  - 5.524691197276116
  - 5.49595762193203
  - 5.530129703879357
  - 5.494081765413284
  - 5.454819437861443
  - 5.48848682641983
  - 5.5163938999176025
  - 5.502830365300179
  - 5.558696019649506
  - 5.50603692829609
  - 6.1931025922298435
  - 5.744392231106758
  - 5.666513735055924
  - 5.690913558006287
  - 5.6034627825021746
  - 5.5545369565486915
  - 5.533469551801682
  - 5.563643035292626
  - 5.602502653002739
  validation_losses:
  - 0.5980139374732971
  - 1.30275559425354
  - 0.4034557342529297
  - 0.47842806577682495
  - 0.41359421610832214
  - 0.3942257761955261
  - 0.43752560019493103
  - 0.3966573476791382
  - 0.7879468202590942
  - 1.823930025100708
  - 0.4367281198501587
  - 0.46106523275375366
  - 0.5532949566841125
  - 0.38290640711784363
  - 0.391984224319458
  - 0.39174625277519226
  - 0.37950074672698975
  - 11.642863273620605
  - 0.5259253978729248
  - 0.744013249874115
  - 0.5045665502548218
  - 0.38142329454421997
  - 0.4770776331424713
  - 0.747541606426239
  - 0.38539543747901917
  - 0.38366565108299255
  - 0.37792786955833435
  - 1.289242148399353
  - 0.4052143096923828
  - 3.717787027359009
  - 0.39375782012939453
  - 0.40294110774993896
  - 0.39257746934890747
  - 0.39434805512428284
  - 0.390318900346756
  - 0.40561503171920776
  - 0.3842213451862335
  - 0.37907618284225464
  - 0.40285608172416687
  - 0.39090341329574585
  - 0.40697669982910156
  - 4.102845191955566
  - 4.673483371734619
  - 0.5000906586647034
  - 0.40771302580833435
  - 0.4020169675350189
  - 0.42408880591392517
  - 0.5807122588157654
  - 0.5249290466308594
  - 3.2118184566497803
  - 0.4175975024700165
  - 2.352827310562134
  - 0.3822086453437805
  - 0.3811032176017761
  - 0.39144784212112427
  - 0.4132055640220642
  - 0.44837844371795654
  - 0.40034106373786926
  - 0.4520508646965027
  - 1.1701945066452026
  - 0.3861469328403473
  - 0.5369283556938171
  - 0.4639596939086914
  - 0.41797584295272827
  - 0.3896772265434265
  - 0.9327409863471985
  - 0.6747892498970032
  - 0.43743059039115906
  - 0.49789759516716003
  - 1.1140058040618896
  - 3.7988834381103516
  - 0.5967625379562378
  - 2.049672842025757
  - 2.7195093631744385
  - 0.971642017364502
  - 0.4007752239704132
  - 0.39094385504722595
  - 0.4063674211502075
  - 0.3915702700614929
  - 0.3976367115974426
  - 0.3892260193824768
  - 0.38778555393218994
  - 0.3876401484012604
  - 0.3887568414211273
loss_records_fold1:
  train_losses:
  - 5.559402036666871
  - 5.555017498135567
  - 5.587910598516465
  - 5.565360471606255
  - 5.611451613903046
  - 5.58548783659935
  - 5.584660544991493
  - 5.574882930517197
  - 5.54644877910614
  - 5.559428077936173
  - 5.597475665807725
  - 5.547506529092789
  - 5.576589497923852
  - 5.577621909976006
  - 5.632385310530663
  - 5.541368928551674
  - 5.541614486277104
  - 5.562659743428231
  - 5.585535138845444
  - 5.554331579804421
  - 5.516417148709298
  - 5.5996764481067665
  validation_losses:
  - 0.3838282823562622
  - 0.3932149410247803
  - 0.38516414165496826
  - 0.39768511056900024
  - 0.3859170973300934
  - 0.38539791107177734
  - 0.4073169529438019
  - 0.3856154680252075
  - 0.38639184832572937
  - 0.3885074555873871
  - 0.3907102346420288
  - 0.3852856755256653
  - 0.397449254989624
  - 0.3944088816642761
  - 0.384836345911026
  - 0.40809205174446106
  - 0.3902341425418854
  - 0.38906341791152954
  - 0.3900481164455414
  - 0.38532891869544983
  - 0.3846254050731659
  - 0.38313013315200806
loss_records_fold2:
  train_losses:
  - 5.537953591346741
  - 5.533940514922143
  - 5.523411527276039
  - 5.590820497274399
  - 5.5834376364946365
  - 5.575606185197831
  - 5.608310344815255
  - 5.5688898980617525
  - 5.585842248797417
  - 5.603139477968217
  - 5.622810423374176
  validation_losses:
  - 0.3885742723941803
  - 0.3874436318874359
  - 0.4045420289039612
  - 0.3870786130428314
  - 0.38650602102279663
  - 0.3866080343723297
  - 0.38351133465766907
  - 0.38918131589889526
  - 0.39079004526138306
  - 0.39135369658470154
  - 0.38716912269592285
loss_records_fold3:
  train_losses:
  - 5.567936110496522
  - 5.610576930642129
  - 5.625713539123535
  - 5.584145447611809
  - 5.589476478099823
  - 5.612273246049881
  - 5.563433954119683
  - 5.553489688038827
  - 5.638080111145974
  - 5.5872774690389635
  - 5.5945920854806905
  - 5.61021588742733
  - 5.564967900514603
  - 5.6114439338445665
  - 5.599633541703224
  - 5.5733095273375515
  validation_losses:
  - 0.36439669132232666
  - 0.38992100954055786
  - 0.3735538721084595
  - 0.36871883273124695
  - 0.3687537908554077
  - 0.3837415874004364
  - 0.39152100682258606
  - 0.3666398823261261
  - 0.37097615003585815
  - 0.3850504159927368
  - 0.36432334780693054
  - 0.36970818042755127
  - 0.36946552991867065
  - 0.37057945132255554
  - 0.36942654848098755
  - 0.37318068742752075
loss_records_fold4:
  train_losses:
  - 5.56405209004879
  - 5.547669684886933
  - 5.5434058755636215
  - 5.560448265075684
  - 5.58638942539692
  - 5.537305456399918
  - 5.5828857839107515
  - 5.586924684047699
  - 5.556352859735489
  - 5.541736033558846
  - 5.57621209025383
  - 5.571619608998299
  - 5.589004236459733
  - 5.569442385435105
  - 5.581799772381783
  - 5.585449513792992
  validation_losses:
  - 0.3705422878265381
  - 0.37299618124961853
  - 0.3715640902519226
  - 0.3733825087547302
  - 0.376386433839798
  - 0.3805798888206482
  - 0.39344388246536255
  - 0.3855637013912201
  - 0.372921347618103
  - 0.38339778780937195
  - 0.3751169443130493
  - 0.3794844150543213
  - 0.3800564110279083
  - 0.3798398971557617
  - 0.3831378221511841
  - 0.39219236373901367
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 84 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 22 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:16:16.646583'
