config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:30:30.732246'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_89fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_89fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_89fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_89fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_89fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_89fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_89fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_89fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_89fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_89fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 47.40002845525742
  - 9.872937738895416
  - 6.790662693977357
  - 12.616409182548523
  - 21.657989954948427
  - 5.94029148221016
  - 6.1894301861524585
  - 5.295265921950341
  - 7.964582926034928
  - 7.5082853019237525
  - 7.143405169248581
  - 7.3678669929504395
  - 4.817849844694138
  - 10.147471004724503
  - 6.531364512443543
  - 8.617931109666825
  - 6.704785203933716
  - 6.876158976554871
  - 4.605152955651284
  - 3.062166684865952
  - 3.5396143138408664
  - 3.606331667304039
  - 4.921291622519494
  - 3.7216796696186067
  - 6.900069788098335
  - 3.4562205851078036
  - 4.276224339008332
  - 4.970547649264336
  - 5.164774018526078
  - 3.214794969558716
  - 3.778708240389824
  validation_losses:
  - 0.8313596844673157
  - 1.5645174980163574
  - 0.7740017771720886
  - 1.014014720916748
  - 0.4916742146015167
  - 0.3881056010723114
  - 0.415250688791275
  - 0.41110527515411377
  - 0.4897443354129791
  - 0.4088616371154785
  - 0.5575413107872009
  - 0.3989918828010559
  - 0.47540023922920227
  - 0.6790890097618103
  - 0.7086420655250549
  - 0.43182650208473206
  - 0.39753594994544983
  - 0.7083669304847717
  - 0.403752863407135
  - 0.3837892711162567
  - 0.39002999663352966
  - 0.43684154748916626
  - 0.44799795746803284
  - 0.38281363248825073
  - 0.40148302912712097
  - 0.3982655107975006
  - 0.3881719410419464
  - 0.3939268887042999
  - 0.3957914710044861
  - 0.39649683237075806
  - 0.38904115557670593
loss_records_fold1:
  train_losses:
  - 3.4958839535713198
  - 3.9401369363069536
  - 3.5188443809747696
  - 5.243448382616044
  - 3.7440397381782535
  - 3.8535097658634188
  - 3.4791213989257814
  - 3.9103245437145233
  - 3.8410589665174486
  - 2.977031308412552
  - 3.349407631158829
  - 3.185977041721344
  - 2.9762818664312363
  - 3.468541118502617
  - 3.2202919065952305
  - 4.47955476641655
  - 2.9779583096504214
  - 3.0063169270753862
  - 3.817316043376923
  - 3.563382837176323
  - 3.043847581744194
  validation_losses:
  - 0.3974382281303406
  - 0.41182613372802734
  - 0.4036341607570648
  - 0.40098899602890015
  - 0.40085744857788086
  - 0.41475850343704224
  - 0.4046366512775421
  - 0.4036625623703003
  - 0.4538612961769104
  - 0.40329891443252563
  - 0.4119933247566223
  - 0.40325817465782166
  - 0.4344235360622406
  - 0.39893612265586853
  - 0.4717258810997009
  - 0.44973239302635193
  - 0.4118097722530365
  - 0.4199703633785248
  - 0.39740467071533203
  - 0.39618951082229614
  - 0.3992118239402771
loss_records_fold2:
  train_losses:
  - 3.213288724422455
  - 3.2657266438007357
  - 3.4389515936374666
  - 2.9693350195884705
  - 3.092543876171112
  - 3.8630368232727053
  - 3.994301044940949
  - 3.6596782445907596
  - 3.115630322694779
  - 3.171703335642815
  - 3.1355732291936875
  - 3.0729085087776187
  - 2.9103587567806244
  - 3.0274913907051086
  - 3.164286541938782
  - 3.0824444741010666
  - 3.0763473868370057
  - 2.9964769691228867
  - 2.9995852023363114
  - 2.9728798627853394
  - 2.9428404539823534
  - 3.1876492470502855
  - 3.0680941343307495
  - 2.8808977007865906
  - 2.9660255372524262
  - 2.8841185718774796
  - 2.8879723340272907
  - 2.96630095243454
  - 2.980583661794663
  validation_losses:
  - 0.4013795554637909
  - 0.37843209505081177
  - 0.38230034708976746
  - 0.5043498277664185
  - 0.38189107179641724
  - 0.41170939803123474
  - 0.42306676506996155
  - 0.41671493649482727
  - 0.579775869846344
  - 0.39518359303474426
  - 0.3817039430141449
  - 0.3828222155570984
  - 0.3934340476989746
  - 0.3928438425064087
  - 0.3775175213813782
  - 0.4021439552307129
  - 0.3814108371734619
  - 0.4075673520565033
  - 0.3824358880519867
  - 0.3843681216239929
  - 0.3816699981689453
  - 0.3778388798236847
  - 0.4389018714427948
  - 0.4099191427230835
  - 0.38150647282600403
  - 0.3722068965435028
  - 0.3794023096561432
  - 0.38044968247413635
  - 0.37910014390945435
loss_records_fold3:
  train_losses:
  - 2.8569104164838794
  - 2.8853852212429048
  - 2.8777332186698916
  - 2.9615990579128266
  - 2.9976600348949436
  - 2.921735662221909
  - 2.906775176525116
  - 2.851515990495682
  - 3.0577572256326677
  - 2.989556157588959
  - 3.0232439517974856
  - 2.833677807450295
  - 2.8415120869874957
  - 2.8431920766830445
  - 2.966815185546875
  - 2.892253947257996
  validation_losses:
  - 0.3767377734184265
  - 0.3848733603954315
  - 0.37817248702049255
  - 0.41012656688690186
  - 0.4331691563129425
  - 0.41892874240875244
  - 0.3903820812702179
  - 0.4059081971645355
  - 0.3896815776824951
  - 0.4115539789199829
  - 0.38111236691474915
  - 0.37628915905952454
  - 0.38087278604507446
  - 0.37784767150878906
  - 0.3762790262699127
  - 0.3777191638946533
loss_records_fold4:
  train_losses:
  - 2.854528492689133
  - 2.829948508739472
  - 2.833906263113022
  - 2.97033863067627
  - 2.8946449637413028
  - 2.8407358646392824
  - 2.9002950668334964
  - 2.891239079833031
  - 2.872587239742279
  - 2.8520905256271365
  - 2.800522670149803
  - 2.8859232515096664
  - 2.830642133951187
  - 2.8801417469978334
  - 2.878257519006729
  - 2.8594538867473602
  - 2.8645495951175692
  - 2.8073262333869935
  - 2.924172648787499
  - 2.824473649263382
  - 2.840300887823105
  - 3.13249734044075
  - 3.1325709462165836
  - 2.947620540857315
  - 2.8173708319664
  - 2.8563877642154694
  - 2.9883520841598514
  - 2.9129960119724276
  - 2.8730843633413317
  - 2.8757011175155642
  - 2.877626615762711
  - 2.8347046017646793
  - 2.8662533164024353
  - 2.8477677643299106
  validation_losses:
  - 0.3736608028411865
  - 0.3683922290802002
  - 0.38024482131004333
  - 0.3681252598762512
  - 0.3981979191303253
  - 0.3759543001651764
  - 0.3898426592350006
  - 0.40691447257995605
  - 0.3684990406036377
  - 0.4001445472240448
  - 0.36439964175224304
  - 0.36581742763519287
  - 0.3968718349933624
  - 0.37247711420059204
  - 0.37264159321784973
  - 0.3896370232105255
  - 0.507337749004364
  - 0.4126802086830139
  - 0.6146005988121033
  - 0.3806712329387665
  - 0.3813682198524475
  - 0.4232175052165985
  - 0.45152655243873596
  - 0.3710588812828064
  - 0.36997637152671814
  - 0.40952709317207336
  - 0.3680180013179779
  - 0.4075009524822235
  - 0.3744756579399109
  - 0.3683548867702484
  - 0.36694610118865967
  - 0.3726644217967987
  - 0.37176981568336487
  - 0.3659612834453583
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 31 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 21 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 16 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 34 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8608247422680413]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.024096385542168676]'
  mean_eval_accuracy: 0.8589574012837968
  mean_f1_accuracy: 0.004819277108433735
  total_train_time: '0:11:57.024844'
