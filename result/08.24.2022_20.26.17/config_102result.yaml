config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 22:42:33.906255'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_102fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_102fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_102fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_102fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_102fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_102fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_102fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_102fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_102fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_102fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.9113062798976899
  - 1.7596881151199342
  - 1.6501718640327454
  - 1.6145346820354463
  - 1.6697614371776581
  - 1.709467387199402
  - 1.6542867779731751
  - 1.547521921992302
  - 1.5612036883831024
  - 1.5705436050891877
  - 1.5716675579547883
  - 1.5785463213920594
  - 1.5577349781990053
  - 1.6725008428096773
  - 1.645621109008789
  - 1.5704007923603058
  - 1.5792979776859284
  - 1.5674830108880997
  validation_losses:
  - 0.7815216183662415
  - 0.4058956205844879
  - 0.4036516845226288
  - 0.3947579562664032
  - 0.40703651309013367
  - 0.4179139733314514
  - 0.3985706865787506
  - 0.401652991771698
  - 0.405307799577713
  - 0.4074401557445526
  - 0.40739625692367554
  - 0.43101805448532104
  - 0.4124179780483246
  - 0.39246323704719543
  - 0.38602831959724426
  - 0.39136987924575806
  - 0.3964332938194275
  - 0.4013992249965668
loss_records_fold1:
  train_losses:
  - 1.5470935881137848
  - 1.6352279841899873
  - 1.5469451248645782
  - 1.8902549624443055
  - 1.6301681458950044
  - 1.7942065358161927
  - 1.5725113153457642
  - 1.5429114341735841
  - 1.6059217631816864
  - 1.5791468501091004
  - 1.5792109727859498
  - 1.5972743809223175
  - 1.5370705842971804
  - 1.5148399949073792
  - 1.546508938074112
  - 1.8304872274398805
  - 1.7223429679870605
  - 1.7081816971302033
  - 1.627651643753052
  - 1.5676557242870333
  - 1.572894060611725
  - 1.5800145149230957
  - 1.547656035423279
  - 1.481022220849991
  - 1.556277644634247
  - 1.5941467344760896
  - 1.5780695080757141
  - 1.531570938229561
  - 1.5721200466156007
  - 1.5961545884609223
  - 1.5636208057403564
  - 1.564875614643097
  - 1.566628956794739
  - 1.6405181407928469
  - 1.5428400337696075
  - 1.5191159665584566
  - 1.5472711861133577
  - 1.670401531457901
  - 1.4974377930164338
  - 1.5677663505077364
  - 1.5435969233512878
  - 1.5613385319709778
  - 1.564825737476349
  - 1.5421121001243592
  - 1.5144943177700043
  - 1.4794584095478058
  - 1.5266897320747377
  - 1.5585960090160371
  - 1.5060051381587982
  - 1.545931613445282
  - 1.5073252677917481
  - 1.5664769291877747
  - 1.5409121930599214
  - 1.5992813557386398
  - 1.5592896163463594
  - 1.5053420484066011
  - 1.4695842355489732
  - 1.5709165275096895
  - 1.511069440841675
  - 1.5334293484687807
  - 1.5250279605388641
  - 1.5409801006317139
  - 1.5335206151008607
  - 1.6454014539718629
  - 1.5544625639915468
  - 1.5504643678665162
  - 1.4818521261215212
  - 1.5724764168262482
  - 1.5195341110229492
  - 1.527269059419632
  - 1.5589999735355378
  - 1.5475184321403503
  - 1.5471587181091309
  - 1.5610342264175416
  validation_losses:
  - 0.40639522671699524
  - 0.4132266640663147
  - 0.3922552764415741
  - 0.414378821849823
  - 0.3949618637561798
  - 0.4002715051174164
  - 0.4082474112510681
  - 0.41912126541137695
  - 0.4049724042415619
  - 0.40420615673065186
  - 0.40043166279792786
  - 0.4249546527862549
  - 0.40075212717056274
  - 0.4114490747451782
  - 0.4063228666782379
  - 0.3917606472969055
  - 0.3917938470840454
  - 0.38536426424980164
  - 0.3928314745426178
  - 0.4078293442726135
  - 0.39396336674690247
  - 0.38999906182289124
  - 0.3842940032482147
  - 0.40268608927726746
  - 0.39078691601753235
  - 0.3933243453502655
  - 0.40512213110923767
  - 0.3911122679710388
  - 0.38917362689971924
  - 0.3917877674102783
  - 0.4072163999080658
  - 0.40216174721717834
  - 0.3932970464229584
  - 0.4278559982776642
  - 0.40239688754081726
  - 0.40122267603874207
  - 0.38748234510421753
  - 0.3883172571659088
  - 0.4107135534286499
  - 0.39818239212036133
  - 0.4112629294395447
  - 0.41586366295814514
  - 0.4089133143424988
  - 0.41908156871795654
  - 0.3994649350643158
  - 0.39012467861175537
  - 0.39540615677833557
  - 0.4374582767486572
  - 0.39959418773651123
  - 0.3900272250175476
  - 0.3936646282672882
  - 0.4226313531398773
  - 0.38509419560432434
  - 0.3934210538864136
  - 0.393949031829834
  - 0.4246181845664978
  - 0.3903180956840515
  - 0.386989951133728
  - 0.38531920313835144
  - 0.39754176139831543
  - 0.3968178927898407
  - 0.38716745376586914
  - 0.5170308947563171
  - 0.3948490023612976
  - 0.3949570953845978
  - 0.4021221101284027
  - 0.3885478973388672
  - 0.4023458957672119
  - 0.4053577780723572
  - 0.40380093455314636
  - 0.396587073802948
  - 0.39166566729545593
  - 0.3949665129184723
  - 0.39533770084381104
loss_records_fold2:
  train_losses:
  - 1.5673884809017182
  - 1.5184496343135834
  - 1.5438968181610109
  - 1.4593142747879029
  - 1.48991237282753
  - 1.4936849653720856
  - 1.4976346135139467
  - 1.50253786444664
  - 1.507403701543808
  - 1.5337197542190553
  - 1.5329185009002686
  - 1.574042308330536
  validation_losses:
  - 0.3873763680458069
  - 0.38941818475723267
  - 0.4616636037826538
  - 0.4305298328399658
  - 0.40987712144851685
  - 0.5601257085800171
  - 0.5471807718276978
  - 0.39166778326034546
  - 0.3815440237522125
  - 0.38752976059913635
  - 0.38969096541404724
  - 0.3865143358707428
loss_records_fold3:
  train_losses:
  - 1.5315552294254304
  - 1.4893658280372621
  - 1.5187244713306427
  - 1.526069873571396
  - 1.49635928273201
  - 1.5884327173233033
  - 1.5046481400728227
  - 1.4910037636756899
  - 1.593232047557831
  - 1.4956562638282778
  - 1.548161679506302
  - 1.6184784770011902
  - 1.4979720354080202
  - 1.5486013472080231
  - 1.5135505259037019
  - 1.5604612112045289
  - 1.526720929145813
  - 1.5564777851104736
  - 1.5396745383739472
  - 1.5218947589397431
  - 1.497429394721985
  - 1.5528261303901674
  - 1.5214270651340485
  - 1.5434696316719057
  - 1.6289706826210022
  - 1.5121807754039764
  - 1.4971490859985352
  - 1.4967140913009644
  - 1.5514828264713287
  - 1.5499670743942262
  - 1.5323174417018892
  - 1.5249883830547333
  - 1.500441461801529
  - 1.5006596982479097
  - 1.5033891618251802
  - 1.5496770143508911
  - 1.550475686788559
  - 1.4891786932945252
  - 1.473393613100052
  - 1.535385024547577
  - 1.4840607941150665
  - 1.574591338634491
  - 1.5397101283073427
  - 1.5057641088962557
  - 1.5206406831741335
  - 1.5130851447582245
  - 1.5713126480579378
  - 1.5246157586574556
  - 1.5457215547561647
  - 1.5927316308021546
  - 1.5270487785339357
  - 1.5348342895507814
  - 1.569933331012726
  - 1.4652548134326935
  - 1.5552828311920166
  - 1.5747039675712586
  - 1.5616046011447908
  - 1.5203477323055268
  - 1.5532419562339783
  - 1.523264503479004
  - 1.5782231628894807
  - 1.5450602889060976
  - 1.531051516532898
  - 1.5431735038757326
  - 1.5251850366592408
  - 1.4907454520463945
  - 1.5136542558670045
  - 1.5844003796577455
  - 1.5234840899705888
  - 1.5287597537040711
  - 1.5488605856895448
  - 1.5375159442424775
  - 1.564045524597168
  - 1.5158915519714355
  - 1.475025099515915
  - 1.5063122808933258
  - 1.496941363811493
  - 1.50886949300766
  - 1.5191625952720642
  - 1.4687492847442627
  - 1.5321861028671266
  - 1.5263789594173431
  - 1.5372571110725404
  - 1.507869827747345
  - 1.4854357838630676
  - 1.5163202822208406
  - 1.5030320882797241
  - 1.5333419978618623
  - 1.4749681293964387
  - 1.5232071340084077
  - 1.5665697813034059
  - 1.4901043415069581
  - 1.5189781308174135
  - 1.5192795097827911
  - 1.5514143466949464
  - 1.5105546832084658
  - 1.4795992970466614
  - 1.4913637042045593
  - 1.56036034822464
  - 1.5345421016216279
  validation_losses:
  - 0.3917679786682129
  - 0.4131626486778259
  - 0.3920457661151886
  - 0.381137877702713
  - 0.3762975037097931
  - 0.45071282982826233
  - 0.39297613501548767
  - 0.3766731023788452
  - 0.39322251081466675
  - 0.4131740927696228
  - 0.40297892689704895
  - 0.3896087110042572
  - 0.4480326175689697
  - 0.418924480676651
  - 2.850759983062744
  - 0.42780035734176636
  - 0.4141441583633423
  - 0.377678781747818
  - 0.3852039873600006
  - 0.38539403676986694
  - 0.5292078852653503
  - 0.4018622934818268
  - 0.41204917430877686
  - 0.5946979522705078
  - 0.5870562791824341
  - 0.38927900791168213
  - 2.2555203437805176
  - 1.3755543231964111
  - 1.0359506607055664
  - 0.3968658745288849
  - 0.7985765933990479
  - 1.1573630571365356
  - 0.9668756127357483
  - 1.6747251749038696
  - 1.0884921550750732
  - 1.2870991230010986
  - 0.9365667104721069
  - 0.3849514424800873
  - 0.3947831988334656
  - 0.607867956161499
  - 0.9739935994148254
  - 1.1908659934997559
  - 0.513839066028595
  - 0.46036234498023987
  - 0.7159207463264465
  - 0.47445785999298096
  - 1.8624508380889893
  - 2.3023626804351807
  - 0.39419037103652954
  - 0.7577732801437378
  - 0.9527581930160522
  - 1.5967106819152832
  - 0.5131787657737732
  - 0.9175466299057007
  - 1.3180153369903564
  - 0.6146895885467529
  - 0.9629632830619812
  - 0.40044093132019043
  - 0.5163306593894958
  - 0.393489271402359
  - 0.8601104617118835
  - 0.3947606384754181
  - 1.3493642807006836
  - 1.235437273979187
  - 0.681441605091095
  - 1.6282541751861572
  - 1.7160207033157349
  - 1.1634111404418945
  - 0.9586117267608643
  - 3.95255970954895
  - 0.4044141471385956
  - 0.4730380177497864
  - 0.41958311200141907
  - 0.37236884236335754
  - 0.4040031135082245
  - 0.3821493983268738
  - 0.4238443076610565
  - 0.3901982307434082
  - 0.6379154920578003
  - 0.49070078134536743
  - 0.723784863948822
  - 0.9882315397262573
  - 0.4905588924884796
  - 0.4198351502418518
  - 0.4162690341472626
  - 0.4025324285030365
  - 1.1629000902175903
  - 0.4207540452480316
  - 0.5622389912605286
  - 0.6151642203330994
  - 0.7602174282073975
  - 0.7752246260643005
  - 0.5387015342712402
  - 1.1937335729599
  - 0.4826177656650543
  - 0.8281000852584839
  - 1.1083858013153076
  - 0.7982261776924133
  - 0.44537588953971863
  - 0.4150753617286682
loss_records_fold4:
  train_losses:
  - 1.5170442938804627
  - 1.5140994489192963
  - 1.488727605342865
  - 1.547718244791031
  - 1.497588324546814
  - 1.5007250785827637
  - 1.509460309147835
  - 1.5617065012454987
  - 1.5328187942504883
  - 1.5102539360523224
  - 1.4738847702741624
  - 1.5284578740596773
  - 1.492258882522583
  - 1.5391539931297302
  - 1.4873715698719026
  - 1.4819669902324677
  - 1.5303762197494508
  - 1.4726093709468842
  - 1.5036014556884767
  - 1.4998672723770143
  - 1.4501408696174622
  - 1.4738210022449494
  - 1.4832167744636537
  - 1.54015811085701
  - 1.5615273416042328
  - 1.4959479570388794
  - 1.4907846271991732
  - 1.5039533495903017
  - 1.530600303411484
  - 1.482600623369217
  - 1.4863439440727235
  - 1.476634779572487
  - 1.4708431720733643
  - 1.4546393871307375
  - 1.5157909810543062
  - 1.459299910068512
  - 1.5078149020671845
  - 1.4683609068393708
  - 1.484689646959305
  - 1.510318273305893
  - 1.446069622039795
  - 1.4734532952308657
  - 1.5034844160079956
  - 1.5112904727458956
  - 1.479157441854477
  - 1.5118658423423768
  - 1.5360583603382112
  - 1.4553731739521027
  - 1.4849398076534273
  validation_losses:
  - 0.3869621753692627
  - 0.39125311374664307
  - 0.3928115665912628
  - 0.4328933656215668
  - 0.3872241973876953
  - 0.3809323310852051
  - 0.4069553315639496
  - 0.38683393597602844
  - 0.38193246722221375
  - 0.3775647282600403
  - 0.3835165202617645
  - 0.4660043716430664
  - 0.40083956718444824
  - 0.3885226547718048
  - 0.37618517875671387
  - 0.3749949634075165
  - 0.3912827670574188
  - 0.39985761046409607
  - 0.40894198417663574
  - 0.4035235047340393
  - 0.3762560784816742
  - 0.5344893932342529
  - 0.4053800106048584
  - 0.47076717019081116
  - 0.613197386264801
  - 0.36854684352874756
  - 0.4020792841911316
  - 0.6021658778190613
  - 0.43582668900489807
  - 0.3818211555480957
  - 0.3732336759567261
  - 0.38517290353775024
  - 0.44694581627845764
  - 0.3920443654060364
  - 0.7369702458381653
  - 0.3792760372161865
  - 0.3969675898551941
  - 0.506786048412323
  - 0.3842882513999939
  - 0.8017027378082275
  - 0.3714402914047241
  - 0.394560307264328
  - 0.43335092067718506
  - 0.39803051948547363
  - 0.37572556734085083
  - 0.3755470812320709
  - 0.3843417465686798
  - 0.37787604331970215
  - 0.37757501006126404
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 18 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 74 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 49 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:22:40.717163'
