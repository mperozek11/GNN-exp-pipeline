config:
  aggregation: mean
  batch_size: 256
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.0
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:25:29.092865'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_99fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_99fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_99fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_99fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_99fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_99fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_99fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_99fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_99fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_99fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.034659343957901
  - 0.8464782774448395
  - 0.8059642970561982
  - 0.794736260175705
  - 0.8889446020126344
  - 0.8106838881969453
  - 0.7739279806613922
  - 0.7400286108255387
  - 0.763571983575821
  - 0.7544843316078187
  - 0.775406152009964
  - 0.7698984503746034
  - 0.7630985379219055
  validation_losses:
  - 0.6890783309936523
  - 0.4339603781700134
  - 0.4256957173347473
  - 0.4067455232143402
  - 0.3862414062023163
  - 0.38705042004585266
  - 0.39837411046028137
  - 0.39756515622138977
  - 0.38973331451416016
  - 0.3886895179748535
  - 0.3962441086769104
  - 0.3898371458053589
  - 0.3826671838760376
loss_records_fold1:
  train_losses:
  - 0.8029311120510102
  - 0.7529903650283813
  - 0.7314953923225403
  - 0.7467015802860261
  - 0.823590213060379
  - 0.7802412629127503
  - 0.808518648147583
  - 0.9052585959434509
  - 0.8359619617462158
  - 0.7824018776416779
  - 0.8553917288780213
  - 0.7528663277626038
  validation_losses:
  - 0.3967677652835846
  - 0.41193291544914246
  - 0.39699649810791016
  - 0.40193232893943787
  - 0.4017716348171234
  - 0.42106014490127563
  - 0.41662582755088806
  - 0.3883841037750244
  - 0.3921120762825012
  - 0.3911743760108948
  - 0.39034777879714966
  - 0.39681971073150635
loss_records_fold2:
  train_losses:
  - 0.812317031621933
  - 0.7957109093666077
  - 0.7883589148521424
  - 0.7971867084503175
  - 0.8398512423038483
  - 0.8148985922336579
  - 0.7742321789264679
  - 0.7719247937202454
  - 0.7927846252918244
  - 0.7983426272869111
  - 0.7782259047031403
  - 0.7494597613811493
  - 0.7343098670244217
  - 0.792107903957367
  - 0.7239240616559983
  - 0.7489162147045136
  - 0.737565952539444
  - 0.7474829375743867
  - 0.7728032767772675
  - 0.7752495229244233
  - 0.747651892900467
  - 0.7351589381694794
  - 0.7383325457572938
  - 0.740809839963913
  - 0.7450261473655702
  - 0.7549027442932129
  - 0.7454940676689148
  - 0.7324380636215211
  - 0.7310090124607087
  - 0.8153866231441498
  - 0.7465170681476594
  - 0.7731844604015351
  - 0.7521473586559296
  - 0.778723293542862
  - 0.7735271811485291
  - 0.767267394065857
  - 0.7509130477905274
  - 0.7759141504764557
  - 0.7721051812171936
  - 0.8335077285766602
  - 0.7503531932830811
  - 0.7368857651948929
  - 0.7838228404521943
  - 0.7438655585050583
  - 0.8351233899593353
  - 0.8258707523345947
  - 0.7617975056171418
  - 0.7947373628616333
  - 0.7401669800281525
  - 0.73011574447155
  - 0.8019931852817536
  - 0.734021282196045
  - 0.7688191056251527
  - 0.7385418713092804
  - 0.7554091691970826
  - 0.7010993972420693
  - 0.769780957698822
  - 0.769092220067978
  - 0.8073227643966675
  - 0.7504400432109833
  - 0.7915445983409882
  - 0.7557403743267059
  - 0.7680974006652832
  - 0.7079751521348954
  - 0.7545284032821655
  - 0.748071837425232
  - 0.7387489378452301
  - 0.7595890462398529
  - 0.763825958967209
  - 0.7480031549930573
  - 0.7330478549003602
  - 0.7515915334224701
  - 0.7661106348037721
  - 0.7258723378181458
  - 0.753926008939743
  - 0.7452950209379197
  - 0.789204865694046
  - 0.7385838389396668
  - 0.7766189277172089
  - 0.7712324559688568
  - 0.7514597594738007
  - 0.7454764306545258
  - 0.7732768297195435
  - 0.7510040163993836
  - 0.7209354519844056
  - 0.7124481052160263
  - 0.8026915192604065
  - 0.7509225904941559
  - 0.8152716219425202
  - 0.7878152966499329
  - 0.743903285264969
  - 0.7651327788829804
  - 0.7639480113983155
  - 0.7681025862693787
  - 0.7531867384910584
  - 0.7356978297233582
  - 0.7430389285087586
  - 0.805380791425705
  - 0.804216742515564
  - 0.7614731192588806
  validation_losses:
  - 0.4336537718772888
  - 0.3895569145679474
  - 0.40442731976509094
  - 0.40393179655075073
  - 0.3963842988014221
  - 0.38423046469688416
  - 0.4073735177516937
  - 0.38970085978507996
  - 0.3862749934196472
  - 0.4166729748249054
  - 0.3846393823623657
  - 0.38381436467170715
  - 0.39279139041900635
  - 0.4175412654876709
  - 0.379888117313385
  - 0.39173421263694763
  - 0.3848343789577484
  - 0.3891163468360901
  - 0.37899357080459595
  - 0.3948420286178589
  - 0.3797556459903717
  - 0.4004063904285431
  - 0.38489076495170593
  - 0.4124893248081207
  - 0.38679927587509155
  - 0.38869234919548035
  - 0.37598755955696106
  - 0.38441070914268494
  - 0.38092461228370667
  - 0.41438916325569153
  - 0.442085862159729
  - 0.37855836749076843
  - 0.39284658432006836
  - 0.38463664054870605
  - 0.4012981057167053
  - 0.3849482834339142
  - 0.39180895686149597
  - 0.3932560086250305
  - 0.4026184678077698
  - 0.44842833280563354
  - 0.3901737928390503
  - 0.396068811416626
  - 0.4302564859390259
  - 0.38535961508750916
  - 0.5396149754524231
  - 0.38046106696128845
  - 0.40617620944976807
  - 0.40294119715690613
  - 0.38872280716896057
  - 0.3873366713523865
  - 0.40859082341194153
  - 0.38759058713912964
  - 0.40408265590667725
  - 0.37753164768218994
  - 0.43113917112350464
  - 1.0225350856781006
  - 0.45111358165740967
  - 0.3918307423591614
  - 0.42940279841423035
  - 0.38410136103630066
  - 0.4050552248954773
  - 0.37635114789009094
  - 0.4133606553077698
  - 0.3775283098220825
  - 0.5541868805885315
  - 0.43158242106437683
  - 0.3886924088001251
  - 0.37721356749534607
  - 0.39925873279571533
  - 1.2467639446258545
  - 0.45135948061943054
  - 0.3772774636745453
  - 0.39324185252189636
  - 0.3871251344680786
  - 0.5329766869544983
  - 0.39440786838531494
  - 0.44194185733795166
  - 0.4399659037590027
  - 0.4495336711406708
  - 0.468645840883255
  - 0.37753206491470337
  - 0.3871873617172241
  - 0.40685248374938965
  - 0.5259159803390503
  - 0.392976850271225
  - 0.3797270357608795
  - 0.4732576608657837
  - 0.4608667492866516
  - 0.5308950543403625
  - 0.39980649948120117
  - 0.45026060938835144
  - 0.3922058045864105
  - 0.38690170645713806
  - 0.3804025948047638
  - 0.394181489944458
  - 0.3813697397708893
  - 0.3814485967159271
  - 0.39739876985549927
  - 0.37902960181236267
  - 0.37814199924468994
loss_records_fold3:
  train_losses:
  - 0.7311314344406128
  - 0.7431226253509522
  - 0.7593227624893188
  - 0.8005504012107849
  - 0.7423985302448273
  - 0.7360634803771973
  - 0.7254468083381653
  - 0.7826151490211487
  - 0.763162261247635
  - 0.7711221039295197
  - 0.7550537765026093
  - 0.7384544551372528
  - 0.7460305035114289
  - 0.7428320527076722
  - 0.7876392960548402
  - 0.7607060074806213
  - 0.7418910801410675
  - 0.7303114533424377
  - 0.7351428389549256
  - 0.7242605090141296
  - 0.7726775646209717
  - 0.7383498072624207
  - 0.7627957463264465
  - 0.7778673887252808
  - 0.7385498285293579
  - 0.7467361927032471
  - 0.7623243272304535
  - 0.780982494354248
  - 0.741306209564209
  - 0.7730937957763673
  - 0.7364596247673035
  - 0.7939237833023072
  - 0.7385616064071656
  - 0.7468793272972107
  - 0.7551728904247285
  - 0.7475432455539703
  - 0.7244998455047608
  - 0.7335859715938569
  - 0.7502990782260895
  - 0.7959026992321014
  - 0.8192105352878571
  - 0.783290183544159
  - 0.7610479295253754
  - 0.7466491520404817
  - 0.7242426842451096
  - 0.7586730420589447
  - 0.8141378700733185
  - 0.7490782916545868
  - 0.7698657989501954
  - 0.7958177924156189
  - 0.7726278960704804
  - 0.7545738458633423
  - 0.7364817321300507
  - 0.7297237634658814
  - 0.7403945147991181
  - 0.7372210979461671
  - 0.7471709072589875
  - 0.7543323040008545
  - 0.7219374775886536
  - 0.7577882587909699
  - 0.7585554361343384
  - 0.7302019774913788
  - 0.7399954617023469
  - 0.7317569553852081
  - 0.7153908669948579
  - 0.6916174322366715
  - 0.768449866771698
  - 0.7215591818094254
  - 0.7302461564540863
  - 0.773896223306656
  - 0.7087859630584717
  - 0.7437807977199555
  - 0.8228452980518342
  - 0.7747788369655609
  - 0.7462110757827759
  - 0.7460464239120483
  - 0.7452316701412202
  - 0.7375527381896974
  - 0.7707933008670808
  - 0.794419950246811
  - 0.7106106162071228
  - 0.7417188286781311
  - 0.7641763925552368
  - 0.7506894052028656
  - 0.7195620357990266
  - 0.7295928299427032
  - 0.7708612978458405
  - 0.7292264342308045
  - 0.7710658848285675
  - 0.7292140603065491
  - 0.7507985949516297
  - 0.7478837788105012
  - 0.7265410423278809
  - 0.7419260203838349
  - 0.7042108684778214
  - 0.7547115445137025
  - 0.7169840544462205
  - 0.7386087954044342
  - 0.7466790318489075
  - 0.7770086705684662
  validation_losses:
  - 0.3623211681842804
  - 0.37291786074638367
  - 0.3674013912677765
  - 0.4150766432285309
  - 0.37380269169807434
  - 0.4070096015930176
  - 0.3980657160282135
  - 0.3733157515525818
  - 0.3591611981391907
  - 0.38418689370155334
  - 0.5861138105392456
  - 0.3750147223472595
  - 0.4461674094200134
  - 0.4111430048942566
  - 0.44675713777542114
  - 0.371554970741272
  - 0.40981197357177734
  - 0.47104594111442566
  - 0.8264732956886292
  - 0.4502279758453369
  - 0.5527169108390808
  - 0.37839242815971375
  - 0.4442731440067291
  - 0.4117569327354431
  - 0.36334651708602905
  - 0.3861124515533447
  - 0.3972984254360199
  - 0.557805061340332
  - 0.4844898581504822
  - 0.4576554298400879
  - 0.49803605675697327
  - 0.3708890378475189
  - 1.2589986324310303
  - 0.7088902592658997
  - 0.3881627321243286
  - 0.42909061908721924
  - 0.5331218242645264
  - 2.8395323753356934
  - 0.44876354932785034
  - 1.5134332180023193
  - 0.4300055503845215
  - 0.39390069246292114
  - 0.41735342144966125
  - 0.37353256344795227
  - 0.37988588213920593
  - 0.39013198018074036
  - 0.37485772371292114
  - 0.3674555718898773
  - 0.4074232578277588
  - 1.9854190349578857
  - 0.5235057473182678
  - 0.40640002489089966
  - 0.38305073976516724
  - 0.3793914020061493
  - 0.37166136503219604
  - 0.3898283541202545
  - 0.3877679407596588
  - 0.4033902585506439
  - 0.422481507062912
  - 0.5172574520111084
  - 1.6975493431091309
  - 0.4718725383281708
  - 0.4375830292701721
  - 0.40791943669319153
  - 0.4314548969268799
  - 0.5830227732658386
  - 0.4147360026836395
  - 2.4900100231170654
  - 1.5578991174697876
  - 0.4442708194255829
  - 0.43430718779563904
  - 0.47828567028045654
  - 0.4710273742675781
  - 0.3839110732078552
  - 0.3857499361038208
  - 0.3844199478626251
  - 0.3767656981945038
  - 0.4490843415260315
  - 0.41838526725769043
  - 0.45063501596450806
  - 0.37267693877220154
  - 0.3637818694114685
  - 0.3742271959781647
  - 0.40403494238853455
  - 0.39632606506347656
  - 0.42659324407577515
  - 0.505422055721283
  - 0.4336779713630676
  - 0.42771780490875244
  - 0.3893107771873474
  - 0.42894649505615234
  - 0.4031223654747009
  - 0.4363914132118225
  - 0.5642841458320618
  - 0.4444909393787384
  - 0.418149471282959
  - 0.44687607884407043
  - 0.7018624544143677
  - 0.46993082761764526
  - 0.41662561893463135
loss_records_fold4:
  train_losses:
  - 0.7290799260139466
  - 0.7399368643760682
  - 0.7480950653553009
  - 0.760893851518631
  - 0.7764388740062714
  - 0.7693323194980621
  - 0.7731986403465272
  - 0.7288166522979737
  - 0.7073792725801469
  - 0.7245856761932373
  - 0.696776807308197
  validation_losses:
  - 0.3895367980003357
  - 0.4114615023136139
  - 0.4165427088737488
  - 0.4734770357608795
  - 0.3940119445323944
  - 0.38494086265563965
  - 0.38166317343711853
  - 0.38579198718070984
  - 0.37013962864875793
  - 0.37908732891082764
  - 0.3865191340446472
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 13 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 12 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8319039451114922,
    0.8539518900343642]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.15517241379310345, 0.08602150537634409]'
  mean_eval_accuracy: 0.8520939800651919
  mean_f1_accuracy: 0.048238783833889506
  total_train_time: '0:20:10.980130'
