config:
  aggregation: mean
  batch_size: 128
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:52:47.346405'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_22fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_22fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_22fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_22fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_22fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_22fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_22fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_22fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_22fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_22fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 1.9121254920959474
  - 1.6379500150680544
  - 1.6505291223526002
  - 1.6253752291202546
  - 1.617000621557236
  - 1.601822090148926
  - 1.5908202171325685
  - 1.6178212463855743
  - 1.5779531300067902
  - 1.578106051683426
  - 1.5129217237234116
  validation_losses:
  - 0.4132719933986664
  - 0.4265602231025696
  - 0.40694037079811096
  - 0.39616939425468445
  - 0.4030025601387024
  - 0.39886486530303955
  - 0.39141395688056946
  - 0.39590251445770264
  - 0.3946240544319153
  - 0.3981421887874603
  - 0.39498311281204224
loss_records_fold1:
  train_losses:
  - 1.5393427044153214
  - 1.6161844789981843
  - 1.5417645871639252
  - 1.5643060743808748
  - 1.587636262178421
  - 1.5583484768867493
  - 1.5786075592041016
  - 1.5085952818393709
  - 1.5692936778068542
  - 1.5340888738632203
  - 1.5699818789958955
  - 1.5536522626876832
  - 1.5145381689071655
  - 1.5721138298511506
  - 1.5652922809123995
  - 1.5607456266880035
  - 1.5368405878543854
  - 1.587725579738617
  - 1.48012832403183
  - 1.530901771783829
  - 1.5531066536903382
  - 1.5177259147167206
  - 1.564980572462082
  - 1.5131404876708985
  validation_losses:
  - 0.39470380544662476
  - 0.4009355902671814
  - 0.40296468138694763
  - 0.3978695869445801
  - 0.3921736478805542
  - 0.388180673122406
  - 0.41720834374427795
  - 0.39983001351356506
  - 0.39617592096328735
  - 0.40835416316986084
  - 0.39606842398643494
  - 0.3954068720340729
  - 0.3906012177467346
  - 0.42558804154396057
  - 0.39542505145072937
  - 0.39190176129341125
  - 0.39105135202407837
  - 0.4177956283092499
  - 0.39447492361068726
  - 0.401301771402359
  - 0.39955011010169983
  - 0.3912537395954132
  - 0.39761778712272644
  - 0.3866863548755646
loss_records_fold2:
  train_losses:
  - 1.7019569277763367
  - 1.5672716051340103
  - 1.587178683280945
  - 1.599309939146042
  - 1.713325881958008
  - 1.559410321712494
  - 1.5579907953739167
  - 1.6026186466217043
  - 1.526615470647812
  - 1.579550862312317
  - 1.5461783766746522
  - 1.5067005574703218
  - 1.5421625912189485
  - 1.5269328355789185
  - 1.6176621556282045
  - 1.5532453238964081
  - 1.5592931389808655
  - 1.5199110507965088
  - 1.4683833658695222
  - 1.5673037052154541
  - 1.6060163080692291
  - 1.5421745777130127
  - 1.516171568632126
  - 1.5895233392715455
  - 1.6345413029193878
  - 1.533885830640793
  - 1.5440552949905397
  - 1.563183081150055
  validation_losses:
  - 0.39472073316574097
  - 0.39067915081977844
  - 0.3999848961830139
  - 0.3996596932411194
  - 0.3917490839958191
  - 0.39223209023475647
  - 0.3972223997116089
  - 0.3934172987937927
  - 0.40753173828125
  - 0.3949279487133026
  - 0.3960082530975342
  - 0.39252179861068726
  - 0.4033997058868408
  - 0.39230719208717346
  - 0.44273462891578674
  - 0.40086179971694946
  - 0.3909125030040741
  - 0.4161002039909363
  - 0.40260711312294006
  - 0.48402199149131775
  - 0.3939848244190216
  - 0.40488770604133606
  - 0.3978959918022156
  - 0.39392849802970886
  - 0.39768195152282715
  - 0.3961738049983978
  - 0.3850564956665039
  - 0.38624319434165955
loss_records_fold3:
  train_losses:
  - 1.5040254294872284
  - 1.5205044329166413
  - 1.5130012392997743
  - 1.5762047231197358
  - 1.5641045570373535
  - 1.533666801452637
  - 1.539630049467087
  - 1.5839653968811036
  - 1.498829084634781
  - 1.5298908531665802
  - 1.5295374393463135
  - 1.5308116972446442
  - 1.5232888817787171
  - 1.560049569606781
  - 1.5397695124149324
  - 1.5603688001632692
  - 1.5154780805110932
  - 1.529470705986023
  - 1.5033158957958221
  - 1.4858988910913469
  - 1.5440493881702424
  - 1.5540369272232057
  - 1.545830136537552
  - 1.6037550568580627
  - 1.577866965532303
  - 1.5668572127819063
  - 1.6132154107093812
  - 1.6221626818180086
  - 1.5574476778507234
  - 1.5043154895305635
  - 1.550412154197693
  - 1.5652990818023682
  - 1.5101671099662781
  - 1.5582506239414216
  - 1.5695887744426729
  - 1.580259758234024
  - 1.5316105961799622
  - 1.5552924811840059
  - 1.5993385910987854
  - 1.6043675303459168
  - 1.5622084856033327
  - 1.5499889075756075
  - 1.5507833302021028
  - 1.6127278149127962
  - 1.5365681946277618
  - 1.550495618581772
  - 1.5333595693111421
  - 1.5589177787303925
  validation_losses:
  - 0.38916173577308655
  - 0.371389240026474
  - 0.38674238324165344
  - 0.3876073360443115
  - 0.3864237368106842
  - 0.38455983996391296
  - 0.3743230104446411
  - 0.42543286085128784
  - 0.3738420307636261
  - 0.38475745916366577
  - 0.38086482882499695
  - 0.3865756392478943
  - 0.37317079305648804
  - 0.3855488896369934
  - 0.4013209044933319
  - 0.3844480812549591
  - 0.3870605230331421
  - 0.38332828879356384
  - 0.3681897222995758
  - 0.3770693838596344
  - 0.39101892709732056
  - 0.3782501518726349
  - 0.3682973086833954
  - 0.3934147357940674
  - 0.3757479190826416
  - 0.37154319882392883
  - 0.37623482942581177
  - 0.4026314914226532
  - 0.37736186385154724
  - 0.4019770920276642
  - 0.3828146457672119
  - 0.3766259253025055
  - 0.3753639757633209
  - 0.3870084881782532
  - 0.39834538102149963
  - 0.37398120760917664
  - 0.3745626211166382
  - 0.3962222933769226
  - 0.3852118253707886
  - 0.3828575313091278
  - 0.37616392970085144
  - 0.3877008259296417
  - 0.3737657368183136
  - 0.3725391626358032
  - 0.37131068110466003
  - 0.37785133719444275
  - 0.37349778413772583
  - 0.3687038719654083
loss_records_fold4:
  train_losses:
  - 1.5150472462177278
  - 1.5355034649372101
  - 1.5208955824375154
  - 1.5268472254276277
  - 1.5412145972251894
  - 1.511967170238495
  - 1.536889374256134
  - 1.536039024591446
  - 1.6121108055114748
  - 1.4965755581855775
  - 1.546973502635956
  - 1.5701692283153534
  - 1.532672393321991
  - 1.5359142541885378
  - 1.536324381828308
  - 1.5043469965457916
  - 1.5174416422843935
  - 1.5597258150577546
  - 1.580392223596573
  - 1.545456337928772
  - 1.548621189594269
  - 1.6192977011203766
  - 1.5920581340789797
  - 1.538076561689377
  - 1.5183695793151857
  - 1.5549682557582856
  - 1.534386867284775
  - 1.5185857534408571
  - 1.5194984555244446
  validation_losses:
  - 0.37641724944114685
  - 0.37921085953712463
  - 0.37715500593185425
  - 0.3924737274646759
  - 0.37595969438552856
  - 0.3828319311141968
  - 0.37844234704971313
  - 0.38891440629959106
  - 0.38454166054725647
  - 0.3782166540622711
  - 0.3936842083930969
  - 0.3988020718097687
  - 0.377446711063385
  - 0.38094109296798706
  - 0.37550097703933716
  - 0.38596680760383606
  - 0.38117125630378723
  - 0.37535178661346436
  - 0.40124598145484924
  - 0.38131317496299744
  - 0.37305641174316406
  - 0.3732883334159851
  - 0.3891110420227051
  - 0.382038414478302
  - 0.385662704706192
  - 0.3793429136276245
  - 0.38459512591362
  - 0.3802005648612976
  - 0.37548530101776123
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 11 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 24 epochs
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 28 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 48 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 29 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.023809523809523808, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0047619047619047615
  total_train_time: '0:11:47.553550'
