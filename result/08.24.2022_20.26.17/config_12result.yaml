config:
  aggregation: sum
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.838308'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_12fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_12fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_12fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_12fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_12fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_12fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_12fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_12fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_12fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_12fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 84.3065953206271
  - 30.919190970063212
  - 19.442183980345728
  - 24.70543276369572
  - 21.058140832185746
  - 21.133281181752682
  - 23.56622327566147
  - 26.114135411381724
  - 13.14976730644703
  - 13.09833055138588
  - 18.359907026588917
  - 9.005014884471894
  - 8.839167830348016
  - 8.576611098647119
  - 9.08024247288704
  - 11.175406834483148
  - 8.094073294103145
  - 10.664684972167016
  - 10.104134461283685
  - 9.231073924899102
  - 7.602781370282173
  - 8.66564975976944
  - 7.420215025544167
  - 7.73491077721119
  - 6.78210481107235
  - 7.202408084273339
  - 7.114188942313195
  - 7.1318458139896395
  - 7.812249350547791
  - 8.071732696890832
  - 6.354601475596429
  - 7.520278218388558
  - 7.8571401059627535
  - 6.478029984235764
  - 6.4219053298234945
  - 6.4500435382127765
  - 6.8663979351520545
  - 6.936261641979218
  - 6.577243039011956
  - 6.728147786855698
  - 6.715052559971809
  - 6.623752593994141
  - 6.518483340740204
  - 6.831071513891221
  - 6.609804543852807
  - 6.5254562318325045
  - 6.588529974222183
  - 6.397236612439156
  - 6.873724341392517
  - 6.469519349932671
  - 6.913306960463524
  validation_losses:
  - 1.802850365638733
  - 0.6806835532188416
  - 0.4761335849761963
  - 0.752157986164093
  - 0.46266308426856995
  - 0.47194886207580566
  - 1.0293471813201904
  - 1.0614057779312134
  - 0.41200777888298035
  - 0.4083988070487976
  - 0.4230330288410187
  - 0.4186290502548218
  - 0.41522216796875
  - 0.45858556032180786
  - 0.40762796998023987
  - 0.3924117386341095
  - 0.5302477478981018
  - 0.4363851547241211
  - 0.4716779291629791
  - 0.44974613189697266
  - 0.39981839060783386
  - 0.47021108865737915
  - 0.4084920287132263
  - 0.40746045112609863
  - 0.4376892149448395
  - 0.39866307377815247
  - 0.39350399374961853
  - 0.4566291868686676
  - 0.47044914960861206
  - 0.4335014224052429
  - 0.40157145261764526
  - 0.42124444246292114
  - 0.4331064522266388
  - 0.4140796959400177
  - 0.40363234281539917
  - 0.4273746609687805
  - 0.48902058601379395
  - 0.4430276155471802
  - 0.46933022141456604
  - 0.412962406873703
  - 0.4037173092365265
  - 0.44699177145957947
  - 0.41638848185539246
  - 0.40655311942100525
  - 0.42309075593948364
  - 0.42887163162231445
  - 0.41672468185424805
  - 0.41529539227485657
  - 0.40559273958206177
  - 0.40069565176963806
  - 0.4004904627799988
loss_records_fold1:
  train_losses:
  - 6.79653836786747
  - 6.35469808280468
  - 6.534420818090439
  - 6.446485257148743
  - 6.57678302526474
  - 6.467384639382363
  - 6.614449763298035
  - 7.207354563474656
  - 7.609629887342454
  - 6.724138426780701
  - 6.880679526925087
  - 6.452213528752328
  - 6.367482402920723
  - 6.5596961140632635
  - 8.820921784639358
  - 10.046074545383455
  - 8.607622289657593
  - 6.859059235453606
  - 7.176189649105073
  - 7.3838573664426805
  - 6.646203243732453
  - 6.412413874268532
  - 6.678104609251022
  - 6.619835251569748
  - 7.051178035140038
  - 6.6667287230491645
  - 6.619711467623711
  - 6.771706223487854
  - 6.648040565848351
  - 6.338390982151032
  - 6.827189183235169
  - 6.629160848259926
  - 6.4311051100492485
  - 6.667418658733368
  - 6.799936139583588
  - 6.588266962766648
  - 6.733293941617013
  - 6.2927989423275
  - 6.453952774405479
  - 6.80808941423893
  - 6.656777775287629
  - 6.6208211705088615
  - 6.654187846183778
  - 6.553699576854706
  - 6.511101564764977
  - 6.529916331171989
  - 6.467698773741723
  - 7.498003888130189
  - 7.072707423567772
  - 6.336109465360642
  - 6.6733284264802935
  - 6.214886918663979
  - 6.382441610097885
  - 6.32194310426712
  - 6.518212021887303
  - 6.489186984300614
  - 6.336194422841072
  - 6.458064995706081
  - 6.478566682338715
  - 6.820883734524251
  - 6.427301567792893
  - 6.1930741488933565
  - 6.510114344954491
  - 6.366015765070916
  - 6.297114849090576
  - 6.384002864360809
  - 6.37064098417759
  - 6.373707264661789
  - 6.831392008066178
  - 6.821932262182236
  - 6.450382086634637
  - 6.54925394654274
  - 6.413597717881203
  - 6.513350084424019
  - 9.224830728769303
  - 8.24518884420395
  - 10.876264962553979
  - 7.819608077406883
  - 7.225884330272675
  - 6.762077963352204
  - 6.909113681316376
  - 6.474043166637421
  - 6.7650192260742195
  - 6.698720046877861
  - 6.557630106806755
  - 6.663185715675354
  - 6.583600962162018
  - 6.559965087473393
  - 6.57759959101677
  - 6.6046102195978165
  - 6.644225281476975
  - 8.024812322854997
  - 6.85517507493496
  - 6.287515598535538
  - 6.542006567120552
  - 6.277312949299812
  - 6.484300374984741
  - 6.18412336409092
  - 6.35374872982502
  - 6.2369689181447034
  validation_losses:
  - 0.4054683744907379
  - 0.41192349791526794
  - 0.4644148349761963
  - 0.41900748014450073
  - 0.4374507665634155
  - 0.47059980034828186
  - 0.8881893754005432
  - 0.5469253659248352
  - 0.4457959830760956
  - 1.3494988679885864
  - 0.4390162527561188
  - 0.43865838646888733
  - 0.4172441065311432
  - 0.42693766951560974
  - 0.4249056279659271
  - 0.4568537473678589
  - 0.40283000469207764
  - 0.41714081168174744
  - 0.41059768199920654
  - 0.43264704942703247
  - 0.39788195490837097
  - 0.4449879229068756
  - 0.4159472584724426
  - 0.44149675965309143
  - 0.41147223114967346
  - 0.4176289737224579
  - 0.467145711183548
  - 0.4572901725769043
  - 0.40829622745513916
  - 0.41221579909324646
  - 0.4460233151912689
  - 0.4325985908508301
  - 0.43662866950035095
  - 0.4366968870162964
  - 0.43150264024734497
  - 0.47901591658592224
  - 0.4640159010887146
  - 0.4750404953956604
  - 0.42536354064941406
  - 0.41286250948905945
  - 0.4204736649990082
  - 0.4899415671825409
  - 0.4345097541809082
  - 0.4336642324924469
  - 0.42831435799598694
  - 0.4146221876144409
  - 0.41058534383773804
  - 0.44990643858909607
  - 0.4667748510837555
  - 0.42725154757499695
  - 0.42860132455825806
  - 0.4124594032764435
  - 0.45611122250556946
  - 0.4406070411205292
  - 0.4434383809566498
  - 0.4232758581638336
  - 0.4199170470237732
  - 0.45146629214286804
  - 0.4302630126476288
  - 0.43725571036338806
  - 0.41558995842933655
  - 0.42670169472694397
  - 0.41532859206199646
  - 0.43676334619522095
  - 5438.125
  - 0.43245840072631836
  - 0.4135800004005432
  - 0.4271104335784912
  - 0.42583563923835754
  - 0.40881240367889404
  - 0.41257813572883606
  - 0.4889863133430481
  - 316.36676025390625
  - 4990377472.0
  - 0.6560440063476562
  - 0.6227248311042786
  - 0.41347232460975647
  - 0.5059561133384705
  - 4.1568522453308105
  - 0.4350437521934509
  - 0.5620599985122681
  - 0.4298730790615082
  - 0.47057661414146423
  - 0.4118143916130066
  - 0.5273601412773132
  - 0.43642523884773254
  - 0.405843585729599
  - 0.42214125394821167
  - 0.4970420002937317
  - 0.4451411962509155
  - 0.42345693707466125
  - 0.5231732726097107
  - 0.3976520299911499
  - 0.5198226571083069
  - 0.4102203845977783
  - 0.4815176725387573
  - 0.4686886668205261
  - 0.4031945765018463
  - 0.41061118245124817
  - 0.40668657422065735
loss_records_fold2:
  train_losses:
  - 6.492168208956719
  - 6.588741058111191
  - 6.574270853400231
  - 6.530284914374352
  - 7.51509470641613
  - 7.2247312605381016
  - 6.475713643431664
  - 6.664013794064522
  - 6.46853977739811
  - 6.516143348813057
  - 7.296526670455933
  - 6.937555810809136
  - 6.594916242361069
  - 6.812755793333054
  - 6.856723326444627
  - 6.873117059469223
  - 6.538522690534592
  - 6.537256500124932
  - 6.545589527487755
  - 6.68249618113041
  - 6.491923981904984
  - 7.131177380681038
  - 6.591835325956345
  - 6.43341638147831
  - 6.3099875271320345
  - 6.486918959021569
  - 6.592105823755265
  - 6.393546807765961
  - 6.735576856136323
  - 6.7264506936073305
  - 6.532535755634308
  - 6.645946872234345
  - 7.221564811468125
  - 6.8693512171506885
  - 6.387282201647759
  - 6.426979139447212
  - 7.099651491641999
  - 6.4704521924257286
  - 6.463544651865959
  - 6.5535234928131105
  - 6.280655103921891
  - 6.744336667656899
  - 6.5933892875909805
  - 6.966560670733452
  - 6.477094602584839
  - 6.659306633472443
  - 6.466034090518952
  - 6.669289341568947
  - 6.437019386887551
  - 6.496200367808342
  - 6.345821884274483
  - 6.834105277061463
  - 6.591167840361596
  - 6.516297239065171
  - 6.5193511128425605
  - 6.321406203508378
  - 6.520050460100174
  - 6.712384530901909
  - 6.660593211650848
  - 6.384962642192841
  - 6.475940549373627
  - 7.1142727673053745
  - 6.709888279438019
  - 6.7323435097932816
  - 6.644783955812454
  - 6.564820727705956
  - 6.886261188983918
  - 6.471151643991471
  - 6.473493167757988
  - 6.642639189958572
  - 6.428083670139313
  - 6.738995689153672
  - 6.484462097287178
  - 6.448993051052094
  - 6.81636572778225
  - 6.614899627864361
  - 6.550009912252427
  - 6.383869084715844
  - 6.49851126074791
  - 6.660994495451451
  - 6.464888972043991
  - 6.5140886276960375
  - 6.468998205661774
  - 6.550981554389001
  - 6.547224241495133
  - 6.762822476029396
  - 6.574870517849923
  - 6.623610284924507
  - 6.584464105963708
  - 6.34938876926899
  - 6.676727759838105
  - 6.465291568636895
  - 6.552947780489922
  - 6.4727260887622835
  - 6.517508378624917
  - 6.7088468194007875
  - 6.777084416151047
  - 6.504349499940872
  - 6.402263420820237
  - 6.560977184772492
  validation_losses:
  - 0.39082399010658264
  - 0.3948690593242645
  - 40.25778579711914
  - 0.4677199125289917
  - 0.44382065534591675
  - 40.0886116027832
  - 0.4315718710422516
  - 7978.015625
  - 0.4033587872982025
  - 0.4041649103164673
  - 0.45604774355888367
  - 0.519307553768158
  - 0.6438223123550415
  - 1.349029302597046
  - 0.5078490376472473
  - 0.4711657166481018
  - 0.4559013843536377
  - 0.3899765908718109
  - 0.4106738865375519
  - 0.40139415860176086
  - 0.42331650853157043
  - 0.5343460440635681
  - 0.4127342998981476
  - 0.41287660598754883
  - 0.40309202671051025
  - 0.4313579797744751
  - 0.4103226661682129
  - 0.3944510221481323
  - 0.4635377824306488
  - 0.44481176137924194
  - 0.4208589196205139
  - 0.5334016680717468
  - 22.399362564086914
  - 0.4974193572998047
  - 0.49211710691452026
  - 0.45919182896614075
  - 8463.9111328125
  - 0.5015423893928528
  - 0.5409528017044067
  - 24.138490676879883
  - 8.475571632385254
  - 2.502314567565918
  - 0.4969426989555359
  - 0.4035026729106903
  - 913.776611328125
  - 962.7838745117188
  - 0.41877347230911255
  - 0.4090249538421631
  - 7.872826099395752
  - 0.5155689120292664
  - 8.55337905883789
  - 0.4630112946033478
  - 292.8885498046875
  - 0.4336802661418915
  - 521.697509765625
  - 566.0389404296875
  - 513.4057006835938
  - 0.4127962291240692
  - 0.4285185933113098
  - 0.41177281737327576
  - 0.42130282521247864
  - 0.5915531516075134
  - 302.9722595214844
  - 0.3973718583583832
  - 0.44033610820770264
  - 0.4148314297199249
  - 0.4229755401611328
  - 0.4049730896949768
  - 0.42075836658477783
  - 0.42824435234069824
  - 0.4040631651878357
  - 0.4170949459075928
  - 0.40285423398017883
  - 0.44331803917884827
  - 0.3982692062854767
  - 0.41760173439979553
  - 0.4148067235946655
  - 0.4327223300933838
  - 0.40893125534057617
  - 0.40982961654663086
  - 0.39571720361709595
  - 0.40099117159843445
  - 0.38841181993484497
  - 0.40486741065979004
  - 0.4297087788581848
  - 0.49971023201942444
  - 0.3975813686847687
  - 0.4350660443305969
  - 0.39934664964675903
  - 0.40511804819107056
  - 0.40900930762290955
  - 0.4348697364330292
  - 0.40186721086502075
  - 0.43235886096954346
  - 0.43241721391677856
  - 0.41377028822898865
  - 0.43856459856033325
  - 0.4189336895942688
  - 0.4054791033267975
  - 0.4283128082752228
loss_records_fold3:
  train_losses:
  - 6.608100885152817
  - 6.531671172380448
  - 6.677830165624619
  - 6.642305395007134
  - 6.435614120960236
  - 6.413212153315545
  - 6.465889033675194
  - 6.641190031170845
  - 6.5987317919731145
  - 6.604013603925705
  - 6.75542785525322
  - 6.624657002091408
  - 6.443311417102814
  - 6.394653007388115
  - 6.575622794032097
  - 6.490605700016022
  - 7.028052523732185
  - 6.290644517540932
  - 6.4905378252267845
  - 6.744271907210351
  - 6.7531010746955875
  - 6.616247394680977
  - 6.197750681638718
  - 6.350917562842369
  - 6.74614731669426
  - 6.529001063108445
  - 6.42094474285841
  - 6.3248615026474
  - 6.339323124289513
  - 6.811711063981057
  - 6.30965115427971
  - 6.432394102215767
  - 6.449037343263626
  - 6.565533703565598
  - 6.274316567182542
  - 6.622505781054497
  - 6.4769562721252445
  - 6.560303598642349
  - 6.596373444795609
  - 6.490452802181244
  - 6.537678268551827
  - 6.434278476238251
  - 6.3511425942182544
  - 6.7483952164649965
  - 6.543194219470024
  - 6.343844869732857
  - 6.441031351685524
  - 6.605543422698975
  - 6.515476888418198
  - 6.57131020128727
  - 6.572766619920731
  - 6.598816680908204
  - 6.525585627555848
  - 6.661486712098122
  - 6.426782044768334
  - 6.291214522719383
  - 6.325256744027138
  - 6.559144219756127
  - 6.45174243748188
  - 6.594738432765007
  - 6.630397394299507
  - 6.508642566204071
  - 6.3740649133920675
  - 6.404551142454148
  - 6.420886680483818
  - 6.3679617047309875
  - 6.527857422828674
  - 6.42742866575718
  - 6.842902493476868
  - 6.525645512342454
  - 6.4674437850713735
  - 6.530293509364128
  - 6.7012225002050405
  - 6.401776587963105
  - 6.320850288867951
  - 6.33475405573845
  - 6.556830203533173
  - 6.578237384557724
  - 6.901484698057175
  - 6.506970793008804
  - 6.320922338962555
  - 6.40565853714943
  - 6.588945645093919
  - 6.596989232301713
  - 6.6114493966102605
  - 6.51698440015316
  - 6.438737988471985
  - 6.802256888151169
  - 6.548621475696564
  - 6.537818005681038
  validation_losses:
  - 0.4772895574569702
  - 0.49264955520629883
  - 0.4616442918777466
  - 0.4126167595386505
  - 0.43259724974632263
  - 0.4975936710834503
  - 0.4415527880191803
  - 0.4168643355369568
  - 0.4488949477672577
  - 0.44824114441871643
  - 0.43236514925956726
  - 5655.404296875
  - 0.4581347107887268
  - 0.42448559403419495
  - 0.40757113695144653
  - 0.4719323217868805
  - 0.41438502073287964
  - 0.42687177658081055
  - 0.4516292214393616
  - 0.43107181787490845
  - 0.5155437588691711
  - 0.5092606544494629
  - 0.42797958850860596
  - 0.42776235938072205
  - 0.43934932351112366
  - 0.43815380334854126
  - 0.40662527084350586
  - 0.41979870200157166
  - 0.5132138729095459
  - 0.413251131772995
  - 0.4174911677837372
  - 0.4099479913711548
  - 0.4089609384536743
  - 436.15447998046875
  - 546878095360.0
  - 820031979520.0
  - 1012476870656.0
  - 998726696960.0
  - 923786477568.0
  - 1006421868544.0
  - 950822830080.0
  - 869397168128.0
  - 1073851465728.0
  - 872966651904.0
  - 905722003456.0
  - 966189121536.0
  - 931942105088.0
  - 665086590976.0
  - 867658956800.0
  - 826395656192.0
  - 848111271936.0
  - 904126529536.0
  - 1026419523584.0
  - 942086946816.0
  - 1002787438592.0
  - 824421580800.0
  - 956344762368.0
  - 973316816896.0
  - 1007766929408.0
  - 1008550739968.0
  - 879315910656.0
  - 1006556545024.0
  - 897728708608.0
  - 1009088266240.0
  - 1073301356544.0
  - 1046415081472.0
  - 717195247616.0
  - 870479429632.0
  - 825617219584.0
  - 765585981440.0
  - 632650596352.0
  - 717116211200.0
  - 702283448320.0
  - 703680086016.0
  - 594767642624.0
  - 808745172992.0
  - 928486785024.0
  - 927253200896.0
  - 917857107968.0
  - 787412811776.0
  - 793972113408.0
  - 948530839552.0
  - 799129796608.0
  - 833028358144.0
  - 784998793216.0
  - 716156043264.0
  - 522826874880.0
  - 427134812160.0
  - 419977232384.0
  - 314245742592.0
loss_records_fold4:
  train_losses:
  - 6.4783707767725
  - 6.378156378865242
  - 6.442647847533227
  - 6.432159978151322
  - 6.374500572681427
  - 6.353650683164597
  - 7.118651404976845
  - 6.700544771552086
  - 6.69626169204712
  - 6.430600473284722
  - 6.478569272160531
  - 6.759438049793244
  - 6.6368752241134645
  - 6.656914579868317
  - 6.645607805252076
  - 6.860660642385483
  - 6.5298182547092445
  - 6.689422076940537
  - 6.982955306768417
  - 6.51115685403347
  - 6.503622740507126
  - 6.519113105535507
  - 6.327140045166016
  - 6.61164795011282
  - 6.6934372842311864
  - 6.465146547555924
  - 6.440968576073647
  - 6.419194561243057
  - 6.505159950256348
  - 6.493236604332925
  - 6.5220201343297965
  - 6.552770879864693
  - 6.492947852611542
  - 7.029356813430787
  - 6.450711792707444
  - 6.383299496769905
  - 7.01255231499672
  - 6.370707306265832
  - 6.681637370586396
  - 6.712064878642559
  - 6.469444373250008
  - 6.5524702131748205
  - 6.640322577953339
  - 6.626271054148674
  - 6.8283178627491
  - 6.347227713465691
  - 6.987669271230698
  - 6.714323484897614
  - 6.598395866155625
  - 6.525121676921845
  - 6.468122482299805
  - 6.608453160524369
  - 6.4431406468153005
  - 6.424965167045594
  - 6.824212580919266
  - 6.527831202745438
  - 6.531172817945481
  - 6.595262724161149
  - 6.607772183418274
  - 6.530979961156845
  - 6.556674802303315
  - 6.622809374332428
  - 6.3363367497921
  - 6.409316745400429
  - 6.563306078314781
  - 6.42066565155983
  - 6.554153198003769
  - 6.567562079429627
  - 6.371718528866769
  - 6.310749459266663
  - 6.5134929627180105
  - 6.387154716253281
  - 6.507715353369713
  - 6.685635361075402
  - 6.637608009576798
  - 6.62380285859108
  - 6.531601080298424
  - 6.362387150526047
  - 6.674501669406891
  - 6.59660369604826
  - 6.696038402616978
  - 6.492603081464768
  validation_losses:
  - 130970181632.0
  - 74236346368.0
  - 90196090880.0
  - 119465295872.0
  - 181265678336.0
  - 180875345920.0
  - 211535183872.0
  - 139477417984.0
  - 156979789824.0
  - 125938573312.0
  - 156979773440.0
  - 125544194048.0
  - 93607108608.0
  - 181618147328.0
  - 128680255488.0
  - 75625103360.0
  - 116687478784.0
  - 120334401536.0
  - 248625512448.0
  - 177622990848.0
  - 97101357056.0
  - 244105789440.0
  - 181531131904.0
  - 241550196736.0
  - 268640468992.0
  - 228653334528.0
  - 315089453056.0
  - 151687086080.0
  - 154214875136.0
  - 154865451008.0
  - 91981864960.0
  - 115250888704.0
  - 120334401536.0
  - 152203935744.0
  - 130970181632.0
  - 93283958784.0
  - 68568776704.0
  - 128762667008.0
  - 100581949440.0
  - 117997518848.0
  - 100008845312.0
  - 77566803968.0
  - 130887794688.0
  - 60552892416.0
  - 91341291520.0
  - 142531362816.0
  - 91260870656.0
  - 129897742336.0
  - 191961546752.0
  - 156979789824.0
  - 171490312192.0
  - 186618511360.0
  - 158770053120.0
  - 77647699968.0
  - 86876012544.0
  - 126027554816.0
  - 75538784256.0
  - 125853868032.0
  - 130887794688.0
  - 90604077056.0
  - 115899604992.0
  - 30736097280.0
  - 51631575040.0
  - 38180438016.0
  - 7503137792.0
  - 52286726144.0
  - 118895828992.0
  - 165324357632.0
  - 55191269376.0
  - 28869775360.0
  - 28630659072.0
  - 30649192448.0
  - 91653283840.0
  - 82265538560.0
  - 43612188672.0
  - 79338774528.0
  - 35587112960.0
  - 13733026816.0
  - 11466404864.0
  - 0.41021257638931274
  - 0.41221264004707336
  - 0.4165167808532715
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 51 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 90 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 82 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:41:52.476923'
