config:
  aggregation: mean
  batch_size: 32
  class_weights: false
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 64
  - 64
  - 64
  - 64
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-22 18:27:07.852644'
fold_0_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_100fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_100fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_100fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_100fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_100fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_100fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_100fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_100fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_100fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.22.2022_16.16.52/config_100fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 7.18069136440754
  - 6.779608422517777
  - 6.7878461450338365
  - 7.334794235229492
  - 6.884807679057122
  - 6.464136877655983
  - 6.644564211368561
  - 6.37658812403679
  - 6.338379606604576
  - 6.4906317561864855
  - 6.408136034011841
  - 6.472259160876274
  - 6.3928520590066915
  - 6.060418203473091
  - 6.072209519147873
  - 6.036005011200905
  - 5.951324835419655
  - 6.019784206151963
  - 6.0481828391551975
  - 6.059448769688607
  - 5.928156036138535
  - 5.960386109352112
  - 6.083263111114502
  - 6.027843260765076
  - 6.048360389471054
  - 6.031164953112603
  - 6.004647544026375
  - 5.974857977032662
  - 6.110437178611756
  - 6.023125585913658
  validation_losses:
  - 0.4086691439151764
  - 0.41746166348457336
  - 0.4272405207157135
  - 0.4016234278678894
  - 0.3869708776473999
  - 0.39976397156715393
  - 0.4038301408290863
  - 0.3941766619682312
  - 0.40527817606925964
  - 0.39656904339790344
  - 0.4111137092113495
  - 0.3872012794017792
  - 0.4671497046947479
  - 1.1927064657211304
  - 0.3913536071777344
  - 0.8427385687828064
  - 2.8258519172668457
  - 0.5680524110794067
  - 1.8596378564834595
  - 0.40097716450691223
  - 0.3869204521179199
  - 0.4340546727180481
  - 0.3821035325527191
  - 6.04409122467041
  - 1.1872667074203491
  - 0.3861043155193329
  - 0.38910990953445435
  - 0.390317440032959
  - 0.3928014039993286
  - 0.3981040120124817
loss_records_fold1:
  train_losses:
  - 5.96041676402092
  - 5.967571330070496
  - 5.968527492880821
  - 5.933011543750763
  - 6.018694454431534
  - 6.003842496871949
  - 5.948576274514199
  - 5.989470827579499
  - 5.9086693137884145
  - 6.112310767173767
  - 5.9624824881553655
  - 6.061654245853425
  - 6.061197221279144
  - 5.871747553348541
  - 6.0136816769838335
  - 5.896686041355133
  - 5.78612408041954
  - 5.8732004344463355
  - 6.033898651599884
  - 6.055942732095719
  - 6.049991393089295
  - 6.052853456139565
  - 6.0354088127613075
  - 5.961256259679795
  - 5.911366870999337
  - 6.159130090475083
  - 5.955604594945908
  - 5.939482498168946
  - 5.830420795083047
  - 5.866789168119431
  - 5.892856535315514
  - 5.8070991218090064
  - 5.824718552827836
  - 5.870313608646393
  - 5.92168449163437
  - 5.913384500145913
  - 5.974526271224022
  - 5.934733337163926
  - 5.913275277614594
  - 5.965269556641579
  - 5.777082866430283
  - 5.881773155927658
  - 5.8834631711244585
  - 5.795035041868687
  - 5.848075196146965
  - 5.8263418018817905
  - 5.750692600011826
  - 5.858197531104088
  - 5.930247655510903
  - 5.89230050444603
  - 5.773496899008752
  - 5.864152297377586
  - 5.824096137285233
  - 5.921464103460313
  - 6.241661575436592
  - 5.953271317481995
  - 5.99234910607338
  - 5.935958826541901
  - 5.963270011544228
  - 5.79244396686554
  - 5.925160849094391
  - 5.781042239069939
  - 5.915773421525955
  - 5.929976916313172
  - 5.854359832406044
  - 5.850384876132011
  - 6.007405546307564
  - 5.861788061261177
  - 5.832296350598336
  - 5.731400096416474
  - 5.791744065284729
  - 5.79536244571209
  - 5.987228620052338
  - 5.946892955899239
  - 5.885067632794381
  - 5.851687619090081
  - 5.970007809996606
  - 5.797162333130837
  - 5.9697048082947735
  - 5.956736087799072
  - 5.7640374392271045
  - 5.896203497052193
  - 5.998450526595116
  - 5.791745004057884
  - 6.115872490406037
  - 5.896675693988801
  - 5.837909451127053
  - 5.773829472064972
  - 5.859844228625298
  - 5.9616793364286425
  - 5.855423188209534
  - 5.898505142331124
  - 5.91317589879036
  - 6.025277063250542
  - 5.858899953961373
  - 5.815121665596962
  - 5.8406898975372314
  - 5.883246544003487
  - 5.794602939486504
  - 5.8250509142875675
  validation_losses:
  - 0.393503338098526
  - 0.3961126208305359
  - 0.4057551324367523
  - 0.3980042338371277
  - 0.3952206075191498
  - 0.5344165563583374
  - 0.418912410736084
  - 0.39673301577568054
  - 0.9260337948799133
  - 0.3903583884239197
  - 0.3995407223701477
  - 0.3907143771648407
  - 0.393258273601532
  - 0.40441158413887024
  - 0.4011906087398529
  - 0.4133409559726715
  - 0.39779406785964966
  - 0.38922804594039917
  - 0.393612802028656
  - 0.3976469337940216
  - 0.38770970702171326
  - 0.4034545421600342
  - 0.38663214445114136
  - 0.3863009214401245
  - 0.39171281456947327
  - 0.40328294038772583
  - 0.40329036116600037
  - 0.4916555881500244
  - 0.39680877327919006
  - 0.42480015754699707
  - 0.3990427255630493
  - 0.5557636618614197
  - 0.4257330894470215
  - 0.44356998801231384
  - 0.7954931855201721
  - 0.5017210245132446
  - 0.4283186197280884
  - 0.4481677711009979
  - 0.4376027286052704
  - 0.5083031058311462
  - 0.8517126441001892
  - 2.4674859046936035
  - 2.7127091884613037
  - 0.4669307470321655
  - 0.43245062232017517
  - 0.6203479170799255
  - 0.5622352957725525
  - 0.42230111360549927
  - 0.6700098514556885
  - 0.41045668721199036
  - 0.5310771465301514
  - 0.5377686023712158
  - 0.6040951609611511
  - 0.4078989028930664
  - 0.6089567542076111
  - 0.44500336050987244
  - 0.41056063771247864
  - 0.43118762969970703
  - 0.4233580231666565
  - 0.3978498876094818
  - 0.4696520268917084
  - 0.5773427486419678
  - 0.39894646406173706
  - 0.4433414340019226
  - 0.517977237701416
  - 0.5451188087463379
  - 0.39275842905044556
  - 0.39320528507232666
  - 0.4091153144836426
  - 0.4954022765159607
  - 0.5165129899978638
  - 0.5096721649169922
  - 0.524625301361084
  - 0.43944862484931946
  - 0.4276021718978882
  - 0.47147712111473083
  - 0.4569700062274933
  - 0.40111133456230164
  - 0.41977593302726746
  - 0.45370206236839294
  - 0.47947514057159424
  - 0.41924020648002625
  - 0.431163489818573
  - 0.416648805141449
  - 0.4795190095901489
  - 0.3885439336299896
  - 0.4825938045978546
  - 0.4603721797466278
  - 0.5098896026611328
  - 0.4162061810493469
  - 0.4515208303928375
  - 0.44044509530067444
  - 0.45040684938430786
  - 0.4464975595474243
  - 0.43488386273384094
  - 0.5199596881866455
  - 0.4949081242084503
  - 0.42178741097450256
  - 0.45868000388145447
  - 0.5020851492881775
loss_records_fold2:
  train_losses:
  - 5.816481372714043
  - 5.87507416009903
  - 5.7472461581230165
  - 5.823431053757668
  - 5.8413579076528555
  - 5.888853019475937
  - 5.7832149237394335
  - 5.746476909518242
  - 5.7873113572597505
  - 5.896977505087853
  - 5.825470900535584
  - 5.939918547868729
  - 5.8618990868330005
  - 5.7945738494396215
  - 5.890484783053399
  - 5.838303625583649
  - 5.808918929100037
  - 5.861568552255631
  - 5.6800112485885625
  - 5.907995727658272
  - 5.819746914505959
  - 5.979762527346612
  - 5.968385088443757
  - 5.868684884905815
  - 5.914679992198945
  - 5.903623306751252
  - 5.837669721245766
  - 5.949899756908417
  - 5.968789486587048
  - 5.760705211758614
  - 5.930240327119828
  - 5.813409942388535
  - 5.846955707669259
  - 6.088358461856842
  - 6.1294713616371155
  - 6.000091469287873
  - 5.913388484716416
  - 5.932257194817066
  - 6.05617302954197
  - 5.94677197933197
  - 5.990645116567612
  - 5.839085814356804
  - 5.863436332345009
  - 5.893788531422615
  - 5.845497593283653
  validation_losses:
  - 0.4497203826904297
  - 0.6013638377189636
  - 0.3889833688735962
  - 0.4824369549751282
  - 0.4139921963214874
  - 0.4660375714302063
  - 0.48905670642852783
  - 0.40532049536705017
  - 0.5070865750312805
  - 0.5298731327056885
  - 0.6739990711212158
  - 0.4113824665546417
  - 0.3881866931915283
  - 0.38557592034339905
  - 0.6248026490211487
  - 0.443185031414032
  - 1.0020530223846436
  - 0.7005097270011902
  - 0.7607671022415161
  - 0.6229419112205505
  - 0.38897737860679626
  - 0.3925236761569977
  - 0.5701937675476074
  - 0.427616149187088
  - 0.40774843096733093
  - 0.45935311913490295
  - 0.4747726023197174
  - 0.4022040367126465
  - 0.4423043131828308
  - 0.39557698369026184
  - 0.44546473026275635
  - 0.41672590374946594
  - 0.41940486431121826
  - 5.71204137802124
  - 1181.2142333984375
  - 2038151.125
  - 224912.921875
  - 39586.19921875
  - 51642.984375
  - 2.4191935062408447
  - 0.4012475311756134
  - 0.3975944519042969
  - 0.38962510228157043
  - 0.3982431888580322
  - 0.377976655960083
loss_records_fold3:
  train_losses:
  - 5.926324591040611
  - 6.091604954004288
  - 5.918424087762833
  - 5.842391660809517
  - 5.771704256534576
  - 6.0953685522079475
  - 6.0246249586343765
  - 5.811616748571396
  - 5.86455920636654
  - 6.029605329036713
  - 5.921586069464684
  - 5.760481610894203
  - 5.950482332706452
  - 5.843806976079941
  - 5.9187795460224155
  - 5.79051805138588
  - 6.030325147509576
  - 5.837152692675591
  - 6.048335066437722
  - 5.918349429965019
  - 5.918064093589783
  - 5.935804116725922
  - 5.870695900917053
  - 5.863041239976884
  - 5.89296727180481
  - 5.875194078683854
  - 5.9424727916717535
  - 6.022396287322045
  - 5.852464520931244
  - 5.937114235758782
  - 5.921938720345498
  - 5.992316228151322
  - 5.951742857694626
  - 5.679698038101197
  - 5.837972921133042
  - 5.844839596748352
  - 5.761399361491204
  - 5.9649776563048365
  - 5.83200461268425
  - 5.719066464900971
  - 5.758766633272171
  - 5.847303166985512
  - 5.8611261576414115
  - 5.827933976054192
  - 5.89500482082367
  - 5.954307824373245
  - 5.850124904513359
  - 5.794709965586662
  - 5.923973348736763
  - 5.84873902797699
  - 5.699052357673645
  - 5.745677408576012
  - 5.847670221328736
  - 5.81880239546299
  - 5.716338664293289
  - 5.845545601844788
  - 5.84645833671093
  - 5.8968002080917366
  - 5.915828973054886
  - 5.905401208996773
  - 5.963400357961655
  - 5.922836199402809
  - 5.911907342076302
  - 5.807404029369355
  - 5.756138199567795
  - 5.888438421487809
  - 5.836745205521584
  - 5.72019017636776
  - 5.869312542676926
  - 5.806507033109665
  - 5.900634071230889
  - 5.8207224160432816
  - 5.8255321949720384
  - 5.832947620749474
  - 5.8072134256362915
  - 5.9559152066707615
  - 5.8226052045822145
  - 5.866511228680611
  - 5.879938992857934
  - 5.729324138164521
  - 5.87531835436821
  - 5.853562164306641
  - 5.889631763100624
  - 5.885983654856682
  - 5.801804530620576
  - 5.803812956809998
  - 5.984915488958359
  - 5.851144948601723
  - 5.884649556875229
  - 5.827887785434723
  - 5.863823211193085
  - 5.8775834262371065
  - 5.795611417293549
  - 5.794253197312355
  - 5.878721332550049
  - 5.94981065094471
  - 5.931592479348183
  - 5.822050142288209
  - 5.956568789482117
  - 5.7778012663126
  validation_losses:
  - 0.3805086016654968
  - 2.8833162784576416
  - 0.3858056962490082
  - 0.41749465465545654
  - 3.308371067047119
  - 1.141560673713684
  - 1.003825306892395
  - 13.700028419494629
  - 16.368762969970703
  - 18.712862014770508
  - 5.30192232131958
  - 18.31399917602539
  - 0.40535035729408264
  - 9.769426345825195
  - 2.556927442550659
  - 24.361743927001953
  - 13.122397422790527
  - 22.471206665039062
  - 17.49356460571289
  - 6.580939769744873
  - 8.092496871948242
  - 1.1732664108276367
  - 2.8846492767333984
  - 1.4564311504364014
  - 2.6056571006774902
  - 1.4254157543182373
  - 1.9258294105529785
  - 0.9794986844062805
  - 1.3512296676635742
  - 6.474854946136475
  - 4.515402793884277
  - 2.4790496826171875
  - 5.963513374328613
  - 3.634361982345581
  - 3.1244852542877197
  - 2.4437224864959717
  - 2.0298783779144287
  - 2.7478764057159424
  - 1.278733253479004
  - 2.7604482173919678
  - 9.68001651763916
  - 5.31822395324707
  - 8.813950538635254
  - 1.9301289319992065
  - 7.0910115242004395
  - 4.352567672729492
  - 1.0866858959197998
  - 1.5957533121109009
  - 2.3923068046569824
  - 0.9666846990585327
  - 1.4803111553192139
  - 1.98434579372406
  - 3.601250648498535
  - 2.49575138092041
  - 3.96925687789917
  - 3.2515575885772705
  - 5.505486011505127
  - 1.1253868341445923
  - 1.0752981901168823
  - 0.3858036398887634
  - 1.3288849592208862
  - 0.8224818110466003
  - 1.083065390586853
  - 0.7414613962173462
  - 1.7776497602462769
  - 1.17630934715271
  - 0.5565571784973145
  - 1.016015887260437
  - 0.5151349306106567
  - 0.7975291609764099
  - 0.5547689199447632
  - 0.7272939085960388
  - 0.5814657211303711
  - 0.9055914282798767
  - 1.602026104927063
  - 2.5461342334747314
  - 0.45390406250953674
  - 0.8712558150291443
  - 0.4172777831554413
  - 0.8603637218475342
  - 2.6451971530914307
  - 3.013991594314575
  - 1.3761866092681885
  - 1.0384036302566528
  - 1.0492143630981445
  - 1.047867774963379
  - 1.1614669561386108
  - 0.4956856369972229
  - 0.743950366973877
  - 0.7739752531051636
  - 1.049225926399231
  - 0.7035334706306458
  - 0.5377318263053894
  - 0.5327522158622742
  - 0.5412226915359497
  - 0.6033633947372437
  - 0.823944628238678
  - 0.6959023475646973
  - 0.8339601755142212
  - 0.5423332452774048
loss_records_fold4:
  train_losses:
  - 5.920161819458008
  - 5.743224376440049
  - 5.752657818794251
  - 5.72067586183548
  - 5.752483120560647
  - 5.829314222931862
  - 5.8663147509098055
  - 5.7606964915990835
  - 5.820860081911087
  - 5.740653061866761
  - 5.854922133684159
  - 5.788299617171288
  - 5.857477807998658
  - 5.900265344977379
  - 5.805124923586845
  - 5.719411733746529
  - 5.752549776434899
  - 6.003992876410485
  - 5.703019767999649
  - 5.883838596940041
  - 5.899186405539513
  - 5.840237158536912
  - 6.025100234150887
  - 5.875656533241273
  - 5.830698519945145
  - 5.847238677740098
  - 5.919892701506615
  - 5.89468292593956
  - 6.029483586549759
  - 5.829953122138978
  - 5.840010043978691
  - 5.866878637671471
  - 5.88201867043972
  - 5.925833782553673
  - 5.934893110394478
  - 5.924592277407647
  - 5.751356610655785
  - 5.8077191621065145
  - 5.881858968734742
  - 5.817689684033394
  - 5.922848442196846
  - 5.8567655265331275
  - 5.852612158656121
  - 5.92813473045826
  - 5.880423933267593
  - 5.7243261456489565
  - 5.81916675567627
  - 5.873325949907303
  - 5.857552725076676
  - 5.90635014474392
  - 5.905067983269692
  - 5.816550797224045
  - 5.841901814937592
  - 5.979661661386491
  - 5.842178225517273
  - 5.888453716039658
  - 5.858937552571297
  - 5.7686817228794105
  - 5.887217745184898
  - 5.882142761349678
  - 5.8398260384798055
  - 5.990837952494622
  - 5.859663271903992
  - 5.9008201450109485
  - 5.811995190382004
  - 5.842278701066971
  - 5.915887168049813
  - 5.837314853072167
  - 5.960519927740098
  - 5.820860502123833
  - 5.90844074189663
  - 5.849173927307129
  - 5.778160667419434
  - 5.909268140792847
  - 5.804040855169297
  - 5.685053977370263
  - 5.766812658309937
  - 5.787949639558793
  - 5.831062632799149
  - 5.907289278507233
  - 5.78492925465107
  - 5.881981709599495
  - 5.824921780824662
  - 5.792207434773445
  - 5.830108827352524
  - 5.980974397063256
  - 5.854894410073758
  - 5.735536426305771
  - 5.770523017644883
  - 5.819230982661248
  - 5.831269532442093
  - 5.773281848430634
  - 5.7439966887235645
  - 5.829328030347824
  - 5.985785421729088
  - 5.964659160375596
  - 5.876808127760888
  - 5.902930349111557
  - 5.945351457595826
  - 5.765944015979767
  validation_losses:
  - 0.4580753445625305
  - 0.42082253098487854
  - 0.5626892447471619
  - 0.8107448220252991
  - 0.8033551573753357
  - 0.5999017357826233
  - 0.6677872538566589
  - 0.5543645620346069
  - 0.6131507158279419
  - 0.5960131883621216
  - 0.6348230838775635
  - 0.5801045894622803
  - 0.48453640937805176
  - 0.4355388581752777
  - 0.4872152507305145
  - 0.8415921926498413
  - 0.4334867298603058
  - 0.4080570340156555
  - 0.5995439291000366
  - 0.40445491671562195
  - 0.42138949036598206
  - 0.42028599977493286
  - 0.5226579308509827
  - 0.5309250354766846
  - 0.45538729429244995
  - 0.4535681903362274
  - 0.5323101878166199
  - 0.4991261065006256
  - 0.40187886357307434
  - 0.46505212783813477
  - 0.37482550740242004
  - 0.4113454818725586
  - 0.4169926345348358
  - 0.4063706398010254
  - 0.40083760023117065
  - 0.4278595447540283
  - 0.6907622218132019
  - 0.47502321004867554
  - 0.41689035296440125
  - 0.49926578998565674
  - 0.4289258122444153
  - 0.43806836009025574
  - 0.5158036947250366
  - 0.4795428514480591
  - 0.4815981090068817
  - 0.4211457669734955
  - 0.4122697710990906
  - 0.3815070688724518
  - 0.4041876494884491
  - 0.40196940302848816
  - 0.46302711963653564
  - 0.3858010768890381
  - 0.4522395730018616
  - 0.42994004487991333
  - 0.4234609305858612
  - 0.5216991305351257
  - 0.4934002459049225
  - 0.5025272965431213
  - 0.41124001145362854
  - 0.9368045330047607
  - 0.39713844656944275
  - 0.38606882095336914
  - 0.49903351068496704
  - 0.3770328760147095
  - 0.37761175632476807
  - 0.42013829946517944
  - 0.4658382534980774
  - 0.38327357172966003
  - 0.394121378660202
  - 0.41167768836021423
  - 0.4750538170337677
  - 0.4187324047088623
  - 0.4216310679912567
  - 0.4384286105632782
  - 0.42119672894477844
  - 0.48316341638565063
  - 0.5007769465446472
  - 0.5295435190200806
  - 0.45110052824020386
  - 0.5475262403488159
  - 0.5208185911178589
  - 0.39761579036712646
  - 0.4730531871318817
  - 0.489105224609375
  - 0.437177449464798
  - 0.39765363931655884
  - 0.37550437450408936
  - 0.39758774638175964
  - 0.37063321471214294
  - 0.38118574023246765
  - 0.37871435284614563
  - 0.40898650884628296
  - 0.36660459637641907
  - 0.39854058623313904
  - 0.4322536289691925
  - 0.4123937785625458
  - 0.44504061341285706
  - 0.4673362374305725
  - 0.4034654498100281
  - 0.5486757755279541
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 30 epochs
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 45 epochs
  fold 3 training message: completed 100 epochs without stopping early
  fold 4 training message: completed 100 epochs without stopping early
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8576329331046312,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.8582707054988712
  mean_f1_accuracy: 0.0
  total_train_time: '0:42:30.957642'
