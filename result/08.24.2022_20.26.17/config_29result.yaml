config:
  aggregation: sum
  batch_size: 64
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: false
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 21:01:22.367329'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_29fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_29fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_29fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_29fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_29fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_29fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_29fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_29fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_29fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_29fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 46.35080571174622
  - 15.604998052120209
  - 9.956900647282602
  - 6.12494620680809
  - 8.741114470362664
  - 4.746782517433167
  - 5.322083622217178
  - 11.58317769765854
  - 7.053783637285233
  - 5.397159522771836
  - 6.770178383588791
  - 4.603667199611664
  - 5.882204008102417
  - 5.542310696840286
  - 4.93485918045044
  - 4.847593194246293
  - 5.176704162359238
  - 8.68724616765976
  - 8.80169254243374
  - 8.564258903265
  - 6.868541985750198
  - 5.471419030427933
  - 4.0049903869628904
  - 5.878487092256546
  - 9.883416277170182
  - 5.590712398290634
  - 7.481847447156906
  - 8.861645513772965
  - 4.36350057721138
  - 3.3436689347028734
  - 3.462290108203888
  - 5.814488565921784
  - 4.129805356264114
  - 5.019116055965424
  - 4.018760007619858
  - 3.9015602409839634
  - 5.832809564471245
  - 4.8482410490512855
  - 5.869356632232666
  - 4.107876747846603
  - 3.641971039772034
  - 3.4620323479175568
  - 4.024397122859955
  - 5.654618072509766
  - 3.7302895128726963
  - 3.2067520797252658
  - 3.7877948880195618
  - 3.213539677858353
  - 3.3786931455135347
  - 3.512540340423584
  - 3.4951655209064487
  - 3.497610414028168
  - 3.240147063136101
  - 3.410198211669922
  - 3.5145340085029604
  - 3.4571453452110292
  - 3.4379931807518007
  - 3.847453820705414
  - 3.4526623606681826
  - 4.736861377954483
  - 3.4975669324398044
  - 3.3037619948387147
  - 3.2398065418004993
  - 3.4016207396984104
  - 3.5046208232641223
  - 3.515094792842865
  - 3.4794279575347904
  - 3.2684521049261095
  - 3.365798032283783
  - 3.345509797334671
  - 3.4521622717380525
  - 3.550975126028061
  - 3.1904402673244476
  - 3.337362676858902
  - 3.222597283124924
  - 3.112838008999825
  - 3.1875866234302523
  - 3.3966181159019473
  - 3.1959602534770966
  - 3.2844114720821382
  - 3.1394831359386446
  - 3.3897683858871464
  - 3.2934404492378238
  - 3.326927137374878
  - 3.189812767505646
  - 3.4984050750732423
  - 3.150835180282593
  - 3.3174560844898227
  - 3.2609770357608796
  - 3.1893077284097675
  - 3.114280253648758
  - 3.200617635250092
  - 3.11685466170311
  - 3.216159164905548
  - 3.259807801246643
  - 3.1999440848827363
  - 3.228283977508545
  - 3.2854455888271334
  - 3.3109112203121187
  - 3.35212277173996
  validation_losses:
  - 5.822249412536621
  - 0.8647621870040894
  - 0.4577177166938782
  - 0.5004560351371765
  - 0.4746246337890625
  - 0.47880175709724426
  - 1.1153630018234253
  - 0.5269387364387512
  - 0.42130547761917114
  - 0.4822983145713806
  - 0.6184582710266113
  - 0.4733428359031677
  - 0.4247291088104248
  - 0.544844388961792
  - 0.46088898181915283
  - 0.44976285099983215
  - 0.43882086873054504
  - 0.3987998366355896
  - 0.44651171565055847
  - 0.49157077074050903
  - 0.4149739146232605
  - 0.4529421031475067
  - 0.4413871169090271
  - 0.5911920666694641
  - 0.40605995059013367
  - 0.4836088716983795
  - 0.48001837730407715
  - 0.4126182794570923
  - 0.4280066192150116
  - 0.46320533752441406
  - 0.4082563519477844
  - 0.4280335307121277
  - 0.533168375492096
  - 0.4119418263435364
  - 0.4082108438014984
  - 0.41210120916366577
  - 0.40237054228782654
  - 0.3976786136627197
  - 0.4252486526966095
  - 0.4306090474128723
  - 0.3951316773891449
  - 0.41496968269348145
  - 0.4261374771595001
  - 0.42373892664909363
  - 0.40251272916793823
  - 0.3979027271270752
  - 0.42294445633888245
  - 0.3905879557132721
  - 0.3925420045852661
  - 0.3975032866001129
  - 0.40130963921546936
  - 0.40052878856658936
  - 0.4192836880683899
  - 0.397722452878952
  - 0.44415441155433655
  - 0.4007698595523834
  - 0.4118426442146301
  - 0.4086662530899048
  - 0.42973196506500244
  - 0.4275016486644745
  - 0.41775768995285034
  - 0.4257861375808716
  - 0.42001017928123474
  - 0.4165334701538086
  - 0.4515377879142761
  - 0.4172605872154236
  - 0.411079078912735
  - 0.43514519929885864
  - 0.419334352016449
  - 0.4028993844985962
  - 0.6654702425003052
  - 0.4209480285644531
  - 0.40624091029167175
  - 0.40738701820373535
  - 0.4264836311340332
  - 0.420590341091156
  - 0.42471763491630554
  - 0.4118095338344574
  - 0.39996057748794556
  - 0.4154392182826996
  - 0.4125829041004181
  - 0.39670634269714355
  - 0.4326269030570984
  - 0.4088647663593292
  - 0.4712980091571808
  - 0.43451571464538574
  - 0.41095179319381714
  - 0.4304959774017334
  - 0.449660986661911
  - 0.4219050407409668
  - 0.43393459916114807
  - 0.41768917441368103
  - 0.4602317810058594
  - 0.4106232821941376
  - 0.407927006483078
  - 0.44586846232414246
  - 0.4231111705303192
  - 0.44537052512168884
  - 0.4202682673931122
  - 0.4153287410736084
loss_records_fold1:
  train_losses:
  - 3.1658446848392487
  - 3.3365924596786503
  - 3.4787438035011293
  - 3.16595978140831
  - 3.3646309971809387
  - 3.381024205684662
  - 3.1841595947742465
  - 3.1526084661483766
  - 3.2548447817564012
  - 3.1607007503509523
  - 3.306503361463547
  - 3.305067437887192
  - 3.4428007125854494
  - 3.5198892772197725
  - 3.3651718854904176
  - 3.865533447265625
  - 3.365535962581635
  - 3.475326746702194
  - 3.4130776166915897
  - 3.2831216156482697
  - 3.1310133874416355
  - 3.1807861566543583
  - 3.4329676926136017
  - 3.3625329732894897
  - 3.1337827384471897
  - 3.246100735664368
  - 3.2322613894939423
  - 3.312030529975891
  - 3.3862797617912292
  - 3.3883517384529114
  - 3.107457804679871
  - 3.2218416035175323
  - 3.5229937076568607
  - 3.089651730656624
  - 3.409379088878632
  - 3.2328233361244205
  - 3.3324727296829226
  - 3.4183724582195283
  - 3.1799918055534366
  - 3.287926083803177
  - 3.174361765384674
  - 3.125561547279358
  - 3.155695700645447
  - 3.1892313778400423
  - 3.269130474328995
  - 3.2809217751026156
  - 3.1072062790393833
  - 3.2031541883945467
  - 3.3121634006500247
  - 3.139940690994263
  - 3.2626297265291218
  - 3.2713880419731143
  - 3.3885089427232744
  - 4.379127150774003
  - 3.6302288621664047
  - 3.5182861328125004
  - 3.3616510421037678
  - 3.388397175073624
  - 3.6390085846185687
  - 3.200876295566559
  - 3.304193848371506
  - 3.3484092593193058
  - 3.2682638943195346
  - 3.350209951400757
  - 3.2430086195468903
  - 3.2320226907730105
  - 3.2688202619552613
  - 3.2636749625205996
  - 3.2306736141443255
  - 3.2581328213214875
  - 3.2237616002559664
  - 3.2475620687007907
  - 3.240288251638413
  - 3.279082500934601
  - 3.1343182146549227
  - 3.212683302164078
  - 3.144154924154282
  - 3.196177971363068
  - 3.213792359828949
  - 3.172620177268982
  - 3.289538300037384
  - 3.293133792281151
  - 3.1881064355373385
  - 3.14205065369606
  - 3.1263373643159866
  - 3.226383012533188
  - 3.195072638988495
  - 3.1650314033031464
  - 3.1001196801662445
  - 3.193745493888855
  - 3.2108151435852053
  - 3.165948444604874
  - 3.225139939785004
  - 3.1689945936203006
  - 3.19928075671196
  - 3.2310527801513675
  - 3.249450290203095
  - 3.2844364643096924
  - 3.1696042835712435
  - 3.1783735632896426
  validation_losses:
  - 0.42852750420570374
  - 0.5424121022224426
  - 0.42756327986717224
  - 0.45467454195022583
  - 0.41070684790611267
  - 0.4515426456928253
  - 0.42260608077049255
  - 0.47252169251441956
  - 0.4204193949699402
  - 0.41089746356010437
  - 0.4112192392349243
  - 0.40911176800727844
  - 0.4224845767021179
  - 0.426419734954834
  - 0.47966206073760986
  - 0.4273403286933899
  - 0.41274377703666687
  - 0.4379025399684906
  - 0.4403914213180542
  - 0.4244154095649719
  - 0.42439818382263184
  - 0.4507065415382385
  - 0.41933178901672363
  - 0.42264145612716675
  - 0.4359251856803894
  - 0.5012524127960205
  - 0.4245448112487793
  - 0.40837380290031433
  - 0.4969196617603302
  - 0.41871190071105957
  - 0.41668957471847534
  - 0.4602927267551422
  - 0.4235932528972626
  - 0.4181702733039856
  - 3.443845748901367
  - 1.688433289527893
  - 0.5913209915161133
  - 0.46063950657844543
  - 0.5204636454582214
  - 1.128238320350647
  - 0.8708797693252563
  - 1.9187052249908447
  - 3.195340871810913
  - 7.268764972686768
  - 0.42987626791000366
  - 22884.724609375
  - 2324.811767578125
  - 145.68360900878906
  - 4.28942346572876
  - 0.43061962723731995
  - 0.9409289956092834
  - 126.55523681640625
  - 2.4382338523864746
  - 0.42537426948547363
  - 0.42490532994270325
  - 2.512228488922119
  - 0.42739158868789673
  - 0.44838830828666687
  - 0.42855679988861084
  - 2.470654249191284
  - 0.4817233979701996
  - 0.4290713965892792
  - 0.4468684494495392
  - 0.4805646538734436
  - 0.4166935086250305
  - 0.4247659146785736
  - 0.425292044878006
  - 0.43761950731277466
  - 0.4993571937084198
  - 0.4405880570411682
  - 0.4783659279346466
  - 0.4188149571418762
  - 0.4409090280532837
  - 0.4221099317073822
  - 0.414529025554657
  - 0.42565983533859253
  - 0.44802799820899963
  - 0.4141932725906372
  - 0.42048025131225586
  - 0.455528199672699
  - 0.42229723930358887
  - 0.42331990599632263
  - 0.4116865396499634
  - 0.4226689040660858
  - 0.47972211241722107
  - 0.4131615161895752
  - 0.4145757555961609
  - 0.4095427691936493
  - 0.4379023015499115
  - 0.4830580949783325
  - 0.44164109230041504
  - 0.43680763244628906
  - 0.4035758078098297
  - 0.42529982328414917
  - 0.4244488775730133
  - 0.42235246300697327
  - 0.4273887872695923
  - 0.4209935665130615
  - 0.4373309314250946
  - 0.4413462281227112
loss_records_fold2:
  train_losses:
  - 3.2752851247787476
  - 3.187471652030945
  - 3.306250673532486
  - 3.281098705530167
  - 3.298020875453949
  - 3.2370646595954895
  - 3.2493337094783783
  - 3.2544099628925327
  - 3.1685049772262577
  - 3.267294013500214
  - 3.1898527801036836
  - 3.190816676616669
  - 3.2864585518836975
  - 3.2304365038871765
  - 3.274175810813904
  - 3.2186098098754883
  - 3.2177988767623904
  - 3.305164411664009
  - 3.231296753883362
  - 3.339354115724564
  - 3.2691942274570467
  - 3.4117546260356906
  - 3.2544850766658784
  - 3.153997153043747
  - 3.1932969748973847
  - 3.2291153192520143
  - 3.212577033042908
  - 3.207791215181351
  - 3.2758450984954837
  - 3.2191360235214237
  - 3.2705818474292756
  - 3.2789285719394687
  - 3.301912832260132
  - 3.44940841794014
  - 3.2430076897144318
  - 3.2544473290443423
  - 3.3852679431438446
  - 3.526322728395462
  - 3.2403462529182434
  - 3.156315851211548
  - 3.3242227613925937
  - 3.200091075897217
  - 3.2514269530773166
  - 3.2363566160202026
  - 3.240370362997055
  - 3.1719934046268463
  - 3.198789928853512
  - 3.320181065797806
  - 3.2791656374931337
  - 3.4049404948949817
  - 3.3744320929050446
  - 3.2046044707298282
  - 3.2307945787906647
  - 3.3929852068424227
  - 3.302332553267479
  - 3.2130713760852814
  - 3.2558870732784273
  - 3.209089690446854
  - 3.209462353587151
  - 3.277714413404465
  - 3.203131651878357
  - 3.372184628248215
  - 3.2047824561595917
  - 3.2152005791664124
  - 3.331391280889511
  - 3.2448300600051883
  - 3.3026163041591645
  - 3.209130656719208
  - 3.4637477815151216
  - 3.2764884471893314
  - 3.3555931746959686
  - 3.2557771503925323
  - 3.472435712814331
  - 3.396498227119446
  - 3.153386241197586
  - 3.3009169518947603
  - 3.2483822226524355
  - 3.251608842611313
  - 3.360143142938614
  - 3.188306632637978
  - 3.26069330573082
  - 3.2442683041095735
  - 3.1221597462892534
  - 3.2605477899312976
  - 3.39109862446785
  - 3.225479888916016
  - 3.2070110082626346
  - 3.2264850556850435
  - 3.3843946158885956
  - 3.481692445278168
  - 3.2748515546321872
  - 3.2163105428218843
  - 3.340003114938736
  - 3.19456040263176
  - 3.2243218779563905
  - 3.098587027192116
  - 3.2583875566720963
  validation_losses:
  - 0.4184936583042145
  - 0.3948459327220917
  - 0.4026907682418823
  - 0.5065054297447205
  - 0.40689945220947266
  - 0.41484057903289795
  - 0.4112239181995392
  - 0.3915550410747528
  - 0.3915967047214508
  - 0.41378524899482727
  - 0.41670098900794983
  - 0.4817308783531189
  - 0.40168169140815735
  - 0.4125010371208191
  - 0.40684112906455994
  - 0.4355047643184662
  - 0.3967283368110657
  - 0.4055635929107666
  - 0.4553694427013397
  - 0.4031623601913452
  - 0.450386643409729
  - 0.3923749625682831
  - 0.40445753931999207
  - 0.4174956679344177
  - 0.40998518466949463
  - 0.423787921667099
  - 0.5153300166130066
  - 0.40346887707710266
  - 0.4212881028652191
  - 0.40122100710868835
  - 0.3973623812198639
  - 0.4041266441345215
  - 0.4155707061290741
  - 0.3930048644542694
  - 0.40460819005966187
  - 0.40010571479797363
  - 0.39977434277534485
  - 0.41012290120124817
  - 0.39413437247276306
  - 0.41127657890319824
  - 0.3973546624183655
  - 0.41834935545921326
  - 0.5099960565567017
  - 0.507848858833313
  - 0.39889413118362427
  - 0.40379416942596436
  - 0.4117047190666199
  - 0.40579965710639954
  - 0.42462417483329773
  - 0.4107705354690552
  - 0.3961975574493408
  - 0.41680264472961426
  - 0.40644127130508423
  - 0.4211832880973816
  - 0.40600475668907166
  - 0.40155091881752014
  - 0.44101786613464355
  - 0.39980220794677734
  - 0.4055836498737335
  - 0.40200889110565186
  - 0.4046345353126526
  - 0.40377604961395264
  - 0.4156295657157898
  - 0.44683966040611267
  - 0.4083385467529297
  - 0.4156036376953125
  - 0.4181918203830719
  - 0.4068576395511627
  - 0.4078342020511627
  - 0.42363446950912476
  - 0.40701454877853394
  - 0.4315488636493683
  - 0.3998531103134155
  - 0.433908075094223
  - 0.3986036479473114
  - 0.43582600355148315
  - 0.4307059645652771
  - 0.4915410876274109
  - 0.42482882738113403
  - 0.40522682666778564
  - 0.4090491235256195
  - 0.3963039219379425
  - 0.41687995195388794
  - 0.4361903667449951
  - 0.4014623761177063
  - 0.4167662262916565
  - 0.4154587686061859
  - 0.4073050022125244
  - 0.40240734815597534
  - 0.4112173020839691
  - 0.4520549476146698
  - 0.42380908131599426
  - 0.4178694486618042
  - 0.41362252831459045
  - 0.4051806330680847
  - 0.4125252068042755
  - 0.40225282311439514
loss_records_fold3:
  train_losses:
  - 3.2522284209728243
  - 3.4089839220047
  - 3.2442675590515138
  - 3.374107372760773
  - 3.255174830555916
  - 3.101130145788193
  - 3.4359236776828768
  - 3.251666396856308
  - 3.265496611595154
  - 3.2552994698286057
  - 3.2897398948669436
  - 6.963444811105728
  - 3.6438312917947773
  - 4.269892609119416
  - 3.737766420841217
  - 3.4954984545707704
  - 3.608632129430771
  validation_losses:
  - 0.4050648510456085
  - 0.4162854850292206
  - 0.4134443998336792
  - 0.49395230412483215
  - 0.40691056847572327
  - 0.41346603631973267
  - 0.41734549403190613
  - 0.42224830389022827
  - 0.4582057297229767
  - 0.4075522720813751
  - 2063.126220703125
  - 0.48479241132736206
  - 0.4507869780063629
  - 0.4127666652202606
  - 0.41733479499816895
  - 0.4088372588157654
  - 0.41108188033103943
loss_records_fold4:
  train_losses:
  - 3.8337826490402223
  - 3.341413623094559
  - 3.2272050708532336
  - 3.810318195819855
  - 3.5592878401279453
  - 3.2535415172576907
  - 3.616252386569977
  - 3.3092798709869387
  - 3.294391942024231
  - 3.398316353559494
  - 3.3728221297264103
  - 3.7415304243564607
  - 3.3111946046352387
  - 3.2815371304750443
  - 3.384183657169342
  - 3.3472881734371187
  - 3.8975538939237597
  - 3.269620418548584
  - 3.1876000583171846
  - 3.2706829547882084
  - 3.6715903639793397
  - 3.410050702095032
  - 3.198420351743698
  - 3.172620350122452
  - 3.1896205604076386
  - 3.2348788678646088
  - 3.117353081703186
  - 3.2713130652904514
  - 3.193107703328133
  - 3.3036568343639376
  - 3.1055363893508914
  - 3.1370170146226886
  validation_losses:
  - 0.4189055562019348
  - 0.4287967085838318
  - 0.4480148255825043
  - 0.4406963884830475
  - 0.4659295976161957
  - 0.4348834753036499
  - 0.4116394817829132
  - 0.3947070837020874
  - 0.40871909260749817
  - 0.41507473587989807
  - 0.45777222514152527
  - 0.4254557192325592
  - 0.4028990566730499
  - 0.413933664560318
  - 0.41011282801628113
  - 0.399308443069458
  - 0.41068035364151
  - 0.4220283031463623
  - 0.4218817949295044
  - 0.40529367327690125
  - 0.49848005175590515
  - 0.41339224576950073
  - 0.41026830673217773
  - 0.41585785150527954
  - 0.3972216248512268
  - 0.4182513356208801
  - 0.42531144618988037
  - 0.4347273111343384
  - 0.4279495179653168
  - 0.4047751724720001
  - 0.4020383656024933
  - 0.40097200870513916
training fold messages:
  fold 0 training message: completed 100 epochs without stopping early
  fold 1 training message: completed 100 epochs without stopping early
  fold 2 training message: stopped training due to stagnating improvement on validation
    loss after 97 epochs
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 32 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:31:09.634683'
