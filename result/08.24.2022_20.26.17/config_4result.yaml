config:
  aggregation: mean
  batch_size: 32
  class_weights: true
  data:
    dataset: WICO
    pre_filter: filter_5g_non
    pre_transform: wico_data_to_custom
    root: wico
  dataset: wico
  device: cuda
  dropout: 0.5
  epochs: 100
  hidden_units:
  - 32
  - 32
  - 32
  improvement_threshold: 0.01
  kfolds: 5
  loss_fn: CrossEntropyLoss
  lr: 0.01
  model: GIN
  optimizer: Adam
  patience: 7
  train_eps: true
  transform: wico_5g_vs_non_conspiracy
experiment_run_start: '2022-08-24 20:32:57.857023'
fold_0_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_4fold_0_optim_dict.pt
fold_0_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_4fold_0_state_dict.pt
fold_1_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_4fold_1_optim_dict.pt
fold_1_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_4fold_1_state_dict.pt
fold_2_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_4fold_2_optim_dict.pt
fold_2_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_4fold_2_state_dict.pt
fold_3_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_4fold_3_optim_dict.pt
fold_3_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_4fold_3_state_dict.pt
fold_4_optim_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_4fold_4_optim_dict.pt
fold_4_state_dict: GNN-exp-pipeline/result/08.24.2022_20.26.17/config_4fold_4_state_dict.pt
loss_records_fold0:
  train_losses:
  - 6.936672246456147
  - 6.373450157046318
  - 6.255472764372826
  - 6.334501251578331
  - 6.161334428191186
  - 6.1103320807218555
  - 6.334545454382897
  - 6.148260286450387
  - 6.331595534086228
  - 6.116129660606385
  - 6.228740176558495
  - 6.1843774855136875
  - 6.044750028848648
  - 5.839489227533341
  - 6.069744437932968
  - 5.955724206566811
  - 5.95204861164093
  - 6.169040814042091
  - 5.90589249432087
  - 5.948168355226517
  - 6.011242979764939
  - 6.083964645862579
  - 6.04342959523201
  - 6.07604561150074
  - 5.982434010505677
  validation_losses:
  - 0.5236392021179199
  - 0.39992207288742065
  - 0.4137170612812042
  - 0.4127502143383026
  - 0.42310836911201477
  - 0.3865983188152313
  - 0.40394270420074463
  - 0.408363938331604
  - 0.3962675929069519
  - 0.4266596734523773
  - 0.3889859616756439
  - 0.40188971161842346
  - 0.40098515152931213
  - 0.3942772150039673
  - 0.41124427318573
  - 0.3978610634803772
  - 0.4050178527832031
  - 0.38777032494544983
  - 0.39873170852661133
  - 0.4032246768474579
  - 0.403794527053833
  - 0.4135811924934387
  - 0.39438962936401367
  - 0.3968329131603241
  - 0.39421507716178894
loss_records_fold1:
  train_losses:
  - 5.938555172085763
  - 6.030363008379936
  - 6.006439495086671
  - 6.006292128562928
  - 5.987224969267846
  - 6.026303109526634
  - 6.118561220169068
  - 6.049310678243637
  - 6.11325421333313
  - 5.878101411461831
  - 5.986525431275368
  - 6.033108627796174
  - 5.875230240821839
  - 6.076393315196038
  - 6.128699779510498
  - 5.934839516878128
  - 5.941956061124802
  - 6.030850350856781
  - 6.067480969429017
  - 6.012045088410378
  - 6.042693796753884
  - 5.932593142986298
  - 6.131441894173623
  - 5.906316131353378
  - 5.972261515259743
  - 5.88473479449749
  - 5.938344115018845
  - 5.855839902162552
  - 6.06268934905529
  - 6.025721696019173
  - 6.037087935209275
  - 5.974864676594734
  - 6.038199865818024
  - 6.090928417444229
  - 6.070392626523972
  - 6.095031723380089
  - 6.030208703875542
  - 6.1215055912733085
  - 5.913664230704308
  - 5.887068283557892
  - 5.95618494451046
  - 5.937799581885338
  - 5.910199895501137
  - 5.9172365218400955
  - 5.871184369921685
  - 6.050549057126045
  - 6.0068931758403785
  - 5.888899499177933
  - 5.909758824110032
  - 5.877081283926964
  - 5.934509971737862
  - 5.985371625423432
  - 6.061428490281106
  - 5.8376070171594625
  - 5.917013224959374
  - 5.851978844404221
  - 6.051789310574532
  - 5.87019890844822
  - 5.982714495062829
  validation_losses:
  - 0.3895762860774994
  - 0.39190298318862915
  - 0.393286794424057
  - 0.3902217447757721
  - 0.38555994629859924
  - 0.3848041594028473
  - 0.39101460576057434
  - 0.3913396894931793
  - 0.4894639551639557
  - 0.39513105154037476
  - 0.40551188588142395
  - 0.3985575735569
  - 0.38825735449790955
  - 0.4190821647644043
  - 0.44401925802230835
  - 0.3973173201084137
  - 0.45873594284057617
  - 0.39993947744369507
  - 0.38948625326156616
  - 0.3902668356895447
  - 0.3840788006782532
  - 0.3906741738319397
  - 0.5392414331436157
  - 0.4260933995246887
  - 0.3950085937976837
  - 0.5515725016593933
  - 0.6276749968528748
  - 0.446479469537735
  - 0.41356343030929565
  - 0.5444910526275635
  - 0.674772322177887
  - 0.40156009793281555
  - 0.44404229521751404
  - 0.41437461972236633
  - 0.4845771789550781
  - 0.45128563046455383
  - 0.654289186000824
  - 0.45405906438827515
  - 0.5222797393798828
  - 0.49200916290283203
  - 0.48540788888931274
  - 0.6619121432304382
  - 0.5605288147926331
  - 0.4797530472278595
  - 0.5787184238433838
  - 0.6631248593330383
  - 0.41895171999931335
  - 0.458917498588562
  - 0.5345129370689392
  - 0.4082823097705841
  - 0.44431614875793457
  - 0.47260481119155884
  - 0.5506874322891235
  - 0.5284298062324524
  - 0.5276705026626587
  - 0.5332674980163574
  - 0.5115812420845032
  - 0.456648051738739
  - 0.40841203927993774
loss_records_fold2:
  train_losses:
  - 5.9274973422288895
  - 5.945669558644295
  - 5.83103688955307
  - 5.849368098378182
  - 5.940454611182213
  - 5.965900778770447
  - 6.033805486559868
  - 5.9197872459888465
  - 5.9805264443159105
  - 5.920541432499886
  - 5.914160457253456
  - 5.852316826581955
  - 5.988468036055565
  - 5.842469722032547
  - 5.9274512976408005
  - 5.825762262940407
  - 5.889115881919861
  - 5.875791448354722
  - 5.997490182518959
  - 5.9517909616231925
  - 5.770360884070397
  - 5.8976952821016315
  - 5.841367787122727
  - 5.769957107305527
  - 5.932499426603318
  - 6.026813018321992
  - 5.943363827466965
  - 5.813474020361901
  - 5.82003065943718
  - 5.949735620617867
  - 5.799327725172043
  - 5.875350278615952
  - 5.893634554743767
  - 5.951597464084625
  - 5.937212428450585
  - 5.857512754201889
  - 5.982421684265137
  - 5.691679504513741
  - 5.772778123617172
  - 5.900444319844246
  - 5.9050304859876634
  - 5.88391995728016
  - 5.779489514231682
  - 5.858769047260285
  - 5.825901743769646
  - 5.93200131058693
  - 5.858920988440514
  - 6.045137220621109
  - 5.8279934346675875
  - 5.833561223745346
  - 5.75689469575882
  - 5.880983972549439
  - 5.9131776064634325
  - 5.903537672758103
  - 5.930924952030182
  - 5.942760699987412
  - 5.685276606678963
  - 5.994091802835465
  - 5.877325674891472
  - 5.801479250192642
  - 5.891736906766892
  - 5.82871964275837
  - 5.928060251474381
  - 5.9332978457212455
  - 5.984030598402024
  - 5.843712118268013
  - 5.881535279750825
  - 5.871451222896576
  - 5.94718074798584
  - 5.836170941591263
  - 5.766960728168488
  - 5.942880275845528
  - 5.912693643569947
  - 5.923390409350396
  - 5.926795941591263
  - 5.827057021856309
  - 5.88117610514164
  - 5.820919546484948
  - 6.3671405702829365
  - 6.00449610054493
  - 5.980191716551781
  - 6.026610404253006
  - 5.799667844176293
  - 6.005760982632637
  - 5.817162188887597
  - 5.963679453730584
  - 5.973922181129456
  - 5.7852459222078325
  - 5.994322782754899
  - 5.849177959561349
  - 6.011424788832665
  - 5.85214042365551
  - 5.763628220558167
  - 5.715341213345528
  - 5.817634731531143
  - 6.041295921802521
  - 5.79669210612774
  - 5.680482083559037
  - 5.933969965577126
  - 5.835158318281174
  validation_losses:
  - 0.4605617821216583
  - 0.553907573223114
  - 0.614071249961853
  - 0.5064383149147034
  - 0.4306013882160187
  - 0.45496293902397156
  - 0.6280933022499084
  - 0.4549068510532379
  - 0.6651592254638672
  - 0.4440753757953644
  - 0.4262069761753082
  - 0.47311267256736755
  - 0.44496336579322815
  - 0.5147944688796997
  - 0.451082319021225
  - 0.7914294004440308
  - 0.5683834552764893
  - 0.6270355582237244
  - 0.43515634536743164
  - 0.45538991689682007
  - 0.5970202684402466
  - 0.5488455891609192
  - 0.47861793637275696
  - 0.4754124879837036
  - 0.8022471070289612
  - 0.5350019335746765
  - 0.3962748944759369
  - 0.5517390966415405
  - 0.6936580538749695
  - 0.9335480332374573
  - 0.5294610261917114
  - 0.5169113874435425
  - 0.4662671387195587
  - 0.4796258509159088
  - 0.5239208340644836
  - 0.5391747355461121
  - 0.5273120403289795
  - 0.44401732087135315
  - 0.6506811380386353
  - 0.6548727750778198
  - 0.4954564571380615
  - 0.48639994859695435
  - 0.4461459517478943
  - 0.5347620248794556
  - 0.42927873134613037
  - 0.5126993060112
  - 0.45517173409461975
  - 0.4978494346141815
  - 0.5535035729408264
  - 0.5248852968215942
  - 0.5324500799179077
  - 0.46949997544288635
  - 0.48058804869651794
  - 0.47712674736976624
  - 0.5478800535202026
  - 0.47808295488357544
  - 0.48266538977622986
  - 1.2464354038238525
  - 0.7078250646591187
  - 0.5508346557617188
  - 0.4393899142742157
  - 0.5399972796440125
  - 0.5895995497703552
  - 0.7374718189239502
  - 0.5141385197639465
  - 0.5785664916038513
  - 0.460043340921402
  - 0.7484296560287476
  - 0.6001164317131042
  - 0.6536670923233032
  - 0.520046055316925
  - 0.47763434052467346
  - 0.6129725575447083
  - 0.4431415796279907
  - 0.5072072148323059
  - 0.4835391640663147
  - 0.5310102105140686
  - 0.575661838054657
  - 0.4487360417842865
  - 0.5083906054496765
  - 0.5279922485351562
  - 0.48268449306488037
  - 0.48485979437828064
  - 0.4667525887489319
  - 0.4856281578540802
  - 0.524980366230011
  - 0.48026785254478455
  - 0.5240575075149536
  - 0.49127092957496643
  - 0.4517131447792053
  - 0.4820038080215454
  - 0.47407954931259155
  - 0.448789119720459
  - 0.8152874112129211
  - 0.6015947461128235
  - 0.538651168346405
  - 0.6011126637458801
  - 0.6069093942642212
  - 0.622340202331543
  - 1.8670326471328735
loss_records_fold3:
  train_losses:
  - 6.0724664539098745
  - 5.877351477742195
  - 5.798225125670434
  - 5.820521923899651
  - 5.861828941106797
  - 5.997782987356186
  - 5.931804516911507
  - 5.9415395826101305
  - 6.155709707736969
  - 6.015534490346909
  - 6.063903751969338
  - 5.97401177585125
  - 5.910480755567551
  - 5.932669073343277
  - 5.874871519207955
  - 5.929718372225762
  - 5.900851067900658
  - 5.982336688041688
  - 6.064085096120834
  - 5.815007948875428
  - 5.937117546796799
  - 6.017076846957207
  - 5.751348945498467
  - 5.978483626246453
  - 6.022268041968346
  - 5.829202470183373
  - 5.968126752972603
  - 6.0460589826107025
  - 5.957184040546418
  - 5.864448499679566
  - 5.7509944379329685
  - 5.96156622171402
  - 5.852782914042473
  - 5.8385304600000385
  - 5.986534363031388
  - 5.96634978055954
  - 5.856994849443436
  - 5.802102559804917
  - 5.795271334052086
  - 5.869629830121994
  - 5.689086401462555
  - 5.855070698261262
  - 5.801982963085175
  - 5.888944822549821
  - 5.990718343853951
  - 5.864642298221589
  - 5.920908063650131
  - 5.950036883354187
  - 5.785065832734109
  - 5.894484457373619
  - 6.0351166307926185
  - 5.858551999926568
  - 6.020291030406952
  - 6.000812613964081
  - 5.882136401534081
  - 5.832552435994149
  - 5.7464646935462955
  - 5.8052735269069675
  - 5.834552845358849
  - 5.7665580958127975
  - 5.9043454200029375
  - 6.1266253054142
  - 5.7620131283998495
  - 6.018333905935288
  - 5.92074146270752
  - 5.877262628078461
  - 5.736143007874489
  - 5.873732581734657
  - 5.771316757798195
  - 5.90722571015358
  - 5.8648531407117845
  - 5.746677109599114
  - 5.817937564849854
  - 5.8395195543766025
  - 5.859789577126503
  - 5.772209635376931
  - 5.960620433092117
  - 6.1588205933570865
  - 6.10429262816906
  - 5.9997157037258155
  - 5.927035266160965
  - 5.871871495246888
  - 5.924080473184586
  - 5.9765370935201645
  - 5.915830153226853
  - 6.101133114099503
  - 5.980102139711381
  - 6.024341660737992
  - 5.948448422551156
  validation_losses:
  - 0.7328841090202332
  - 0.7716624736785889
  - 0.8018524646759033
  - 0.7035768628120422
  - 0.7636744379997253
  - 0.7199985980987549
  - 0.5931468605995178
  - 0.8968579173088074
  - 0.5240721702575684
  - 0.4836892783641815
  - 0.4960455596446991
  - 0.4964926242828369
  - 0.5498695373535156
  - 0.48879697918891907
  - 0.5538923740386963
  - 0.5074341893196106
  - 0.5523343682289124
  - 0.8597932457923889
  - 0.45487701892852783
  - 1.1035033464431763
  - 1.1488906145095825
  - 0.6511443257331848
  - 0.7262791991233826
  - 0.6069346070289612
  - 0.7337118983268738
  - 1.0270591974258423
  - 1.5249289274215698
  - 0.5186655521392822
  - 0.5338988304138184
  - 0.6229022145271301
  - 0.606574535369873
  - 0.583259642124176
  - 0.5920040607452393
  - 0.6047543287277222
  - 0.5524453520774841
  - 0.6449549794197083
  - 0.558959424495697
  - 0.7348138093948364
  - 0.5917744040489197
  - 0.5660538077354431
  - 0.6841267943382263
  - 0.5913664698600769
  - 0.6666810512542725
  - 0.6805781126022339
  - 0.5178455114364624
  - 0.5342234373092651
  - 0.5504124164581299
  - 0.578565776348114
  - 0.5919477343559265
  - 0.7413496375083923
  - 0.4361148774623871
  - 0.46891331672668457
  - 0.46849575638771057
  - 0.5733132362365723
  - 0.44966045022010803
  - 0.5528172254562378
  - 0.5232929587364197
  - 0.655342161655426
  - 0.5869117379188538
  - 0.5516311526298523
  - 0.5619895458221436
  - 0.48707345128059387
  - 0.4856412708759308
  - 0.5697720050811768
  - 0.49797680974006653
  - 0.4381729066371918
  - 0.5522260069847107
  - 0.5106805562973022
  - 0.48148632049560547
  - 0.546843409538269
  - 0.5722350478172302
  - 0.526830792427063
  - 0.4453551769256592
  - 0.4999922811985016
  - 0.607178270816803
  - 0.6340889930725098
  - 0.5284041166305542
  - 0.4904484152793884
  - 0.4520649015903473
  - 0.4397493600845337
  - 0.4267739951610565
  - 0.44885390996932983
  - 0.4901985228061676
  - 0.45075729489326477
  - 0.4542187750339508
  - 0.43907231092453003
  - 0.4252747595310211
  - 0.4167037606239319
  - 0.41278499364852905
loss_records_fold4:
  train_losses:
  - 5.958040791749955
  - 5.933393907546997
  - 5.991376215219498
  - 5.866832050681115
  - 5.950284391641617
  - 6.013265174627304
  - 5.911353597044945
  - 6.047267764806747
  - 5.812135097384453
  - 5.930007180571557
  - 5.868269589543343
  - 5.88879234790802
  - 5.992414271831513
  - 5.988588757812977
  - 5.9805702626705175
  - 5.937687730789185
  - 5.799170330166817
  validation_losses:
  - 0.4173721969127655
  - 0.4447055160999298
  - 0.43769025802612305
  - 0.3891390860080719
  - 0.4167363941669464
  - 0.4005248546600342
  - 0.4067509174346924
  - 0.38663384318351746
  - 0.4077107608318329
  - 0.39848166704177856
  - 0.41827744245529175
  - 0.4270116686820984
  - 0.40118512511253357
  - 0.3989233076572418
  - 0.3877008855342865
  - 0.38511303067207336
  - 0.3842901885509491
training fold messages:
  fold 0 training message: stopped training due to stagnating improvement on validation
    loss after 25 epochs
  fold 1 training message: stopped training due to stagnating improvement on validation
    loss after 59 epochs
  fold 2 training message: completed 100 epochs without stopping early
  fold 3 training message: stopped training due to stagnating improvement on validation
    loss after 89 epochs
  fold 4 training message: stopped training due to stagnating improvement on validation
    loss after 17 epochs
training_metrics:
  fold_eval_accs: '[0.8576329331046312, 0.8576329331046312, 0.8593481989708405, 0.8593481989708405,
    0.8591065292096219]'
  fold_eval_f1: '[0.0, 0.0, 0.0, 0.0, 0.0]'
  mean_eval_accuracy: 0.858613758672113
  mean_f1_accuracy: 0.0
  total_train_time: '0:28:42.411441'
